nohup: ignoring input
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_0-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.089 (0.673) data 0.000 (0.196) loss 1.1331 (1.0811) acc 8.5938 (17.3438) lr 2.0000e-03 eta 0:05:33
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [64/160] noisy rate: 0.00 --> 0.00 --> 0.00 <<<
epoch [2/100] batch [5/5] time 0.235 (0.429) data 0.000 (0.184) loss 2.5598 (2.0687) acc 25.0000 (28.9892) lr 1.9995e-03 eta 0:03:30
epoch [3/100] batch [5/5] time 0.044 (0.256) data 0.000 (0.156) loss 1.9498 (1.9733) acc 30.3571 (32.6449) lr 1.9980e-03 eta 0:02:04
epoch [4/100] batch [5/5] time 0.238 (0.338) data 0.000 (0.185) loss 1.7435 (1.7303) acc 50.0000 (48.1869) lr 1.9956e-03 eta 0:02:42
epoch [5/100] batch [5/5] time 0.042 (0.273) data 0.000 (0.169) loss 1.8652 (1.6510) acc 25.0000 (48.9375) lr 1.9921e-03 eta 0:02:09
epoch [6/100] batch [5/5] time 0.039 (0.246) data 0.000 (0.196) loss 1.5458 (1.5068) acc 47.7273 (55.2933) lr 1.9877e-03 eta 0:01:55
>>> samples [66/160] noisy rate: 0.00 --> 0.42 --> 0.03 <<<
epoch [7/100] batch [5/5] time 0.054 (0.377) data 0.000 (0.206) loss 1.3280 (1.4641) acc 67.8571 (58.5202) lr 1.9823e-03 eta 0:02:55
epoch [8/100] batch [5/5] time 0.045 (0.234) data 0.000 (0.179) loss 1.5014 (1.4494) acc 52.0833 (60.4217) lr 1.9759e-03 eta 0:01:47
epoch [9/100] batch [5/5] time 0.050 (0.257) data 0.000 (0.199) loss 1.5100 (1.3518) acc 67.8571 (63.2054) lr 1.9686e-03 eta 0:01:57
epoch [10/100] batch [5/5] time 0.043 (0.231) data 0.000 (0.174) loss 1.4280 (1.3517) acc 61.3636 (69.9189) lr 1.9603e-03 eta 0:01:44
epoch [11/100] batch [5/5] time 0.059 (0.234) data 0.000 (0.172) loss 1.2714 (1.3788) acc 70.5882 (65.2486) lr 1.9511e-03 eta 0:01:44
>>> samples [69/160] noisy rate: 0.00 --> 0.19 --> 0.04 <<<
epoch [12/100] batch [5/5] time 0.057 (0.244) data 0.000 (0.189) loss 1.3208 (1.3333) acc 62.5000 (64.3132) lr 1.9409e-03 eta 0:01:47
epoch [13/100] batch [5/5] time 0.036 (0.299) data 0.000 (0.176) loss 1.0860 (1.2224) acc 87.5000 (71.6557) lr 1.9298e-03 eta 0:02:10
epoch [14/100] batch [5/5] time 0.049 (0.250) data 0.000 (0.194) loss 1.2141 (1.2073) acc 75.0000 (77.7364) lr 1.9178e-03 eta 0:01:47
epoch [15/100] batch [5/5] time 0.052 (0.235) data 0.000 (0.183) loss 1.3344 (1.0758) acc 73.5294 (82.5456) lr 1.9048e-03 eta 0:01:40
epoch [16/100] batch [5/5] time 0.044 (0.341) data 0.001 (0.204) loss 1.4191 (1.1326) acc 68.1818 (78.2530) lr 1.8910e-03 eta 0:02:23
>>> samples [76/160] noisy rate: 0.00 --> 0.19 --> 0.08 <<<
epoch [17/100] batch [5/5] time 0.040 (0.238) data 0.000 (0.182) loss 0.8028 (0.9904) acc 91.6667 (85.5751) lr 1.8763e-03 eta 0:01:38
epoch [18/100] batch [5/5] time 0.042 (0.211) data 0.000 (0.157) loss 1.2271 (1.0234) acc 77.0833 (81.1167) lr 1.8607e-03 eta 0:01:26
epoch [19/100] batch [5/5] time 0.058 (0.258) data 0.000 (0.204) loss 1.0154 (1.0180) acc 77.7778 (82.8271) lr 1.8443e-03 eta 0:01:44
epoch [20/100] batch [5/5] time 0.063 (0.238) data 0.000 (0.178) loss 1.1544 (1.0266) acc 83.3333 (83.3425) lr 1.8271e-03 eta 0:01:35
epoch [21/100] batch [5/5] time 0.050 (0.240) data 0.000 (0.182) loss 0.9595 (1.0014) acc 82.1429 (84.3601) lr 1.8090e-03 eta 0:01:34
>>> samples [76/160] noisy rate: 0.00 --> 0.22 --> 0.08 <<<
epoch [22/100] batch [5/5] time 0.042 (0.235) data 0.000 (0.181) loss 1.3245 (0.9976) acc 65.9091 (81.8901) lr 1.7902e-03 eta 0:01:31
epoch [23/100] batch [5/5] time 0.046 (0.285) data 0.000 (0.230) loss 0.7773 (0.9935) acc 93.7500 (86.2010) lr 1.7705e-03 eta 0:01:49
epoch [24/100] batch [5/5] time 0.051 (0.377) data 0.000 (0.238) loss 0.8887 (1.0040) acc 83.9286 (84.5476) lr 1.7501e-03 eta 0:02:23
epoch [25/100] batch [5/5] time 0.051 (0.233) data 0.000 (0.178) loss 1.1800 (0.9882) acc 76.7857 (86.6349) lr 1.7290e-03 eta 0:01:27
epoch [26/100] batch [5/5] time 0.052 (0.256) data 0.000 (0.195) loss 1.0182 (0.9638) acc 85.0000 (88.8304) lr 1.7071e-03 eta 0:01:34
>>> samples [78/160] noisy rate: 0.00 --> 0.24 --> 0.09 <<<
epoch [27/100] batch [5/5] time 0.064 (0.255) data 0.000 (0.192) loss 0.9554 (0.9190) acc 90.2778 (89.1215) lr 1.6845e-03 eta 0:01:33
epoch [28/100] batch [5/5] time 0.058 (0.249) data 0.000 (0.188) loss 0.9186 (0.9769) acc 87.5000 (83.4699) lr 1.6613e-03 eta 0:01:29
epoch [29/100] batch [5/5] time 0.051 (0.266) data 0.000 (0.210) loss 0.9866 (0.9750) acc 91.0714 (82.8423) lr 1.6374e-03 eta 0:01:34
epoch [30/100] batch [5/5] time 0.049 (0.237) data 0.000 (0.178) loss 1.1991 (0.9452) acc 75.0000 (87.2174) lr 1.6129e-03 eta 0:01:22
epoch [31/100] batch [5/5] time 0.056 (0.279) data 0.000 (0.224) loss 0.7835 (0.8970) acc 89.0625 (86.5291) lr 1.5878e-03 eta 0:01:36
>>> samples [78/160] noisy rate: 0.00 --> 0.24 --> 0.09 <<<
epoch [32/100] batch [5/5] time 0.060 (0.257) data 0.000 (0.196) loss 1.2771 (0.9426) acc 75.0000 (85.9444) lr 1.5621e-03 eta 0:01:27
epoch [33/100] batch [5/5] time 0.060 (0.248) data 0.000 (0.186) loss 0.9921 (0.9536) acc 84.7222 (86.1290) lr 1.5358e-03 eta 0:01:22
epoch [34/100] batch [5/5] time 0.056 (0.232) data 0.000 (0.171) loss 1.0524 (0.9034) acc 79.6875 (89.9930) lr 1.5090e-03 eta 0:01:16
epoch [35/100] batch [5/5] time 0.064 (0.243) data 0.000 (0.180) loss 1.0741 (0.8919) acc 88.1579 (90.3861) lr 1.4818e-03 eta 0:01:18
epoch [36/100] batch [5/5] time 0.049 (0.235) data 0.000 (0.178) loss 1.0451 (0.9280) acc 80.3571 (86.0662) lr 1.4540e-03 eta 0:01:15
>>> samples [78/160] noisy rate: 0.00 --> 0.24 --> 0.09 <<<
epoch [37/100] batch [5/5] time 0.050 (0.291) data 0.000 (0.230) loss 0.9828 (0.9373) acc 88.3333 (85.8333) lr 1.4258e-03 eta 0:01:31
epoch [38/100] batch [5/5] time 0.044 (0.251) data 0.000 (0.189) loss 0.7801 (0.8852) acc 93.7500 (91.7892) lr 1.3971e-03 eta 0:01:17
epoch [39/100] batch [5/5] time 0.057 (0.246) data 0.000 (0.186) loss 0.7999 (0.9411) acc 91.1765 (87.2954) lr 1.3681e-03 eta 0:01:15
epoch [40/100] batch [5/5] time 0.049 (0.250) data 0.000 (0.191) loss 0.8108 (0.9138) acc 89.0625 (87.6122) lr 1.3387e-03 eta 0:01:14
epoch [41/100] batch [5/5] time 0.054 (0.223) data 0.000 (0.168) loss 1.0641 (0.8938) acc 79.6875 (88.0289) lr 1.3090e-03 eta 0:01:05
>>> samples [80/160] noisy rate: 0.00 --> 0.22 --> 0.11 <<<
epoch [42/100] batch [5/5] time 0.055 (0.232) data 0.000 (0.180) loss 0.8206 (0.9047) acc 95.3125 (90.0337) lr 1.2790e-03 eta 0:01:07
epoch [43/100] batch [5/5] time 0.056 (0.248) data 0.000 (0.186) loss 1.1908 (0.9041) acc 82.8947 (88.4430) lr 1.2487e-03 eta 0:01:10
epoch [44/100] batch [5/5] time 0.059 (0.228) data 0.000 (0.171) loss 0.7833 (0.9275) acc 94.1176 (88.1139) lr 1.2181e-03 eta 0:01:03
epoch [45/100] batch [5/5] time 0.063 (0.315) data 0.000 (0.177) loss 0.7657 (0.8971) acc 92.1053 (90.9860) lr 1.1874e-03 eta 0:01:26
epoch [46/100] batch [5/5] time 0.056 (0.244) data 0.000 (0.185) loss 0.9298 (0.8968) acc 85.2941 (88.7869) lr 1.1564e-03 eta 0:01:05
>>> samples [87/160] noisy rate: 0.00 --> 0.24 --> 0.13 <<<
epoch [47/100] batch [5/5] time 0.060 (0.271) data 0.000 (0.208) loss 0.8709 (0.9179) acc 88.2353 (86.4953) lr 1.1253e-03 eta 0:01:11
epoch [48/100] batch [5/5] time 0.067 (0.259) data 0.000 (0.194) loss 0.9309 (0.9065) acc 85.0000 (87.9922) lr 1.0941e-03 eta 0:01:07
epoch [49/100] batch [5/5] time 0.060 (0.248) data 0.000 (0.189) loss 0.9460 (0.8958) acc 92.1053 (89.4848) lr 1.0628e-03 eta 0:01:03
epoch [50/100] batch [5/5] time 0.074 (0.254) data 0.000 (0.190) loss 0.9609 (0.8734) acc 84.0909 (86.3557) lr 1.0314e-03 eta 0:01:03
epoch [51/100] batch [5/5] time 0.057 (0.230) data 0.000 (0.167) loss 0.5781 (0.8378) acc 95.5882 (90.9926) lr 1.0000e-03 eta 0:00:56
>>> samples [89/160] noisy rate: 0.00 --> 0.24 --> 0.13 <<<
epoch [52/100] batch [5/5] time 0.065 (0.243) data 0.000 (0.183) loss 0.9764 (0.8898) acc 88.1579 (88.7155) lr 9.6859e-04 eta 0:00:58
epoch [53/100] batch [5/5] time 0.065 (0.264) data 0.000 (0.201) loss 0.8867 (0.8680) acc 90.0000 (89.9007) lr 9.3721e-04 eta 0:01:02
epoch [54/100] batch [5/5] time 0.059 (0.225) data 0.000 (0.160) loss 1.0172 (0.8661) acc 82.3529 (88.7623) lr 9.0589e-04 eta 0:00:51
epoch [55/100] batch [5/5] time 0.062 (0.276) data 0.000 (0.214) loss 0.9098 (0.8746) acc 88.1579 (86.5183) lr 8.7467e-04 eta 0:01:02
epoch [56/100] batch [5/5] time 0.457 (0.302) data 0.000 (0.163) loss 0.7240 (0.9131) acc 84.7826 (87.5083) lr 8.4357e-04 eta 0:01:06
>>> samples [91/160] noisy rate: 0.00 --> 0.26 --> 0.14 <<<
epoch [57/100] batch [5/5] time 0.065 (0.300) data 0.000 (0.233) loss 0.9142 (0.8735) acc 87.5000 (87.6066) lr 8.1262e-04 eta 0:01:04
epoch [58/100] batch [5/5] time 0.072 (0.241) data 0.000 (0.176) loss 0.7396 (0.8777) acc 95.4545 (87.8686) lr 7.8186e-04 eta 0:00:50
epoch [59/100] batch [5/5] time 0.488 (0.365) data 0.000 (0.218) loss 0.8549 (0.8727) acc 93.7500 (89.3542) lr 7.5131e-04 eta 0:01:14
epoch [60/100] batch [5/5] time 0.058 (0.269) data 0.000 (0.203) loss 0.7928 (0.8347) acc 88.8889 (89.9107) lr 7.2101e-04 eta 0:00:53
epoch [61/100] batch [5/5] time 0.068 (0.256) data 0.000 (0.192) loss 0.7500 (0.8357) acc 92.8571 (91.1610) lr 6.9098e-04 eta 0:00:49
>>> samples [91/160] noisy rate: 0.00 --> 0.26 --> 0.14 <<<
epoch [62/100] batch [5/5] time 0.063 (0.265) data 0.000 (0.201) loss 0.7983 (0.8100) acc 92.6471 (90.9280) lr 6.6126e-04 eta 0:00:50
epoch [63/100] batch [5/5] time 0.059 (0.235) data 0.000 (0.171) loss 0.9106 (0.8356) acc 91.6667 (89.7381) lr 6.3188e-04 eta 0:00:43
epoch [64/100] batch [5/5] time 0.064 (0.232) data 0.000 (0.169) loss 0.8251 (0.8535) acc 92.5000 (89.8619) lr 6.0285e-04 eta 0:00:41
epoch [65/100] batch [5/5] time 0.061 (0.238) data 0.000 (0.175) loss 0.7883 (0.8134) acc 85.5263 (89.9683) lr 5.7422e-04 eta 0:00:41
epoch [66/100] batch [5/5] time 0.063 (0.255) data 0.000 (0.194) loss 0.8739 (0.8439) acc 89.2857 (89.1465) lr 5.4601e-04 eta 0:00:43
>>> samples [92/160] noisy rate: 0.00 --> 0.24 --> 0.14 <<<
epoch [67/100] batch [5/5] time 0.064 (0.237) data 0.000 (0.172) loss 0.9410 (0.8414) acc 92.8571 (89.1211) lr 5.1825e-04 eta 0:00:39
epoch [68/100] batch [5/5] time 0.061 (0.238) data 0.000 (0.178) loss 0.8853 (0.8353) acc 81.5789 (89.5511) lr 4.9096e-04 eta 0:00:38
epoch [69/100] batch [5/5] time 0.063 (0.305) data 0.001 (0.241) loss 0.9044 (0.8292) acc 94.7368 (88.2222) lr 4.6417e-04 eta 0:00:47
epoch [70/100] batch [5/5] time 0.063 (0.288) data 0.000 (0.229) loss 0.7988 (0.8528) acc 84.0909 (88.1638) lr 4.3792e-04 eta 0:00:43
epoch [71/100] batch [5/5] time 0.064 (0.284) data 0.000 (0.224) loss 0.8222 (0.8475) acc 88.6364 (88.7550) lr 4.1221e-04 eta 0:00:41
>>> samples [92/160] noisy rate: 0.00 --> 0.26 --> 0.14 <<<
epoch [72/100] batch [5/5] time 0.045 (0.347) data 0.000 (0.200) loss 0.6329 (0.8025) acc 94.2308 (90.2149) lr 3.8709e-04 eta 0:00:48
epoch [73/100] batch [5/5] time 0.050 (0.303) data 0.000 (0.243) loss 0.6851 (0.8717) acc 96.4286 (90.2024) lr 3.6258e-04 eta 0:00:40
epoch [74/100] batch [5/5] time 0.065 (0.281) data 0.000 (0.221) loss 0.8003 (0.8204) acc 91.6667 (91.3632) lr 3.3869e-04 eta 0:00:36
epoch [75/100] batch [5/5] time 0.055 (0.266) data 0.000 (0.204) loss 0.7402 (0.8623) acc 93.4211 (86.5177) lr 3.1545e-04 eta 0:00:33
epoch [76/100] batch [5/5] time 0.067 (0.244) data 0.000 (0.177) loss 0.7791 (0.8478) acc 84.5238 (88.1990) lr 2.9289e-04 eta 0:00:29
>>> samples [92/160] noisy rate: 0.00 --> 0.25 --> 0.14 <<<
epoch [77/100] batch [5/5] time 0.069 (0.219) data 0.000 (0.154) loss 0.7647 (0.8388) acc 90.4762 (90.0207) lr 2.7103e-04 eta 0:00:25
epoch [78/100] batch [5/5] time 0.056 (0.239) data 0.000 (0.178) loss 0.7016 (0.8451) acc 97.3684 (87.7041) lr 2.4989e-04 eta 0:00:26
epoch [79/100] batch [5/5] time 0.044 (0.277) data 0.000 (0.214) loss 0.9302 (0.8195) acc 85.7143 (88.6859) lr 2.2949e-04 eta 0:00:29
epoch [80/100] batch [5/5] time 0.053 (0.275) data 0.000 (0.214) loss 0.7584 (0.8273) acc 91.6667 (89.2982) lr 2.0984e-04 eta 0:00:27
epoch [81/100] batch [5/5] time 0.057 (0.275) data 0.000 (0.214) loss 0.8667 (0.8444) acc 89.4737 (90.3203) lr 1.9098e-04 eta 0:00:26
>>> samples [92/160] noisy rate: 0.00 --> 0.24 --> 0.14 <<<
epoch [82/100] batch [5/5] time 0.060 (0.285) data 0.001 (0.226) loss 0.7350 (0.8328) acc 91.6667 (90.3958) lr 1.7292e-04 eta 0:00:25
epoch [83/100] batch [5/5] time 0.061 (0.260) data 0.000 (0.197) loss 0.7454 (0.8292) acc 87.5000 (90.8170) lr 1.5567e-04 eta 0:00:22
epoch [84/100] batch [5/5] time 0.063 (0.283) data 0.000 (0.215) loss 0.9117 (0.8494) acc 91.2500 (88.4425) lr 1.3926e-04 eta 0:00:22
epoch [85/100] batch [5/5] time 0.063 (0.271) data 0.000 (0.209) loss 0.9453 (0.8084) acc 90.0000 (90.3142) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.055 (0.285) data 0.000 (0.220) loss 1.0568 (0.8329) acc 83.8235 (90.3502) lr 1.0899e-04 eta 0:00:19
>>> samples [92/160] noisy rate: 0.00 --> 0.24 --> 0.14 <<<
epoch [87/100] batch [5/5] time 0.055 (0.285) data 0.001 (0.223) loss 0.9352 (0.8444) acc 88.8889 (87.5784) lr 9.5173e-05 eta 0:00:18
epoch [88/100] batch [5/5] time 0.051 (0.294) data 0.000 (0.231) loss 0.7177 (0.8121) acc 98.3333 (92.3589) lr 8.2245e-05 eta 0:00:17
epoch [89/100] batch [5/5] time 0.052 (0.276) data 0.000 (0.210) loss 0.8761 (0.8373) acc 97.0588 (91.8447) lr 7.0224e-05 eta 0:00:15
epoch [90/100] batch [5/5] time 0.057 (0.297) data 0.000 (0.233) loss 0.9120 (0.8283) acc 100.0000 (91.1506) lr 5.9119e-05 eta 0:00:14
epoch [91/100] batch [5/5] time 0.061 (0.273) data 0.000 (0.211) loss 1.0215 (0.8191) acc 88.1579 (92.3637) lr 4.8943e-05 eta 0:00:12
>>> samples [93/160] noisy rate: 0.00 --> 0.26 --> 0.14 <<<
epoch [92/100] batch [5/5] time 0.054 (0.282) data 0.001 (0.216) loss 0.6558 (0.8140) acc 86.1111 (89.5107) lr 3.9706e-05 eta 0:00:11
epoch [93/100] batch [5/5] time 0.059 (0.263) data 0.001 (0.198) loss 0.6975 (0.8097) acc 95.8333 (89.6867) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.056 (0.262) data 0.000 (0.200) loss 0.8298 (0.8268) acc 93.4211 (89.9368) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.046 (0.247) data 0.000 (0.187) loss 0.9121 (0.8352) acc 88.4615 (90.3160) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.066 (0.278) data 0.000 (0.214) loss 0.9495 (0.8361) acc 83.3333 (88.4236) lr 1.2312e-05 eta 0:00:05
>>> samples [93/160] noisy rate: 0.00 --> 0.26 --> 0.14 <<<
epoch [97/100] batch [5/5] time 0.057 (0.292) data 0.000 (0.227) loss 0.9504 (0.8055) acc 84.3750 (90.9765) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.058 (0.293) data 0.000 (0.229) loss 0.6928 (0.7891) acc 93.7500 (90.0875) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.064 (0.265) data 0.000 (0.206) loss 0.8552 (0.8400) acc 91.2500 (89.3106) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.056 (0.267) data 0.000 (0.198) loss 0.9687 (0.8017) acc 90.6250 (92.6189) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.0, 0.42, 0.19, 0.19, 0.22, 0.24, 0.24, 0.24, 0.22, 0.24, 0.24, 0.26, 0.26, 0.24, 0.26, 0.25, 0.24, 0.24, 0.26, 0.26]
* learned noise rate: [0.0, 0.03, 0.04, 0.08, 0.08, 0.09, 0.09, 0.09, 0.11, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:50,  1.39s/it]  4%|▎         | 3/81 [00:01<00:32,  2.42it/s]  6%|▌         | 5/81 [00:01<00:18,  4.13it/s]  9%|▊         | 7/81 [00:01<00:12,  5.81it/s] 11%|█         | 9/81 [00:01<00:09,  7.60it/s] 14%|█▎        | 11/81 [00:02<00:07,  9.28it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.42it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.72it/s] 21%|██        | 17/81 [00:02<00:05, 12.78it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.18it/s] 26%|██▌       | 21/81 [00:02<00:04, 13.82it/s] 28%|██▊       | 23/81 [00:02<00:04, 14.20it/s] 31%|███       | 25/81 [00:03<00:03, 14.06it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.54it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.92it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.45it/s] 41%|████      | 33/81 [00:03<00:03, 14.48it/s] 43%|████▎     | 35/81 [00:03<00:03, 14.88it/s] 46%|████▌     | 37/81 [00:03<00:03, 14.37it/s] 48%|████▊     | 39/81 [00:03<00:02, 14.80it/s] 51%|█████     | 41/81 [00:04<00:02, 14.71it/s] 53%|█████▎    | 43/81 [00:04<00:02, 14.79it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.63it/s] 58%|█████▊    | 47/81 [00:04<00:02, 14.93it/s] 60%|██████    | 49/81 [00:04<00:02, 15.21it/s] 63%|██████▎   | 51/81 [00:04<00:02, 14.95it/s] 65%|██████▌   | 53/81 [00:04<00:01, 14.92it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.13it/s] 70%|███████   | 57/81 [00:05<00:01, 14.30it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.33it/s] 75%|███████▌  | 61/81 [00:05<00:01, 14.78it/s] 78%|███████▊  | 63/81 [00:05<00:01, 14.67it/s] 80%|████████  | 65/81 [00:05<00:01, 14.94it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.01it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.28it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.51it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.68it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.80it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.89it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.95it/s]100%|██████████| 81/81 [00:06<00:00, 15.99it/s]100%|██████████| 81/81 [00:06<00:00, 11.64it/s]
=> result
* total: 8,100
* correct: 4,920
* accuracy: 60.7%
* error: 39.3%
* macro_f1: 56.1%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 535	acc: 59.4%
* class: 1 (Forest)	total: 900	correct: 892	acc: 99.1%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 52	acc: 5.8%
* class: 3 (Highway or Road)	total: 750	correct: 393	acc: 52.4%
* class: 4 (Industrial Buildings)	total: 750	correct: 668	acc: 89.1%
* class: 5 (Pasture Land)	total: 600	correct: 474	acc: 79.0%
* class: 6 (Permanent Crop Land)	total: 750	correct: 613	acc: 81.7%
* class: 7 (Residential Buildings)	total: 900	correct: 847	acc: 94.1%
* class: 8 (River)	total: 750	correct: 445	acc: 59.3%
* class: 9 (Sea or Lake)	total: 900	correct: 1	acc: 0.1%
* average: 62.0%
Elapsed: 0:04:25
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_0-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.677) data 0.000 (0.217) loss 1.0639 (1.0964) acc 20.3125 (18.7500) lr 2.0000e-03 eta 0:05:35
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [85/160] noisy rate: 0.00 --> 0.14 --> 0.06 <<<
epoch [2/100] batch [5/5] time 0.432 (0.519) data 0.000 (0.184) loss 2.2471 (2.1358) acc 11.1111 (24.3399) lr 1.9995e-03 eta 0:04:14
epoch [3/100] batch [5/5] time 0.434 (0.407) data 0.000 (0.213) loss 1.9201 (2.0906) acc 39.4737 (31.3435) lr 1.9980e-03 eta 0:03:17
epoch [4/100] batch [5/5] time 0.380 (0.306) data 0.000 (0.182) loss 2.0203 (1.9284) acc 34.3750 (38.7094) lr 1.9956e-03 eta 0:02:27
epoch [5/100] batch [5/5] time 0.058 (0.311) data 0.000 (0.190) loss 2.0891 (1.8764) acc 43.0556 (41.3532) lr 1.9921e-03 eta 0:02:27
epoch [6/100] batch [5/5] time 0.059 (0.290) data 0.000 (0.230) loss 2.0860 (1.9522) acc 35.5263 (38.7792) lr 1.9877e-03 eta 0:02:16
>>> samples [86/160] noisy rate: 0.00 --> 0.23 --> 0.07 <<<
epoch [7/100] batch [5/5] time 0.048 (0.318) data 0.000 (0.181) loss 1.9542 (1.9091) acc 46.8750 (39.5524) lr 1.9823e-03 eta 0:02:27
epoch [8/100] batch [5/5] time 0.052 (0.243) data 0.000 (0.186) loss 1.5792 (1.7816) acc 50.0000 (49.3045) lr 1.9759e-03 eta 0:01:51
epoch [9/100] batch [5/5] time 0.060 (0.239) data 0.000 (0.178) loss 1.6663 (1.7620) acc 63.0952 (49.4663) lr 1.9686e-03 eta 0:01:48
epoch [10/100] batch [5/5] time 0.054 (0.261) data 0.000 (0.200) loss 1.6539 (1.7264) acc 60.9375 (52.2104) lr 1.9603e-03 eta 0:01:57
epoch [11/100] batch [5/5] time 0.052 (0.267) data 0.000 (0.207) loss 1.9381 (1.6668) acc 48.4375 (57.2917) lr 1.9511e-03 eta 0:01:58
>>> samples [88/160] noisy rate: 0.00 --> 0.31 --> 0.09 <<<
epoch [12/100] batch [5/5] time 0.053 (0.319) data 0.000 (0.255) loss 1.5892 (1.6800) acc 63.8889 (60.0208) lr 1.9409e-03 eta 0:02:20
epoch [13/100] batch [5/5] time 0.059 (0.276) data 0.001 (0.216) loss 1.9102 (1.6900) acc 44.4444 (59.2339) lr 1.9298e-03 eta 0:02:00
epoch [14/100] batch [5/5] time 0.055 (0.265) data 0.000 (0.205) loss 1.5781 (1.5932) acc 72.2222 (66.8403) lr 1.9178e-03 eta 0:01:54
epoch [15/100] batch [5/5] time 0.052 (0.424) data 0.000 (0.228) loss 1.5822 (1.5603) acc 64.0625 (64.6740) lr 1.9048e-03 eta 0:03:00
epoch [16/100] batch [5/5] time 0.058 (0.260) data 0.001 (0.203) loss 1.8680 (1.5814) acc 27.7778 (58.1562) lr 1.8910e-03 eta 0:01:49
>>> samples [92/160] noisy rate: 0.00 --> 0.29 --> 0.12 <<<
epoch [17/100] batch [5/5] time 0.053 (0.266) data 0.000 (0.204) loss 1.4898 (1.5513) acc 66.1765 (64.6695) lr 1.8763e-03 eta 0:01:50
epoch [18/100] batch [5/5] time 0.060 (0.209) data 0.000 (0.148) loss 1.4695 (1.5243) acc 68.4211 (65.7643) lr 1.8607e-03 eta 0:01:25
epoch [19/100] batch [5/5] time 0.053 (0.255) data 0.000 (0.193) loss 1.4167 (1.5248) acc 68.0555 (67.8580) lr 1.8443e-03 eta 0:01:43
epoch [20/100] batch [5/5] time 0.064 (0.253) data 0.000 (0.187) loss 1.4800 (1.5012) acc 70.0000 (69.2339) lr 1.8271e-03 eta 0:01:41
epoch [21/100] batch [5/5] time 0.060 (0.253) data 0.000 (0.190) loss 1.3843 (1.4667) acc 61.1111 (70.3056) lr 1.8090e-03 eta 0:01:40
>>> samples [94/160] noisy rate: 0.00 --> 0.26 --> 0.13 <<<
epoch [22/100] batch [5/5] time 0.056 (0.355) data 0.000 (0.208) loss 1.5956 (1.4826) acc 62.5000 (68.7824) lr 1.7902e-03 eta 0:02:18
epoch [23/100] batch [5/5] time 0.068 (0.290) data 0.000 (0.225) loss 1.5077 (1.4354) acc 82.1429 (75.0397) lr 1.7705e-03 eta 0:01:51
epoch [24/100] batch [5/5] time 0.060 (0.272) data 0.000 (0.205) loss 1.5415 (1.4459) acc 79.4118 (75.3368) lr 1.7501e-03 eta 0:01:43
epoch [25/100] batch [5/5] time 0.064 (0.291) data 0.000 (0.226) loss 1.6161 (1.4118) acc 81.2500 (76.4898) lr 1.7290e-03 eta 0:01:48
epoch [26/100] batch [5/5] time 0.060 (0.370) data 0.000 (0.218) loss 1.4654 (1.4582) acc 75.0000 (74.2343) lr 1.7071e-03 eta 0:02:16
>>> samples [96/160] noisy rate: 0.00 --> 0.19 --> 0.15 <<<
epoch [27/100] batch [5/5] time 0.072 (0.280) data 0.000 (0.213) loss 1.4571 (1.4094) acc 72.7273 (75.7677) lr 1.6845e-03 eta 0:01:42
epoch [28/100] batch [5/5] time 0.071 (0.251) data 0.000 (0.187) loss 1.3818 (1.4391) acc 79.5455 (75.9383) lr 1.6613e-03 eta 0:01:30
epoch [29/100] batch [5/5] time 0.057 (0.259) data 0.000 (0.192) loss 1.2556 (1.4204) acc 76.4706 (75.7076) lr 1.6374e-03 eta 0:01:31
epoch [30/100] batch [5/5] time 0.067 (0.268) data 0.000 (0.206) loss 1.3249 (1.4000) acc 84.0909 (76.4381) lr 1.6129e-03 eta 0:01:33
epoch [31/100] batch [5/5] time 0.060 (0.266) data 0.000 (0.201) loss 1.6061 (1.4360) acc 67.1053 (74.6591) lr 1.5878e-03 eta 0:01:31
>>> samples [96/160] noisy rate: 0.00 --> 0.19 --> 0.15 <<<
epoch [32/100] batch [5/5] time 0.069 (0.288) data 0.000 (0.224) loss 1.2600 (1.3953) acc 70.2381 (75.7148) lr 1.5621e-03 eta 0:01:37
epoch [33/100] batch [5/5] time 0.064 (0.353) data 0.000 (0.202) loss 1.4063 (1.3968) acc 78.9474 (76.0968) lr 1.5358e-03 eta 0:01:58
epoch [34/100] batch [5/5] time 0.064 (0.288) data 0.000 (0.223) loss 1.3674 (1.3772) acc 78.7500 (78.5065) lr 1.5090e-03 eta 0:01:35
epoch [35/100] batch [5/5] time 0.056 (0.273) data 0.000 (0.206) loss 1.4727 (1.3724) acc 77.6316 (79.9203) lr 1.4818e-03 eta 0:01:28
epoch [36/100] batch [5/5] time 0.052 (0.272) data 0.000 (0.212) loss 1.4373 (1.3716) acc 81.2500 (80.5203) lr 1.4540e-03 eta 0:01:27
>>> samples [96/160] noisy rate: 0.00 --> 0.18 --> 0.15 <<<
epoch [37/100] batch [5/5] time 0.056 (0.263) data 0.000 (0.200) loss 1.6442 (1.3766) acc 64.4737 (80.4758) lr 1.4258e-03 eta 0:01:22
epoch [38/100] batch [5/5] time 0.051 (0.260) data 0.000 (0.195) loss 1.4230 (1.3539) acc 73.5294 (80.0486) lr 1.3971e-03 eta 0:01:20
epoch [39/100] batch [5/5] time 0.058 (0.355) data 0.000 (0.205) loss 1.4851 (1.4079) acc 78.9474 (77.8973) lr 1.3681e-03 eta 0:01:48
epoch [40/100] batch [5/5] time 0.078 (0.268) data 0.000 (0.205) loss 1.3551 (1.3829) acc 77.8846 (77.3002) lr 1.3387e-03 eta 0:01:20
epoch [41/100] batch [5/5] time 0.048 (0.250) data 0.000 (0.184) loss 1.0572 (1.3318) acc 90.6250 (84.6561) lr 1.3090e-03 eta 0:01:13
>>> samples [97/160] noisy rate: 0.00 --> 0.21 --> 0.14 <<<
epoch [42/100] batch [5/5] time 0.057 (0.259) data 0.000 (0.193) loss 1.5560 (1.3720) acc 73.7500 (78.1042) lr 1.2790e-03 eta 0:01:15
epoch [43/100] batch [5/5] time 0.057 (0.281) data 0.000 (0.217) loss 1.2792 (1.3632) acc 76.2500 (78.1553) lr 1.2487e-03 eta 0:01:20
epoch [44/100] batch [5/5] time 0.056 (0.276) data 0.000 (0.212) loss 1.1915 (1.3716) acc 82.8947 (79.2729) lr 1.2181e-03 eta 0:01:17
epoch [45/100] batch [5/5] time 0.055 (0.264) data 0.000 (0.198) loss 1.3684 (1.3572) acc 86.8421 (82.2156) lr 1.1874e-03 eta 0:01:12
epoch [46/100] batch [5/5] time 0.060 (0.259) data 0.000 (0.196) loss 1.1271 (1.3208) acc 76.3158 (83.7303) lr 1.1564e-03 eta 0:01:09
>>> samples [97/160] noisy rate: 0.00 --> 0.17 --> 0.14 <<<
epoch [47/100] batch [5/5] time 0.052 (0.311) data 0.000 (0.248) loss 1.1953 (1.3358) acc 81.6667 (81.9912) lr 1.1253e-03 eta 0:01:22
epoch [48/100] batch [5/5] time 0.053 (0.277) data 0.000 (0.214) loss 1.2848 (1.3438) acc 84.7222 (82.1478) lr 1.0941e-03 eta 0:01:12
epoch [49/100] batch [5/5] time 0.050 (0.353) data 0.000 (0.205) loss 1.2207 (1.3411) acc 79.6875 (80.9929) lr 1.0628e-03 eta 0:01:30
epoch [50/100] batch [5/5] time 0.060 (0.253) data 0.000 (0.189) loss 1.4316 (1.3161) acc 86.8421 (83.4770) lr 1.0314e-03 eta 0:01:03
epoch [51/100] batch [5/5] time 0.044 (0.263) data 0.000 (0.201) loss 1.2535 (1.3142) acc 83.9286 (81.5527) lr 1.0000e-03 eta 0:01:04
>>> samples [97/160] noisy rate: 0.00 --> 0.17 --> 0.14 <<<
epoch [52/100] batch [5/5] time 0.063 (0.271) data 0.000 (0.210) loss 1.3953 (1.3148) acc 73.7500 (81.0760) lr 9.6859e-04 eta 0:01:05
epoch [53/100] batch [5/5] time 0.071 (0.272) data 0.000 (0.206) loss 1.1205 (1.3646) acc 88.0435 (82.7510) lr 9.3721e-04 eta 0:01:03
epoch [54/100] batch [5/5] time 0.072 (0.265) data 0.000 (0.203) loss 1.3992 (1.3139) acc 71.1538 (83.4606) lr 9.0589e-04 eta 0:01:00
epoch [55/100] batch [5/5] time 0.058 (0.226) data 0.000 (0.161) loss 1.2717 (1.3473) acc 88.2353 (82.4176) lr 8.7467e-04 eta 0:00:50
epoch [56/100] batch [5/5] time 0.062 (0.280) data 0.000 (0.216) loss 1.2844 (1.2846) acc 83.3333 (85.4155) lr 8.4357e-04 eta 0:01:01
>>> samples [98/160] noisy rate: 0.00 --> 0.18 --> 0.15 <<<
epoch [57/100] batch [5/5] time 0.060 (0.256) data 0.000 (0.193) loss 1.5770 (1.3577) acc 85.5263 (82.5899) lr 8.1262e-04 eta 0:00:54
epoch [58/100] batch [5/5] time 0.057 (0.251) data 0.000 (0.189) loss 1.4193 (1.3324) acc 76.2500 (80.8064) lr 7.8186e-04 eta 0:00:52
epoch [59/100] batch [5/5] time 0.055 (0.251) data 0.000 (0.190) loss 1.5290 (1.3249) acc 78.9474 (84.4055) lr 7.5131e-04 eta 0:00:51
epoch [60/100] batch [5/5] time 0.058 (0.244) data 0.000 (0.180) loss 1.2309 (1.3421) acc 81.2500 (83.6292) lr 7.2101e-04 eta 0:00:48
epoch [61/100] batch [5/5] time 0.068 (0.258) data 0.000 (0.194) loss 1.3108 (1.3217) acc 75.0000 (84.1704) lr 6.9098e-04 eta 0:00:50
>>> samples [98/160] noisy rate: 0.00 --> 0.22 --> 0.15 <<<
epoch [62/100] batch [5/5] time 0.054 (0.252) data 0.000 (0.189) loss 1.4595 (1.3070) acc 70.8333 (82.6078) lr 6.6126e-04 eta 0:00:47
epoch [63/100] batch [5/5] time 0.043 (0.260) data 0.000 (0.199) loss 1.1694 (1.3123) acc 92.8571 (84.8846) lr 6.3188e-04 eta 0:00:48
epoch [64/100] batch [5/5] time 0.059 (0.253) data 0.000 (0.189) loss 1.3501 (1.3124) acc 84.2105 (84.4953) lr 6.0285e-04 eta 0:00:45
epoch [65/100] batch [5/5] time 0.054 (0.242) data 0.000 (0.179) loss 1.3876 (1.3125) acc 78.9474 (82.3196) lr 5.7422e-04 eta 0:00:42
epoch [66/100] batch [5/5] time 0.055 (0.215) data 0.000 (0.154) loss 1.4234 (1.3189) acc 82.8947 (84.8083) lr 5.4601e-04 eta 0:00:36
>>> samples [98/160] noisy rate: 0.00 --> 0.19 --> 0.15 <<<
epoch [67/100] batch [5/5] time 0.062 (0.228) data 0.000 (0.163) loss 1.3878 (1.2900) acc 83.7500 (85.2696) lr 5.1825e-04 eta 0:00:37
epoch [68/100] batch [5/5] time 0.072 (0.264) data 0.001 (0.199) loss 1.3507 (1.2761) acc 89.1304 (87.1685) lr 4.9096e-04 eta 0:00:42
epoch [69/100] batch [5/5] time 0.067 (0.268) data 0.001 (0.200) loss 1.1840 (1.3000) acc 94.0476 (85.8622) lr 4.6417e-04 eta 0:00:41
epoch [70/100] batch [5/5] time 0.068 (0.260) data 0.000 (0.194) loss 1.3915 (1.2931) acc 80.6818 (86.7344) lr 4.3792e-04 eta 0:00:38
epoch [71/100] batch [5/5] time 0.062 (0.284) data 0.000 (0.221) loss 1.0644 (1.3056) acc 90.4762 (85.1668) lr 4.1221e-04 eta 0:00:41
>>> samples [98/160] noisy rate: 0.00 --> 0.26 --> 0.15 <<<
epoch [72/100] batch [5/5] time 0.060 (0.264) data 0.000 (0.201) loss 1.4605 (1.3035) acc 81.5789 (86.1579) lr 3.8709e-04 eta 0:00:36
epoch [73/100] batch [5/5] time 0.043 (0.219) data 0.000 (0.158) loss 1.2335 (1.2907) acc 89.2857 (85.7486) lr 3.6258e-04 eta 0:00:29
epoch [74/100] batch [5/5] time 0.061 (0.238) data 0.000 (0.176) loss 1.1637 (1.2949) acc 89.2857 (86.3381) lr 3.3869e-04 eta 0:00:30
epoch [75/100] batch [5/5] time 0.058 (0.283) data 0.000 (0.219) loss 1.1855 (1.2837) acc 88.8889 (86.5930) lr 3.1545e-04 eta 0:00:35
epoch [76/100] batch [5/5] time 0.045 (0.251) data 0.000 (0.186) loss 1.3850 (1.2939) acc 78.5714 (84.9691) lr 2.9289e-04 eta 0:00:30
>>> samples [98/160] noisy rate: 0.00 --> 0.21 --> 0.15 <<<
epoch [77/100] batch [5/5] time 0.073 (0.262) data 0.000 (0.197) loss 1.1641 (1.3004) acc 92.7083 (84.2961) lr 2.7103e-04 eta 0:00:30
epoch [78/100] batch [5/5] time 0.059 (0.236) data 0.000 (0.172) loss 1.3583 (1.2858) acc 77.6316 (85.3975) lr 2.4989e-04 eta 0:00:25
epoch [79/100] batch [5/5] time 0.062 (0.223) data 0.000 (0.157) loss 1.2159 (1.3324) acc 88.7500 (82.4293) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.066 (0.275) data 0.000 (0.212) loss 1.3379 (1.2865) acc 83.3333 (85.0368) lr 2.0984e-04 eta 0:00:27
epoch [81/100] batch [5/5] time 0.076 (0.265) data 0.000 (0.203) loss 1.1630 (1.2868) acc 84.6154 (86.3906) lr 1.9098e-04 eta 0:00:25
>>> samples [98/160] noisy rate: 0.00 --> 0.20 --> 0.15 <<<
epoch [82/100] batch [5/5] time 0.058 (0.264) data 0.000 (0.200) loss 1.5945 (1.2813) acc 76.3889 (84.7503) lr 1.7292e-04 eta 0:00:23
epoch [83/100] batch [5/5] time 0.063 (0.270) data 0.000 (0.208) loss 1.3717 (1.2846) acc 84.0909 (86.9318) lr 1.5567e-04 eta 0:00:22
epoch [84/100] batch [5/5] time 0.055 (0.272) data 0.000 (0.209) loss 1.1973 (1.3081) acc 90.7895 (84.2297) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.054 (0.253) data 0.001 (0.187) loss 1.4092 (1.2944) acc 80.5555 (85.1688) lr 1.2369e-04 eta 0:00:18
epoch [86/100] batch [5/5] time 0.072 (0.286) data 0.000 (0.220) loss 1.3421 (1.2977) acc 83.3333 (85.3048) lr 1.0899e-04 eta 0:00:20
>>> samples [99/160] noisy rate: 0.00 --> 0.21 --> 0.16 <<<
epoch [87/100] batch [5/5] time 0.056 (0.236) data 0.000 (0.170) loss 0.8755 (1.2766) acc 97.3684 (85.5569) lr 9.5173e-05 eta 0:00:15
epoch [88/100] batch [5/5] time 0.066 (0.253) data 0.000 (0.185) loss 1.3804 (1.2753) acc 83.3333 (85.9166) lr 8.2245e-05 eta 0:00:15
epoch [89/100] batch [5/5] time 0.065 (0.279) data 0.000 (0.215) loss 1.4456 (1.2695) acc 84.0909 (86.6603) lr 7.0224e-05 eta 0:00:15
epoch [90/100] batch [5/5] time 0.072 (0.278) data 0.000 (0.213) loss 1.3032 (1.3102) acc 85.8696 (85.7329) lr 5.9119e-05 eta 0:00:13
epoch [91/100] batch [5/5] time 0.064 (0.253) data 0.000 (0.189) loss 1.2472 (1.2801) acc 87.5000 (86.4379) lr 4.8943e-05 eta 0:00:11
>>> samples [99/160] noisy rate: 0.00 --> 0.19 --> 0.16 <<<
epoch [92/100] batch [5/5] time 0.072 (0.265) data 0.000 (0.194) loss 1.2435 (1.2706) acc 84.3750 (85.9607) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.068 (0.263) data 0.000 (0.198) loss 1.0807 (1.3163) acc 84.5238 (84.1381) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.059 (0.266) data 0.000 (0.201) loss 1.2186 (1.2922) acc 83.7500 (85.6524) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.060 (0.254) data 0.000 (0.188) loss 1.1705 (1.2699) acc 90.7895 (85.6276) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.060 (0.263) data 0.000 (0.198) loss 1.3252 (1.2818) acc 86.1111 (86.8225) lr 1.2312e-05 eta 0:00:05
>>> samples [99/160] noisy rate: 0.00 --> 0.22 --> 0.16 <<<
epoch [97/100] batch [5/5] time 0.052 (0.288) data 0.000 (0.222) loss 1.3738 (1.3055) acc 80.0000 (84.0561) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.061 (0.279) data 0.000 (0.212) loss 1.5861 (1.2980) acc 78.9474 (86.0902) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.060 (0.290) data 0.000 (0.221) loss 1.1672 (1.2723) acc 86.7647 (85.4125) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.065 (0.308) data 0.000 (0.239) loss 1.4649 (1.2813) acc 88.7500 (87.4242) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.14, 0.23, 0.31, 0.29, 0.26, 0.19, 0.19, 0.18, 0.21, 0.17, 0.17, 0.18, 0.22, 0.19, 0.26, 0.21, 0.2, 0.21, 0.19, 0.22]
* learned noise rate: [0.06, 0.07, 0.09, 0.12, 0.13, 0.15, 0.15, 0.15, 0.14, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:45,  1.32s/it]  4%|▎         | 3/81 [00:01<00:32,  2.44it/s]  6%|▌         | 5/81 [00:01<00:18,  4.13it/s]  9%|▊         | 7/81 [00:01<00:12,  5.74it/s] 11%|█         | 9/81 [00:02<00:10,  7.08it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.68it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.20it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.53it/s] 21%|██        | 17/81 [00:02<00:05, 12.55it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.44it/s] 26%|██▌       | 21/81 [00:02<00:04, 14.04it/s] 28%|██▊       | 23/81 [00:02<00:03, 14.57it/s] 31%|███       | 25/81 [00:03<00:03, 14.96it/s] 33%|███▎      | 27/81 [00:03<00:03, 15.13it/s] 36%|███▌      | 29/81 [00:03<00:03, 15.37it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.30it/s] 41%|████      | 33/81 [00:03<00:03, 15.48it/s] 43%|████▎     | 35/81 [00:03<00:03, 15.30it/s] 46%|████▌     | 37/81 [00:03<00:02, 15.48it/s] 48%|████▊     | 39/81 [00:03<00:02, 15.61it/s] 51%|█████     | 41/81 [00:04<00:02, 15.37it/s] 53%|█████▎    | 43/81 [00:04<00:02, 14.73it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.81it/s] 58%|█████▊    | 47/81 [00:04<00:02, 14.92it/s] 60%|██████    | 49/81 [00:04<00:02, 14.82it/s] 63%|██████▎   | 51/81 [00:04<00:01, 15.14it/s] 65%|██████▌   | 53/81 [00:04<00:01, 15.33it/s] 68%|██████▊   | 55/81 [00:04<00:01, 15.49it/s] 70%|███████   | 57/81 [00:05<00:01, 15.26it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.38it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.36it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.13it/s] 80%|████████  | 65/81 [00:05<00:01, 15.37it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.54it/s] 85%|████████▌ | 69/81 [00:05<00:00, 15.68it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.78it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.84it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.89it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.91it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.94it/s]100%|██████████| 81/81 [00:06<00:00, 15.95it/s]100%|██████████| 81/81 [00:06<00:00, 11.87it/s]
=> result
* total: 8,100
* correct: 5,095
* accuracy: 62.9%
* error: 37.1%
* macro_f1: 60.3%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 490	acc: 54.4%
* class: 1 (Forest)	total: 900	correct: 884	acc: 98.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 68	acc: 7.6%
* class: 3 (Highway or Road)	total: 750	correct: 347	acc: 46.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 735	acc: 98.0%
* class: 5 (Pasture Land)	total: 600	correct: 337	acc: 56.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 674	acc: 89.9%
* class: 7 (Residential Buildings)	total: 900	correct: 741	acc: 82.3%
* class: 8 (River)	total: 750	correct: 287	acc: 38.3%
* class: 9 (Sea or Lake)	total: 900	correct: 532	acc: 59.1%
* average: 63.0%
Elapsed: 0:04:28
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_0-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.752) data 0.000 (0.243) loss 1.1455 (1.1174) acc 17.9688 (15.3125) lr 2.0000e-03 eta 0:06:12
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [70/160] noisy rate: 0.00 --> 0.21 --> 0.00 <<<
epoch [2/100] batch [5/5] time 0.050 (0.429) data 0.000 (0.151) loss 2.2494 (2.2034) acc 13.4615 (18.6448) lr 1.9995e-03 eta 0:03:30
epoch [3/100] batch [5/5] time 0.318 (0.379) data 0.000 (0.205) loss 1.9256 (2.0266) acc 34.0909 (27.0409) lr 1.9980e-03 eta 0:03:03
epoch [4/100] batch [5/5] time 0.045 (0.355) data 0.000 (0.184) loss 2.0098 (1.9717) acc 29.1667 (32.4244) lr 1.9956e-03 eta 0:02:50
epoch [5/100] batch [5/5] time 0.042 (0.255) data 0.000 (0.197) loss 1.8692 (1.8749) acc 58.3333 (41.8718) lr 1.9921e-03 eta 0:02:01
epoch [6/100] batch [5/5] time 0.038 (0.238) data 0.000 (0.184) loss 1.8693 (1.7853) acc 40.9091 (46.9318) lr 1.9877e-03 eta 0:01:52
>>> samples [79/160] noisy rate: 0.00 --> 0.29 --> 0.08 <<<
epoch [7/100] batch [5/5] time 0.052 (0.365) data 0.000 (0.236) loss 1.7760 (1.8086) acc 41.1765 (45.3101) lr 1.9823e-03 eta 0:02:49
epoch [8/100] batch [5/5] time 0.055 (0.305) data 0.000 (0.246) loss 1.6750 (1.7721) acc 66.6667 (52.7145) lr 1.9759e-03 eta 0:02:20
epoch [9/100] batch [5/5] time 0.052 (0.273) data 0.001 (0.212) loss 1.7587 (1.6668) acc 45.0000 (55.6181) lr 1.9686e-03 eta 0:02:04
epoch [10/100] batch [5/5] time 0.051 (0.265) data 0.001 (0.207) loss 1.3744 (1.5812) acc 64.0625 (62.1426) lr 1.9603e-03 eta 0:01:59
epoch [11/100] batch [5/5] time 0.039 (0.314) data 0.000 (0.190) loss 1.5999 (1.5608) acc 60.4167 (64.2500) lr 1.9511e-03 eta 0:02:19
>>> samples [83/160] noisy rate: 0.00 --> 0.32 --> 0.08 <<<
epoch [12/100] batch [5/5] time 0.434 (0.363) data 0.000 (0.229) loss 1.5420 (1.5283) acc 75.0000 (65.1579) lr 1.9409e-03 eta 0:02:39
epoch [13/100] batch [5/5] time 0.062 (0.244) data 0.000 (0.184) loss 1.6364 (1.5296) acc 65.4762 (66.2539) lr 1.9298e-03 eta 0:01:46
epoch [14/100] batch [5/5] time 0.049 (0.308) data 0.000 (0.178) loss 1.4191 (1.5291) acc 65.6250 (62.1160) lr 1.9178e-03 eta 0:02:12
epoch [15/100] batch [5/5] time 0.044 (0.234) data 0.000 (0.174) loss 1.8272 (1.5295) acc 56.2500 (66.3947) lr 1.9048e-03 eta 0:01:39
epoch [16/100] batch [5/5] time 0.047 (0.302) data 0.000 (0.242) loss 1.6532 (1.4997) acc 63.3333 (69.7306) lr 1.8910e-03 eta 0:02:06
>>> samples [83/160] noisy rate: 0.00 --> 0.44 --> 0.08 <<<
epoch [17/100] batch [5/5] time 0.054 (0.263) data 0.000 (0.209) loss 1.3236 (1.4810) acc 79.6875 (68.8763) lr 1.8763e-03 eta 0:01:49
epoch [18/100] batch [5/5] time 0.052 (0.270) data 0.000 (0.216) loss 1.3439 (1.4456) acc 63.8889 (66.7262) lr 1.8607e-03 eta 0:01:50
epoch [19/100] batch [5/5] time 0.045 (0.234) data 0.000 (0.177) loss 1.3939 (1.4229) acc 75.0000 (68.7644) lr 1.8443e-03 eta 0:01:34
epoch [20/100] batch [5/5] time 0.056 (0.299) data 0.000 (0.244) loss 1.2969 (1.4117) acc 78.9474 (73.4900) lr 1.8271e-03 eta 0:01:59
epoch [21/100] batch [5/5] time 0.044 (0.255) data 0.000 (0.197) loss 1.5996 (1.4073) acc 57.6923 (70.5396) lr 1.8090e-03 eta 0:01:40
>>> samples [85/160] noisy rate: 0.00 --> 0.32 --> 0.09 <<<
epoch [22/100] batch [5/5] time 0.059 (0.261) data 0.000 (0.201) loss 1.3028 (1.3534) acc 75.0000 (77.6775) lr 1.7902e-03 eta 0:01:41
epoch [23/100] batch [5/5] time 0.044 (0.335) data 0.000 (0.186) loss 1.6647 (1.3983) acc 75.0000 (74.1476) lr 1.7705e-03 eta 0:02:09
epoch [24/100] batch [5/5] time 0.488 (0.325) data 0.000 (0.181) loss 1.2827 (1.3548) acc 75.0000 (72.7475) lr 1.7501e-03 eta 0:02:03
epoch [25/100] batch [5/5] time 0.043 (0.297) data 0.000 (0.239) loss 1.4816 (1.3714) acc 68.1818 (73.4152) lr 1.7290e-03 eta 0:01:51
epoch [26/100] batch [5/5] time 0.061 (0.244) data 0.000 (0.183) loss 1.4457 (1.3035) acc 69.4444 (77.3366) lr 1.7071e-03 eta 0:01:30
>>> samples [85/160] noisy rate: 0.00 --> 0.26 --> 0.09 <<<
epoch [27/100] batch [5/5] time 0.051 (0.259) data 0.000 (0.201) loss 1.0797 (1.2943) acc 88.3333 (76.5607) lr 1.6845e-03 eta 0:01:34
epoch [28/100] batch [5/5] time 0.049 (0.269) data 0.001 (0.211) loss 1.0947 (1.3016) acc 75.0000 (77.7390) lr 1.6613e-03 eta 0:01:36
epoch [29/100] batch [5/5] time 0.064 (0.273) data 0.000 (0.214) loss 1.1589 (1.3286) acc 86.2500 (76.0376) lr 1.6374e-03 eta 0:01:36
epoch [30/100] batch [5/5] time 0.056 (0.258) data 0.000 (0.192) loss 1.3035 (1.3105) acc 76.5625 (75.4771) lr 1.6129e-03 eta 0:01:30
epoch [31/100] batch [5/5] time 0.053 (0.293) data 0.000 (0.236) loss 1.3310 (1.2592) acc 70.5882 (79.3077) lr 1.5878e-03 eta 0:01:41
>>> samples [88/160] noisy rate: 0.00 --> 0.26 --> 0.10 <<<
epoch [32/100] batch [5/5] time 0.063 (0.294) data 0.000 (0.233) loss 1.5035 (1.3169) acc 78.7500 (77.3986) lr 1.5621e-03 eta 0:01:39
epoch [33/100] batch [5/5] time 0.043 (0.324) data 0.001 (0.175) loss 1.1854 (1.2612) acc 81.2500 (78.8925) lr 1.5358e-03 eta 0:01:48
epoch [34/100] batch [5/5] time 0.050 (0.250) data 0.000 (0.193) loss 1.2212 (1.2924) acc 91.0714 (82.0645) lr 1.5090e-03 eta 0:01:22
epoch [35/100] batch [5/5] time 0.050 (0.256) data 0.000 (0.195) loss 1.4611 (1.3083) acc 75.0000 (78.8235) lr 1.4818e-03 eta 0:01:23
epoch [36/100] batch [5/5] time 0.055 (0.259) data 0.000 (0.198) loss 1.2313 (1.3050) acc 72.2222 (76.7681) lr 1.4540e-03 eta 0:01:22
>>> samples [89/160] noisy rate: 0.00 --> 0.25 --> 0.11 <<<
epoch [37/100] batch [5/5] time 0.060 (0.254) data 0.000 (0.193) loss 1.3006 (1.2746) acc 73.6111 (80.4049) lr 1.4258e-03 eta 0:01:20
epoch [38/100] batch [5/5] time 0.060 (0.249) data 0.000 (0.183) loss 1.1871 (1.2648) acc 80.5555 (77.8852) lr 1.3971e-03 eta 0:01:17
epoch [39/100] batch [5/5] time 0.061 (0.246) data 0.000 (0.182) loss 0.9712 (1.2692) acc 84.2105 (80.5149) lr 1.3681e-03 eta 0:01:15
epoch [40/100] batch [5/5] time 0.060 (0.304) data 0.000 (0.243) loss 1.2692 (1.2368) acc 76.3889 (78.0779) lr 1.3387e-03 eta 0:01:31
epoch [41/100] batch [5/5] time 0.053 (0.256) data 0.000 (0.195) loss 0.8849 (1.2479) acc 92.6471 (82.6618) lr 1.3090e-03 eta 0:01:15
>>> samples [91/160] noisy rate: 0.00 --> 0.20 --> 0.12 <<<
epoch [42/100] batch [5/5] time 0.055 (0.283) data 0.000 (0.221) loss 1.3288 (1.2283) acc 77.7778 (82.0782) lr 1.2790e-03 eta 0:01:22
epoch [43/100] batch [5/5] time 0.054 (0.290) data 0.000 (0.224) loss 1.3468 (1.2514) acc 83.8235 (78.1148) lr 1.2487e-03 eta 0:01:22
epoch [44/100] batch [5/5] time 0.066 (0.270) data 0.000 (0.205) loss 1.2170 (1.2329) acc 82.9545 (83.5212) lr 1.2181e-03 eta 0:01:15
epoch [45/100] batch [5/5] time 0.055 (0.258) data 0.000 (0.194) loss 1.2637 (1.2566) acc 80.5555 (82.2129) lr 1.1874e-03 eta 0:01:10
epoch [46/100] batch [5/5] time 0.055 (0.283) data 0.000 (0.219) loss 1.2054 (1.2358) acc 83.3333 (82.2752) lr 1.1564e-03 eta 0:01:16
>>> samples [92/160] noisy rate: 0.00 --> 0.24 --> 0.13 <<<
epoch [47/100] batch [5/5] time 0.052 (0.329) data 0.000 (0.265) loss 1.3073 (1.2196) acc 75.0000 (83.1796) lr 1.1253e-03 eta 0:01:27
epoch [48/100] batch [5/5] time 0.051 (0.295) data 0.001 (0.233) loss 1.2988 (1.2028) acc 79.6875 (79.6178) lr 1.0941e-03 eta 0:01:16
epoch [49/100] batch [5/5] time 0.055 (0.274) data 0.000 (0.208) loss 1.2051 (1.2523) acc 79.1667 (80.1327) lr 1.0628e-03 eta 0:01:09
epoch [50/100] batch [5/5] time 0.062 (0.285) data 0.000 (0.223) loss 1.2466 (1.2245) acc 88.0952 (81.4480) lr 1.0314e-03 eta 0:01:11
epoch [51/100] batch [5/5] time 0.054 (0.270) data 0.001 (0.205) loss 1.2716 (1.2488) acc 70.5882 (80.5572) lr 1.0000e-03 eta 0:01:06
>>> samples [94/160] noisy rate: 0.00 --> 0.25 --> 0.15 <<<
epoch [52/100] batch [5/5] time 0.050 (0.323) data 0.001 (0.263) loss 1.1999 (1.2476) acc 85.9375 (80.5916) lr 9.6859e-04 eta 0:01:17
epoch [53/100] batch [5/5] time 0.057 (0.308) data 0.000 (0.247) loss 1.3118 (1.2624) acc 75.0000 (77.4481) lr 9.3721e-04 eta 0:01:12
epoch [54/100] batch [5/5] time 0.055 (0.308) data 0.000 (0.247) loss 1.2266 (1.2300) acc 80.8824 (81.7642) lr 9.0589e-04 eta 0:01:10
epoch [55/100] batch [5/5] time 0.059 (0.272) data 0.000 (0.207) loss 1.0891 (1.2663) acc 82.5000 (80.5788) lr 8.7467e-04 eta 0:01:01
epoch [56/100] batch [5/5] time 0.052 (0.262) data 0.000 (0.198) loss 1.2843 (1.2182) acc 80.8824 (83.2934) lr 8.4357e-04 eta 0:00:57
>>> samples [94/160] noisy rate: 0.00 --> 0.24 --> 0.15 <<<
epoch [57/100] batch [5/5] time 0.064 (0.308) data 0.000 (0.246) loss 1.2318 (1.2062) acc 70.0000 (80.6284) lr 8.1262e-04 eta 0:01:06
epoch [58/100] batch [5/5] time 0.068 (0.271) data 0.000 (0.202) loss 1.3087 (1.1887) acc 85.7143 (82.4223) lr 7.8186e-04 eta 0:00:57
epoch [59/100] batch [5/5] time 0.063 (0.260) data 0.000 (0.192) loss 1.1921 (1.2184) acc 76.3158 (84.9695) lr 7.5131e-04 eta 0:00:53
epoch [60/100] batch [5/5] time 0.062 (0.249) data 0.000 (0.183) loss 1.2044 (1.2483) acc 83.3333 (80.3373) lr 7.2101e-04 eta 0:00:49
epoch [61/100] batch [5/5] time 0.071 (0.296) data 0.000 (0.231) loss 1.1409 (1.2191) acc 82.9545 (80.7412) lr 6.9098e-04 eta 0:00:57
>>> samples [94/160] noisy rate: 0.00 --> 0.24 --> 0.15 <<<
epoch [62/100] batch [5/5] time 0.071 (0.400) data 0.000 (0.241) loss 1.5369 (1.2156) acc 70.4545 (82.3617) lr 6.6126e-04 eta 0:01:15
epoch [63/100] batch [5/5] time 0.060 (0.240) data 0.000 (0.176) loss 1.3637 (1.2120) acc 82.3529 (83.0773) lr 6.3188e-04 eta 0:00:44
epoch [64/100] batch [5/5] time 0.063 (0.245) data 0.000 (0.181) loss 1.1335 (1.2108) acc 80.0000 (84.8929) lr 6.0285e-04 eta 0:00:44
epoch [65/100] batch [5/5] time 0.055 (0.225) data 0.000 (0.161) loss 1.0688 (1.2065) acc 89.7059 (83.5835) lr 5.7422e-04 eta 0:00:39
epoch [66/100] batch [5/5] time 0.068 (0.233) data 0.000 (0.172) loss 1.0782 (1.1955) acc 79.7619 (84.9574) lr 5.4601e-04 eta 0:00:39
>>> samples [94/160] noisy rate: 0.00 --> 0.24 --> 0.15 <<<
epoch [67/100] batch [5/5] time 0.044 (0.293) data 0.000 (0.228) loss 1.1374 (1.1882) acc 80.3571 (84.7019) lr 5.1825e-04 eta 0:00:48
epoch [68/100] batch [5/5] time 0.049 (0.240) data 0.000 (0.179) loss 0.8698 (1.2127) acc 83.9286 (82.9928) lr 4.9096e-04 eta 0:00:38
epoch [69/100] batch [5/5] time 0.062 (0.248) data 0.000 (0.183) loss 1.1144 (1.2342) acc 83.7500 (85.6253) lr 4.6417e-04 eta 0:00:38
epoch [70/100] batch [5/5] time 0.053 (0.240) data 0.000 (0.177) loss 1.0652 (1.2014) acc 87.5000 (85.0714) lr 4.3792e-04 eta 0:00:35
epoch [71/100] batch [5/5] time 0.071 (0.254) data 0.000 (0.179) loss 1.4462 (1.2169) acc 86.9048 (84.3502) lr 4.1221e-04 eta 0:00:36
>>> samples [94/160] noisy rate: 0.00 --> 0.26 --> 0.15 <<<
epoch [72/100] batch [5/5] time 0.060 (0.299) data 0.000 (0.233) loss 1.0057 (1.1791) acc 83.8235 (86.2041) lr 3.8709e-04 eta 0:00:41
epoch [73/100] batch [5/5] time 0.060 (0.254) data 0.000 (0.189) loss 1.2667 (1.2285) acc 77.7778 (82.0490) lr 3.6258e-04 eta 0:00:34
epoch [74/100] batch [5/5] time 0.060 (0.284) data 0.000 (0.218) loss 1.0425 (1.1921) acc 84.7222 (82.9875) lr 3.3869e-04 eta 0:00:36
epoch [75/100] batch [5/5] time 0.059 (0.260) data 0.000 (0.194) loss 1.3258 (1.1890) acc 80.8824 (84.1775) lr 3.1545e-04 eta 0:00:32
epoch [76/100] batch [5/5] time 0.057 (0.314) data 0.000 (0.250) loss 1.2082 (1.2219) acc 86.1111 (84.6594) lr 2.9289e-04 eta 0:00:37
>>> samples [95/160] noisy rate: 0.00 --> 0.24 --> 0.16 <<<
epoch [77/100] batch [5/5] time 0.064 (0.259) data 0.000 (0.192) loss 1.3793 (1.2148) acc 78.7500 (81.9762) lr 2.7103e-04 eta 0:00:29
epoch [78/100] batch [5/5] time 0.068 (0.297) data 0.000 (0.228) loss 0.9660 (1.2322) acc 83.3333 (81.8144) lr 2.4989e-04 eta 0:00:32
epoch [79/100] batch [5/5] time 0.057 (0.264) data 0.000 (0.195) loss 1.6095 (1.2170) acc 68.7500 (81.0122) lr 2.2949e-04 eta 0:00:27
epoch [80/100] batch [5/5] time 0.053 (0.341) data 0.000 (0.276) loss 1.0073 (1.2347) acc 85.2941 (82.4100) lr 2.0984e-04 eta 0:00:34
epoch [81/100] batch [5/5] time 0.072 (0.299) data 0.000 (0.230) loss 1.1021 (1.2051) acc 90.9091 (86.0808) lr 1.9098e-04 eta 0:00:28
>>> samples [95/160] noisy rate: 0.00 --> 0.26 --> 0.16 <<<
epoch [82/100] batch [5/5] time 0.054 (0.250) data 0.000 (0.184) loss 1.1030 (1.1995) acc 83.3333 (82.5820) lr 1.7292e-04 eta 0:00:22
epoch [83/100] batch [5/5] time 0.073 (0.245) data 0.000 (0.176) loss 1.3849 (1.2124) acc 76.0870 (83.1732) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.060 (0.269) data 0.000 (0.203) loss 1.0358 (1.1796) acc 79.4118 (83.8929) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.068 (0.284) data 0.000 (0.218) loss 1.1095 (1.2273) acc 83.3333 (81.9027) lr 1.2369e-04 eta 0:00:21
epoch [86/100] batch [5/5] time 0.058 (0.241) data 0.000 (0.182) loss 1.1573 (1.2072) acc 87.5000 (84.1283) lr 1.0899e-04 eta 0:00:16
>>> samples [95/160] noisy rate: 0.00 --> 0.24 --> 0.16 <<<
epoch [87/100] batch [5/5] time 0.056 (0.254) data 0.000 (0.190) loss 1.1130 (1.2153) acc 82.3529 (81.1802) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.055 (0.250) data 0.000 (0.181) loss 1.1159 (1.1717) acc 84.7222 (85.8578) lr 8.2245e-05 eta 0:00:15
epoch [89/100] batch [5/5] time 0.050 (0.255) data 0.000 (0.195) loss 1.1564 (1.2191) acc 87.5000 (83.3095) lr 7.0224e-05 eta 0:00:14
epoch [90/100] batch [5/5] time 0.064 (0.260) data 0.000 (0.197) loss 1.1623 (1.2080) acc 85.0000 (80.9876) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.051 (0.243) data 0.000 (0.176) loss 1.2344 (1.2163) acc 83.3333 (83.8966) lr 4.8943e-05 eta 0:00:10
>>> samples [95/160] noisy rate: 0.00 --> 0.26 --> 0.16 <<<
epoch [92/100] batch [5/5] time 0.065 (0.235) data 0.000 (0.172) loss 1.2051 (1.1884) acc 85.2273 (85.0460) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.059 (0.250) data 0.000 (0.188) loss 1.2471 (1.1811) acc 88.8889 (86.9941) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.057 (0.290) data 0.000 (0.225) loss 1.1829 (1.2071) acc 75.0000 (83.0909) lr 2.4083e-05 eta 0:00:08
epoch [95/100] batch [5/5] time 0.058 (0.253) data 0.000 (0.187) loss 1.3899 (1.2101) acc 83.3333 (84.6018) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.068 (0.252) data 0.000 (0.183) loss 1.3568 (1.1936) acc 75.0000 (83.0455) lr 1.2312e-05 eta 0:00:05
>>> samples [96/160] noisy rate: 0.00 --> 0.26 --> 0.16 <<<
epoch [97/100] batch [5/5] time 0.057 (0.251) data 0.000 (0.187) loss 1.2678 (1.1702) acc 84.7222 (84.9971) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.059 (0.243) data 0.000 (0.178) loss 1.0552 (1.2000) acc 90.7895 (84.4647) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.059 (0.248) data 0.000 (0.181) loss 1.2889 (1.1899) acc 82.3529 (86.1333) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.060 (0.239) data 0.000 (0.173) loss 1.3039 (1.1645) acc 84.7222 (84.8787) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.21, 0.29, 0.32, 0.44, 0.32, 0.26, 0.26, 0.25, 0.2, 0.24, 0.25, 0.24, 0.24, 0.24, 0.26, 0.24, 0.26, 0.24, 0.26, 0.26]
* learned noise rate: [0.0, 0.08, 0.08, 0.08, 0.09, 0.09, 0.1, 0.11, 0.12, 0.13, 0.15, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:20,  1.00s/it]  4%|▎         | 3/81 [00:01<00:25,  3.12it/s]  6%|▌         | 5/81 [00:01<00:14,  5.08it/s]  9%|▊         | 7/81 [00:01<00:11,  6.64it/s] 11%|█         | 9/81 [00:01<00:08,  8.04it/s] 14%|█▎        | 11/81 [00:01<00:07,  9.16it/s] 16%|█▌        | 13/81 [00:01<00:06, 10.03it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.41it/s] 21%|██        | 17/81 [00:02<00:05, 12.10it/s] 23%|██▎       | 19/81 [00:02<00:04, 12.98it/s] 26%|██▌       | 21/81 [00:02<00:04, 13.72it/s] 28%|██▊       | 23/81 [00:02<00:04, 14.04it/s] 31%|███       | 25/81 [00:02<00:03, 14.56it/s] 33%|███▎      | 27/81 [00:02<00:03, 14.93it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.73it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.07it/s] 41%|████      | 33/81 [00:03<00:03, 14.89it/s] 43%|████▎     | 35/81 [00:03<00:03, 15.17it/s] 46%|████▌     | 37/81 [00:03<00:02, 15.38it/s] 48%|████▊     | 39/81 [00:03<00:02, 15.15it/s] 51%|█████     | 41/81 [00:03<00:02, 15.02it/s] 53%|█████▎    | 43/81 [00:03<00:02, 15.28it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.31it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.48it/s] 60%|██████    | 49/81 [00:04<00:02, 15.07it/s] 63%|██████▎   | 51/81 [00:04<00:01, 15.07it/s] 65%|██████▌   | 53/81 [00:04<00:01, 14.83it/s] 68%|██████▊   | 55/81 [00:04<00:01, 14.77it/s] 70%|███████   | 57/81 [00:04<00:01, 14.93it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.71it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.00it/s] 78%|███████▊  | 63/81 [00:05<00:01, 14.88it/s] 80%|████████  | 65/81 [00:05<00:01, 15.18it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.29it/s] 85%|████████▌ | 69/81 [00:05<00:00, 15.49it/s] 88%|████████▊ | 71/81 [00:05<00:00, 15.66it/s] 90%|█████████ | 73/81 [00:05<00:00, 15.79it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.87it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.93it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.97it/s]100%|██████████| 81/81 [00:06<00:00, 16.00it/s]100%|██████████| 81/81 [00:06<00:00, 12.34it/s]
=> result
* total: 8,100
* correct: 5,188
* accuracy: 64.0%
* error: 36.0%
* macro_f1: 60.4%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 604	acc: 67.1%
* class: 1 (Forest)	total: 900	correct: 892	acc: 99.1%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 11	acc: 1.2%
* class: 3 (Highway or Road)	total: 750	correct: 517	acc: 68.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 720	acc: 96.0%
* class: 5 (Pasture Land)	total: 600	correct: 257	acc: 42.8%
* class: 6 (Permanent Crop Land)	total: 750	correct: 604	acc: 80.5%
* class: 7 (Residential Buildings)	total: 900	correct: 763	acc: 84.8%
* class: 8 (River)	total: 750	correct: 435	acc: 58.0%
* class: 9 (Sea or Lake)	total: 900	correct: 385	acc: 42.8%
* average: 64.1%
Elapsed: 0:04:33
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_2-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.618) data 0.000 (0.218) loss 1.1727 (1.1094) acc 7.8125 (15.6250) lr 2.0000e-03 eta 0:05:05
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [59/160] noisy rate: 0.12 --> 0.12 --> 0.03 <<<
epoch [2/100] batch [5/5] time 0.036 (0.397) data 0.000 (0.191) loss 2.6371 (2.1456) acc 15.6250 (27.6635) lr 1.9995e-03 eta 0:03:14
epoch [3/100] batch [5/5] time 0.315 (0.326) data 0.000 (0.179) loss 1.8250 (1.9719) acc 35.4167 (32.0658) lr 1.9980e-03 eta 0:02:38
epoch [4/100] batch [5/5] time 0.038 (0.383) data 0.000 (0.199) loss 1.5915 (1.7353) acc 62.5000 (53.0367) lr 1.9956e-03 eta 0:03:03
epoch [5/100] batch [5/5] time 0.036 (0.246) data 0.000 (0.194) loss 1.8042 (1.6859) acc 32.5000 (49.7381) lr 1.9921e-03 eta 0:01:56
epoch [6/100] batch [5/5] time 0.039 (0.237) data 0.000 (0.189) loss 1.4882 (1.5289) acc 57.5000 (54.3324) lr 1.9877e-03 eta 0:01:51
>>> samples [59/160] noisy rate: 0.12 --> 0.42 --> 0.03 <<<
epoch [7/100] batch [5/5] time 0.038 (0.366) data 0.000 (0.198) loss 1.5185 (1.5131) acc 59.0909 (56.0385) lr 1.9823e-03 eta 0:02:50
epoch [8/100] batch [5/5] time 0.037 (0.256) data 0.000 (0.208) loss 1.3257 (1.3965) acc 67.5000 (61.5065) lr 1.9759e-03 eta 0:01:57
epoch [9/100] batch [5/5] time 0.047 (0.337) data 0.001 (0.236) loss 1.5896 (1.4525) acc 42.3077 (58.5806) lr 1.9686e-03 eta 0:02:33
epoch [10/100] batch [5/5] time 0.042 (0.244) data 0.000 (0.193) loss 1.5373 (1.3984) acc 54.5455 (60.4805) lr 1.9603e-03 eta 0:01:49
epoch [11/100] batch [5/5] time 0.051 (0.274) data 0.000 (0.226) loss 1.1703 (1.3827) acc 71.8750 (64.2106) lr 1.9511e-03 eta 0:02:01
>>> samples [61/160] noisy rate: 0.12 --> 0.21 --> 0.05 <<<
epoch [12/100] batch [5/5] time 0.040 (0.281) data 0.001 (0.230) loss 1.1276 (1.2949) acc 77.0833 (68.2517) lr 1.9409e-03 eta 0:02:03
epoch [13/100] batch [5/5] time 0.032 (0.274) data 0.000 (0.223) loss 1.0572 (1.2264) acc 85.7143 (73.8175) lr 1.9298e-03 eta 0:01:59
epoch [14/100] batch [5/5] time 0.034 (0.324) data 0.000 (0.196) loss 1.3208 (1.2226) acc 75.0000 (73.9733) lr 1.9178e-03 eta 0:02:19
epoch [15/100] batch [5/5] time 0.049 (0.277) data 0.000 (0.228) loss 1.3932 (1.1087) acc 68.3333 (78.2900) lr 1.9048e-03 eta 0:01:57
epoch [16/100] batch [5/5] time 0.044 (0.340) data 0.000 (0.215) loss 1.3910 (1.1582) acc 65.9091 (79.2235) lr 1.8910e-03 eta 0:02:22
>>> samples [63/160] noisy rate: 0.12 --> 0.22 --> 0.08 <<<
epoch [17/100] batch [5/5] time 0.043 (0.257) data 0.000 (0.205) loss 1.1318 (1.0828) acc 71.1538 (79.8697) lr 1.8763e-03 eta 0:01:46
epoch [18/100] batch [5/5] time 0.040 (0.329) data 0.000 (0.202) loss 1.2411 (1.0812) acc 70.4545 (80.9307) lr 1.8607e-03 eta 0:02:14
epoch [19/100] batch [5/5] time 0.043 (0.252) data 0.000 (0.202) loss 0.8801 (1.0968) acc 86.5385 (79.0934) lr 1.8443e-03 eta 0:01:41
epoch [20/100] batch [5/5] time 0.051 (0.262) data 0.000 (0.211) loss 1.2973 (1.0753) acc 82.8125 (84.1231) lr 1.8271e-03 eta 0:01:44
epoch [21/100] batch [5/5] time 0.038 (0.265) data 0.000 (0.213) loss 1.0631 (1.0068) acc 75.0000 (81.0123) lr 1.8090e-03 eta 0:01:44
>>> samples [66/160] noisy rate: 0.12 --> 0.29 --> 0.11 <<<
epoch [22/100] batch [5/5] time 0.035 (0.266) data 0.000 (0.214) loss 1.3538 (1.0855) acc 61.1111 (78.0395) lr 1.7902e-03 eta 0:01:43
epoch [23/100] batch [5/5] time 0.037 (0.300) data 0.000 (0.247) loss 0.7661 (1.0457) acc 88.6364 (80.8225) lr 1.7705e-03 eta 0:01:55
epoch [24/100] batch [5/5] time 0.036 (0.289) data 0.000 (0.238) loss 0.9416 (1.0552) acc 86.1111 (82.6984) lr 1.7501e-03 eta 0:01:49
epoch [25/100] batch [5/5] time 0.041 (0.250) data 0.000 (0.196) loss 1.2504 (1.0753) acc 67.3077 (80.2015) lr 1.7290e-03 eta 0:01:33
epoch [26/100] batch [5/5] time 0.047 (0.273) data 0.000 (0.217) loss 1.1302 (1.0688) acc 75.0000 (82.9808) lr 1.7071e-03 eta 0:01:40
>>> samples [66/160] noisy rate: 0.12 --> 0.28 --> 0.11 <<<
epoch [27/100] batch [5/5] time 0.056 (0.253) data 0.000 (0.200) loss 0.9606 (1.0270) acc 85.2941 (82.3396) lr 1.6845e-03 eta 0:01:32
epoch [28/100] batch [5/5] time 0.039 (0.266) data 0.000 (0.221) loss 1.1964 (1.0368) acc 79.1667 (82.0238) lr 1.6613e-03 eta 0:01:35
epoch [29/100] batch [5/5] time 0.053 (0.255) data 0.000 (0.201) loss 1.1071 (1.0746) acc 83.3333 (78.4874) lr 1.6374e-03 eta 0:01:30
epoch [30/100] batch [5/5] time 0.047 (0.242) data 0.000 (0.191) loss 1.2261 (1.0416) acc 78.8462 (80.6795) lr 1.6129e-03 eta 0:01:24
epoch [31/100] batch [5/5] time 0.042 (0.262) data 0.000 (0.214) loss 0.9372 (0.9722) acc 81.8182 (86.3628) lr 1.5878e-03 eta 0:01:30
>>> samples [77/160] noisy rate: 0.12 --> 0.34 --> 0.16 <<<
epoch [32/100] batch [5/5] time 0.052 (0.250) data 0.000 (0.195) loss 1.1423 (0.9735) acc 77.9412 (84.4777) lr 1.5621e-03 eta 0:01:25
epoch [33/100] batch [5/5] time 0.057 (0.245) data 0.000 (0.189) loss 0.9701 (0.9805) acc 77.9412 (83.1022) lr 1.5358e-03 eta 0:01:22
epoch [34/100] batch [5/5] time 0.049 (0.232) data 0.000 (0.172) loss 1.2824 (0.9686) acc 70.3125 (86.3781) lr 1.5090e-03 eta 0:01:16
epoch [35/100] batch [5/5] time 0.058 (0.235) data 0.000 (0.177) loss 1.0055 (0.9265) acc 93.7500 (86.4568) lr 1.4818e-03 eta 0:01:16
epoch [36/100] batch [5/5] time 0.052 (0.254) data 0.000 (0.202) loss 1.0120 (0.9593) acc 85.0000 (85.4509) lr 1.4540e-03 eta 0:01:21
>>> samples [77/160] noisy rate: 0.12 --> 0.32 --> 0.16 <<<
epoch [37/100] batch [5/5] time 0.043 (0.235) data 0.000 (0.180) loss 1.0790 (0.9851) acc 91.6667 (85.2591) lr 1.4258e-03 eta 0:01:13
epoch [38/100] batch [5/5] time 0.042 (0.216) data 0.000 (0.157) loss 0.7423 (0.9250) acc 93.1818 (87.4219) lr 1.3971e-03 eta 0:01:06
epoch [39/100] batch [5/5] time 0.058 (0.261) data 0.000 (0.204) loss 0.8296 (0.9734) acc 83.7500 (84.1386) lr 1.3681e-03 eta 0:01:19
epoch [40/100] batch [5/5] time 0.045 (0.253) data 0.000 (0.198) loss 0.8147 (0.9465) acc 86.6667 (85.6694) lr 1.3387e-03 eta 0:01:15
epoch [41/100] batch [5/5] time 0.057 (0.241) data 0.000 (0.186) loss 0.9533 (0.9143) acc 83.3333 (85.9524) lr 1.3090e-03 eta 0:01:11
>>> samples [77/160] noisy rate: 0.12 --> 0.34 --> 0.16 <<<
epoch [42/100] batch [5/5] time 0.054 (0.229) data 0.000 (0.173) loss 0.9191 (0.9443) acc 90.2778 (88.4048) lr 1.2790e-03 eta 0:01:06
epoch [43/100] batch [5/5] time 0.057 (0.222) data 0.000 (0.163) loss 1.1080 (0.9586) acc 77.7778 (82.9722) lr 1.2487e-03 eta 0:01:03
epoch [44/100] batch [5/5] time 0.053 (0.238) data 0.000 (0.180) loss 0.8819 (0.9351) acc 83.3333 (83.3635) lr 1.2181e-03 eta 0:01:06
epoch [45/100] batch [5/5] time 0.057 (0.244) data 0.000 (0.191) loss 0.6768 (0.9405) acc 91.6667 (86.2024) lr 1.1874e-03 eta 0:01:07
epoch [46/100] batch [5/5] time 0.048 (0.228) data 0.000 (0.169) loss 1.0022 (0.9331) acc 84.3750 (85.6223) lr 1.1564e-03 eta 0:01:01
>>> samples [77/160] noisy rate: 0.12 --> 0.36 --> 0.16 <<<
epoch [47/100] batch [5/5] time 0.051 (0.254) data 0.000 (0.200) loss 0.7955 (0.9381) acc 95.0000 (86.9715) lr 1.1253e-03 eta 0:01:07
epoch [48/100] batch [5/5] time 0.054 (0.240) data 0.000 (0.183) loss 1.0123 (0.9502) acc 79.6875 (87.3831) lr 1.0941e-03 eta 0:01:02
epoch [49/100] batch [5/5] time 0.057 (0.250) data 0.001 (0.194) loss 1.0008 (0.9379) acc 92.6471 (87.8329) lr 1.0628e-03 eta 0:01:03
epoch [50/100] batch [5/5] time 0.057 (0.267) data 0.000 (0.213) loss 0.9400 (0.8975) acc 90.2778 (87.9079) lr 1.0314e-03 eta 0:01:06
epoch [51/100] batch [5/5] time 0.038 (0.252) data 0.001 (0.195) loss 0.5588 (0.8769) acc 100.0000 (89.3661) lr 1.0000e-03 eta 0:01:01
>>> samples [78/160] noisy rate: 0.12 --> 0.34 --> 0.17 <<<
epoch [52/100] batch [5/5] time 0.054 (0.327) data 0.000 (0.199) loss 0.9238 (0.9288) acc 90.6250 (84.6486) lr 9.6859e-04 eta 0:01:18
epoch [53/100] batch [5/5] time 0.050 (0.280) data 0.000 (0.227) loss 0.9225 (0.9058) acc 84.3750 (87.7159) lr 9.3721e-04 eta 0:01:05
epoch [54/100] batch [5/5] time 0.048 (0.286) data 0.000 (0.232) loss 1.0321 (0.8816) acc 73.2143 (87.9167) lr 9.0589e-04 eta 0:01:05
epoch [55/100] batch [5/5] time 0.053 (0.248) data 0.000 (0.194) loss 1.0950 (0.9269) acc 92.1875 (85.0833) lr 8.7467e-04 eta 0:00:55
epoch [56/100] batch [5/5] time 0.067 (0.261) data 0.000 (0.196) loss 0.8259 (0.9400) acc 84.5238 (88.1674) lr 8.4357e-04 eta 0:00:57
>>> samples [78/160] noisy rate: 0.12 --> 0.34 --> 0.17 <<<
epoch [57/100] batch [5/5] time 0.055 (0.245) data 0.000 (0.185) loss 0.9893 (0.9366) acc 86.7647 (83.6716) lr 8.1262e-04 eta 0:00:52
epoch [58/100] batch [5/5] time 0.063 (0.249) data 0.000 (0.190) loss 0.8368 (0.9211) acc 95.2381 (86.9940) lr 7.8186e-04 eta 0:00:52
epoch [59/100] batch [5/5] time 0.055 (0.246) data 0.000 (0.187) loss 0.9394 (0.9246) acc 82.8947 (84.7029) lr 7.5131e-04 eta 0:00:50
epoch [60/100] batch [5/5] time 0.055 (0.277) data 0.000 (0.222) loss 0.9300 (0.8837) acc 88.2353 (88.2112) lr 7.2101e-04 eta 0:00:55
epoch [61/100] batch [5/5] time 0.050 (0.246) data 0.000 (0.190) loss 0.7357 (0.9035) acc 88.3333 (89.1454) lr 6.9098e-04 eta 0:00:48
>>> samples [78/160] noisy rate: 0.12 --> 0.36 --> 0.17 <<<
epoch [62/100] batch [5/5] time 0.050 (0.216) data 0.000 (0.164) loss 0.8899 (0.9021) acc 93.3333 (87.1978) lr 6.6126e-04 eta 0:00:41
epoch [63/100] batch [5/5] time 0.055 (0.222) data 0.000 (0.167) loss 0.9443 (0.8624) acc 85.2941 (89.5525) lr 6.3188e-04 eta 0:00:41
epoch [64/100] batch [5/5] time 0.043 (0.258) data 0.000 (0.204) loss 0.8412 (0.9042) acc 92.8571 (87.2103) lr 6.0285e-04 eta 0:00:46
epoch [65/100] batch [5/5] time 0.054 (0.260) data 0.000 (0.204) loss 0.6441 (0.8688) acc 96.8750 (89.4721) lr 5.7422e-04 eta 0:00:45
epoch [66/100] batch [5/5] time 0.050 (0.252) data 0.000 (0.196) loss 1.0603 (0.8853) acc 85.0000 (87.8689) lr 5.4601e-04 eta 0:00:42
>>> samples [78/160] noisy rate: 0.12 --> 0.34 --> 0.17 <<<
epoch [67/100] batch [5/5] time 0.058 (0.246) data 0.000 (0.191) loss 1.1186 (0.8915) acc 86.1111 (89.2580) lr 5.1825e-04 eta 0:00:40
epoch [68/100] batch [5/5] time 0.061 (0.253) data 0.001 (0.198) loss 0.8662 (0.8890) acc 89.4737 (88.1300) lr 4.9096e-04 eta 0:00:40
epoch [69/100] batch [5/5] time 0.057 (0.275) data 0.000 (0.216) loss 0.8740 (0.8641) acc 94.1176 (89.7481) lr 4.6417e-04 eta 0:00:42
epoch [70/100] batch [5/5] time 0.048 (0.266) data 0.000 (0.213) loss 0.9428 (0.8840) acc 81.2500 (86.5847) lr 4.3792e-04 eta 0:00:39
epoch [71/100] batch [5/5] time 0.053 (0.265) data 0.000 (0.209) loss 0.8806 (0.8998) acc 93.7500 (88.6271) lr 4.1221e-04 eta 0:00:38
>>> samples [80/160] noisy rate: 0.12 --> 0.33 --> 0.19 <<<
epoch [72/100] batch [5/5] time 0.039 (0.358) data 0.000 (0.218) loss 0.7485 (0.8636) acc 89.5833 (87.3240) lr 3.8709e-04 eta 0:00:50
epoch [73/100] batch [5/5] time 0.042 (0.288) data 0.000 (0.231) loss 0.7402 (0.9244) acc 90.9091 (89.3197) lr 3.6258e-04 eta 0:00:38
epoch [74/100] batch [5/5] time 0.053 (0.261) data 0.000 (0.206) loss 0.8567 (0.8941) acc 91.6667 (90.3743) lr 3.3869e-04 eta 0:00:33
epoch [75/100] batch [5/5] time 0.049 (0.281) data 0.000 (0.225) loss 0.9047 (0.9395) acc 82.8125 (85.3342) lr 3.1545e-04 eta 0:00:35
epoch [76/100] batch [5/5] time 0.049 (0.256) data 0.000 (0.198) loss 0.8691 (0.9077) acc 85.9375 (88.2433) lr 2.9289e-04 eta 0:00:30
>>> samples [80/160] noisy rate: 0.12 --> 0.34 --> 0.19 <<<
epoch [77/100] batch [5/5] time 0.058 (0.295) data 0.000 (0.236) loss 0.7230 (0.9146) acc 91.6667 (86.9249) lr 2.7103e-04 eta 0:00:33
epoch [78/100] batch [5/5] time 0.050 (0.266) data 0.000 (0.207) loss 0.8189 (0.9067) acc 98.3333 (86.8922) lr 2.4989e-04 eta 0:00:29
epoch [79/100] batch [5/5] time 0.044 (0.263) data 0.000 (0.207) loss 1.0919 (0.8994) acc 83.3333 (88.7803) lr 2.2949e-04 eta 0:00:27
epoch [80/100] batch [5/5] time 0.058 (0.253) data 0.000 (0.193) loss 0.9821 (0.8891) acc 78.9474 (87.5040) lr 2.0984e-04 eta 0:00:25
epoch [81/100] batch [5/5] time 0.057 (0.278) data 0.000 (0.216) loss 0.9246 (0.9088) acc 92.1053 (88.4394) lr 1.9098e-04 eta 0:00:26
>>> samples [80/160] noisy rate: 0.12 --> 0.32 --> 0.19 <<<
epoch [82/100] batch [5/5] time 0.053 (0.272) data 0.000 (0.219) loss 0.8559 (0.8903) acc 79.6875 (89.4589) lr 1.7292e-04 eta 0:00:24
epoch [83/100] batch [5/5] time 0.054 (0.231) data 0.000 (0.172) loss 0.7512 (0.8869) acc 90.6250 (90.4277) lr 1.5567e-04 eta 0:00:19
epoch [84/100] batch [5/5] time 0.055 (0.259) data 0.000 (0.203) loss 1.0329 (0.9149) acc 86.7647 (88.7436) lr 1.3926e-04 eta 0:00:20
epoch [85/100] batch [5/5] time 0.044 (0.260) data 0.000 (0.202) loss 0.9209 (0.8876) acc 96.4286 (90.2800) lr 1.2369e-04 eta 0:00:19
epoch [86/100] batch [5/5] time 0.049 (0.251) data 0.000 (0.194) loss 1.1248 (0.9043) acc 83.9286 (86.9584) lr 1.0899e-04 eta 0:00:17
>>> samples [80/160] noisy rate: 0.12 --> 0.29 --> 0.19 <<<
epoch [87/100] batch [5/5] time 0.045 (0.253) data 0.000 (0.194) loss 0.9903 (0.8912) acc 80.0000 (87.5635) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.053 (0.238) data 0.000 (0.179) loss 0.9125 (0.8698) acc 90.0000 (90.1131) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.045 (0.269) data 0.000 (0.215) loss 1.1787 (0.8833) acc 83.3333 (90.0226) lr 7.0224e-05 eta 0:00:14
epoch [90/100] batch [5/5] time 0.056 (0.248) data 0.000 (0.193) loss 1.1226 (0.8924) acc 88.2353 (89.3591) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.061 (0.276) data 0.000 (0.219) loss 0.9453 (0.8807) acc 88.8889 (91.1700) lr 4.8943e-05 eta 0:00:12
>>> samples [81/160] noisy rate: 0.12 --> 0.33 --> 0.20 <<<
epoch [92/100] batch [5/5] time 0.039 (0.289) data 0.000 (0.231) loss 0.7420 (0.8982) acc 85.4167 (88.3009) lr 3.9706e-05 eta 0:00:11
epoch [93/100] batch [5/5] time 0.047 (0.277) data 0.000 (0.217) loss 0.7776 (0.8981) acc 92.8571 (88.5195) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.051 (0.290) data 0.001 (0.238) loss 0.8627 (0.9445) acc 90.0000 (86.3333) lr 2.4083e-05 eta 0:00:08
epoch [95/100] batch [5/5] time 0.048 (0.256) data 0.000 (0.199) loss 0.9222 (0.8895) acc 91.0714 (91.8742) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.062 (0.252) data 0.000 (0.191) loss 0.9730 (0.8960) acc 85.0000 (90.0920) lr 1.2312e-05 eta 0:00:05
>>> samples [82/160] noisy rate: 0.12 --> 0.34 --> 0.21 <<<
epoch [97/100] batch [5/5] time 0.046 (0.267) data 0.000 (0.210) loss 1.0101 (0.8942) acc 86.5385 (89.1307) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.050 (0.221) data 0.000 (0.164) loss 0.8030 (0.9014) acc 93.3333 (88.4286) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.056 (0.243) data 0.000 (0.183) loss 0.9753 (0.9082) acc 79.6875 (88.1937) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.053 (0.256) data 0.000 (0.200) loss 1.0318 (0.8689) acc 85.9375 (91.8438) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.12, 0.42, 0.21, 0.22, 0.29, 0.28, 0.34, 0.32, 0.34, 0.36, 0.34, 0.34, 0.36, 0.34, 0.33, 0.34, 0.32, 0.29, 0.33, 0.34]
* learned noise rate: [0.03, 0.03, 0.05, 0.08, 0.11, 0.11, 0.16, 0.16, 0.16, 0.16, 0.17, 0.17, 0.17, 0.17, 0.19, 0.19, 0.19, 0.19, 0.2, 0.21]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:29,  1.87s/it]  4%|▎         | 3/81 [00:01<00:41,  1.88it/s]  6%|▌         | 5/81 [00:02<00:22,  3.45it/s]  9%|▊         | 7/81 [00:02<00:14,  5.15it/s] 11%|█         | 9/81 [00:02<00:10,  6.90it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.48it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.04it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.39it/s] 21%|██        | 17/81 [00:02<00:05, 12.29it/s] 23%|██▎       | 19/81 [00:03<00:04, 13.22it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.82it/s] 28%|██▊       | 23/81 [00:03<00:04, 14.08it/s] 31%|███       | 25/81 [00:03<00:03, 14.58it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.96it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.84it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.15it/s] 41%|████      | 33/81 [00:03<00:03, 15.36it/s] 43%|████▎     | 35/81 [00:04<00:02, 15.38it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.52it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.64it/s] 51%|█████     | 41/81 [00:04<00:02, 15.68it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.37it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.50it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.27it/s] 60%|██████    | 49/81 [00:04<00:02, 15.44it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.02it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.22it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.29it/s] 70%|███████   | 57/81 [00:05<00:01, 15.26it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.41it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.53it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.27it/s] 80%|████████  | 65/81 [00:06<00:01, 15.46it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.60it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.71it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.81it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.89it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.94it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.98it/s] 98%|█████████▊| 79/81 [00:06<00:00, 16.01it/s]100%|██████████| 81/81 [00:07<00:00, 16.03it/s]100%|██████████| 81/81 [00:07<00:00, 11.25it/s]
=> result
* total: 8,100
* correct: 4,449
* accuracy: 54.9%
* error: 45.1%
* macro_f1: 49.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 342	acc: 38.0%
* class: 1 (Forest)	total: 900	correct: 875	acc: 97.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 17	acc: 1.9%
* class: 3 (Highway or Road)	total: 750	correct: 276	acc: 36.8%
* class: 4 (Industrial Buildings)	total: 750	correct: 630	acc: 84.0%
* class: 5 (Pasture Land)	total: 600	correct: 379	acc: 63.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 664	acc: 88.5%
* class: 7 (Residential Buildings)	total: 900	correct: 847	acc: 94.1%
* class: 8 (River)	total: 750	correct: 416	acc: 55.5%
* class: 9 (Sea or Lake)	total: 900	correct: 3	acc: 0.3%
* average: 56.0%
Elapsed: 0:04:25
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_2-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.084 (0.617) data 0.000 (0.179) loss 1.0761 (1.0985) acc 20.3125 (19.0625) lr 2.0000e-03 eta 0:05:05
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [82/160] noisy rate: 0.12 --> 0.18 --> 0.06 <<<
epoch [2/100] batch [5/5] time 0.398 (0.517) data 0.000 (0.166) loss 2.2100 (2.1133) acc 15.7895 (27.8454) lr 1.9995e-03 eta 0:04:13
epoch [3/100] batch [5/5] time 0.056 (0.341) data 0.000 (0.160) loss 1.8601 (1.9806) acc 35.5263 (32.0279) lr 1.9980e-03 eta 0:02:45
epoch [4/100] batch [5/5] time 0.047 (0.308) data 0.000 (0.180) loss 2.0361 (1.9186) acc 31.6667 (35.6004) lr 1.9956e-03 eta 0:02:27
epoch [5/100] batch [5/5] time 0.057 (0.268) data 0.000 (0.204) loss 2.0857 (1.9316) acc 21.8750 (35.2200) lr 1.9921e-03 eta 0:02:07
epoch [6/100] batch [5/5] time 0.059 (0.229) data 0.000 (0.166) loss 2.0326 (1.8944) acc 38.8889 (40.6710) lr 1.9877e-03 eta 0:01:47
>>> samples [88/160] noisy rate: 0.12 --> 0.29 --> 0.12 <<<
epoch [7/100] batch [5/5] time 0.053 (0.367) data 0.000 (0.219) loss 1.8533 (1.8189) acc 67.1875 (50.5381) lr 1.9823e-03 eta 0:02:50
epoch [8/100] batch [5/5] time 0.061 (0.255) data 0.000 (0.192) loss 1.6185 (1.7281) acc 57.5000 (51.2547) lr 1.9759e-03 eta 0:01:57
epoch [9/100] batch [5/5] time 0.058 (0.247) data 0.000 (0.188) loss 1.7819 (1.8181) acc 62.5000 (49.9510) lr 1.9686e-03 eta 0:01:52
epoch [10/100] batch [5/5] time 0.064 (0.265) data 0.000 (0.202) loss 1.8563 (1.8554) acc 57.1429 (50.2341) lr 1.9603e-03 eta 0:01:59
epoch [11/100] batch [5/5] time 0.061 (0.260) data 0.000 (0.195) loss 1.9674 (1.7456) acc 48.6842 (54.8991) lr 1.9511e-03 eta 0:01:55
>>> samples [98/160] noisy rate: 0.12 --> 0.22 --> 0.17 <<<
epoch [12/100] batch [5/5] time 0.057 (0.246) data 0.000 (0.178) loss 1.6719 (1.6986) acc 68.7500 (56.9013) lr 1.9409e-03 eta 0:01:48
epoch [13/100] batch [5/5] time 0.067 (0.255) data 0.000 (0.187) loss 1.8512 (1.6785) acc 51.1905 (53.0013) lr 1.9298e-03 eta 0:01:50
epoch [14/100] batch [5/5] time 0.058 (0.321) data 0.000 (0.173) loss 1.6185 (1.6447) acc 72.2222 (58.4277) lr 1.9178e-03 eta 0:02:18
epoch [15/100] batch [5/5] time 0.043 (0.384) data 0.000 (0.242) loss 1.5593 (1.5957) acc 76.9231 (68.5904) lr 1.9048e-03 eta 0:02:43
epoch [16/100] batch [5/5] time 0.067 (0.291) data 0.000 (0.227) loss 1.7472 (1.5885) acc 50.0000 (64.1784) lr 1.8910e-03 eta 0:02:02
>>> samples [98/160] noisy rate: 0.12 --> 0.24 --> 0.17 <<<
epoch [17/100] batch [5/5] time 0.058 (0.274) data 0.000 (0.211) loss 1.4691 (1.5786) acc 70.8333 (66.9394) lr 1.8763e-03 eta 0:01:53
epoch [18/100] batch [5/5] time 0.062 (0.283) data 0.000 (0.215) loss 1.6203 (1.5492) acc 73.8095 (66.6667) lr 1.8607e-03 eta 0:01:55
epoch [19/100] batch [5/5] time 0.061 (0.316) data 0.000 (0.250) loss 1.3727 (1.5172) acc 77.6316 (70.7030) lr 1.8443e-03 eta 0:02:07
epoch [20/100] batch [5/5] time 0.063 (0.316) data 0.000 (0.251) loss 1.4754 (1.5036) acc 68.7500 (73.2876) lr 1.8271e-03 eta 0:02:06
epoch [21/100] batch [5/5] time 0.053 (0.305) data 0.000 (0.240) loss 1.3791 (1.4937) acc 68.0555 (70.7517) lr 1.8090e-03 eta 0:02:00
>>> samples [98/160] noisy rate: 0.12 --> 0.18 --> 0.17 <<<
epoch [22/100] batch [5/5] time 0.060 (0.373) data 0.000 (0.225) loss 1.5471 (1.4924) acc 73.6111 (72.9626) lr 1.7902e-03 eta 0:02:25
epoch [23/100] batch [5/5] time 0.062 (0.269) data 0.000 (0.202) loss 1.4640 (1.4716) acc 72.5000 (76.2843) lr 1.7705e-03 eta 0:01:43
epoch [24/100] batch [5/5] time 0.064 (0.369) data 0.000 (0.213) loss 1.5682 (1.4991) acc 80.5555 (73.2728) lr 1.7501e-03 eta 0:02:20
epoch [25/100] batch [5/5] time 0.062 (0.236) data 0.000 (0.171) loss 1.6772 (1.4732) acc 78.7500 (75.3591) lr 1.7290e-03 eta 0:01:28
epoch [26/100] batch [5/5] time 0.067 (0.253) data 0.000 (0.183) loss 1.5417 (1.4670) acc 72.6190 (76.2823) lr 1.7071e-03 eta 0:01:33
>>> samples [100/160] noisy rate: 0.12 --> 0.15 --> 0.18 <<<
epoch [27/100] batch [5/5] time 0.073 (0.362) data 0.000 (0.295) loss 1.4740 (1.4565) acc 75.0000 (74.8839) lr 1.6845e-03 eta 0:02:12
epoch [28/100] batch [5/5] time 0.066 (0.284) data 0.000 (0.214) loss 1.4115 (1.4694) acc 80.9524 (75.1675) lr 1.6613e-03 eta 0:01:42
epoch [29/100] batch [5/5] time 0.057 (0.287) data 0.000 (0.223) loss 1.3022 (1.4585) acc 80.8824 (74.4352) lr 1.6374e-03 eta 0:01:41
epoch [30/100] batch [5/5] time 0.070 (0.265) data 0.000 (0.193) loss 1.4298 (1.4181) acc 81.2500 (80.7561) lr 1.6129e-03 eta 0:01:32
epoch [31/100] batch [5/5] time 0.074 (0.250) data 0.000 (0.179) loss 1.6757 (1.4666) acc 67.8571 (74.8901) lr 1.5878e-03 eta 0:01:26
>>> samples [100/160] noisy rate: 0.12 --> 0.09 --> 0.18 <<<
epoch [32/100] batch [5/5] time 0.075 (0.308) data 0.000 (0.242) loss 1.3616 (1.4310) acc 67.7083 (77.1779) lr 1.5621e-03 eta 0:01:44
epoch [33/100] batch [5/5] time 0.064 (0.349) data 0.000 (0.190) loss 1.4127 (1.4450) acc 71.2500 (73.5292) lr 1.5358e-03 eta 0:01:56
epoch [34/100] batch [5/5] time 0.059 (0.252) data 0.000 (0.180) loss 1.4035 (1.4035) acc 81.2500 (78.2045) lr 1.5090e-03 eta 0:01:23
epoch [35/100] batch [5/5] time 0.061 (0.243) data 0.000 (0.180) loss 1.4622 (1.4165) acc 73.8095 (79.6508) lr 1.4818e-03 eta 0:01:18
epoch [36/100] batch [5/5] time 0.056 (0.246) data 0.000 (0.175) loss 1.4156 (1.4101) acc 79.6875 (79.8264) lr 1.4540e-03 eta 0:01:18
>>> samples [100/160] noisy rate: 0.12 --> 0.15 --> 0.18 <<<
epoch [37/100] batch [5/5] time 0.069 (0.262) data 0.000 (0.191) loss 1.6446 (1.3918) acc 71.4286 (79.1872) lr 1.4258e-03 eta 0:01:22
epoch [38/100] batch [5/5] time 0.062 (0.234) data 0.000 (0.168) loss 1.5028 (1.4042) acc 63.7500 (78.5317) lr 1.3971e-03 eta 0:01:12
epoch [39/100] batch [5/5] time 0.076 (0.300) data 0.000 (0.234) loss 1.5743 (1.4398) acc 61.4583 (73.7290) lr 1.3681e-03 eta 0:01:31
epoch [40/100] batch [5/5] time 0.531 (0.336) data 0.000 (0.178) loss 1.4587 (1.3988) acc 73.2759 (82.6380) lr 1.3387e-03 eta 0:01:40
epoch [41/100] batch [5/5] time 0.062 (0.265) data 0.000 (0.196) loss 1.0770 (1.3785) acc 86.8421 (83.0973) lr 1.3090e-03 eta 0:01:18
>>> samples [100/160] noisy rate: 0.12 --> 0.15 --> 0.18 <<<
epoch [42/100] batch [5/5] time 0.057 (0.252) data 0.000 (0.189) loss 1.5742 (1.4013) acc 78.9474 (79.4107) lr 1.2790e-03 eta 0:01:12
epoch [43/100] batch [5/5] time 0.065 (0.242) data 0.000 (0.179) loss 1.3627 (1.3955) acc 76.0870 (79.3640) lr 1.2487e-03 eta 0:01:09
epoch [44/100] batch [5/5] time 0.060 (0.226) data 0.000 (0.162) loss 1.2072 (1.3858) acc 86.1111 (80.3483) lr 1.2181e-03 eta 0:01:03
epoch [45/100] batch [5/5] time 0.056 (0.292) data 0.000 (0.225) loss 1.4042 (1.3793) acc 88.2353 (81.6524) lr 1.1874e-03 eta 0:01:20
epoch [46/100] batch [5/5] time 0.059 (0.239) data 0.000 (0.175) loss 1.0128 (1.3418) acc 84.2105 (81.5631) lr 1.1564e-03 eta 0:01:04
>>> samples [100/160] noisy rate: 0.12 --> 0.13 --> 0.18 <<<
epoch [47/100] batch [5/5] time 0.047 (0.244) data 0.000 (0.178) loss 1.2867 (1.3649) acc 76.6667 (82.0990) lr 1.1253e-03 eta 0:01:04
epoch [48/100] batch [5/5] time 0.054 (0.234) data 0.000 (0.170) loss 1.4547 (1.3916) acc 77.7778 (82.2917) lr 1.0941e-03 eta 0:01:00
epoch [49/100] batch [5/5] time 0.061 (0.256) data 0.000 (0.189) loss 1.4096 (1.3841) acc 72.3684 (83.1458) lr 1.0628e-03 eta 0:01:05
epoch [50/100] batch [5/5] time 0.065 (0.242) data 0.000 (0.175) loss 1.3651 (1.3506) acc 78.7500 (80.6298) lr 1.0314e-03 eta 0:01:00
epoch [51/100] batch [5/5] time 0.051 (0.269) data 0.000 (0.200) loss 1.3437 (1.3537) acc 85.7143 (80.9664) lr 1.0000e-03 eta 0:01:05
>>> samples [100/160] noisy rate: 0.12 --> 0.17 --> 0.18 <<<
epoch [52/100] batch [5/5] time 0.063 (0.239) data 0.000 (0.171) loss 1.4750 (1.3562) acc 75.0000 (80.0564) lr 9.6859e-04 eta 0:00:57
epoch [53/100] batch [5/5] time 0.070 (0.233) data 0.000 (0.168) loss 1.1373 (1.3950) acc 92.7083 (83.1723) lr 9.3721e-04 eta 0:00:54
epoch [54/100] batch [5/5] time 0.075 (0.270) data 0.000 (0.202) loss 1.3675 (1.3635) acc 79.1667 (79.9151) lr 9.0589e-04 eta 0:01:01
epoch [55/100] batch [5/5] time 0.065 (0.268) data 0.000 (0.202) loss 1.3678 (1.3791) acc 82.8947 (79.6743) lr 8.7467e-04 eta 0:01:00
epoch [56/100] batch [5/5] time 0.059 (0.259) data 0.001 (0.188) loss 1.2822 (1.3245) acc 83.7500 (84.9846) lr 8.4357e-04 eta 0:00:56
>>> samples [103/160] noisy rate: 0.12 --> 0.16 --> 0.17 <<<
epoch [57/100] batch [5/5] time 0.058 (0.229) data 0.000 (0.161) loss 1.6248 (1.3519) acc 81.2500 (80.0424) lr 8.1262e-04 eta 0:00:49
epoch [58/100] batch [5/5] time 0.067 (0.227) data 0.000 (0.160) loss 1.4010 (1.3194) acc 86.3636 (83.1351) lr 7.8186e-04 eta 0:00:47
epoch [59/100] batch [5/5] time 0.054 (0.234) data 0.000 (0.169) loss 1.5059 (1.3252) acc 83.3333 (83.8592) lr 7.5131e-04 eta 0:00:48
epoch [60/100] batch [5/5] time 0.062 (0.272) data 0.000 (0.201) loss 1.1712 (1.3271) acc 90.7895 (83.9555) lr 7.2101e-04 eta 0:00:54
epoch [61/100] batch [5/5] time 0.076 (0.244) data 0.000 (0.177) loss 1.2869 (1.3156) acc 71.1538 (83.4248) lr 6.9098e-04 eta 0:00:47
>>> samples [103/160] noisy rate: 0.12 --> 0.19 --> 0.17 <<<
epoch [62/100] batch [5/5] time 0.057 (0.268) data 0.000 (0.202) loss 1.3245 (1.2962) acc 75.0000 (83.0045) lr 6.6126e-04 eta 0:00:50
epoch [63/100] batch [5/5] time 0.041 (0.303) data 0.000 (0.234) loss 1.3058 (1.3133) acc 78.8462 (81.5418) lr 6.3188e-04 eta 0:00:55
epoch [64/100] batch [5/5] time 0.071 (0.303) data 0.000 (0.239) loss 1.4208 (1.3134) acc 82.9545 (81.3339) lr 6.0285e-04 eta 0:00:54
epoch [65/100] batch [5/5] time 0.060 (0.244) data 0.001 (0.178) loss 1.3926 (1.3221) acc 73.6842 (79.6064) lr 5.7422e-04 eta 0:00:42
epoch [66/100] batch [5/5] time 0.059 (0.237) data 0.000 (0.167) loss 1.4361 (1.3309) acc 76.3889 (81.9485) lr 5.4601e-04 eta 0:00:40
>>> samples [103/160] noisy rate: 0.12 --> 0.20 --> 0.17 <<<
epoch [67/100] batch [5/5] time 0.058 (0.277) data 0.000 (0.211) loss 1.2707 (1.3294) acc 84.2105 (80.7775) lr 5.1825e-04 eta 0:00:45
epoch [68/100] batch [5/5] time 0.064 (0.265) data 0.000 (0.199) loss 1.3162 (1.3155) acc 77.2727 (82.7587) lr 4.9096e-04 eta 0:00:42
epoch [69/100] batch [5/5] time 0.066 (0.260) data 0.000 (0.192) loss 1.1860 (1.3122) acc 84.7826 (82.5801) lr 4.6417e-04 eta 0:00:40
epoch [70/100] batch [5/5] time 0.071 (0.247) data 0.000 (0.180) loss 1.4266 (1.3222) acc 83.3333 (85.2193) lr 4.3792e-04 eta 0:00:37
epoch [71/100] batch [5/5] time 0.070 (0.263) data 0.000 (0.194) loss 1.0815 (1.3067) acc 87.5000 (85.7877) lr 4.1221e-04 eta 0:00:38
>>> samples [103/160] noisy rate: 0.12 --> 0.20 --> 0.17 <<<
epoch [72/100] batch [5/5] time 0.067 (0.287) data 0.000 (0.218) loss 1.3678 (1.3142) acc 75.0000 (82.4812) lr 3.8709e-04 eta 0:00:40
epoch [73/100] batch [5/5] time 0.059 (0.241) data 0.000 (0.170) loss 1.1617 (1.3064) acc 91.1765 (82.5121) lr 3.6258e-04 eta 0:00:32
epoch [74/100] batch [5/5] time 0.071 (0.283) data 0.000 (0.213) loss 1.3066 (1.3202) acc 78.2609 (84.3886) lr 3.3869e-04 eta 0:00:36
epoch [75/100] batch [5/5] time 0.056 (0.281) data 0.000 (0.213) loss 1.2571 (1.3137) acc 78.9474 (82.4051) lr 3.1545e-04 eta 0:00:35
epoch [76/100] batch [5/5] time 0.045 (0.278) data 0.000 (0.209) loss 1.2787 (1.2858) acc 86.5385 (84.3415) lr 2.9289e-04 eta 0:00:33
>>> samples [103/160] noisy rate: 0.12 --> 0.15 --> 0.17 <<<
epoch [77/100] batch [5/5] time 0.067 (0.241) data 0.000 (0.177) loss 1.2687 (1.3077) acc 86.4583 (83.6253) lr 2.7103e-04 eta 0:00:27
epoch [78/100] batch [5/5] time 0.054 (0.236) data 0.000 (0.167) loss 1.4709 (1.3073) acc 69.4444 (83.7316) lr 2.4989e-04 eta 0:00:26
epoch [79/100] batch [5/5] time 0.067 (0.246) data 0.000 (0.176) loss 1.2082 (1.3361) acc 92.0455 (82.2667) lr 2.2949e-04 eta 0:00:25
epoch [80/100] batch [5/5] time 0.073 (0.258) data 0.000 (0.187) loss 1.4111 (1.2850) acc 78.4091 (83.8939) lr 2.0984e-04 eta 0:00:25
epoch [81/100] batch [5/5] time 0.077 (0.263) data 0.000 (0.195) loss 1.1186 (1.2972) acc 86.5385 (86.5097) lr 1.9098e-04 eta 0:00:24
>>> samples [103/160] noisy rate: 0.12 --> 0.17 --> 0.17 <<<
epoch [82/100] batch [5/5] time 0.062 (0.255) data 0.000 (0.188) loss 1.4332 (1.2746) acc 78.9474 (83.9079) lr 1.7292e-04 eta 0:00:22
epoch [83/100] batch [5/5] time 0.076 (0.270) data 0.000 (0.200) loss 1.2979 (1.3087) acc 85.4167 (82.4336) lr 1.5567e-04 eta 0:00:22
epoch [84/100] batch [5/5] time 0.065 (0.287) data 0.000 (0.222) loss 1.1215 (1.3147) acc 87.5000 (82.8638) lr 1.3926e-04 eta 0:00:22
epoch [85/100] batch [5/5] time 0.058 (0.270) data 0.000 (0.201) loss 1.4270 (1.2957) acc 73.7500 (84.3160) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.072 (0.237) data 0.000 (0.171) loss 1.3200 (1.3162) acc 81.2500 (82.5873) lr 1.0899e-04 eta 0:00:16
>>> samples [103/160] noisy rate: 0.12 --> 0.14 --> 0.17 <<<
epoch [87/100] batch [5/5] time 0.058 (0.252) data 0.000 (0.187) loss 0.9182 (1.2750) acc 91.2500 (85.9703) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.075 (0.243) data 0.000 (0.175) loss 1.3953 (1.2990) acc 81.2500 (84.6510) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.068 (0.252) data 0.000 (0.182) loss 1.4595 (1.2951) acc 87.5000 (86.7731) lr 7.0224e-05 eta 0:00:13
epoch [90/100] batch [5/5] time 0.070 (0.248) data 0.000 (0.178) loss 1.2917 (1.3309) acc 80.4348 (83.6037) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.068 (0.233) data 0.000 (0.164) loss 1.2755 (1.3054) acc 84.5238 (83.9336) lr 4.8943e-05 eta 0:00:10
>>> samples [103/160] noisy rate: 0.12 --> 0.16 --> 0.17 <<<
epoch [92/100] batch [5/5] time 0.070 (0.246) data 0.000 (0.182) loss 1.2265 (1.2890) acc 87.5000 (84.0618) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.068 (0.277) data 0.000 (0.208) loss 1.1195 (1.3368) acc 87.5000 (82.5662) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.064 (0.240) data 0.001 (0.173) loss 1.2658 (1.2911) acc 85.0000 (85.4167) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.053 (0.238) data 0.000 (0.171) loss 1.2638 (1.2844) acc 81.9444 (84.9161) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.060 (0.238) data 0.000 (0.171) loss 1.4959 (1.2897) acc 80.2632 (84.2544) lr 1.2312e-05 eta 0:00:04
>>> samples [103/160] noisy rate: 0.12 --> 0.17 --> 0.17 <<<
epoch [97/100] batch [5/5] time 0.053 (0.298) data 0.000 (0.227) loss 1.2725 (1.3190) acc 86.7647 (81.5972) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.065 (0.300) data 0.000 (0.234) loss 1.4992 (1.3123) acc 78.4091 (82.3167) lr 4.4380e-06 eta 0:00:03
epoch [99/100] batch [5/5] time 0.058 (0.267) data 0.000 (0.200) loss 1.2056 (1.2890) acc 79.1667 (82.6339) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.065 (0.253) data 0.000 (0.187) loss 1.3851 (1.2870) acc 90.0000 (84.4916) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.18, 0.29, 0.22, 0.24, 0.18, 0.15, 0.09, 0.15, 0.15, 0.13, 0.17, 0.16, 0.19, 0.2, 0.2, 0.15, 0.17, 0.14, 0.16, 0.17]
* learned noise rate: [0.06, 0.12, 0.17, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:27,  1.09s/it]  2%|▏         | 2/81 [00:01<00:40,  1.96it/s]  5%|▍         | 4/81 [00:01<00:18,  4.17it/s]  7%|▋         | 6/81 [00:01<00:12,  6.03it/s] 10%|▉         | 8/81 [00:01<00:09,  7.61it/s] 12%|█▏        | 10/81 [00:01<00:08,  8.79it/s] 15%|█▍        | 12/81 [00:02<00:07,  9.70it/s] 17%|█▋        | 14/81 [00:02<00:06, 11.14it/s] 20%|█▉        | 16/81 [00:02<00:05, 12.33it/s] 22%|██▏       | 18/81 [00:02<00:04, 13.28it/s] 25%|██▍       | 20/81 [00:02<00:04, 14.01it/s] 27%|██▋       | 22/81 [00:02<00:04, 14.40it/s] 30%|██▉       | 24/81 [00:02<00:03, 14.85it/s] 32%|███▏      | 26/81 [00:02<00:03, 15.17it/s] 35%|███▍      | 28/81 [00:03<00:03, 15.39it/s] 37%|███▋      | 30/81 [00:03<00:03, 15.54it/s] 40%|███▉      | 32/81 [00:03<00:03, 15.42it/s] 42%|████▏     | 34/81 [00:03<00:03, 15.57it/s] 44%|████▍     | 36/81 [00:03<00:02, 15.34it/s] 47%|████▋     | 38/81 [00:03<00:02, 15.51it/s] 49%|████▉     | 40/81 [00:03<00:02, 14.69it/s] 52%|█████▏    | 42/81 [00:03<00:02, 14.78it/s] 54%|█████▍    | 44/81 [00:04<00:02, 15.11it/s] 57%|█████▋    | 46/81 [00:04<00:02, 15.20it/s] 59%|█████▉    | 48/81 [00:04<00:02, 14.74it/s] 62%|██████▏   | 50/81 [00:04<00:02, 15.08it/s] 64%|██████▍   | 52/81 [00:04<00:01, 15.04it/s] 67%|██████▋   | 54/81 [00:04<00:01, 15.30it/s] 69%|██████▉   | 56/81 [00:04<00:01, 15.40it/s] 72%|███████▏  | 58/81 [00:04<00:01, 15.53it/s] 74%|███████▍  | 60/81 [00:05<00:01, 15.66it/s] 77%|███████▋  | 62/81 [00:05<00:01, 15.39it/s] 79%|███████▉  | 64/81 [00:05<00:01, 15.55it/s] 81%|████████▏ | 66/81 [00:05<00:00, 15.50it/s] 84%|████████▍ | 68/81 [00:05<00:00, 15.61it/s] 86%|████████▋ | 70/81 [00:05<00:00, 15.70it/s] 89%|████████▉ | 72/81 [00:05<00:00, 15.80it/s] 91%|█████████▏| 74/81 [00:06<00:00, 15.88it/s] 94%|█████████▍| 76/81 [00:06<00:00, 15.93it/s] 96%|█████████▋| 78/81 [00:06<00:00, 15.95it/s] 99%|█████████▉| 80/81 [00:06<00:00, 15.97it/s]100%|██████████| 81/81 [00:06<00:00, 12.30it/s]
=> result
* total: 8,100
* correct: 5,002
* accuracy: 61.8%
* error: 38.2%
* macro_f1: 59.4%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 445	acc: 49.4%
* class: 1 (Forest)	total: 900	correct: 876	acc: 97.3%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 105	acc: 11.7%
* class: 3 (Highway or Road)	total: 750	correct: 367	acc: 48.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 741	acc: 98.8%
* class: 5 (Pasture Land)	total: 600	correct: 410	acc: 68.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 641	acc: 85.5%
* class: 7 (Residential Buildings)	total: 900	correct: 670	acc: 74.4%
* class: 8 (River)	total: 750	correct: 269	acc: 35.9%
* class: 9 (Sea or Lake)	total: 900	correct: 478	acc: 53.1%
* average: 62.3%
Elapsed: 0:04:24
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_2-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.639) data 0.000 (0.189) loss 1.1733 (1.1445) acc 17.9688 (15.4688) lr 2.0000e-03 eta 0:05:16
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [68/160] noisy rate: 0.12 --> 0.27 --> 0.01 <<<
epoch [2/100] batch [5/5] time 0.052 (0.425) data 0.000 (0.177) loss 2.2872 (2.1913) acc 6.8182 (19.1486) lr 1.9995e-03 eta 0:03:28
epoch [3/100] batch [5/5] time 0.301 (0.420) data 0.000 (0.182) loss 1.9051 (2.0288) acc 35.0000 (27.7272) lr 1.9980e-03 eta 0:03:23
epoch [4/100] batch [5/5] time 0.331 (0.466) data 0.000 (0.180) loss 1.9734 (1.9883) acc 35.4167 (35.5972) lr 1.9956e-03 eta 0:03:43
epoch [5/100] batch [5/5] time 0.038 (0.214) data 0.000 (0.160) loss 2.1088 (1.9040) acc 47.5000 (49.2454) lr 1.9921e-03 eta 0:01:41
epoch [6/100] batch [5/5] time 0.043 (0.230) data 0.000 (0.179) loss 1.9468 (1.8110) acc 52.2727 (53.1661) lr 1.9877e-03 eta 0:01:48
>>> samples [71/160] noisy rate: 0.12 --> 0.26 --> 0.03 <<<
epoch [7/100] batch [5/5] time 0.051 (0.204) data 0.000 (0.147) loss 1.7256 (1.8298) acc 46.4286 (44.2857) lr 1.9823e-03 eta 0:01:34
epoch [8/100] batch [5/5] time 0.051 (0.236) data 0.000 (0.177) loss 1.5991 (1.7483) acc 73.3333 (59.6989) lr 1.9759e-03 eta 0:01:48
epoch [9/100] batch [5/5] time 0.049 (0.237) data 0.000 (0.181) loss 1.7840 (1.6744) acc 53.8462 (57.9231) lr 1.9686e-03 eta 0:01:47
epoch [10/100] batch [5/5] time 0.048 (0.235) data 0.000 (0.182) loss 1.4643 (1.6214) acc 50.0000 (63.0913) lr 1.9603e-03 eta 0:01:45
epoch [11/100] batch [5/5] time 0.042 (0.219) data 0.000 (0.164) loss 1.5325 (1.5943) acc 68.1818 (63.7188) lr 1.9511e-03 eta 0:01:37
>>> samples [82/160] noisy rate: 0.12 --> 0.22 --> 0.06 <<<
epoch [12/100] batch [5/5] time 0.430 (0.271) data 0.000 (0.143) loss 1.6403 (1.6320) acc 65.2174 (63.8597) lr 1.9409e-03 eta 0:01:59
epoch [13/100] batch [5/5] time 0.427 (0.352) data 0.000 (0.140) loss 1.4781 (1.5501) acc 80.0000 (64.7061) lr 1.9298e-03 eta 0:02:33
epoch [14/100] batch [5/5] time 0.058 (0.225) data 0.000 (0.165) loss 1.4635 (1.5293) acc 81.2500 (68.8416) lr 1.9178e-03 eta 0:01:36
epoch [15/100] batch [5/5] time 0.051 (0.246) data 0.000 (0.189) loss 1.7820 (1.5276) acc 55.3571 (68.7227) lr 1.9048e-03 eta 0:01:44
epoch [16/100] batch [5/5] time 0.048 (0.238) data 0.000 (0.177) loss 1.4935 (1.5201) acc 77.0833 (72.3056) lr 1.8910e-03 eta 0:01:40
>>> samples [84/160] noisy rate: 0.12 --> 0.17 --> 0.07 <<<
epoch [17/100] batch [5/5] time 0.060 (0.253) data 0.000 (0.192) loss 1.3386 (1.4762) acc 76.3889 (70.8799) lr 1.8763e-03 eta 0:01:45
epoch [18/100] batch [5/5] time 0.059 (0.320) data 0.000 (0.180) loss 1.3504 (1.4590) acc 70.5882 (70.9567) lr 1.8607e-03 eta 0:02:10
epoch [19/100] batch [5/5] time 0.053 (0.236) data 0.000 (0.176) loss 1.3512 (1.4195) acc 78.1250 (73.9797) lr 1.8443e-03 eta 0:01:35
epoch [20/100] batch [5/5] time 0.058 (0.308) data 0.001 (0.167) loss 1.3058 (1.5026) acc 75.0000 (67.4603) lr 1.8271e-03 eta 0:02:03
epoch [21/100] batch [5/5] time 0.046 (0.228) data 0.000 (0.170) loss 1.5340 (1.4393) acc 75.0000 (70.7491) lr 1.8090e-03 eta 0:01:29
>>> samples [85/160] noisy rate: 0.12 --> 0.19 --> 0.08 <<<
epoch [22/100] batch [5/5] time 0.059 (0.230) data 0.000 (0.165) loss 1.3692 (1.3904) acc 72.5000 (74.4375) lr 1.7902e-03 eta 0:01:29
epoch [23/100] batch [5/5] time 0.040 (0.325) data 0.000 (0.178) loss 1.6181 (1.4687) acc 81.2500 (72.6006) lr 1.7705e-03 eta 0:02:05
epoch [24/100] batch [5/5] time 0.067 (0.228) data 0.000 (0.168) loss 1.2055 (1.4096) acc 73.8095 (74.3651) lr 1.7501e-03 eta 0:01:26
epoch [25/100] batch [5/5] time 0.049 (0.234) data 0.000 (0.173) loss 1.7342 (1.4405) acc 75.0000 (76.0000) lr 1.7290e-03 eta 0:01:27
epoch [26/100] batch [5/5] time 0.049 (0.240) data 0.000 (0.181) loss 1.3504 (1.3878) acc 75.0000 (75.9069) lr 1.7071e-03 eta 0:01:28
>>> samples [86/160] noisy rate: 0.12 --> 0.22 --> 0.08 <<<
epoch [27/100] batch [5/5] time 0.053 (0.225) data 0.000 (0.166) loss 1.3457 (1.3833) acc 86.1111 (77.6702) lr 1.6845e-03 eta 0:01:22
epoch [28/100] batch [5/5] time 0.052 (0.237) data 0.000 (0.177) loss 1.1288 (1.3445) acc 84.3750 (80.7181) lr 1.6613e-03 eta 0:01:25
epoch [29/100] batch [5/5] time 0.051 (0.236) data 0.000 (0.179) loss 1.0424 (1.3807) acc 91.1765 (79.3454) lr 1.6374e-03 eta 0:01:23
epoch [30/100] batch [5/5] time 0.046 (0.238) data 0.000 (0.176) loss 1.3944 (1.3784) acc 80.0000 (78.7839) lr 1.6129e-03 eta 0:01:23
epoch [31/100] batch [5/5] time 0.040 (0.227) data 0.000 (0.168) loss 1.3291 (1.3405) acc 77.0833 (79.0049) lr 1.5878e-03 eta 0:01:18
>>> samples [87/160] noisy rate: 0.12 --> 0.23 --> 0.09 <<<
epoch [32/100] batch [5/5] time 0.071 (0.230) data 0.000 (0.169) loss 1.6546 (1.3898) acc 73.9130 (77.8784) lr 1.5621e-03 eta 0:01:18
epoch [33/100] batch [5/5] time 0.044 (0.319) data 0.000 (0.174) loss 1.1989 (1.3182) acc 75.0000 (75.9375) lr 1.5358e-03 eta 0:01:46
epoch [34/100] batch [5/5] time 0.044 (0.223) data 0.000 (0.164) loss 1.2443 (1.3666) acc 89.2857 (80.0280) lr 1.5090e-03 eta 0:01:13
epoch [35/100] batch [5/5] time 0.059 (0.231) data 0.000 (0.169) loss 1.6837 (1.3736) acc 76.3889 (78.8560) lr 1.4818e-03 eta 0:01:14
epoch [36/100] batch [5/5] time 0.053 (0.239) data 0.001 (0.176) loss 1.2382 (1.3674) acc 69.4444 (75.6693) lr 1.4540e-03 eta 0:01:16
>>> samples [88/160] noisy rate: 0.12 --> 0.27 --> 0.10 <<<
epoch [37/100] batch [5/5] time 0.062 (0.230) data 0.000 (0.167) loss 1.3147 (1.3455) acc 76.3158 (79.1838) lr 1.4258e-03 eta 0:01:12
epoch [38/100] batch [5/5] time 0.051 (0.241) data 0.000 (0.184) loss 1.1886 (1.3265) acc 91.1765 (78.2908) lr 1.3971e-03 eta 0:01:14
epoch [39/100] batch [5/5] time 0.062 (0.228) data 0.000 (0.168) loss 1.1943 (1.3538) acc 80.2632 (81.5712) lr 1.3681e-03 eta 0:01:09
epoch [40/100] batch [5/5] time 0.061 (0.230) data 0.000 (0.167) loss 1.3333 (1.3339) acc 79.1667 (79.5139) lr 1.3387e-03 eta 0:01:09
epoch [41/100] batch [5/5] time 0.051 (0.236) data 0.000 (0.177) loss 1.0981 (1.3351) acc 85.0000 (78.9031) lr 1.3090e-03 eta 0:01:09
>>> samples [91/160] noisy rate: 0.12 --> 0.24 --> 0.12 <<<
epoch [42/100] batch [5/5] time 0.054 (0.262) data 0.000 (0.202) loss 1.3106 (1.2960) acc 79.6875 (84.7451) lr 1.2790e-03 eta 0:01:15
epoch [43/100] batch [5/5] time 0.054 (0.248) data 0.000 (0.189) loss 1.6477 (1.3374) acc 81.9444 (80.7320) lr 1.2487e-03 eta 0:01:10
epoch [44/100] batch [5/5] time 0.071 (0.247) data 0.000 (0.185) loss 1.3075 (1.3107) acc 88.6364 (81.9315) lr 1.2181e-03 eta 0:01:09
epoch [45/100] batch [5/5] time 0.061 (0.247) data 0.000 (0.186) loss 1.3354 (1.3417) acc 80.5555 (80.0167) lr 1.1874e-03 eta 0:01:07
epoch [46/100] batch [5/5] time 0.055 (0.229) data 0.000 (0.169) loss 1.2900 (1.3189) acc 78.9474 (80.6670) lr 1.1564e-03 eta 0:01:01
>>> samples [92/160] noisy rate: 0.12 --> 0.30 --> 0.13 <<<
epoch [47/100] batch [5/5] time 0.048 (0.231) data 0.000 (0.171) loss 1.2281 (1.3089) acc 84.3750 (85.0832) lr 1.1253e-03 eta 0:01:01
epoch [48/100] batch [5/5] time 0.053 (0.241) data 0.000 (0.178) loss 1.3236 (1.3157) acc 84.7222 (79.8556) lr 1.0941e-03 eta 0:01:02
epoch [49/100] batch [5/5] time 0.051 (0.242) data 0.000 (0.179) loss 1.2529 (1.3125) acc 77.9412 (83.0882) lr 1.0628e-03 eta 0:01:01
epoch [50/100] batch [5/5] time 0.066 (0.235) data 0.000 (0.173) loss 1.4596 (1.3020) acc 82.6087 (81.5115) lr 1.0314e-03 eta 0:00:58
epoch [51/100] batch [5/5] time 0.058 (0.244) data 0.000 (0.177) loss 1.3754 (1.3284) acc 67.5000 (82.1345) lr 1.0000e-03 eta 0:00:59
>>> samples [93/160] noisy rate: 0.12 --> 0.31 --> 0.14 <<<
epoch [52/100] batch [5/5] time 0.054 (0.205) data 0.000 (0.141) loss 1.3873 (1.3507) acc 84.3750 (80.5954) lr 9.6859e-04 eta 0:00:49
epoch [53/100] batch [5/5] time 0.064 (0.218) data 0.000 (0.155) loss 1.3974 (1.3377) acc 76.3158 (81.8197) lr 9.3721e-04 eta 0:00:51
epoch [54/100] batch [5/5] time 0.056 (0.236) data 0.000 (0.173) loss 1.2761 (1.3263) acc 85.2941 (81.9426) lr 9.0589e-04 eta 0:00:54
epoch [55/100] batch [5/5] time 0.061 (0.231) data 0.000 (0.166) loss 1.1031 (1.3506) acc 85.5263 (80.1217) lr 8.7467e-04 eta 0:00:51
epoch [56/100] batch [5/5] time 0.060 (0.231) data 0.000 (0.170) loss 1.3437 (1.2939) acc 87.5000 (85.5014) lr 8.4357e-04 eta 0:00:50
>>> samples [93/160] noisy rate: 0.12 --> 0.31 --> 0.14 <<<
epoch [57/100] batch [5/5] time 0.069 (0.238) data 0.000 (0.175) loss 1.3459 (1.2853) acc 75.0000 (77.6182) lr 8.1262e-04 eta 0:00:51
epoch [58/100] batch [5/5] time 0.067 (0.256) data 0.000 (0.193) loss 1.3743 (1.2827) acc 78.1250 (81.9456) lr 7.8186e-04 eta 0:00:53
epoch [59/100] batch [5/5] time 0.062 (0.233) data 0.000 (0.169) loss 1.2960 (1.2938) acc 80.0000 (85.5196) lr 7.5131e-04 eta 0:00:47
epoch [60/100] batch [5/5] time 0.055 (0.232) data 0.000 (0.173) loss 1.2110 (1.3289) acc 86.1111 (78.6915) lr 7.2101e-04 eta 0:00:46
epoch [61/100] batch [5/5] time 0.062 (0.228) data 0.001 (0.166) loss 1.1905 (1.2891) acc 83.3333 (83.5181) lr 6.9098e-04 eta 0:00:44
>>> samples [93/160] noisy rate: 0.12 --> 0.31 --> 0.14 <<<
epoch [62/100] batch [5/5] time 0.059 (0.262) data 0.001 (0.194) loss 1.6709 (1.3030) acc 81.2500 (83.1482) lr 6.6126e-04 eta 0:00:49
epoch [63/100] batch [5/5] time 0.061 (0.233) data 0.000 (0.171) loss 1.3906 (1.3001) acc 77.6316 (80.5149) lr 6.3188e-04 eta 0:00:43
epoch [64/100] batch [5/5] time 0.062 (0.234) data 0.000 (0.174) loss 1.2443 (1.3003) acc 86.8421 (83.3371) lr 6.0285e-04 eta 0:00:42
epoch [65/100] batch [5/5] time 0.059 (0.246) data 0.000 (0.182) loss 1.2163 (1.2862) acc 93.0555 (84.4335) lr 5.7422e-04 eta 0:00:43
epoch [66/100] batch [5/5] time 0.067 (0.235) data 0.000 (0.172) loss 1.2015 (1.2913) acc 82.1429 (84.0189) lr 5.4601e-04 eta 0:00:39
>>> samples [93/160] noisy rate: 0.12 --> 0.29 --> 0.14 <<<
epoch [67/100] batch [5/5] time 0.053 (0.235) data 0.000 (0.172) loss 1.3963 (1.2838) acc 86.7647 (83.2848) lr 5.1825e-04 eta 0:00:38
epoch [68/100] batch [5/5] time 0.045 (0.238) data 0.000 (0.175) loss 1.1602 (1.3168) acc 78.3333 (79.8279) lr 4.9096e-04 eta 0:00:38
epoch [69/100] batch [5/5] time 0.054 (0.245) data 0.000 (0.184) loss 1.1458 (1.3012) acc 87.5000 (83.0000) lr 4.6417e-04 eta 0:00:37
epoch [70/100] batch [5/5] time 0.058 (0.238) data 0.000 (0.172) loss 1.4302 (1.2825) acc 82.3529 (85.5889) lr 4.3792e-04 eta 0:00:35
epoch [71/100] batch [5/5] time 0.058 (0.251) data 0.000 (0.191) loss 1.4289 (1.2976) acc 83.7500 (79.8382) lr 4.1221e-04 eta 0:00:36
>>> samples [93/160] noisy rate: 0.12 --> 0.34 --> 0.14 <<<
epoch [72/100] batch [5/5] time 0.056 (0.217) data 0.000 (0.154) loss 1.1716 (1.2721) acc 80.5555 (85.5448) lr 3.8709e-04 eta 0:00:30
epoch [73/100] batch [5/5] time 0.055 (0.223) data 0.000 (0.163) loss 1.3550 (1.2823) acc 84.2105 (86.1646) lr 3.6258e-04 eta 0:00:30
epoch [74/100] batch [5/5] time 0.065 (0.238) data 0.000 (0.173) loss 1.3277 (1.2540) acc 82.5000 (83.5058) lr 3.3869e-04 eta 0:00:30
epoch [75/100] batch [5/5] time 0.052 (0.228) data 0.000 (0.166) loss 1.5320 (1.2739) acc 79.4118 (84.0966) lr 3.1545e-04 eta 0:00:28
epoch [76/100] batch [5/5] time 0.059 (0.223) data 0.001 (0.160) loss 1.2770 (1.2953) acc 77.7778 (82.1147) lr 2.9289e-04 eta 0:00:26
>>> samples [94/160] noisy rate: 0.12 --> 0.28 --> 0.15 <<<
epoch [77/100] batch [5/5] time 0.063 (0.235) data 0.000 (0.171) loss 1.3587 (1.2837) acc 83.7500 (83.4193) lr 2.7103e-04 eta 0:00:27
epoch [78/100] batch [5/5] time 0.061 (0.227) data 0.000 (0.163) loss 1.0091 (1.3042) acc 77.3810 (81.6877) lr 2.4989e-04 eta 0:00:24
epoch [79/100] batch [5/5] time 0.047 (0.247) data 0.000 (0.185) loss 1.7055 (1.3215) acc 71.1538 (80.7901) lr 2.2949e-04 eta 0:00:25
epoch [80/100] batch [5/5] time 0.061 (0.242) data 0.000 (0.179) loss 1.1074 (1.2944) acc 84.2105 (83.8596) lr 2.0984e-04 eta 0:00:24
epoch [81/100] batch [5/5] time 0.063 (0.244) data 0.000 (0.184) loss 1.0582 (1.2863) acc 87.5000 (83.9728) lr 1.9098e-04 eta 0:00:23
>>> samples [94/160] noisy rate: 0.12 --> 0.29 --> 0.15 <<<
epoch [82/100] batch [5/5] time 0.066 (0.235) data 0.000 (0.171) loss 1.1706 (1.2711) acc 81.2500 (82.8931) lr 1.7292e-04 eta 0:00:21
epoch [83/100] batch [5/5] time 0.068 (0.244) data 0.000 (0.181) loss 1.2977 (1.2973) acc 76.1905 (83.4112) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.059 (0.237) data 0.000 (0.172) loss 1.2631 (1.2704) acc 72.2222 (81.0016) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.061 (0.233) data 0.000 (0.166) loss 1.1455 (1.2878) acc 83.3333 (82.7694) lr 1.2369e-04 eta 0:00:17
epoch [86/100] batch [5/5] time 0.056 (0.238) data 0.000 (0.177) loss 1.3777 (1.2846) acc 75.0000 (80.5526) lr 1.0899e-04 eta 0:00:16
>>> samples [94/160] noisy rate: 0.12 --> 0.31 --> 0.15 <<<
epoch [87/100] batch [5/5] time 0.049 (0.218) data 0.000 (0.153) loss 0.9543 (1.2638) acc 85.7143 (81.4228) lr 9.5173e-05 eta 0:00:14
epoch [88/100] batch [5/5] time 0.055 (0.232) data 0.000 (0.172) loss 0.9730 (1.2584) acc 88.2353 (82.8513) lr 8.2245e-05 eta 0:00:13
epoch [89/100] batch [5/5] time 0.057 (0.232) data 0.000 (0.169) loss 1.3429 (1.3046) acc 82.3529 (83.3338) lr 7.0224e-05 eta 0:00:12
epoch [90/100] batch [5/5] time 0.063 (0.218) data 0.000 (0.154) loss 1.3401 (1.2819) acc 85.0000 (82.2964) lr 5.9119e-05 eta 0:00:10
epoch [91/100] batch [5/5] time 0.056 (0.243) data 0.000 (0.173) loss 1.3475 (1.2767) acc 85.2941 (83.5247) lr 4.8943e-05 eta 0:00:10
>>> samples [95/160] noisy rate: 0.12 --> 0.29 --> 0.15 <<<
epoch [92/100] batch [5/5] time 0.074 (0.234) data 0.000 (0.168) loss 1.3641 (1.2351) acc 84.3750 (85.6797) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.058 (0.235) data 0.000 (0.171) loss 1.1797 (1.2589) acc 87.5000 (84.3875) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.062 (0.254) data 0.000 (0.195) loss 1.1848 (1.2555) acc 73.7500 (80.8647) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.057 (0.240) data 0.000 (0.173) loss 1.3700 (1.2798) acc 78.7500 (82.9868) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.066 (0.232) data 0.000 (0.167) loss 1.4316 (1.2766) acc 77.1739 (79.8352) lr 1.2312e-05 eta 0:00:04
>>> samples [95/160] noisy rate: 0.12 --> 0.34 --> 0.15 <<<
epoch [97/100] batch [5/5] time 0.054 (0.256) data 0.000 (0.192) loss 1.3682 (1.2320) acc 80.5555 (85.9510) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.067 (0.240) data 0.000 (0.175) loss 1.0517 (1.2726) acc 95.2381 (81.7157) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.054 (0.214) data 0.000 (0.150) loss 1.3589 (1.2564) acc 81.9444 (85.5716) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.054 (0.224) data 0.000 (0.160) loss 1.4796 (1.2362) acc 91.6667 (86.7943) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.27, 0.26, 0.22, 0.17, 0.19, 0.22, 0.23, 0.27, 0.24, 0.3, 0.31, 0.31, 0.31, 0.29, 0.34, 0.28, 0.29, 0.31, 0.29, 0.34]
* learned noise rate: [0.01, 0.03, 0.06, 0.07, 0.08, 0.08, 0.09, 0.1, 0.12, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:20,  1.01s/it]  4%|▎         | 3/81 [00:01<00:24,  3.16it/s]  6%|▌         | 5/81 [00:01<00:14,  5.09it/s]  9%|▊         | 7/81 [00:01<00:10,  6.88it/s] 11%|█         | 9/81 [00:01<00:08,  8.04it/s] 14%|█▎        | 11/81 [00:01<00:07,  9.70it/s] 16%|█▌        | 13/81 [00:01<00:06, 11.10it/s] 19%|█▊        | 15/81 [00:02<00:05, 12.29it/s] 21%|██        | 17/81 [00:02<00:04, 13.05it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.54it/s] 26%|██▌       | 21/81 [00:02<00:04, 14.18it/s] 28%|██▊       | 23/81 [00:02<00:04, 14.29it/s] 31%|███       | 25/81 [00:02<00:03, 14.59it/s] 33%|███▎      | 27/81 [00:02<00:03, 14.96it/s] 36%|███▌      | 29/81 [00:02<00:03, 14.84it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.00it/s] 41%|████      | 33/81 [00:03<00:03, 15.18it/s] 43%|████▎     | 35/81 [00:03<00:03, 15.23it/s] 46%|████▌     | 37/81 [00:03<00:02, 15.04it/s] 48%|████▊     | 39/81 [00:03<00:02, 14.96it/s] 51%|█████     | 41/81 [00:03<00:02, 15.22it/s] 53%|█████▎    | 43/81 [00:03<00:02, 15.16it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.36it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.24it/s] 60%|██████    | 49/81 [00:04<00:02, 15.42it/s] 63%|██████▎   | 51/81 [00:04<00:01, 15.13it/s] 65%|██████▌   | 53/81 [00:04<00:01, 15.31it/s] 68%|██████▊   | 55/81 [00:04<00:01, 15.42it/s] 70%|███████   | 57/81 [00:04<00:01, 15.55it/s] 73%|███████▎  | 59/81 [00:04<00:01, 15.48it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.28it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.34it/s] 80%|████████  | 65/81 [00:05<00:01, 15.44it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.45it/s] 85%|████████▌ | 69/81 [00:05<00:00, 15.59it/s] 88%|████████▊ | 71/81 [00:05<00:00, 15.72it/s] 90%|█████████ | 73/81 [00:05<00:00, 15.82it/s] 93%|█████████▎| 75/81 [00:05<00:00, 15.87it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.92it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.97it/s]100%|██████████| 81/81 [00:06<00:00, 16.00it/s]100%|██████████| 81/81 [00:06<00:00, 12.62it/s]
=> result
* total: 8,100
* correct: 5,158
* accuracy: 63.7%
* error: 36.3%
* macro_f1: 62.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 546	acc: 60.7%
* class: 1 (Forest)	total: 900	correct: 884	acc: 98.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 222	acc: 24.7%
* class: 3 (Highway or Road)	total: 750	correct: 474	acc: 63.2%
* class: 4 (Industrial Buildings)	total: 750	correct: 714	acc: 95.2%
* class: 5 (Pasture Land)	total: 600	correct: 257	acc: 42.8%
* class: 6 (Permanent Crop Land)	total: 750	correct: 601	acc: 80.1%
* class: 7 (Residential Buildings)	total: 900	correct: 706	acc: 78.4%
* class: 8 (River)	total: 750	correct: 476	acc: 63.5%
* class: 9 (Sea or Lake)	total: 900	correct: 278	acc: 30.9%
* average: 63.8%
Elapsed: 0:04:05
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_4-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.595) data 0.000 (0.215) loss 1.1777 (1.1524) acc 7.8125 (14.0625) lr 2.0000e-03 eta 0:04:54
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [47/160] noisy rate: 0.25 --> 0.25 --> 0.02 <<<
epoch [2/100] batch [5/5] time 0.245 (0.374) data 0.000 (0.176) loss 2.4757 (2.1452) acc 25.0000 (26.9583) lr 1.9995e-03 eta 0:03:03
epoch [3/100] batch [5/5] time 0.044 (0.281) data 0.000 (0.180) loss 2.2025 (2.2090) acc 25.0000 (22.4481) lr 1.9980e-03 eta 0:02:16
epoch [4/100] batch [5/5] time 0.214 (0.332) data 0.000 (0.144) loss 1.9622 (1.9543) acc 35.0000 (33.1371) lr 1.9956e-03 eta 0:02:39
epoch [5/100] batch [5/5] time 0.039 (0.198) data 0.000 (0.149) loss 1.7979 (1.8400) acc 35.0000 (42.3644) lr 1.9921e-03 eta 0:01:34
epoch [6/100] batch [5/5] time 0.035 (0.246) data 0.000 (0.149) loss 1.3039 (1.6134) acc 75.0000 (51.1399) lr 1.9877e-03 eta 0:01:55
>>> samples [54/160] noisy rate: 0.25 --> 0.32 --> 0.06 <<<
epoch [7/100] batch [5/5] time 0.049 (0.247) data 0.000 (0.159) loss 1.7135 (1.6189) acc 50.0000 (49.1111) lr 1.9823e-03 eta 0:01:54
epoch [8/100] batch [5/5] time 0.040 (0.263) data 0.000 (0.145) loss 1.5171 (1.5466) acc 52.7778 (57.4257) lr 1.9759e-03 eta 0:02:01
epoch [9/100] batch [5/5] time 0.049 (0.215) data 0.000 (0.164) loss 1.6512 (1.5604) acc 52.0833 (53.3158) lr 1.9686e-03 eta 0:01:37
epoch [10/100] batch [5/5] time 0.035 (0.226) data 0.000 (0.172) loss 1.7659 (1.4931) acc 39.2857 (57.3764) lr 1.9603e-03 eta 0:01:41
epoch [11/100] batch [5/5] time 0.046 (0.230) data 0.000 (0.178) loss 1.1895 (1.4208) acc 85.4167 (66.4610) lr 1.9511e-03 eta 0:01:42
>>> samples [57/160] noisy rate: 0.25 --> 0.27 --> 0.07 <<<
epoch [12/100] batch [5/5] time 0.052 (0.224) data 0.000 (0.169) loss 1.3617 (1.3312) acc 67.8571 (70.1118) lr 1.9409e-03 eta 0:01:38
epoch [13/100] batch [5/5] time 0.035 (0.277) data 0.000 (0.158) loss 1.2866 (1.2930) acc 75.0000 (68.8750) lr 1.9298e-03 eta 0:02:00
epoch [14/100] batch [5/5] time 0.045 (0.227) data 0.000 (0.174) loss 1.3904 (1.2665) acc 68.7500 (72.4756) lr 1.9178e-03 eta 0:01:37
epoch [15/100] batch [5/5] time 0.056 (0.231) data 0.000 (0.179) loss 1.2646 (1.1900) acc 71.8750 (75.0257) lr 1.9048e-03 eta 0:01:38
epoch [16/100] batch [5/5] time 0.036 (0.303) data 0.000 (0.179) loss 1.3191 (1.2463) acc 50.0000 (71.1507) lr 1.8910e-03 eta 0:02:07
>>> samples [57/160] noisy rate: 0.25 --> 0.23 --> 0.07 <<<
epoch [17/100] batch [5/5] time 0.037 (0.223) data 0.000 (0.173) loss 1.1309 (1.1299) acc 68.7500 (79.0952) lr 1.8763e-03 eta 0:01:32
epoch [18/100] batch [5/5] time 0.038 (0.210) data 0.000 (0.157) loss 1.3261 (1.1016) acc 77.7778 (83.5339) lr 1.8607e-03 eta 0:01:25
epoch [19/100] batch [5/5] time 0.039 (0.204) data 0.000 (0.155) loss 1.0790 (1.1169) acc 81.2500 (77.6474) lr 1.8443e-03 eta 0:01:22
epoch [20/100] batch [5/5] time 0.046 (0.202) data 0.000 (0.149) loss 1.0133 (1.0911) acc 92.3077 (84.7280) lr 1.8271e-03 eta 0:01:20
epoch [21/100] batch [5/5] time 0.034 (0.214) data 0.000 (0.163) loss 0.9982 (1.0405) acc 71.8750 (83.2555) lr 1.8090e-03 eta 0:01:24
>>> samples [57/160] noisy rate: 0.25 --> 0.22 --> 0.07 <<<
epoch [22/100] batch [5/5] time 0.028 (0.218) data 0.000 (0.171) loss 1.3354 (1.1123) acc 64.2857 (78.0804) lr 1.7902e-03 eta 0:01:25
epoch [23/100] batch [5/5] time 0.041 (0.244) data 0.000 (0.197) loss 0.9824 (1.0501) acc 95.4545 (87.2121) lr 1.7705e-03 eta 0:01:34
epoch [24/100] batch [5/5] time 0.035 (0.220) data 0.000 (0.166) loss 1.2754 (1.0496) acc 75.0000 (86.6317) lr 1.7501e-03 eta 0:01:23
epoch [25/100] batch [5/5] time 0.034 (0.219) data 0.000 (0.165) loss 1.0228 (1.0612) acc 75.0000 (82.5379) lr 1.7290e-03 eta 0:01:22
epoch [26/100] batch [5/5] time 0.039 (0.224) data 0.000 (0.174) loss 1.2121 (1.0201) acc 81.2500 (86.4103) lr 1.7071e-03 eta 0:01:22
>>> samples [58/160] noisy rate: 0.25 --> 0.23 --> 0.07 <<<
epoch [27/100] batch [5/5] time 0.046 (0.244) data 0.000 (0.193) loss 1.0309 (1.0569) acc 86.5385 (84.3613) lr 1.6845e-03 eta 0:01:28
epoch [28/100] batch [5/5] time 0.038 (0.217) data 0.000 (0.169) loss 1.0564 (1.0230) acc 88.6364 (86.0985) lr 1.6613e-03 eta 0:01:18
epoch [29/100] batch [5/5] time 0.046 (0.240) data 0.000 (0.194) loss 1.1405 (1.0209) acc 84.6154 (87.0897) lr 1.6374e-03 eta 0:01:25
epoch [30/100] batch [5/5] time 0.041 (0.213) data 0.000 (0.164) loss 1.0919 (1.0277) acc 86.5385 (88.4611) lr 1.6129e-03 eta 0:01:14
epoch [31/100] batch [5/5] time 0.034 (0.234) data 0.000 (0.182) loss 0.9655 (0.9380) acc 87.5000 (91.1503) lr 1.5878e-03 eta 0:01:20
>>> samples [63/160] noisy rate: 0.25 --> 0.23 --> 0.10 <<<
epoch [32/100] batch [5/5] time 0.038 (0.295) data 0.000 (0.170) loss 1.1961 (0.9481) acc 82.5000 (86.6268) lr 1.5621e-03 eta 0:01:40
epoch [33/100] batch [5/5] time 0.039 (0.225) data 0.000 (0.173) loss 0.7322 (0.9179) acc 93.7500 (91.3141) lr 1.5358e-03 eta 0:01:15
epoch [34/100] batch [5/5] time 0.046 (0.217) data 0.000 (0.169) loss 1.0588 (0.9009) acc 76.7857 (90.0191) lr 1.5090e-03 eta 0:01:11
epoch [35/100] batch [5/5] time 0.049 (0.244) data 0.000 (0.192) loss 0.9954 (0.8945) acc 91.0714 (91.5684) lr 1.4818e-03 eta 0:01:19
epoch [36/100] batch [5/5] time 0.044 (0.216) data 0.000 (0.161) loss 0.9494 (0.9235) acc 89.5833 (91.2651) lr 1.4540e-03 eta 0:01:09
>>> samples [63/160] noisy rate: 0.25 --> 0.23 --> 0.10 <<<
epoch [37/100] batch [5/5] time 0.036 (0.230) data 0.000 (0.176) loss 0.9517 (0.9314) acc 86.3636 (86.1623) lr 1.4258e-03 eta 0:01:12
epoch [38/100] batch [5/5] time 0.034 (0.217) data 0.000 (0.166) loss 1.0366 (0.8917) acc 100.0000 (93.5173) lr 1.3971e-03 eta 0:01:07
epoch [39/100] batch [5/5] time 0.053 (0.232) data 0.000 (0.183) loss 0.7089 (0.9393) acc 87.5000 (89.3333) lr 1.3681e-03 eta 0:01:10
epoch [40/100] batch [5/5] time 0.047 (0.210) data 0.000 (0.159) loss 0.7444 (0.9003) acc 97.9167 (91.4615) lr 1.3387e-03 eta 0:01:03
epoch [41/100] batch [5/5] time 0.040 (0.238) data 0.000 (0.185) loss 1.1063 (0.9123) acc 97.9167 (91.0569) lr 1.3090e-03 eta 0:01:10
>>> samples [63/160] noisy rate: 0.25 --> 0.19 --> 0.10 <<<
epoch [42/100] batch [5/5] time 0.034 (0.242) data 0.001 (0.190) loss 0.9177 (0.8980) acc 97.5000 (94.5000) lr 1.2790e-03 eta 0:01:10
epoch [43/100] batch [5/5] time 0.042 (0.239) data 0.000 (0.187) loss 1.0104 (0.9292) acc 90.3846 (90.3344) lr 1.2487e-03 eta 0:01:08
epoch [44/100] batch [5/5] time 0.039 (0.231) data 0.001 (0.180) loss 0.9553 (0.9079) acc 89.5833 (90.3205) lr 1.2181e-03 eta 0:01:04
epoch [45/100] batch [5/5] time 0.427 (0.313) data 0.000 (0.190) loss 0.8463 (0.9080) acc 88.8889 (90.9500) lr 1.1874e-03 eta 0:01:26
epoch [46/100] batch [5/5] time 0.041 (0.219) data 0.000 (0.166) loss 1.0260 (0.9132) acc 82.6923 (89.6932) lr 1.1564e-03 eta 0:00:59
>>> samples [64/160] noisy rate: 0.25 --> 0.17 --> 0.09 <<<
epoch [47/100] batch [5/5] time 0.044 (0.224) data 0.000 (0.172) loss 0.8619 (0.8850) acc 94.6429 (91.7491) lr 1.1253e-03 eta 0:00:59
epoch [48/100] batch [5/5] time 0.041 (0.240) data 0.000 (0.194) loss 0.8573 (0.9326) acc 90.3846 (89.7620) lr 1.0941e-03 eta 0:01:02
epoch [49/100] batch [5/5] time 0.056 (0.231) data 0.000 (0.178) loss 0.9846 (0.8766) acc 97.0588 (92.5167) lr 1.0628e-03 eta 0:00:58
epoch [50/100] batch [5/5] time 0.051 (0.224) data 0.000 (0.172) loss 0.8892 (0.8362) acc 92.6471 (94.7415) lr 1.0314e-03 eta 0:00:56
epoch [51/100] batch [5/5] time 0.037 (0.227) data 0.000 (0.175) loss 0.7064 (0.8448) acc 97.2222 (93.6284) lr 1.0000e-03 eta 0:00:55
>>> samples [64/160] noisy rate: 0.25 --> 0.16 --> 0.09 <<<
epoch [52/100] batch [5/5] time 0.043 (0.208) data 0.000 (0.160) loss 0.9933 (0.8688) acc 95.8333 (93.6667) lr 9.6859e-04 eta 0:00:49
epoch [53/100] batch [5/5] time 0.044 (0.218) data 0.000 (0.164) loss 0.6938 (0.8626) acc 91.0714 (92.4691) lr 9.3721e-04 eta 0:00:51
epoch [54/100] batch [5/5] time 0.040 (0.207) data 0.000 (0.157) loss 0.9785 (0.8594) acc 91.6667 (92.7364) lr 9.0589e-04 eta 0:00:47
epoch [55/100] batch [5/5] time 0.050 (0.233) data 0.000 (0.177) loss 0.8580 (0.8381) acc 96.4286 (92.4368) lr 8.7467e-04 eta 0:00:52
epoch [56/100] batch [5/5] time 0.048 (0.233) data 0.000 (0.176) loss 0.8368 (0.8789) acc 89.2857 (92.1122) lr 8.4357e-04 eta 0:00:51
>>> samples [64/160] noisy rate: 0.25 --> 0.36 --> 0.09 <<<
epoch [57/100] batch [5/5] time 0.045 (0.217) data 0.000 (0.163) loss 0.8302 (0.8575) acc 91.0714 (90.1802) lr 8.1262e-04 eta 0:00:46
epoch [58/100] batch [5/5] time 0.048 (0.237) data 0.000 (0.188) loss 0.7062 (0.8497) acc 96.8750 (92.3499) lr 7.8186e-04 eta 0:00:49
epoch [59/100] batch [5/5] time 0.047 (0.225) data 0.000 (0.174) loss 0.7921 (0.8400) acc 91.6667 (94.1667) lr 7.5131e-04 eta 0:00:46
epoch [60/100] batch [5/5] time 0.047 (0.237) data 0.000 (0.180) loss 0.8789 (0.8093) acc 92.8571 (93.3571) lr 7.2101e-04 eta 0:00:47
epoch [61/100] batch [5/5] time 0.043 (0.234) data 0.000 (0.181) loss 0.5188 (0.8080) acc 98.0769 (94.0517) lr 6.9098e-04 eta 0:00:45
>>> samples [70/160] noisy rate: 0.25 --> 0.16 --> 0.09 <<<
epoch [62/100] batch [5/5] time 0.041 (0.307) data 0.000 (0.173) loss 0.7412 (0.8271) acc 90.9091 (93.8366) lr 6.6126e-04 eta 0:00:58
epoch [63/100] batch [5/5] time 0.038 (0.218) data 0.000 (0.167) loss 0.8392 (0.8300) acc 97.7273 (93.1645) lr 6.3188e-04 eta 0:00:40
epoch [64/100] batch [5/5] time 0.053 (0.232) data 0.000 (0.178) loss 0.8804 (0.8113) acc 86.6667 (92.4146) lr 6.0285e-04 eta 0:00:41
epoch [65/100] batch [5/5] time 0.062 (0.230) data 0.000 (0.172) loss 0.7589 (0.7862) acc 92.1053 (94.0286) lr 5.7422e-04 eta 0:00:40
epoch [66/100] batch [5/5] time 0.046 (0.232) data 0.001 (0.176) loss 0.8026 (0.8233) acc 86.6667 (92.1209) lr 5.4601e-04 eta 0:00:39
>>> samples [70/160] noisy rate: 0.25 --> 0.16 --> 0.09 <<<
epoch [67/100] batch [5/5] time 0.045 (0.225) data 0.000 (0.172) loss 0.8306 (0.8105) acc 95.0000 (94.4525) lr 5.1825e-04 eta 0:00:37
epoch [68/100] batch [5/5] time 0.054 (0.237) data 0.000 (0.180) loss 0.7662 (0.8387) acc 95.8333 (94.1795) lr 4.9096e-04 eta 0:00:37
epoch [69/100] batch [5/5] time 0.046 (0.220) data 0.000 (0.162) loss 0.7408 (0.8120) acc 100.0000 (94.2949) lr 4.6417e-04 eta 0:00:34
epoch [70/100] batch [5/5] time 0.041 (0.293) data 0.000 (0.164) loss 0.7521 (0.8028) acc 96.1538 (93.5641) lr 4.3792e-04 eta 0:00:43
epoch [71/100] batch [5/5] time 0.042 (0.231) data 0.000 (0.180) loss 0.8608 (0.8424) acc 96.1538 (92.5817) lr 4.1221e-04 eta 0:00:33
>>> samples [70/160] noisy rate: 0.25 --> 0.19 --> 0.09 <<<
epoch [72/100] batch [5/5] time 0.044 (0.233) data 0.000 (0.181) loss 0.6588 (0.7920) acc 95.8333 (93.7443) lr 3.8709e-04 eta 0:00:32
epoch [73/100] batch [5/5] time 0.041 (0.225) data 0.000 (0.169) loss 0.6778 (0.8445) acc 90.9091 (92.2395) lr 3.6258e-04 eta 0:00:30
epoch [74/100] batch [5/5] time 0.042 (0.227) data 0.000 (0.177) loss 0.7583 (0.8219) acc 96.1538 (92.0623) lr 3.3869e-04 eta 0:00:29
epoch [75/100] batch [5/5] time 0.044 (0.237) data 0.000 (0.179) loss 0.7312 (0.8009) acc 88.6364 (92.7702) lr 3.1545e-04 eta 0:00:29
epoch [76/100] batch [5/5] time 0.054 (0.224) data 0.000 (0.166) loss 0.6788 (0.8429) acc 90.6250 (89.9591) lr 2.9289e-04 eta 0:00:26
>>> samples [72/160] noisy rate: 0.25 --> 0.17 --> 0.10 <<<
epoch [77/100] batch [5/5] time 0.056 (0.223) data 0.000 (0.166) loss 0.8611 (0.8349) acc 95.3125 (93.7797) lr 2.7103e-04 eta 0:00:25
epoch [78/100] batch [5/5] time 0.050 (0.213) data 0.000 (0.155) loss 0.7021 (0.8166) acc 98.2143 (93.0889) lr 2.4989e-04 eta 0:00:23
epoch [79/100] batch [5/5] time 0.042 (0.227) data 0.000 (0.169) loss 0.7496 (0.8012) acc 93.1818 (93.4788) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.048 (0.223) data 0.000 (0.168) loss 0.7493 (0.8104) acc 92.1875 (96.2097) lr 2.0984e-04 eta 0:00:22
epoch [81/100] batch [5/5] time 0.060 (0.241) data 0.000 (0.186) loss 0.7961 (0.8340) acc 96.8750 (92.7321) lr 1.9098e-04 eta 0:00:22
>>> samples [72/160] noisy rate: 0.25 --> 0.16 --> 0.10 <<<
epoch [82/100] batch [5/5] time 0.049 (0.231) data 0.000 (0.171) loss 0.6787 (0.7971) acc 94.6429 (95.2411) lr 1.7292e-04 eta 0:00:20
epoch [83/100] batch [5/5] time 0.054 (0.245) data 0.000 (0.189) loss 0.8819 (0.8197) acc 90.0000 (91.9952) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.050 (0.230) data 0.000 (0.179) loss 0.9985 (0.8471) acc 95.0000 (92.6989) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.046 (0.322) data 0.000 (0.189) loss 0.8592 (0.7776) acc 97.9167 (94.5334) lr 1.2369e-04 eta 0:00:24
epoch [86/100] batch [5/5] time 0.040 (0.212) data 0.000 (0.152) loss 0.9069 (0.8054) acc 90.0000 (94.4154) lr 1.0899e-04 eta 0:00:14
>>> samples [73/160] noisy rate: 0.25 --> 0.19 --> 0.11 <<<
epoch [87/100] batch [5/5] time 0.037 (0.221) data 0.000 (0.166) loss 0.9476 (0.8267) acc 90.9091 (92.3277) lr 9.5173e-05 eta 0:00:14
epoch [88/100] batch [5/5] time 0.052 (0.230) data 0.000 (0.176) loss 0.9838 (0.7994) acc 98.4375 (93.6648) lr 8.2245e-05 eta 0:00:13
epoch [89/100] batch [5/5] time 0.046 (0.254) data 0.001 (0.200) loss 0.8879 (0.8291) acc 94.2308 (93.5337) lr 7.0224e-05 eta 0:00:13
epoch [90/100] batch [5/5] time 0.045 (0.224) data 0.000 (0.170) loss 0.9351 (0.7954) acc 96.6667 (95.3629) lr 5.9119e-05 eta 0:00:11
epoch [91/100] batch [5/5] time 0.059 (0.236) data 0.000 (0.175) loss 0.9026 (0.8243) acc 90.2778 (93.4246) lr 4.8943e-05 eta 0:00:10
>>> samples [75/160] noisy rate: 0.25 --> 0.21 --> 0.12 <<<
epoch [92/100] batch [5/5] time 0.044 (0.248) data 0.000 (0.194) loss 0.6876 (0.8295) acc 91.0714 (94.4034) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.047 (0.238) data 0.000 (0.182) loss 0.8955 (0.8373) acc 90.3846 (92.1814) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.046 (0.223) data 0.000 (0.168) loss 0.8989 (0.8800) acc 95.0000 (92.4076) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.045 (0.231) data 0.000 (0.176) loss 0.9887 (0.8260) acc 89.2857 (93.3507) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.056 (0.221) data 0.000 (0.164) loss 0.9999 (0.8550) acc 88.1579 (91.2030) lr 1.2312e-05 eta 0:00:04
>>> samples [76/160] noisy rate: 0.25 --> 0.18 --> 0.13 <<<
epoch [97/100] batch [5/5] time 0.047 (0.223) data 0.000 (0.167) loss 1.0550 (0.8231) acc 88.3333 (93.5598) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.047 (0.216) data 0.001 (0.159) loss 0.7778 (0.8316) acc 88.3333 (90.0865) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.052 (0.237) data 0.000 (0.182) loss 0.8917 (0.8874) acc 88.2353 (90.1759) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.049 (0.226) data 0.000 (0.170) loss 0.9360 (0.8245) acc 92.1875 (93.4123) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.25, 0.32, 0.27, 0.23, 0.22, 0.23, 0.23, 0.23, 0.19, 0.17, 0.16, 0.36, 0.16, 0.16, 0.19, 0.17, 0.16, 0.19, 0.21, 0.18]
* learned noise rate: [0.02, 0.06, 0.07, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.1, 0.11, 0.12, 0.13]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:33,  1.92s/it]  4%|▎         | 3/81 [00:02<00:42,  1.84it/s]  6%|▌         | 5/81 [00:02<00:22,  3.36it/s]  9%|▊         | 7/81 [00:02<00:14,  4.96it/s] 11%|█         | 9/81 [00:02<00:10,  6.59it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.26it/s] 16%|█▌        | 13/81 [00:02<00:06,  9.80it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.15it/s] 21%|██        | 17/81 [00:02<00:05, 12.26it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.85it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.47it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.72it/s] 31%|███       | 25/81 [00:03<00:04, 13.99it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.46it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.77it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.01it/s] 41%|████      | 33/81 [00:04<00:03, 15.19it/s] 43%|████▎     | 35/81 [00:04<00:03, 15.30it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.37it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.46it/s] 51%|█████     | 41/81 [00:04<00:02, 15.52it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.25it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.22it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.03it/s] 60%|██████    | 49/81 [00:05<00:02, 15.22it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.36it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.42it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.36it/s] 70%|███████   | 57/81 [00:05<00:01, 15.46it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.46it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.53it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.58it/s] 80%|████████  | 65/81 [00:06<00:01, 15.61it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.23it/s] 85%|████████▌ | 69/81 [00:06<00:00, 14.98it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.20it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.36it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.45it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.56it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.66it/s]100%|██████████| 81/81 [00:07<00:00, 15.72it/s]100%|██████████| 81/81 [00:07<00:00, 11.15it/s]
=> result
* total: 8,100
* correct: 4,623
* accuracy: 57.1%
* error: 42.9%
* macro_f1: 54.6%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 242	acc: 26.9%
* class: 1 (Forest)	total: 900	correct: 849	acc: 94.3%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 194	acc: 21.6%
* class: 3 (Highway or Road)	total: 750	correct: 328	acc: 43.7%
* class: 4 (Industrial Buildings)	total: 750	correct: 652	acc: 86.9%
* class: 5 (Pasture Land)	total: 600	correct: 385	acc: 64.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 601	acc: 80.1%
* class: 7 (Residential Buildings)	total: 900	correct: 867	acc: 96.3%
* class: 8 (River)	total: 750	correct: 382	acc: 50.9%
* class: 9 (Sea or Lake)	total: 900	correct: 123	acc: 13.7%
* average: 57.9%
Elapsed: 0:04:01
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_4-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.085 (0.656) data 0.000 (0.178) loss 1.1422 (1.1312) acc 19.5312 (17.8125) lr 2.0000e-03 eta 0:05:24
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [79/160] noisy rate: 0.25 --> 0.30 --> 0.15 <<<
epoch [2/100] batch [5/5] time 0.369 (0.557) data 0.000 (0.166) loss 2.1713 (2.1433) acc 21.4286 (29.4083) lr 1.9995e-03 eta 0:04:32
epoch [3/100] batch [5/5] time 0.449 (0.435) data 0.000 (0.156) loss 1.8608 (1.9976) acc 43.4211 (34.6515) lr 1.9980e-03 eta 0:03:31
epoch [4/100] batch [5/5] time 0.060 (0.288) data 0.001 (0.170) loss 2.0655 (1.9362) acc 33.8235 (38.0371) lr 1.9956e-03 eta 0:02:18
epoch [5/100] batch [5/5] time 0.056 (0.284) data 0.000 (0.165) loss 2.1993 (2.0164) acc 22.0588 (33.0724) lr 1.9921e-03 eta 0:02:15
epoch [6/100] batch [5/5] time 0.059 (0.227) data 0.000 (0.167) loss 1.9990 (1.9777) acc 36.7647 (35.9706) lr 1.9877e-03 eta 0:01:46
>>> samples [91/160] noisy rate: 0.25 --> 0.29 --> 0.19 <<<
epoch [7/100] batch [5/5] time 0.045 (0.312) data 0.000 (0.177) loss 1.9777 (1.9213) acc 48.0769 (47.4696) lr 1.9823e-03 eta 0:02:25
epoch [8/100] batch [5/5] time 0.066 (0.315) data 0.001 (0.172) loss 1.8025 (1.8588) acc 46.0526 (45.5020) lr 1.9759e-03 eta 0:02:24
epoch [9/100] batch [5/5] time 0.064 (0.275) data 0.000 (0.211) loss 1.8550 (1.8549) acc 65.0000 (53.2368) lr 1.9686e-03 eta 0:02:04
epoch [10/100] batch [5/5] time 0.490 (0.322) data 0.000 (0.177) loss 1.7992 (1.8583) acc 56.5217 (45.3490) lr 1.9603e-03 eta 0:02:25
epoch [11/100] batch [5/5] time 0.055 (0.223) data 0.000 (0.164) loss 1.9985 (1.7810) acc 50.0000 (51.6357) lr 1.9511e-03 eta 0:01:39
>>> samples [92/160] noisy rate: 0.25 --> 0.25 --> 0.20 <<<
epoch [12/100] batch [5/5] time 0.069 (0.241) data 0.000 (0.178) loss 1.7752 (1.7935) acc 52.2727 (54.6146) lr 1.9409e-03 eta 0:01:46
epoch [13/100] batch [5/5] time 0.056 (0.225) data 0.000 (0.163) loss 1.9108 (1.7738) acc 50.0000 (50.8841) lr 1.9298e-03 eta 0:01:37
epoch [14/100] batch [5/5] time 0.053 (0.220) data 0.000 (0.158) loss 1.7964 (1.7443) acc 63.8889 (58.3491) lr 1.9178e-03 eta 0:01:34
epoch [15/100] batch [5/5] time 0.052 (0.311) data 0.000 (0.168) loss 1.7296 (1.6955) acc 64.7059 (61.9143) lr 1.9048e-03 eta 0:02:12
epoch [16/100] batch [5/5] time 0.058 (0.241) data 0.000 (0.181) loss 1.7726 (1.7224) acc 48.7500 (58.1465) lr 1.8910e-03 eta 0:01:41
>>> samples [95/160] noisy rate: 0.25 --> 0.25 --> 0.20 <<<
epoch [17/100] batch [5/5] time 0.053 (0.244) data 0.000 (0.179) loss 1.6421 (1.7088) acc 63.8889 (59.9576) lr 1.8763e-03 eta 0:01:41
epoch [18/100] batch [5/5] time 0.057 (0.227) data 0.000 (0.167) loss 1.6504 (1.6605) acc 64.4737 (62.9508) lr 1.8607e-03 eta 0:01:32
epoch [19/100] batch [5/5] time 0.055 (0.249) data 0.000 (0.186) loss 1.5080 (1.6642) acc 65.2778 (62.7230) lr 1.8443e-03 eta 0:01:40
epoch [20/100] batch [5/5] time 0.067 (0.247) data 0.000 (0.184) loss 1.5769 (1.6625) acc 66.6667 (67.4875) lr 1.8271e-03 eta 0:01:38
epoch [21/100] batch [5/5] time 0.052 (0.249) data 0.000 (0.187) loss 1.5643 (1.6440) acc 70.5882 (65.7272) lr 1.8090e-03 eta 0:01:38
>>> samples [97/160] noisy rate: 0.25 --> 0.21 --> 0.22 <<<
epoch [22/100] batch [5/5] time 0.054 (0.210) data 0.000 (0.147) loss 1.6210 (1.6095) acc 76.5625 (67.8874) lr 1.7902e-03 eta 0:01:22
epoch [23/100] batch [5/5] time 0.066 (0.232) data 0.000 (0.168) loss 1.6757 (1.6179) acc 65.4762 (67.7619) lr 1.7705e-03 eta 0:01:29
epoch [24/100] batch [5/5] time 0.050 (0.242) data 0.000 (0.178) loss 1.6819 (1.5896) acc 63.2353 (69.0012) lr 1.7501e-03 eta 0:01:31
epoch [25/100] batch [5/5] time 0.060 (0.208) data 0.000 (0.146) loss 1.6643 (1.5814) acc 84.2105 (72.6019) lr 1.7290e-03 eta 0:01:17
epoch [26/100] batch [5/5] time 0.056 (0.221) data 0.001 (0.156) loss 1.7429 (1.6081) acc 61.8421 (68.3555) lr 1.7071e-03 eta 0:01:21
>>> samples [100/160] noisy rate: 0.25 --> 0.19 --> 0.22 <<<
epoch [27/100] batch [5/5] time 0.070 (0.269) data 0.000 (0.201) loss 1.5566 (1.5808) acc 68.1818 (69.4935) lr 1.6845e-03 eta 0:01:38
epoch [28/100] batch [5/5] time 0.070 (0.282) data 0.001 (0.220) loss 1.5859 (1.6042) acc 77.0833 (67.7893) lr 1.6613e-03 eta 0:01:41
epoch [29/100] batch [5/5] time 0.052 (0.356) data 0.000 (0.204) loss 1.4335 (1.5918) acc 72.0588 (69.0996) lr 1.6374e-03 eta 0:02:06
epoch [30/100] batch [5/5] time 0.066 (0.260) data 0.000 (0.192) loss 1.6348 (1.5915) acc 71.4286 (71.5473) lr 1.6129e-03 eta 0:01:31
epoch [31/100] batch [5/5] time 0.058 (0.249) data 0.000 (0.183) loss 1.8373 (1.6204) acc 66.6667 (68.7682) lr 1.5878e-03 eta 0:01:25
>>> samples [100/160] noisy rate: 0.25 --> 0.16 --> 0.22 <<<
epoch [32/100] batch [5/5] time 0.068 (0.263) data 0.000 (0.194) loss 1.4808 (1.5767) acc 65.9091 (72.3415) lr 1.5621e-03 eta 0:01:29
epoch [33/100] batch [5/5] time 0.056 (0.295) data 0.001 (0.235) loss 1.5852 (1.5782) acc 64.4737 (68.2630) lr 1.5358e-03 eta 0:01:38
epoch [34/100] batch [5/5] time 0.063 (0.245) data 0.000 (0.178) loss 1.4466 (1.5408) acc 75.0000 (71.1563) lr 1.5090e-03 eta 0:01:20
epoch [35/100] batch [5/5] time 0.068 (0.263) data 0.000 (0.197) loss 1.7757 (1.5699) acc 55.6818 (71.5638) lr 1.4818e-03 eta 0:01:25
epoch [36/100] batch [5/5] time 0.050 (0.251) data 0.000 (0.187) loss 1.5487 (1.5554) acc 75.0000 (73.1474) lr 1.4540e-03 eta 0:01:20
>>> samples [100/160] noisy rate: 0.25 --> 0.15 --> 0.22 <<<
epoch [37/100] batch [5/5] time 0.070 (0.231) data 0.000 (0.167) loss 1.7143 (1.5510) acc 68.1818 (74.6073) lr 1.4258e-03 eta 0:01:12
epoch [38/100] batch [5/5] time 0.060 (0.242) data 0.000 (0.176) loss 1.5288 (1.5475) acc 63.1579 (72.1076) lr 1.3971e-03 eta 0:01:15
epoch [39/100] batch [5/5] time 0.069 (0.252) data 0.000 (0.187) loss 1.6747 (1.5388) acc 71.5909 (72.1019) lr 1.3681e-03 eta 0:01:16
epoch [40/100] batch [5/5] time 0.072 (0.245) data 0.000 (0.180) loss 1.5579 (1.5326) acc 71.1538 (75.0992) lr 1.3387e-03 eta 0:01:13
epoch [41/100] batch [5/5] time 0.054 (0.260) data 0.000 (0.195) loss 1.2847 (1.4911) acc 93.0555 (79.7516) lr 1.3090e-03 eta 0:01:16
>>> samples [100/160] noisy rate: 0.25 --> 0.16 --> 0.22 <<<
epoch [42/100] batch [5/5] time 0.054 (0.253) data 0.000 (0.187) loss 1.6627 (1.5453) acc 84.7222 (73.9331) lr 1.2790e-03 eta 0:01:13
epoch [43/100] batch [5/5] time 0.064 (0.247) data 0.000 (0.180) loss 1.4583 (1.5005) acc 73.8636 (73.6616) lr 1.2487e-03 eta 0:01:10
epoch [44/100] batch [5/5] time 0.053 (0.266) data 0.000 (0.201) loss 1.2310 (1.5009) acc 76.5625 (73.4438) lr 1.2181e-03 eta 0:01:14
epoch [45/100] batch [5/5] time 0.049 (0.246) data 0.000 (0.181) loss 1.6863 (1.5124) acc 76.5625 (75.6688) lr 1.1874e-03 eta 0:01:07
epoch [46/100] batch [5/5] time 0.057 (0.286) data 0.000 (0.222) loss 1.2398 (1.4923) acc 78.7500 (76.9022) lr 1.1564e-03 eta 0:01:17
>>> samples [100/160] noisy rate: 0.25 --> 0.16 --> 0.22 <<<
epoch [47/100] batch [5/5] time 0.059 (0.370) data 0.000 (0.220) loss 1.5701 (1.5183) acc 61.1111 (74.0520) lr 1.1253e-03 eta 0:01:38
epoch [48/100] batch [5/5] time 0.069 (0.250) data 0.000 (0.183) loss 1.7035 (1.5144) acc 80.6818 (77.9545) lr 1.0941e-03 eta 0:01:05
epoch [49/100] batch [5/5] time 0.055 (0.263) data 0.000 (0.199) loss 1.4789 (1.5127) acc 68.4211 (73.1490) lr 1.0628e-03 eta 0:01:07
epoch [50/100] batch [5/5] time 0.055 (0.261) data 0.000 (0.196) loss 1.5308 (1.5022) acc 77.6316 (75.6642) lr 1.0314e-03 eta 0:01:05
epoch [51/100] batch [5/5] time 0.060 (0.240) data 0.000 (0.172) loss 1.7458 (1.4852) acc 67.1053 (78.6593) lr 1.0000e-03 eta 0:00:58
>>> samples [100/160] noisy rate: 0.25 --> 0.14 --> 0.22 <<<
epoch [52/100] batch [5/5] time 0.070 (0.258) data 0.000 (0.193) loss 1.6229 (1.4683) acc 66.3043 (78.4420) lr 9.6859e-04 eta 0:01:02
epoch [53/100] batch [5/5] time 0.065 (0.249) data 0.000 (0.184) loss 1.2558 (1.5285) acc 84.7826 (75.9232) lr 9.3721e-04 eta 0:00:58
epoch [54/100] batch [5/5] time 0.067 (0.245) data 0.000 (0.175) loss 1.4783 (1.5193) acc 71.7391 (77.5480) lr 9.0589e-04 eta 0:00:56
epoch [55/100] batch [5/5] time 0.062 (0.252) data 0.000 (0.186) loss 1.4758 (1.5125) acc 77.5000 (75.4314) lr 8.7467e-04 eta 0:00:56
epoch [56/100] batch [5/5] time 0.065 (0.236) data 0.000 (0.169) loss 1.5619 (1.4746) acc 73.8095 (78.7524) lr 8.4357e-04 eta 0:00:51
>>> samples [103/160] noisy rate: 0.25 --> 0.18 --> 0.21 <<<
epoch [57/100] batch [5/5] time 0.062 (0.265) data 0.000 (0.201) loss 1.6782 (1.4796) acc 78.5714 (76.3760) lr 8.1262e-04 eta 0:00:56
epoch [58/100] batch [5/5] time 0.067 (0.254) data 0.000 (0.184) loss 1.5615 (1.4719) acc 71.4286 (77.7076) lr 7.8186e-04 eta 0:00:53
epoch [59/100] batch [5/5] time 0.055 (0.281) data 0.000 (0.215) loss 1.5772 (1.4596) acc 85.2941 (79.6533) lr 7.5131e-04 eta 0:00:57
epoch [60/100] batch [5/5] time 0.061 (0.268) data 0.000 (0.200) loss 1.5453 (1.4814) acc 75.0000 (80.3644) lr 7.2101e-04 eta 0:00:53
epoch [61/100] batch [5/5] time 0.070 (0.259) data 0.000 (0.192) loss 1.3721 (1.4733) acc 75.0000 (76.2099) lr 6.9098e-04 eta 0:00:50
>>> samples [103/160] noisy rate: 0.25 --> 0.16 --> 0.21 <<<
epoch [62/100] batch [5/5] time 0.053 (0.260) data 0.000 (0.193) loss 1.5518 (1.4476) acc 70.3125 (76.1284) lr 6.6126e-04 eta 0:00:49
epoch [63/100] batch [5/5] time 0.058 (0.259) data 0.000 (0.192) loss 1.5468 (1.4566) acc 81.9444 (77.7525) lr 6.3188e-04 eta 0:00:47
epoch [64/100] batch [5/5] time 0.069 (0.256) data 0.000 (0.190) loss 1.6690 (1.4478) acc 62.5000 (77.6755) lr 6.0285e-04 eta 0:00:46
epoch [65/100] batch [5/5] time 0.068 (0.247) data 0.000 (0.180) loss 1.3703 (1.4587) acc 86.9048 (76.1613) lr 5.7422e-04 eta 0:00:43
epoch [66/100] batch [5/5] time 0.055 (0.232) data 0.000 (0.167) loss 1.4320 (1.4740) acc 78.9474 (76.7193) lr 5.4601e-04 eta 0:00:39
>>> samples [103/160] noisy rate: 0.25 --> 0.15 --> 0.21 <<<
epoch [67/100] batch [5/5] time 0.049 (0.256) data 0.000 (0.189) loss 1.3809 (1.4770) acc 75.0000 (74.6928) lr 5.1825e-04 eta 0:00:42
epoch [68/100] batch [5/5] time 0.068 (0.235) data 0.000 (0.171) loss 1.4110 (1.4478) acc 76.1905 (78.5200) lr 4.9096e-04 eta 0:00:37
epoch [69/100] batch [5/5] time 0.070 (0.254) data 0.000 (0.185) loss 1.5521 (1.4720) acc 65.6250 (77.0685) lr 4.6417e-04 eta 0:00:39
epoch [70/100] batch [5/5] time 0.075 (0.249) data 0.000 (0.182) loss 1.6062 (1.4568) acc 74.0000 (80.8802) lr 4.3792e-04 eta 0:00:37
epoch [71/100] batch [5/5] time 0.063 (0.249) data 0.000 (0.181) loss 1.1013 (1.4776) acc 86.9048 (79.8690) lr 4.1221e-04 eta 0:00:36
>>> samples [103/160] noisy rate: 0.25 --> 0.16 --> 0.21 <<<
epoch [72/100] batch [5/5] time 0.059 (0.245) data 0.000 (0.180) loss 1.4333 (1.4425) acc 80.2632 (78.9052) lr 3.8709e-04 eta 0:00:34
epoch [73/100] batch [5/5] time 0.057 (0.243) data 0.001 (0.175) loss 1.3559 (1.4249) acc 80.8824 (77.9568) lr 3.6258e-04 eta 0:00:32
epoch [74/100] batch [5/5] time 0.061 (0.223) data 0.000 (0.153) loss 1.5558 (1.4619) acc 76.1905 (79.2940) lr 3.3869e-04 eta 0:00:28
epoch [75/100] batch [5/5] time 0.055 (0.243) data 0.000 (0.173) loss 1.2904 (1.4424) acc 85.2941 (81.0412) lr 3.1545e-04 eta 0:00:30
epoch [76/100] batch [5/5] time 0.053 (0.247) data 0.000 (0.179) loss 1.4513 (1.4327) acc 82.8125 (80.0145) lr 2.9289e-04 eta 0:00:29
>>> samples [103/160] noisy rate: 0.25 --> 0.16 --> 0.21 <<<
epoch [77/100] batch [5/5] time 0.074 (0.242) data 0.000 (0.175) loss 1.5180 (1.4434) acc 75.0000 (78.1522) lr 2.7103e-04 eta 0:00:27
epoch [78/100] batch [5/5] time 0.060 (0.252) data 0.000 (0.185) loss 1.5644 (1.4288) acc 77.6316 (82.0344) lr 2.4989e-04 eta 0:00:27
epoch [79/100] batch [5/5] time 0.068 (0.260) data 0.000 (0.190) loss 1.4432 (1.4740) acc 84.0909 (78.7714) lr 2.2949e-04 eta 0:00:27
epoch [80/100] batch [5/5] time 0.070 (0.264) data 0.000 (0.196) loss 1.4533 (1.4333) acc 76.0417 (80.5719) lr 2.0984e-04 eta 0:00:26
epoch [81/100] batch [5/5] time 0.078 (0.265) data 0.000 (0.196) loss 1.3561 (1.4364) acc 79.8077 (80.9093) lr 1.9098e-04 eta 0:00:25
>>> samples [103/160] noisy rate: 0.25 --> 0.12 --> 0.21 <<<
epoch [82/100] batch [5/5] time 0.060 (0.254) data 0.000 (0.187) loss 1.5650 (1.4351) acc 68.4211 (78.5179) lr 1.7292e-04 eta 0:00:22
epoch [83/100] batch [5/5] time 0.064 (0.246) data 0.000 (0.176) loss 1.5545 (1.4508) acc 79.5455 (78.9385) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.065 (0.245) data 0.001 (0.175) loss 1.3337 (1.4378) acc 73.8636 (77.8602) lr 1.3926e-04 eta 0:00:19
epoch [85/100] batch [5/5] time 0.068 (0.261) data 0.000 (0.193) loss 1.5417 (1.4197) acc 70.2381 (79.0615) lr 1.2369e-04 eta 0:00:19
epoch [86/100] batch [5/5] time 0.063 (0.252) data 0.000 (0.181) loss 1.3718 (1.4410) acc 85.0000 (80.6964) lr 1.0899e-04 eta 0:00:17
>>> samples [103/160] noisy rate: 0.25 --> 0.14 --> 0.21 <<<
epoch [87/100] batch [5/5] time 0.066 (0.267) data 0.000 (0.201) loss 1.1260 (1.4143) acc 84.7826 (80.6455) lr 9.5173e-05 eta 0:00:17
epoch [88/100] batch [5/5] time 0.062 (0.256) data 0.000 (0.191) loss 1.4270 (1.4263) acc 79.7619 (79.5822) lr 8.2245e-05 eta 0:00:15
epoch [89/100] batch [5/5] time 0.071 (0.223) data 0.000 (0.154) loss 1.5382 (1.4090) acc 73.9130 (80.2964) lr 7.0224e-05 eta 0:00:12
epoch [90/100] batch [5/5] time 0.076 (0.247) data 0.000 (0.177) loss 1.3929 (1.4433) acc 78.0000 (81.9131) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.067 (0.234) data 0.000 (0.167) loss 1.4621 (1.4177) acc 78.1250 (80.1272) lr 4.8943e-05 eta 0:00:10
>>> samples [103/160] noisy rate: 0.25 --> 0.14 --> 0.21 <<<
epoch [92/100] batch [5/5] time 0.073 (0.246) data 0.000 (0.182) loss 1.4375 (1.4071) acc 78.1250 (79.0179) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.070 (0.255) data 0.001 (0.185) loss 1.3941 (1.4514) acc 71.5909 (78.3627) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.057 (0.242) data 0.000 (0.175) loss 1.4428 (1.4393) acc 83.7500 (79.6829) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.055 (0.246) data 0.000 (0.177) loss 1.2838 (1.4178) acc 79.4118 (81.5907) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.056 (0.270) data 0.000 (0.205) loss 1.3835 (1.4292) acc 76.3158 (78.5426) lr 1.2312e-05 eta 0:00:05
>>> samples [103/160] noisy rate: 0.25 --> 0.17 --> 0.21 <<<
epoch [97/100] batch [5/5] time 0.052 (0.261) data 0.000 (0.198) loss 1.3531 (1.4488) acc 80.8824 (78.3810) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.067 (0.267) data 0.000 (0.203) loss 1.6790 (1.4241) acc 78.4091 (80.9763) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.061 (0.219) data 0.000 (0.156) loss 1.4303 (1.4275) acc 76.2500 (77.1811) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.064 (0.244) data 0.000 (0.175) loss 1.4711 (1.4202) acc 82.5000 (81.0957) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.3, 0.29, 0.25, 0.25, 0.21, 0.19, 0.16, 0.15, 0.16, 0.16, 0.14, 0.18, 0.16, 0.15, 0.16, 0.16, 0.12, 0.14, 0.14, 0.17]
* learned noise rate: [0.15, 0.19, 0.2, 0.2, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:31,  1.15s/it]  2%|▏         | 2/81 [00:01<00:46,  1.71it/s]  5%|▍         | 4/81 [00:01<00:20,  3.76it/s]  7%|▋         | 6/81 [00:01<00:13,  5.62it/s] 10%|▉         | 8/81 [00:01<00:10,  7.25it/s] 12%|█▏        | 10/81 [00:01<00:08,  8.62it/s] 15%|█▍        | 12/81 [00:02<00:06, 10.21it/s] 17%|█▋        | 14/81 [00:02<00:05, 11.44it/s] 20%|█▉        | 16/81 [00:02<00:05, 12.55it/s] 22%|██▏       | 18/81 [00:02<00:04, 13.44it/s] 25%|██▍       | 20/81 [00:02<00:04, 14.12it/s] 27%|██▋       | 22/81 [00:02<00:04, 14.59it/s] 30%|██▉       | 24/81 [00:02<00:03, 14.59it/s] 32%|███▏      | 26/81 [00:02<00:03, 14.94it/s] 35%|███▍      | 28/81 [00:03<00:03, 15.15it/s] 37%|███▋      | 30/81 [00:03<00:03, 15.12it/s] 40%|███▉      | 32/81 [00:03<00:03, 15.27it/s] 42%|████▏     | 34/81 [00:03<00:03, 15.28it/s] 44%|████▍     | 36/81 [00:03<00:02, 15.47it/s] 47%|████▋     | 38/81 [00:03<00:02, 15.12it/s] 49%|████▉     | 40/81 [00:03<00:02, 14.90it/s] 52%|█████▏    | 42/81 [00:04<00:02, 15.04it/s] 54%|█████▍    | 44/81 [00:04<00:02, 15.30it/s] 57%|█████▋    | 46/81 [00:04<00:02, 14.94it/s] 59%|█████▉    | 48/81 [00:04<00:02, 15.21it/s] 62%|██████▏   | 50/81 [00:04<00:02, 15.41it/s] 64%|██████▍   | 52/81 [00:04<00:01, 15.47it/s] 67%|██████▋   | 54/81 [00:04<00:01, 15.27it/s] 69%|██████▉   | 56/81 [00:04<00:01, 15.35it/s] 72%|███████▏  | 58/81 [00:05<00:01, 15.55it/s] 74%|███████▍  | 60/81 [00:05<00:01, 15.71it/s] 77%|███████▋  | 62/81 [00:05<00:01, 15.45it/s] 79%|███████▉  | 64/81 [00:05<00:01, 15.61it/s] 81%|████████▏ | 66/81 [00:05<00:00, 15.74it/s] 84%|████████▍ | 68/81 [00:05<00:00, 15.84it/s] 86%|████████▋ | 70/81 [00:05<00:00, 15.95it/s] 89%|████████▉ | 72/81 [00:05<00:00, 16.03it/s] 91%|█████████▏| 74/81 [00:06<00:00, 16.09it/s] 94%|█████████▍| 76/81 [00:06<00:00, 16.13it/s] 96%|█████████▋| 78/81 [00:06<00:00, 16.16it/s] 99%|█████████▉| 80/81 [00:06<00:00, 16.17it/s]100%|██████████| 81/81 [00:06<00:00, 12.17it/s]
=> result
* total: 8,100
* correct: 4,983
* accuracy: 61.5%
* error: 38.5%
* macro_f1: 58.7%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 485	acc: 53.9%
* class: 1 (Forest)	total: 900	correct: 860	acc: 95.6%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 22	acc: 2.4%
* class: 3 (Highway or Road)	total: 750	correct: 354	acc: 47.2%
* class: 4 (Industrial Buildings)	total: 750	correct: 741	acc: 98.8%
* class: 5 (Pasture Land)	total: 600	correct: 349	acc: 58.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 590	acc: 78.7%
* class: 7 (Residential Buildings)	total: 900	correct: 659	acc: 73.2%
* class: 8 (River)	total: 750	correct: 270	acc: 36.0%
* class: 9 (Sea or Lake)	total: 900	correct: 653	acc: 72.6%
* average: 61.7%
Elapsed: 0:04:17
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_4-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.085 (0.714) data 0.000 (0.216) loss 1.1646 (1.1434) acc 19.5312 (14.0625) lr 2.0000e-03 eta 0:05:53
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [60/160] noisy rate: 0.25 --> 0.42 --> 0.07 <<<
epoch [2/100] batch [5/5] time 0.286 (0.437) data 0.000 (0.168) loss 2.3626 (2.1838) acc 10.0000 (21.6212) lr 1.9995e-03 eta 0:03:34
epoch [3/100] batch [5/5] time 0.036 (0.241) data 0.000 (0.193) loss 1.8957 (2.0405) acc 54.5455 (30.4091) lr 1.9980e-03 eta 0:01:56
epoch [4/100] batch [5/5] time 0.038 (0.443) data 0.001 (0.222) loss 2.0223 (1.9948) acc 27.5000 (35.5420) lr 1.9956e-03 eta 0:03:32
epoch [5/100] batch [5/5] time 0.044 (0.276) data 0.000 (0.166) loss 2.1438 (1.9476) acc 32.5000 (39.6610) lr 1.9921e-03 eta 0:02:10
epoch [6/100] batch [5/5] time 0.043 (0.367) data 0.000 (0.205) loss 2.1227 (1.8407) acc 29.5455 (47.0896) lr 1.9877e-03 eta 0:02:52
>>> samples [72/160] noisy rate: 0.25 --> 0.32 --> 0.12 <<<
epoch [7/100] batch [5/5] time 0.050 (0.221) data 0.000 (0.166) loss 1.7572 (1.8033) acc 56.6667 (49.8704) lr 1.9823e-03 eta 0:01:42
epoch [8/100] batch [5/5] time 0.044 (0.256) data 0.000 (0.204) loss 1.6632 (1.7677) acc 66.0714 (56.1296) lr 1.9759e-03 eta 0:01:57
epoch [9/100] batch [5/5] time 0.039 (0.234) data 0.000 (0.179) loss 1.7275 (1.6964) acc 60.4167 (60.9240) lr 1.9686e-03 eta 0:01:46
epoch [10/100] batch [5/5] time 0.043 (0.220) data 0.000 (0.168) loss 1.4751 (1.6785) acc 55.7692 (59.7813) lr 1.9603e-03 eta 0:01:38
epoch [11/100] batch [5/5] time 0.038 (0.272) data 0.000 (0.153) loss 1.4528 (1.6308) acc 72.7273 (62.8064) lr 1.9511e-03 eta 0:02:01
>>> samples [73/160] noisy rate: 0.25 --> 0.36 --> 0.14 <<<
epoch [12/100] batch [5/5] time 0.062 (0.307) data 0.000 (0.180) loss 1.6783 (1.6575) acc 67.5000 (63.4596) lr 1.9409e-03 eta 0:02:15
epoch [13/100] batch [5/5] time 0.051 (0.227) data 0.000 (0.174) loss 1.6146 (1.6168) acc 69.1176 (66.3317) lr 1.9298e-03 eta 0:01:38
epoch [14/100] batch [5/5] time 0.053 (0.243) data 0.000 (0.184) loss 1.5232 (1.6018) acc 82.3529 (70.8346) lr 1.9178e-03 eta 0:01:44
epoch [15/100] batch [5/5] time 0.046 (0.313) data 0.000 (0.175) loss 1.8869 (1.5664) acc 70.4545 (69.1969) lr 1.9048e-03 eta 0:02:12
epoch [16/100] batch [5/5] time 0.045 (0.252) data 0.000 (0.201) loss 1.7387 (1.5943) acc 68.7500 (69.9020) lr 1.8910e-03 eta 0:01:45
>>> samples [73/160] noisy rate: 0.25 --> 0.36 --> 0.14 <<<
epoch [17/100] batch [5/5] time 0.049 (0.297) data 0.000 (0.242) loss 1.4812 (1.5883) acc 68.7500 (67.7532) lr 1.8763e-03 eta 0:02:03
epoch [18/100] batch [5/5] time 0.046 (0.234) data 0.000 (0.179) loss 1.4485 (1.5962) acc 70.0000 (70.6883) lr 1.8607e-03 eta 0:01:35
epoch [19/100] batch [5/5] time 0.054 (0.299) data 0.000 (0.244) loss 1.4529 (1.5250) acc 68.7500 (70.2183) lr 1.8443e-03 eta 0:02:00
epoch [20/100] batch [5/5] time 0.055 (0.294) data 0.000 (0.240) loss 1.5629 (1.5949) acc 79.1667 (69.3333) lr 1.8271e-03 eta 0:01:57
epoch [21/100] batch [5/5] time 0.037 (0.280) data 0.000 (0.223) loss 1.6775 (1.5502) acc 77.2727 (73.0892) lr 1.8090e-03 eta 0:01:50
>>> samples [76/160] noisy rate: 0.25 --> 0.35 --> 0.17 <<<
epoch [22/100] batch [5/5] time 0.058 (0.279) data 0.000 (0.222) loss 1.4989 (1.5395) acc 75.0000 (71.9247) lr 1.7902e-03 eta 0:01:48
epoch [23/100] batch [5/5] time 0.035 (0.241) data 0.000 (0.180) loss 1.4407 (1.5360) acc 82.5000 (72.7778) lr 1.7705e-03 eta 0:01:32
epoch [24/100] batch [5/5] time 0.060 (0.233) data 0.001 (0.179) loss 1.3684 (1.5098) acc 78.7500 (74.1429) lr 1.7501e-03 eta 0:01:28
epoch [25/100] batch [5/5] time 0.044 (0.240) data 0.000 (0.181) loss 1.7037 (1.5187) acc 79.1667 (74.6681) lr 1.7290e-03 eta 0:01:29
epoch [26/100] batch [5/5] time 0.052 (0.236) data 0.001 (0.181) loss 1.5877 (1.4924) acc 68.3333 (73.8542) lr 1.7071e-03 eta 0:01:27
>>> samples [77/160] noisy rate: 0.25 --> 0.31 --> 0.17 <<<
epoch [27/100] batch [5/5] time 0.056 (0.234) data 0.000 (0.181) loss 1.4092 (1.4972) acc 75.0000 (71.9524) lr 1.6845e-03 eta 0:01:25
epoch [28/100] batch [5/5] time 0.042 (0.236) data 0.000 (0.183) loss 1.3607 (1.4595) acc 65.3846 (76.8052) lr 1.6613e-03 eta 0:01:24
epoch [29/100] batch [5/5] time 0.055 (0.248) data 0.000 (0.194) loss 1.4284 (1.4600) acc 76.5625 (80.9673) lr 1.6374e-03 eta 0:01:27
epoch [30/100] batch [5/5] time 0.058 (0.243) data 0.000 (0.183) loss 1.4850 (1.4649) acc 70.3125 (76.4034) lr 1.6129e-03 eta 0:01:25
epoch [31/100] batch [5/5] time 0.045 (0.226) data 0.000 (0.172) loss 1.5213 (1.4250) acc 71.4286 (78.7395) lr 1.5878e-03 eta 0:01:17
>>> samples [79/160] noisy rate: 0.25 --> 0.32 --> 0.16 <<<
epoch [32/100] batch [5/5] time 0.056 (0.259) data 0.000 (0.200) loss 1.7562 (1.4979) acc 67.8571 (72.8716) lr 1.5621e-03 eta 0:01:28
epoch [33/100] batch [5/5] time 0.038 (0.313) data 0.000 (0.182) loss 1.4785 (1.4826) acc 63.6364 (71.9306) lr 1.5358e-03 eta 0:01:44
epoch [34/100] batch [5/5] time 0.044 (0.266) data 0.000 (0.214) loss 1.5029 (1.4676) acc 82.1429 (73.4805) lr 1.5090e-03 eta 0:01:27
epoch [35/100] batch [5/5] time 0.051 (0.304) data 0.000 (0.247) loss 1.6756 (1.4937) acc 73.3333 (76.6914) lr 1.4818e-03 eta 0:01:38
epoch [36/100] batch [5/5] time 0.050 (0.283) data 0.000 (0.227) loss 1.4263 (1.4608) acc 70.0000 (73.9209) lr 1.4540e-03 eta 0:01:30
>>> samples [80/160] noisy rate: 0.25 --> 0.30 --> 0.16 <<<
epoch [37/100] batch [5/5] time 0.050 (0.247) data 0.000 (0.193) loss 1.5601 (1.4514) acc 76.7857 (77.0562) lr 1.4258e-03 eta 0:01:17
epoch [38/100] batch [5/5] time 0.049 (0.269) data 0.000 (0.213) loss 1.2859 (1.4430) acc 87.5000 (75.4749) lr 1.3971e-03 eta 0:01:23
epoch [39/100] batch [5/5] time 0.056 (0.240) data 0.000 (0.185) loss 1.2142 (1.4351) acc 79.4118 (74.4481) lr 1.3681e-03 eta 0:01:13
epoch [40/100] batch [5/5] time 0.052 (0.254) data 0.000 (0.202) loss 1.4574 (1.4181) acc 76.4706 (76.7364) lr 1.3387e-03 eta 0:01:16
epoch [41/100] batch [5/5] time 0.058 (0.248) data 0.000 (0.192) loss 1.2969 (1.4647) acc 90.6250 (73.0024) lr 1.3090e-03 eta 0:01:13
>>> samples [81/160] noisy rate: 0.25 --> 0.22 --> 0.17 <<<
epoch [42/100] batch [5/5] time 0.053 (0.254) data 0.000 (0.199) loss 1.3025 (1.4020) acc 83.8235 (80.4105) lr 1.2790e-03 eta 0:01:13
epoch [43/100] batch [5/5] time 0.060 (0.246) data 0.000 (0.185) loss 1.5608 (1.4289) acc 73.6842 (74.1486) lr 1.2487e-03 eta 0:01:10
epoch [44/100] batch [5/5] time 0.062 (0.250) data 0.000 (0.191) loss 1.3065 (1.4184) acc 83.3333 (80.0464) lr 1.2181e-03 eta 0:01:09
epoch [45/100] batch [5/5] time 0.048 (0.264) data 0.000 (0.208) loss 1.4974 (1.4295) acc 81.2500 (80.8717) lr 1.1874e-03 eta 0:01:12
epoch [46/100] batch [5/5] time 0.048 (0.309) data 0.000 (0.255) loss 1.3864 (1.4321) acc 81.2500 (76.9433) lr 1.1564e-03 eta 0:01:23
>>> samples [81/160] noisy rate: 0.25 --> 0.31 --> 0.17 <<<
epoch [47/100] batch [5/5] time 0.042 (0.348) data 0.000 (0.207) loss 1.3212 (1.3803) acc 77.2727 (78.4474) lr 1.1253e-03 eta 0:01:32
epoch [48/100] batch [5/5] time 0.050 (0.292) data 0.000 (0.238) loss 1.4515 (1.3873) acc 89.2857 (81.3339) lr 1.0941e-03 eta 0:01:15
epoch [49/100] batch [5/5] time 0.051 (0.260) data 0.000 (0.199) loss 1.4360 (1.4210) acc 76.7857 (79.8462) lr 1.0628e-03 eta 0:01:06
epoch [50/100] batch [5/5] time 0.063 (0.244) data 0.000 (0.187) loss 1.3786 (1.3821) acc 79.7619 (79.9681) lr 1.0314e-03 eta 0:01:00
epoch [51/100] batch [5/5] time 0.055 (0.258) data 0.000 (0.199) loss 1.5802 (1.4338) acc 57.8947 (75.9268) lr 1.0000e-03 eta 0:01:03
>>> samples [82/160] noisy rate: 0.25 --> 0.31 --> 0.18 <<<
epoch [52/100] batch [5/5] time 0.047 (0.281) data 0.000 (0.225) loss 1.4043 (1.4406) acc 87.5000 (76.3889) lr 9.6859e-04 eta 0:01:07
epoch [53/100] batch [5/5] time 0.064 (0.264) data 0.000 (0.206) loss 1.4700 (1.4075) acc 71.0526 (77.7569) lr 9.3721e-04 eta 0:01:02
epoch [54/100] batch [5/5] time 0.047 (0.260) data 0.000 (0.202) loss 1.4224 (1.4093) acc 75.0000 (79.7320) lr 9.0589e-04 eta 0:00:59
epoch [55/100] batch [5/5] time 0.046 (0.236) data 0.001 (0.177) loss 1.2688 (1.4053) acc 87.5000 (76.4370) lr 8.7467e-04 eta 0:00:53
epoch [56/100] batch [5/5] time 0.057 (0.282) data 0.001 (0.228) loss 1.4688 (1.4218) acc 81.5789 (78.5870) lr 8.4357e-04 eta 0:01:02
>>> samples [83/160] noisy rate: 0.25 --> 0.34 --> 0.18 <<<
epoch [57/100] batch [5/5] time 0.062 (0.283) data 0.000 (0.225) loss 1.2705 (1.3645) acc 71.0526 (77.8884) lr 8.1262e-04 eta 0:01:00
epoch [58/100] batch [5/5] time 0.052 (0.297) data 0.000 (0.235) loss 1.2639 (1.3718) acc 85.9375 (80.8893) lr 7.8186e-04 eta 0:01:02
epoch [59/100] batch [5/5] time 0.056 (0.273) data 0.000 (0.213) loss 1.2992 (1.3945) acc 72.0588 (78.7653) lr 7.5131e-04 eta 0:00:55
epoch [60/100] batch [5/5] time 0.039 (0.320) data 0.001 (0.172) loss 1.3124 (1.4042) acc 90.9091 (77.0735) lr 7.2101e-04 eta 0:01:03
epoch [61/100] batch [5/5] time 0.064 (0.242) data 0.000 (0.183) loss 1.3701 (1.3929) acc 82.1429 (79.3773) lr 6.9098e-04 eta 0:00:47
>>> samples [84/160] noisy rate: 0.25 --> 0.32 --> 0.18 <<<
epoch [62/100] batch [5/5] time 0.055 (0.240) data 0.000 (0.177) loss 1.4118 (1.3674) acc 85.5263 (82.3744) lr 6.6126e-04 eta 0:00:45
epoch [63/100] batch [5/5] time 0.055 (0.262) data 0.000 (0.206) loss 1.3197 (1.3657) acc 81.9444 (78.3893) lr 6.3188e-04 eta 0:00:48
epoch [64/100] batch [5/5] time 0.057 (0.235) data 0.000 (0.176) loss 1.5075 (1.3648) acc 77.5000 (81.9038) lr 6.0285e-04 eta 0:00:42
epoch [65/100] batch [5/5] time 0.048 (0.274) data 0.000 (0.218) loss 1.5239 (1.3680) acc 85.9375 (83.1352) lr 5.7422e-04 eta 0:00:48
epoch [66/100] batch [5/5] time 0.058 (0.236) data 0.000 (0.179) loss 1.1594 (1.3590) acc 76.3158 (81.4373) lr 5.4601e-04 eta 0:00:40
>>> samples [86/160] noisy rate: 0.25 --> 0.31 --> 0.19 <<<
epoch [67/100] batch [5/5] time 0.041 (0.240) data 0.000 (0.182) loss 1.4248 (1.3872) acc 80.7692 (80.3077) lr 5.1825e-04 eta 0:00:39
epoch [68/100] batch [5/5] time 0.044 (0.221) data 0.000 (0.165) loss 1.4155 (1.3838) acc 77.0833 (77.5950) lr 4.9096e-04 eta 0:00:35
epoch [69/100] batch [5/5] time 0.051 (0.236) data 0.000 (0.183) loss 1.3968 (1.3771) acc 76.3889 (77.2588) lr 4.6417e-04 eta 0:00:36
epoch [70/100] batch [5/5] time 0.051 (0.257) data 0.000 (0.199) loss 1.3174 (1.3536) acc 83.9286 (83.1282) lr 4.3792e-04 eta 0:00:38
epoch [71/100] batch [5/5] time 0.059 (0.211) data 0.000 (0.151) loss 1.6701 (1.3698) acc 86.1111 (80.9477) lr 4.1221e-04 eta 0:00:30
>>> samples [86/160] noisy rate: 0.25 --> 0.33 --> 0.19 <<<
epoch [72/100] batch [5/5] time 0.052 (0.239) data 0.000 (0.179) loss 1.3245 (1.3545) acc 77.9412 (80.9477) lr 3.8709e-04 eta 0:00:33
epoch [73/100] batch [5/5] time 0.060 (0.228) data 0.000 (0.171) loss 1.3488 (1.3577) acc 77.7778 (78.8546) lr 3.6258e-04 eta 0:00:30
epoch [74/100] batch [5/5] time 0.053 (0.219) data 0.000 (0.160) loss 1.2519 (1.3352) acc 76.5625 (82.8026) lr 3.3869e-04 eta 0:00:28
epoch [75/100] batch [5/5] time 0.051 (0.265) data 0.001 (0.203) loss 1.5123 (1.3786) acc 73.2143 (78.3006) lr 3.1545e-04 eta 0:00:33
epoch [76/100] batch [5/5] time 0.056 (0.244) data 0.000 (0.185) loss 1.4171 (1.3789) acc 72.2222 (81.4628) lr 2.9289e-04 eta 0:00:29
>>> samples [88/160] noisy rate: 0.25 --> 0.31 --> 0.19 <<<
epoch [77/100] batch [5/5] time 0.051 (0.229) data 0.000 (0.167) loss 1.3142 (1.3459) acc 83.3333 (80.5325) lr 2.7103e-04 eta 0:00:26
epoch [78/100] batch [5/5] time 0.057 (0.231) data 0.000 (0.173) loss 1.0737 (1.3699) acc 81.2500 (79.2738) lr 2.4989e-04 eta 0:00:25
epoch [79/100] batch [5/5] time 0.053 (0.229) data 0.000 (0.166) loss 1.4369 (1.3404) acc 75.0000 (80.1822) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.062 (0.240) data 0.000 (0.179) loss 1.1689 (1.3611) acc 76.3158 (79.7867) lr 2.0984e-04 eta 0:00:24
epoch [81/100] batch [5/5] time 0.061 (0.243) data 0.000 (0.184) loss 1.2956 (1.3311) acc 83.3333 (80.0983) lr 1.9098e-04 eta 0:00:23
>>> samples [88/160] noisy rate: 0.25 --> 0.32 --> 0.19 <<<
epoch [82/100] batch [5/5] time 0.054 (0.225) data 0.000 (0.169) loss 1.2656 (1.3438) acc 73.6842 (79.6110) lr 1.7292e-04 eta 0:00:20
epoch [83/100] batch [5/5] time 0.065 (0.237) data 0.000 (0.176) loss 1.5424 (1.3481) acc 75.0000 (78.5384) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.052 (0.279) data 0.000 (0.217) loss 1.4511 (1.3362) acc 72.0588 (82.5245) lr 1.3926e-04 eta 0:00:22
epoch [85/100] batch [5/5] time 0.067 (0.267) data 0.000 (0.203) loss 1.2171 (1.3590) acc 79.5455 (78.7216) lr 1.2369e-04 eta 0:00:19
epoch [86/100] batch [5/5] time 0.068 (0.253) data 0.000 (0.192) loss 1.4315 (1.3437) acc 73.8095 (81.6209) lr 1.0899e-04 eta 0:00:17
>>> samples [90/160] noisy rate: 0.25 --> 0.32 --> 0.21 <<<
epoch [87/100] batch [5/5] time 0.047 (0.238) data 0.000 (0.176) loss 1.0721 (1.3330) acc 90.0000 (79.8959) lr 9.5173e-05 eta 0:00:15
epoch [88/100] batch [5/5] time 0.056 (0.367) data 0.000 (0.221) loss 1.1137 (1.2998) acc 84.3750 (82.7889) lr 8.2245e-05 eta 0:00:21
epoch [89/100] batch [5/5] time 0.047 (0.262) data 0.000 (0.198) loss 1.2163 (1.3375) acc 88.3333 (82.2735) lr 7.0224e-05 eta 0:00:14
epoch [90/100] batch [5/5] time 0.062 (0.255) data 0.000 (0.189) loss 1.3933 (1.3438) acc 77.5000 (80.0803) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.050 (0.271) data 0.001 (0.211) loss 1.2455 (1.3221) acc 88.3333 (83.6713) lr 4.8943e-05 eta 0:00:12
>>> samples [92/160] noisy rate: 0.25 --> 0.32 --> 0.22 <<<
epoch [92/100] batch [5/5] time 0.071 (0.293) data 0.000 (0.225) loss 1.4640 (1.3030) acc 80.6818 (80.1327) lr 3.9706e-05 eta 0:00:11
epoch [93/100] batch [5/5] time 0.052 (0.283) data 0.001 (0.223) loss 1.2842 (1.3119) acc 75.0000 (82.6754) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.056 (0.260) data 0.000 (0.198) loss 1.4303 (1.3114) acc 63.8889 (79.5431) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.070 (0.254) data 0.001 (0.191) loss 1.5367 (1.3183) acc 70.2381 (80.7326) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.072 (0.323) data 0.000 (0.171) loss 1.5530 (1.3489) acc 77.0833 (77.7235) lr 1.2312e-05 eta 0:00:06
>>> samples [93/160] noisy rate: 0.25 --> 0.34 --> 0.22 <<<
epoch [97/100] batch [5/5] time 0.064 (0.280) data 0.000 (0.217) loss 1.3725 (1.2915) acc 82.1429 (83.2121) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.056 (0.277) data 0.000 (0.217) loss 1.3761 (1.3187) acc 84.7222 (80.5895) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.048 (0.258) data 0.000 (0.191) loss 1.2903 (1.2833) acc 81.6667 (83.4159) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.059 (0.278) data 0.000 (0.213) loss 1.4236 (1.3057) acc 93.0555 (82.0000) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.42, 0.32, 0.36, 0.36, 0.35, 0.31, 0.32, 0.3, 0.22, 0.31, 0.31, 0.34, 0.32, 0.31, 0.33, 0.31, 0.32, 0.32, 0.32, 0.34]
* learned noise rate: [0.07, 0.12, 0.14, 0.14, 0.17, 0.17, 0.16, 0.16, 0.17, 0.17, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.21, 0.22, 0.22]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:20,  1.00s/it]  2%|▏         | 2/81 [00:01<00:39,  2.01it/s]  5%|▍         | 4/81 [00:01<00:18,  4.28it/s]  7%|▋         | 6/81 [00:01<00:11,  6.27it/s] 10%|▉         | 8/81 [00:01<00:09,  7.79it/s] 12%|█▏        | 10/81 [00:01<00:07,  9.05it/s] 15%|█▍        | 12/81 [00:01<00:07,  9.51it/s] 17%|█▋        | 14/81 [00:02<00:06,  9.67it/s] 20%|█▉        | 16/81 [00:02<00:05, 11.06it/s] 22%|██▏       | 18/81 [00:02<00:05, 12.16it/s] 25%|██▍       | 20/81 [00:02<00:04, 13.12it/s] 27%|██▋       | 22/81 [00:02<00:04, 13.63it/s] 30%|██▉       | 24/81 [00:02<00:04, 14.24it/s] 32%|███▏      | 26/81 [00:02<00:03, 14.50it/s] 35%|███▍      | 28/81 [00:03<00:03, 14.91it/s] 37%|███▋      | 30/81 [00:03<00:03, 14.59it/s] 40%|███▉      | 32/81 [00:03<00:03, 14.61it/s] 42%|████▏     | 34/81 [00:03<00:03, 14.93it/s] 44%|████▍     | 36/81 [00:03<00:02, 15.14it/s] 47%|████▋     | 38/81 [00:03<00:02, 15.37it/s] 49%|████▉     | 40/81 [00:03<00:02, 15.44it/s] 52%|█████▏    | 42/81 [00:03<00:02, 15.42it/s] 54%|█████▍    | 44/81 [00:04<00:02, 15.55it/s] 57%|█████▋    | 46/81 [00:04<00:02, 15.61it/s] 59%|█████▉    | 48/81 [00:04<00:02, 15.33it/s] 62%|██████▏   | 50/81 [00:04<00:02, 15.13it/s] 64%|██████▍   | 52/81 [00:04<00:01, 15.34it/s] 67%|██████▋   | 54/81 [00:04<00:01, 15.44it/s] 69%|██████▉   | 56/81 [00:04<00:01, 15.19it/s] 72%|███████▏  | 58/81 [00:05<00:01, 15.00it/s] 74%|███████▍  | 60/81 [00:05<00:01, 15.25it/s] 77%|███████▋  | 62/81 [00:05<00:01, 15.00it/s] 79%|███████▉  | 64/81 [00:05<00:01, 14.80it/s] 81%|████████▏ | 66/81 [00:05<00:01, 14.77it/s] 84%|████████▍ | 68/81 [00:05<00:00, 15.04it/s] 86%|████████▋ | 70/81 [00:05<00:00, 15.31it/s] 89%|████████▉ | 72/81 [00:05<00:00, 15.50it/s] 91%|█████████▏| 74/81 [00:06<00:00, 15.64it/s] 94%|█████████▍| 76/81 [00:06<00:00, 15.71it/s] 96%|█████████▋| 78/81 [00:06<00:00, 15.78it/s] 99%|█████████▉| 80/81 [00:06<00:00, 15.85it/s]100%|██████████| 81/81 [00:06<00:00, 12.08it/s]
=> result
* total: 8,100
* correct: 4,287
* accuracy: 52.9%
* error: 47.1%
* macro_f1: 48.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 40	acc: 4.4%
* class: 1 (Forest)	total: 900	correct: 878	acc: 97.6%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 16	acc: 1.8%
* class: 3 (Highway or Road)	total: 750	correct: 434	acc: 57.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 728	acc: 97.1%
* class: 5 (Pasture Land)	total: 600	correct: 376	acc: 62.7%
* class: 6 (Permanent Crop Land)	total: 750	correct: 562	acc: 74.9%
* class: 7 (Residential Buildings)	total: 900	correct: 806	acc: 89.6%
* class: 8 (River)	total: 750	correct: 360	acc: 48.0%
* class: 9 (Sea or Lake)	total: 900	correct: 87	acc: 9.7%
* average: 54.4%
Elapsed: 0:04:24
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_6-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.638) data 0.000 (0.216) loss 1.2139 (1.1542) acc 9.3750 (14.6875) lr 2.0000e-03 eta 0:05:15
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [40/160] noisy rate: 0.38 --> 0.38 --> 0.08 <<<
epoch [2/100] batch [5/5] time 0.035 (0.312) data 0.000 (0.177) loss 2.1795 (2.1193) acc 39.2857 (27.0893) lr 1.9995e-03 eta 0:02:32
epoch [3/100] batch [5/5] time 0.038 (0.308) data 0.000 (0.178) loss 1.7907 (1.9438) acc 40.0000 (29.3194) lr 1.9980e-03 eta 0:02:29
epoch [4/100] batch [5/5] time 0.026 (0.317) data 0.000 (0.174) loss 1.3869 (1.6900) acc 85.0000 (61.2540) lr 1.9956e-03 eta 0:02:32
epoch [5/100] batch [5/5] time 0.036 (0.222) data 0.000 (0.175) loss 1.9576 (1.7470) acc 30.5556 (43.5000) lr 1.9921e-03 eta 0:01:45
epoch [6/100] batch [5/5] time 0.039 (0.228) data 0.000 (0.183) loss 1.3775 (1.5893) acc 75.0000 (53.0694) lr 1.9877e-03 eta 0:01:47
>>> samples [53/160] noisy rate: 0.38 --> 0.39 --> 0.11 <<<
epoch [7/100] batch [5/5] time 0.032 (0.327) data 0.000 (0.228) loss 1.7120 (1.5859) acc 58.3333 (63.8333) lr 1.9823e-03 eta 0:02:31
epoch [8/100] batch [5/5] time 0.038 (0.352) data 0.000 (0.182) loss 1.6515 (1.4818) acc 50.0000 (63.3666) lr 1.9759e-03 eta 0:02:42
epoch [9/100] batch [5/5] time 0.040 (0.230) data 0.000 (0.183) loss 1.5349 (1.5186) acc 66.6667 (63.7024) lr 1.9686e-03 eta 0:01:44
epoch [10/100] batch [5/5] time 0.224 (0.262) data 0.000 (0.177) loss 1.7169 (1.4897) acc 54.1667 (69.2094) lr 1.9603e-03 eta 0:01:58
epoch [11/100] batch [5/5] time 0.037 (0.293) data 0.000 (0.179) loss 1.3927 (1.4446) acc 69.4444 (68.8207) lr 1.9511e-03 eta 0:02:10
>>> samples [62/160] noisy rate: 0.38 --> 0.39 --> 0.16 <<<
epoch [12/100] batch [5/5] time 0.043 (0.217) data 0.000 (0.166) loss 1.1839 (1.3485) acc 76.9231 (69.4503) lr 1.9409e-03 eta 0:01:35
epoch [13/100] batch [5/5] time 0.033 (0.277) data 0.000 (0.161) loss 1.2304 (1.3025) acc 87.5000 (75.3873) lr 1.9298e-03 eta 0:02:00
epoch [14/100] batch [5/5] time 0.037 (0.278) data 0.000 (0.154) loss 1.4862 (1.3332) acc 70.4545 (78.4492) lr 1.9178e-03 eta 0:01:59
epoch [15/100] batch [5/5] time 0.048 (0.214) data 0.000 (0.165) loss 1.2445 (1.2222) acc 87.5000 (78.8301) lr 1.9048e-03 eta 0:01:31
epoch [16/100] batch [5/5] time 0.043 (0.244) data 0.000 (0.195) loss 1.3829 (1.2507) acc 70.4545 (76.1742) lr 1.8910e-03 eta 0:01:42
>>> samples [67/160] noisy rate: 0.38 --> 0.36 --> 0.18 <<<
epoch [17/100] batch [5/5] time 0.035 (0.314) data 0.000 (0.185) loss 1.2342 (1.2644) acc 82.5000 (77.6190) lr 1.8763e-03 eta 0:02:10
epoch [18/100] batch [5/5] time 0.046 (0.251) data 0.000 (0.197) loss 1.2534 (1.2220) acc 90.3846 (82.6811) lr 1.8607e-03 eta 0:01:42
epoch [19/100] batch [5/5] time 0.039 (0.238) data 0.000 (0.189) loss 1.5108 (1.2355) acc 56.2500 (78.0918) lr 1.8443e-03 eta 0:01:36
epoch [20/100] batch [5/5] time 0.053 (0.237) data 0.000 (0.183) loss 1.3148 (1.2059) acc 76.5625 (77.6458) lr 1.8271e-03 eta 0:01:34
epoch [21/100] batch [5/5] time 0.049 (0.234) data 0.000 (0.175) loss 1.2844 (1.1846) acc 70.3125 (83.1525) lr 1.8090e-03 eta 0:01:32
>>> samples [68/160] noisy rate: 0.38 --> 0.35 --> 0.19 <<<
epoch [22/100] batch [5/5] time 0.043 (0.244) data 0.000 (0.191) loss 1.2248 (1.1856) acc 81.2500 (80.9288) lr 1.7902e-03 eta 0:01:35
epoch [23/100] batch [5/5] time 0.046 (0.229) data 0.000 (0.177) loss 1.0017 (1.2086) acc 90.3846 (81.0769) lr 1.7705e-03 eta 0:01:28
epoch [24/100] batch [5/5] time 0.048 (0.215) data 0.000 (0.161) loss 1.1117 (1.2173) acc 75.0000 (78.5625) lr 1.7501e-03 eta 0:01:21
epoch [25/100] batch [5/5] time 0.044 (0.232) data 0.000 (0.180) loss 1.5179 (1.1821) acc 66.6667 (82.2550) lr 1.7290e-03 eta 0:01:27
epoch [26/100] batch [5/5] time 0.045 (0.211) data 0.000 (0.156) loss 1.0998 (1.1821) acc 89.5833 (80.2335) lr 1.7071e-03 eta 0:01:18
>>> samples [70/160] noisy rate: 0.38 --> 0.34 --> 0.20 <<<
epoch [27/100] batch [5/5] time 0.054 (0.242) data 0.000 (0.186) loss 1.1118 (1.1569) acc 83.3333 (84.2665) lr 1.6845e-03 eta 0:01:28
epoch [28/100] batch [5/5] time 0.051 (0.233) data 0.000 (0.179) loss 1.1126 (1.1818) acc 82.8125 (81.6426) lr 1.6613e-03 eta 0:01:23
epoch [29/100] batch [5/5] time 0.047 (0.224) data 0.001 (0.170) loss 1.3782 (1.1676) acc 80.0000 (78.4195) lr 1.6374e-03 eta 0:01:19
epoch [30/100] batch [5/5] time 0.051 (0.235) data 0.000 (0.183) loss 1.2037 (1.1560) acc 78.3333 (83.0916) lr 1.6129e-03 eta 0:01:22
epoch [31/100] batch [5/5] time 0.048 (0.228) data 0.000 (0.171) loss 1.0401 (1.0991) acc 70.0000 (82.6789) lr 1.5878e-03 eta 0:01:18
>>> samples [74/160] noisy rate: 0.38 --> 0.34 --> 0.19 <<<
epoch [32/100] batch [5/5] time 0.042 (0.241) data 0.000 (0.188) loss 1.5239 (1.1321) acc 65.3846 (82.1037) lr 1.5621e-03 eta 0:01:21
epoch [33/100] batch [5/5] time 0.057 (0.248) data 0.000 (0.191) loss 0.9834 (1.1186) acc 94.1176 (85.9767) lr 1.5358e-03 eta 0:01:22
epoch [34/100] batch [5/5] time 0.045 (0.296) data 0.000 (0.168) loss 1.0981 (1.0598) acc 78.5714 (85.8109) lr 1.5090e-03 eta 0:01:37
epoch [35/100] batch [5/5] time 0.043 (0.244) data 0.000 (0.187) loss 1.2041 (1.1059) acc 80.3571 (84.2557) lr 1.4818e-03 eta 0:01:19
epoch [36/100] batch [5/5] time 0.046 (0.243) data 0.001 (0.187) loss 1.0816 (1.1590) acc 88.3333 (84.4889) lr 1.4540e-03 eta 0:01:17
>>> samples [75/160] noisy rate: 0.38 --> 0.33 --> 0.19 <<<
epoch [37/100] batch [5/5] time 0.044 (0.202) data 0.000 (0.147) loss 1.3508 (1.1224) acc 84.6154 (81.1957) lr 1.4258e-03 eta 0:01:03
epoch [38/100] batch [5/5] time 0.048 (0.209) data 0.000 (0.151) loss 1.1957 (1.0845) acc 85.7143 (84.0744) lr 1.3971e-03 eta 0:01:04
epoch [39/100] batch [5/5] time 0.049 (0.208) data 0.000 (0.151) loss 0.9985 (1.0950) acc 85.7143 (84.5460) lr 1.3681e-03 eta 0:01:03
epoch [40/100] batch [5/5] time 0.044 (0.242) data 0.000 (0.187) loss 0.8919 (1.1157) acc 89.2857 (83.8343) lr 1.3387e-03 eta 0:01:12
epoch [41/100] batch [5/5] time 0.047 (0.252) data 0.000 (0.198) loss 1.3282 (1.1080) acc 85.7143 (83.8424) lr 1.3090e-03 eta 0:01:14
>>> samples [75/160] noisy rate: 0.38 --> 0.36 --> 0.19 <<<
epoch [42/100] batch [5/5] time 0.055 (0.242) data 0.001 (0.190) loss 1.1752 (1.0746) acc 75.0000 (86.9435) lr 1.2790e-03 eta 0:01:10
epoch [43/100] batch [5/5] time 0.046 (0.307) data 0.000 (0.178) loss 1.3694 (1.1023) acc 76.6667 (83.5897) lr 1.2487e-03 eta 0:01:27
epoch [44/100] batch [5/5] time 0.058 (0.244) data 0.000 (0.186) loss 1.1164 (1.0777) acc 87.5000 (84.2981) lr 1.2181e-03 eta 0:01:08
epoch [45/100] batch [5/5] time 0.048 (0.230) data 0.000 (0.177) loss 0.8486 (1.1227) acc 91.6667 (84.2453) lr 1.1874e-03 eta 0:01:03
epoch [46/100] batch [5/5] time 0.062 (0.232) data 0.000 (0.173) loss 1.1798 (1.0780) acc 77.5000 (85.6154) lr 1.1564e-03 eta 0:01:02
>>> samples [78/160] noisy rate: 0.38 --> 0.36 --> 0.22 <<<
epoch [47/100] batch [5/5] time 0.054 (0.235) data 0.000 (0.178) loss 1.1482 (1.1330) acc 87.5000 (81.6869) lr 1.1253e-03 eta 0:01:02
epoch [48/100] batch [5/5] time 0.050 (0.219) data 0.000 (0.161) loss 1.1112 (1.1142) acc 82.1429 (85.9454) lr 1.0941e-03 eta 0:00:56
epoch [49/100] batch [5/5] time 0.046 (0.201) data 0.000 (0.146) loss 1.1527 (1.1145) acc 78.3333 (82.6995) lr 1.0628e-03 eta 0:00:51
epoch [50/100] batch [5/5] time 0.063 (0.236) data 0.000 (0.177) loss 1.1193 (1.0920) acc 81.2500 (85.8562) lr 1.0314e-03 eta 0:00:58
epoch [51/100] batch [5/5] time 0.041 (0.224) data 0.000 (0.166) loss 1.0778 (1.0665) acc 92.3077 (87.1368) lr 1.0000e-03 eta 0:00:54
>>> samples [79/160] noisy rate: 0.38 --> 0.36 --> 0.23 <<<
epoch [52/100] batch [5/5] time 0.057 (0.238) data 0.000 (0.179) loss 1.0596 (1.0972) acc 93.0555 (84.1042) lr 9.6859e-04 eta 0:00:57
epoch [53/100] batch [5/5] time 0.046 (0.238) data 0.000 (0.173) loss 0.8451 (1.0968) acc 92.3077 (86.4130) lr 9.3721e-04 eta 0:00:55
epoch [54/100] batch [5/5] time 0.047 (0.244) data 0.000 (0.181) loss 1.2407 (1.0903) acc 65.3846 (84.4742) lr 9.0589e-04 eta 0:00:56
epoch [55/100] batch [5/5] time 0.059 (0.230) data 0.000 (0.170) loss 1.0785 (1.0935) acc 86.7647 (84.2338) lr 8.7467e-04 eta 0:00:51
epoch [56/100] batch [5/5] time 0.061 (0.252) data 0.000 (0.193) loss 1.2324 (1.0938) acc 73.6842 (87.0044) lr 8.4357e-04 eta 0:00:55
>>> samples [79/160] noisy rate: 0.38 --> 0.39 --> 0.23 <<<
epoch [57/100] batch [5/5] time 0.060 (0.251) data 0.000 (0.192) loss 1.2164 (1.0967) acc 84.2105 (84.8663) lr 8.1262e-04 eta 0:00:53
epoch [58/100] batch [5/5] time 0.052 (0.220) data 0.000 (0.165) loss 0.9495 (1.0695) acc 95.3125 (85.7597) lr 7.8186e-04 eta 0:00:46
epoch [59/100] batch [5/5] time 0.449 (0.302) data 0.000 (0.170) loss 0.8542 (1.0814) acc 89.2857 (84.7024) lr 7.5131e-04 eta 0:01:01
epoch [60/100] batch [5/5] time 0.057 (0.240) data 0.000 (0.183) loss 1.2174 (1.0569) acc 85.2941 (85.4909) lr 7.2101e-04 eta 0:00:48
epoch [61/100] batch [5/5] time 0.046 (0.204) data 0.000 (0.148) loss 0.9073 (1.0513) acc 88.4615 (85.9660) lr 6.9098e-04 eta 0:00:39
>>> samples [79/160] noisy rate: 0.38 --> 0.40 --> 0.23 <<<
epoch [62/100] batch [5/5] time 0.045 (0.250) data 0.000 (0.193) loss 1.0864 (1.0711) acc 80.3571 (86.8214) lr 6.6126e-04 eta 0:00:47
epoch [63/100] batch [5/5] time 0.047 (0.248) data 0.001 (0.192) loss 1.1801 (1.0647) acc 83.3333 (88.2549) lr 6.3188e-04 eta 0:00:45
epoch [64/100] batch [5/5] time 0.043 (0.227) data 0.000 (0.171) loss 0.9637 (1.0445) acc 87.5000 (87.6981) lr 6.0285e-04 eta 0:00:40
epoch [65/100] batch [5/5] time 0.056 (0.252) data 0.000 (0.194) loss 0.9807 (1.0681) acc 94.1176 (85.5792) lr 5.7422e-04 eta 0:00:44
epoch [66/100] batch [5/5] time 0.057 (0.226) data 0.000 (0.169) loss 1.2193 (1.0702) acc 82.3529 (87.5682) lr 5.4601e-04 eta 0:00:38
>>> samples [80/160] noisy rate: 0.38 --> 0.40 --> 0.24 <<<
epoch [67/100] batch [5/5] time 0.058 (0.262) data 0.000 (0.202) loss 1.1809 (1.0551) acc 83.3333 (84.8546) lr 5.1825e-04 eta 0:00:43
epoch [68/100] batch [5/5] time 0.061 (0.242) data 0.000 (0.184) loss 0.9074 (1.1147) acc 91.6667 (84.9116) lr 4.9096e-04 eta 0:00:38
epoch [69/100] batch [5/5] time 0.049 (0.254) data 0.000 (0.196) loss 1.1404 (1.0666) acc 87.5000 (87.6908) lr 4.6417e-04 eta 0:00:39
epoch [70/100] batch [5/5] time 0.045 (0.237) data 0.000 (0.181) loss 1.2365 (1.0807) acc 76.6667 (85.1319) lr 4.3792e-04 eta 0:00:35
epoch [71/100] batch [5/5] time 0.053 (0.228) data 0.000 (0.169) loss 1.1645 (1.0854) acc 85.2941 (82.8546) lr 4.1221e-04 eta 0:00:33
>>> samples [81/160] noisy rate: 0.38 --> 0.35 --> 0.23 <<<
epoch [72/100] batch [5/5] time 0.046 (0.241) data 0.000 (0.180) loss 0.8367 (1.0424) acc 96.4286 (87.5782) lr 3.8709e-04 eta 0:00:33
epoch [73/100] batch [5/5] time 0.036 (0.242) data 0.000 (0.183) loss 0.8501 (1.0630) acc 90.9091 (87.5819) lr 3.6258e-04 eta 0:00:32
epoch [74/100] batch [5/5] time 0.046 (0.209) data 0.000 (0.153) loss 0.9942 (1.0529) acc 95.0000 (86.6471) lr 3.3869e-04 eta 0:00:27
epoch [75/100] batch [5/5] time 0.043 (0.232) data 0.000 (0.176) loss 0.9560 (1.0723) acc 87.5000 (86.9476) lr 3.1545e-04 eta 0:00:29
epoch [76/100] batch [5/5] time 0.058 (0.203) data 0.000 (0.144) loss 1.2276 (1.1174) acc 73.5294 (85.5035) lr 2.9289e-04 eta 0:00:24
>>> samples [81/160] noisy rate: 0.38 --> 0.40 --> 0.23 <<<
epoch [77/100] batch [5/5] time 0.055 (0.217) data 0.000 (0.159) loss 0.9696 (1.1019) acc 90.7895 (84.3860) lr 2.7103e-04 eta 0:00:24
epoch [78/100] batch [5/5] time 0.056 (0.222) data 0.001 (0.163) loss 1.0727 (1.0890) acc 86.8421 (87.1210) lr 2.4989e-04 eta 0:00:24
epoch [79/100] batch [5/5] time 0.043 (0.227) data 0.000 (0.167) loss 1.2869 (1.0756) acc 80.7692 (88.3516) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.044 (0.227) data 0.000 (0.170) loss 1.0086 (1.0820) acc 85.7143 (86.4322) lr 2.0984e-04 eta 0:00:22
epoch [81/100] batch [5/5] time 0.062 (0.254) data 0.000 (0.196) loss 0.9703 (1.0675) acc 96.4286 (85.7382) lr 1.9098e-04 eta 0:00:24
>>> samples [82/160] noisy rate: 0.38 --> 0.39 --> 0.23 <<<
epoch [82/100] batch [5/5] time 0.047 (0.229) data 0.000 (0.174) loss 0.9540 (1.0777) acc 81.6667 (84.1310) lr 1.7292e-04 eta 0:00:20
epoch [83/100] batch [5/5] time 0.056 (0.216) data 0.000 (0.160) loss 1.0084 (1.0742) acc 91.1765 (86.6071) lr 1.5567e-04 eta 0:00:18
epoch [84/100] batch [5/5] time 0.051 (0.232) data 0.000 (0.172) loss 1.1035 (1.1015) acc 91.6667 (83.2698) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.050 (0.232) data 0.000 (0.171) loss 1.0415 (1.0545) acc 96.4286 (87.3469) lr 1.2369e-04 eta 0:00:17
epoch [86/100] batch [5/5] time 0.035 (0.319) data 0.000 (0.179) loss 1.2472 (1.1139) acc 82.5000 (84.0425) lr 1.0899e-04 eta 0:00:22
>>> samples [82/160] noisy rate: 0.38 --> 0.36 --> 0.23 <<<
epoch [87/100] batch [5/5] time 0.051 (0.236) data 0.000 (0.176) loss 1.3052 (1.0803) acc 85.7143 (84.9118) lr 9.5173e-05 eta 0:00:15
epoch [88/100] batch [5/5] time 0.045 (0.335) data 0.000 (0.192) loss 1.1882 (1.0619) acc 86.6667 (86.3226) lr 8.2245e-05 eta 0:00:20
epoch [89/100] batch [5/5] time 0.050 (0.239) data 0.000 (0.179) loss 1.0518 (1.0507) acc 85.7143 (84.7691) lr 7.0224e-05 eta 0:00:13
epoch [90/100] batch [5/5] time 0.061 (0.253) data 0.000 (0.193) loss 1.1302 (1.0522) acc 89.4737 (87.8424) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.060 (0.259) data 0.000 (0.197) loss 1.2185 (1.0563) acc 82.8947 (88.1112) lr 4.8943e-05 eta 0:00:11
>>> samples [84/160] noisy rate: 0.38 --> 0.37 --> 0.25 <<<
epoch [92/100] batch [5/5] time 0.043 (0.238) data 0.000 (0.182) loss 0.9728 (1.0608) acc 93.7500 (87.4514) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.052 (0.248) data 0.000 (0.187) loss 1.0216 (1.0837) acc 90.6250 (84.1590) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.058 (0.237) data 0.000 (0.178) loss 1.1220 (1.0838) acc 81.9444 (85.6657) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.048 (0.244) data 0.000 (0.183) loss 1.2576 (1.0912) acc 78.3333 (87.1805) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.058 (0.242) data 0.001 (0.183) loss 1.1142 (1.0761) acc 83.3333 (84.4544) lr 1.2312e-05 eta 0:00:04
>>> samples [85/160] noisy rate: 0.38 --> 0.41 --> 0.26 <<<
epoch [97/100] batch [5/5] time 0.052 (0.266) data 0.000 (0.203) loss 1.2806 (1.0890) acc 85.0000 (86.7214) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.053 (0.241) data 0.000 (0.178) loss 0.9805 (1.0482) acc 84.7222 (84.8404) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.048 (0.262) data 0.000 (0.198) loss 0.9698 (1.1088) acc 89.0625 (84.1653) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.049 (0.233) data 0.000 (0.175) loss 1.0295 (1.0569) acc 87.5000 (86.7138) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.38, 0.39, 0.39, 0.36, 0.35, 0.34, 0.34, 0.33, 0.36, 0.36, 0.36, 0.39, 0.4, 0.4, 0.35, 0.4, 0.39, 0.36, 0.37, 0.41]
* learned noise rate: [0.08, 0.11, 0.16, 0.18, 0.19, 0.2, 0.19, 0.19, 0.19, 0.22, 0.23, 0.23, 0.23, 0.24, 0.23, 0.23, 0.23, 0.23, 0.25, 0.26]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<02:43,  2.04s/it]  4%|▎         | 3/81 [00:02<00:44,  1.74it/s]  6%|▌         | 5/81 [00:02<00:23,  3.21it/s]  9%|▊         | 7/81 [00:02<00:15,  4.85it/s] 11%|█         | 9/81 [00:02<00:11,  6.49it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.18it/s] 16%|█▌        | 13/81 [00:02<00:07,  9.71it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.00it/s] 21%|██        | 17/81 [00:03<00:05, 12.18it/s] 23%|██▎       | 19/81 [00:03<00:04, 13.14it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.47it/s] 28%|██▊       | 23/81 [00:03<00:04, 14.10it/s] 31%|███       | 25/81 [00:03<00:03, 14.58it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.69it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.73it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.05it/s] 41%|████      | 33/81 [00:04<00:03, 15.31it/s] 43%|████▎     | 35/81 [00:04<00:02, 15.47it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.33it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.47it/s] 51%|█████     | 41/81 [00:04<00:02, 15.59it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.57it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.99it/s] 58%|█████▊    | 47/81 [00:05<00:02, 15.11it/s] 60%|██████    | 49/81 [00:05<00:02, 15.33it/s] 63%|██████▎   | 51/81 [00:05<00:02, 14.92it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.18it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.92it/s] 70%|███████   | 57/81 [00:05<00:01, 15.18it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.39it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.51it/s] 78%|███████▊  | 63/81 [00:06<00:01, 15.62it/s] 80%|████████  | 65/81 [00:06<00:01, 15.70it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.76it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.70it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.80it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.87it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.92it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.96it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.99it/s]100%|██████████| 81/81 [00:07<00:00, 16.00it/s]100%|██████████| 81/81 [00:07<00:00, 11.05it/s]
=> result
* total: 8,100
* correct: 4,326
* accuracy: 53.4%
* error: 46.6%
* macro_f1: 48.6%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 407	acc: 45.2%
* class: 1 (Forest)	total: 900	correct: 870	acc: 96.7%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 24	acc: 2.7%
* class: 3 (Highway or Road)	total: 750	correct: 414	acc: 55.2%
* class: 4 (Industrial Buildings)	total: 750	correct: 593	acc: 79.1%
* class: 5 (Pasture Land)	total: 600	correct: 309	acc: 51.5%
* class: 6 (Permanent Crop Land)	total: 750	correct: 634	acc: 84.5%
* class: 7 (Residential Buildings)	total: 900	correct: 827	acc: 91.9%
* class: 8 (River)	total: 750	correct: 235	acc: 31.3%
* class: 9 (Sea or Lake)	total: 900	correct: 13	acc: 1.4%
* average: 54.0%
Elapsed: 0:04:08
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_6-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.085 (0.664) data 0.000 (0.237) loss 1.1260 (1.1786) acc 19.5312 (14.3750) lr 2.0000e-03 eta 0:05:28
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [68/160] noisy rate: 0.38 --> 0.37 --> 0.19 <<<
epoch [2/100] batch [5/5] time 0.330 (0.509) data 0.000 (0.164) loss 2.2481 (2.1890) acc 25.0000 (29.0042) lr 1.9995e-03 eta 0:04:09
epoch [3/100] batch [5/5] time 0.631 (0.468) data 0.000 (0.176) loss 1.9161 (2.0381) acc 29.6875 (29.5027) lr 1.9980e-03 eta 0:03:47
epoch [4/100] batch [5/5] time 0.331 (0.291) data 0.000 (0.179) loss 1.9741 (1.9405) acc 29.5455 (34.7799) lr 1.9956e-03 eta 0:02:19
epoch [5/100] batch [5/5] time 0.053 (0.363) data 0.000 (0.179) loss 2.2257 (2.0379) acc 25.0000 (29.8593) lr 1.9921e-03 eta 0:02:52
epoch [6/100] batch [5/5] time 0.045 (0.237) data 0.000 (0.180) loss 2.1764 (2.0663) acc 23.3333 (31.0390) lr 1.9877e-03 eta 0:01:51
>>> samples [77/160] noisy rate: 0.38 --> 0.37 --> 0.21 <<<
epoch [7/100] batch [5/5] time 0.042 (0.202) data 0.000 (0.145) loss 1.9355 (1.9885) acc 54.5455 (43.5675) lr 1.9823e-03 eta 0:01:33
epoch [8/100] batch [5/5] time 0.061 (0.212) data 0.000 (0.156) loss 1.8356 (1.9304) acc 46.0526 (38.7024) lr 1.9759e-03 eta 0:01:37
epoch [9/100] batch [5/5] time 0.061 (0.272) data 0.000 (0.146) loss 1.8578 (1.9278) acc 48.6842 (39.0803) lr 1.9686e-03 eta 0:02:03
epoch [10/100] batch [5/5] time 0.050 (0.231) data 0.001 (0.178) loss 1.8567 (1.9021) acc 41.0714 (43.7078) lr 1.9603e-03 eta 0:01:44
epoch [11/100] batch [5/5] time 0.057 (0.255) data 0.000 (0.199) loss 2.0898 (1.8367) acc 31.2500 (43.0995) lr 1.9511e-03 eta 0:01:53
>>> samples [83/160] noisy rate: 0.38 --> 0.36 --> 0.22 <<<
epoch [12/100] batch [5/5] time 0.462 (0.311) data 0.000 (0.170) loss 1.8087 (1.8623) acc 62.5000 (50.3389) lr 1.9409e-03 eta 0:02:16
epoch [13/100] batch [5/5] time 0.046 (0.246) data 0.001 (0.186) loss 1.9316 (1.8332) acc 41.6667 (48.9951) lr 1.9298e-03 eta 0:01:46
epoch [14/100] batch [5/5] time 0.048 (0.222) data 0.000 (0.167) loss 1.7765 (1.7824) acc 54.6875 (52.6246) lr 1.9178e-03 eta 0:01:35
epoch [15/100] batch [5/5] time 0.049 (0.332) data 0.000 (0.197) loss 1.8240 (1.7450) acc 56.2500 (51.8542) lr 1.9048e-03 eta 0:02:21
epoch [16/100] batch [5/5] time 0.058 (0.257) data 0.001 (0.201) loss 1.9084 (1.7859) acc 37.5000 (50.5368) lr 1.8910e-03 eta 0:01:47
>>> samples [86/160] noisy rate: 0.38 --> 0.30 --> 0.22 <<<
epoch [17/100] batch [5/5] time 0.048 (0.324) data 0.000 (0.187) loss 1.7871 (1.7733) acc 51.5625 (52.5508) lr 1.8763e-03 eta 0:02:14
epoch [18/100] batch [5/5] time 0.068 (0.249) data 0.000 (0.185) loss 1.6846 (1.7180) acc 64.2857 (57.2052) lr 1.8607e-03 eta 0:01:41
epoch [19/100] batch [5/5] time 0.061 (0.243) data 0.000 (0.179) loss 1.6472 (1.6895) acc 68.0555 (56.8065) lr 1.8443e-03 eta 0:01:38
epoch [20/100] batch [5/5] time 0.063 (0.246) data 0.000 (0.187) loss 1.7971 (1.7180) acc 42.8571 (52.0973) lr 1.8271e-03 eta 0:01:38
epoch [21/100] batch [5/5] time 0.048 (0.245) data 0.000 (0.187) loss 1.5125 (1.6925) acc 51.7857 (56.9885) lr 1.8090e-03 eta 0:01:36
>>> samples [86/160] noisy rate: 0.38 --> 0.29 --> 0.22 <<<
epoch [22/100] batch [5/5] time 0.042 (0.247) data 0.001 (0.189) loss 1.7328 (1.6762) acc 65.3846 (60.0940) lr 1.7902e-03 eta 0:01:36
epoch [23/100] batch [5/5] time 0.060 (0.242) data 0.000 (0.178) loss 1.5739 (1.6816) acc 58.8235 (60.0163) lr 1.7705e-03 eta 0:01:33
epoch [24/100] batch [5/5] time 0.061 (0.256) data 0.000 (0.198) loss 1.7925 (1.6585) acc 44.7368 (59.9474) lr 1.7501e-03 eta 0:01:37
epoch [25/100] batch [5/5] time 0.067 (0.239) data 0.000 (0.176) loss 1.9196 (1.6633) acc 58.3333 (58.9703) lr 1.7290e-03 eta 0:01:29
epoch [26/100] batch [5/5] time 0.059 (0.259) data 0.000 (0.196) loss 1.7204 (1.6532) acc 52.7778 (60.5972) lr 1.7071e-03 eta 0:01:35
>>> samples [89/160] noisy rate: 0.38 --> 0.32 --> 0.24 <<<
epoch [27/100] batch [5/5] time 0.053 (0.242) data 0.000 (0.186) loss 1.6584 (1.6386) acc 56.9444 (62.0698) lr 1.6845e-03 eta 0:01:28
epoch [28/100] batch [5/5] time 0.068 (0.239) data 0.000 (0.177) loss 1.6057 (1.6329) acc 75.0000 (63.4942) lr 1.6613e-03 eta 0:01:25
epoch [29/100] batch [5/5] time 0.044 (0.236) data 0.000 (0.178) loss 1.2925 (1.5865) acc 77.0833 (64.1820) lr 1.6374e-03 eta 0:01:23
epoch [30/100] batch [5/5] time 0.067 (0.242) data 0.000 (0.178) loss 1.4971 (1.6210) acc 55.9524 (64.3731) lr 1.6129e-03 eta 0:01:24
epoch [31/100] batch [5/5] time 0.054 (0.236) data 0.000 (0.172) loss 1.9299 (1.6304) acc 45.3125 (62.8788) lr 1.5878e-03 eta 0:01:21
>>> samples [89/160] noisy rate: 0.38 --> 0.26 --> 0.24 <<<
epoch [32/100] batch [5/5] time 0.064 (0.259) data 0.000 (0.198) loss 1.4307 (1.5821) acc 71.2500 (66.4669) lr 1.5621e-03 eta 0:01:28
epoch [33/100] batch [5/5] time 0.061 (0.250) data 0.000 (0.187) loss 1.4892 (1.5956) acc 72.2222 (67.0812) lr 1.5358e-03 eta 0:01:23
epoch [34/100] batch [5/5] time 0.056 (0.234) data 0.000 (0.174) loss 1.5830 (1.5620) acc 64.4737 (66.3200) lr 1.5090e-03 eta 0:01:17
epoch [35/100] batch [5/5] time 0.053 (0.247) data 0.000 (0.188) loss 1.7188 (1.5649) acc 62.5000 (72.6221) lr 1.4818e-03 eta 0:01:20
epoch [36/100] batch [5/5] time 0.052 (0.231) data 0.000 (0.170) loss 1.5739 (1.5727) acc 77.9412 (69.6487) lr 1.4540e-03 eta 0:01:14
>>> samples [91/160] noisy rate: 0.38 --> 0.19 --> 0.23 <<<
epoch [37/100] batch [5/5] time 0.068 (0.244) data 0.000 (0.182) loss 1.6956 (1.5424) acc 69.0476 (70.6660) lr 1.4258e-03 eta 0:01:16
epoch [38/100] batch [5/5] time 0.069 (0.259) data 0.000 (0.197) loss 1.7028 (1.5456) acc 71.4286 (70.7639) lr 1.3971e-03 eta 0:01:20
epoch [39/100] batch [5/5] time 0.500 (0.344) data 0.000 (0.197) loss 1.7798 (1.5628) acc 58.0000 (67.3283) lr 1.3681e-03 eta 0:01:45
epoch [40/100] batch [5/5] time 0.070 (0.245) data 0.000 (0.186) loss 1.4559 (1.5797) acc 61.0000 (69.2175) lr 1.3387e-03 eta 0:01:13
epoch [41/100] batch [5/5] time 0.046 (0.248) data 0.000 (0.188) loss 1.4328 (1.4986) acc 60.0000 (72.7846) lr 1.3090e-03 eta 0:01:13
>>> samples [93/160] noisy rate: 0.38 --> 0.21 --> 0.25 <<<
epoch [42/100] batch [5/5] time 0.051 (0.249) data 0.000 (0.188) loss 1.6396 (1.5619) acc 66.6667 (68.9394) lr 1.2790e-03 eta 0:01:12
epoch [43/100] batch [5/5] time 0.064 (0.237) data 0.000 (0.173) loss 1.4919 (1.5615) acc 63.6364 (65.5841) lr 1.2487e-03 eta 0:01:07
epoch [44/100] batch [5/5] time 0.063 (0.250) data 0.000 (0.187) loss 1.5833 (1.5689) acc 65.0000 (66.1975) lr 1.2181e-03 eta 0:01:10
epoch [45/100] batch [5/5] time 0.048 (0.237) data 0.000 (0.175) loss 1.5534 (1.5154) acc 84.3750 (75.7654) lr 1.1874e-03 eta 0:01:05
epoch [46/100] batch [5/5] time 0.051 (0.244) data 0.000 (0.179) loss 1.0943 (1.4933) acc 76.4706 (74.5540) lr 1.1564e-03 eta 0:01:05
>>> samples [94/160] noisy rate: 0.38 --> 0.22 --> 0.24 <<<
epoch [47/100] batch [5/5] time 0.057 (0.314) data 0.000 (0.169) loss 1.5076 (1.5266) acc 72.3684 (70.4816) lr 1.1253e-03 eta 0:01:23
epoch [48/100] batch [5/5] time 0.061 (0.251) data 0.000 (0.187) loss 1.6573 (1.5204) acc 61.8421 (71.1140) lr 1.0941e-03 eta 0:01:05
epoch [49/100] batch [5/5] time 0.056 (0.327) data 0.000 (0.184) loss 1.4654 (1.5363) acc 77.6316 (70.8144) lr 1.0628e-03 eta 0:01:23
epoch [50/100] batch [5/5] time 0.051 (0.250) data 0.000 (0.186) loss 1.5053 (1.5294) acc 68.3333 (71.0376) lr 1.0314e-03 eta 0:01:02
epoch [51/100] batch [5/5] time 0.054 (0.257) data 0.000 (0.194) loss 1.5853 (1.5220) acc 67.1875 (72.1913) lr 1.0000e-03 eta 0:01:02
>>> samples [95/160] noisy rate: 0.38 --> 0.20 --> 0.25 <<<
epoch [52/100] batch [5/5] time 0.062 (0.249) data 0.000 (0.185) loss 1.6661 (1.4971) acc 68.4211 (74.2767) lr 9.6859e-04 eta 0:00:59
epoch [53/100] batch [5/5] time 0.062 (0.244) data 0.001 (0.180) loss 1.4023 (1.5344) acc 67.8571 (72.8922) lr 9.3721e-04 eta 0:00:57
epoch [54/100] batch [5/5] time 0.065 (0.238) data 0.000 (0.172) loss 1.6328 (1.5428) acc 51.0870 (73.9507) lr 9.0589e-04 eta 0:00:54
epoch [55/100] batch [5/5] time 0.056 (0.255) data 0.000 (0.189) loss 1.4308 (1.5046) acc 72.3684 (75.3308) lr 8.7467e-04 eta 0:00:57
epoch [56/100] batch [5/5] time 0.062 (0.236) data 0.000 (0.173) loss 1.5140 (1.4875) acc 73.6842 (75.4741) lr 8.4357e-04 eta 0:00:52
>>> samples [96/160] noisy rate: 0.38 --> 0.22 --> 0.25 <<<
epoch [57/100] batch [5/5] time 0.057 (0.231) data 0.000 (0.166) loss 1.5634 (1.5147) acc 78.7500 (72.4030) lr 8.1262e-04 eta 0:00:49
epoch [58/100] batch [5/5] time 0.072 (0.235) data 0.000 (0.169) loss 1.6222 (1.4752) acc 64.7727 (76.3625) lr 7.8186e-04 eta 0:00:49
epoch [59/100] batch [5/5] time 0.056 (0.230) data 0.000 (0.166) loss 1.7729 (1.5101) acc 66.1765 (73.4840) lr 7.5131e-04 eta 0:00:47
epoch [60/100] batch [5/5] time 0.048 (0.232) data 0.000 (0.169) loss 1.2933 (1.4882) acc 79.6875 (75.0966) lr 7.2101e-04 eta 0:00:46
epoch [61/100] batch [5/5] time 0.063 (0.245) data 0.000 (0.181) loss 1.4258 (1.4858) acc 71.0526 (76.5738) lr 6.9098e-04 eta 0:00:47
>>> samples [96/160] noisy rate: 0.38 --> 0.16 --> 0.25 <<<
epoch [62/100] batch [5/5] time 0.051 (0.242) data 0.000 (0.177) loss 1.5330 (1.5076) acc 75.0000 (73.8162) lr 6.6126e-04 eta 0:00:45
epoch [63/100] batch [5/5] time 0.055 (0.248) data 0.001 (0.178) loss 1.6080 (1.4739) acc 68.7500 (76.1784) lr 6.3188e-04 eta 0:00:45
epoch [64/100] batch [5/5] time 0.072 (0.240) data 0.000 (0.175) loss 1.4964 (1.4709) acc 80.4348 (79.2309) lr 6.0285e-04 eta 0:00:43
epoch [65/100] batch [5/5] time 0.055 (0.256) data 0.000 (0.189) loss 1.4664 (1.4975) acc 75.0000 (73.5808) lr 5.7422e-04 eta 0:00:44
epoch [66/100] batch [5/5] time 0.051 (0.245) data 0.000 (0.183) loss 1.6885 (1.5057) acc 76.6667 (74.2980) lr 5.4601e-04 eta 0:00:41
>>> samples [96/160] noisy rate: 0.38 --> 0.21 --> 0.25 <<<
epoch [67/100] batch [5/5] time 0.056 (0.240) data 0.000 (0.177) loss 1.4944 (1.4932) acc 78.1250 (73.3472) lr 5.1825e-04 eta 0:00:39
epoch [68/100] batch [5/5] time 0.066 (0.243) data 0.000 (0.178) loss 1.5589 (1.4844) acc 65.4762 (75.5321) lr 4.9096e-04 eta 0:00:38
epoch [69/100] batch [5/5] time 0.058 (0.259) data 0.000 (0.194) loss 1.4047 (1.4827) acc 75.0000 (76.8685) lr 4.6417e-04 eta 0:00:40
epoch [70/100] batch [5/5] time 0.067 (0.265) data 0.000 (0.202) loss 1.3260 (1.4810) acc 80.9524 (76.5463) lr 4.3792e-04 eta 0:00:39
epoch [71/100] batch [5/5] time 0.060 (0.258) data 0.000 (0.192) loss 1.2531 (1.4701) acc 89.4737 (76.8480) lr 4.1221e-04 eta 0:00:37
>>> samples [97/160] noisy rate: 0.38 --> 0.19 --> 0.25 <<<
epoch [72/100] batch [5/5] time 0.053 (0.236) data 0.000 (0.169) loss 1.4858 (1.4632) acc 72.2222 (75.5105) lr 3.8709e-04 eta 0:00:32
epoch [73/100] batch [5/5] time 0.059 (0.233) data 0.000 (0.170) loss 1.4062 (1.4733) acc 83.3333 (76.5066) lr 3.6258e-04 eta 0:00:31
epoch [74/100] batch [5/5] time 0.052 (0.256) data 0.000 (0.189) loss 1.4762 (1.4762) acc 72.0588 (75.6051) lr 3.3869e-04 eta 0:00:33
epoch [75/100] batch [5/5] time 0.056 (0.259) data 0.000 (0.190) loss 1.3881 (1.4705) acc 70.5882 (75.0024) lr 3.1545e-04 eta 0:00:32
epoch [76/100] batch [5/5] time 0.043 (0.325) data 0.000 (0.180) loss 1.4571 (1.4560) acc 83.3333 (79.3507) lr 2.9289e-04 eta 0:00:38
>>> samples [97/160] noisy rate: 0.38 --> 0.19 --> 0.25 <<<
epoch [77/100] batch [5/5] time 0.059 (0.243) data 0.000 (0.179) loss 1.3793 (1.4605) acc 71.2500 (75.8041) lr 2.7103e-04 eta 0:00:27
epoch [78/100] batch [5/5] time 0.056 (0.246) data 0.000 (0.179) loss 1.4699 (1.4534) acc 58.3333 (75.2564) lr 2.4989e-04 eta 0:00:27
epoch [79/100] batch [5/5] time 0.064 (0.262) data 0.000 (0.197) loss 1.4848 (1.4829) acc 73.8095 (74.0409) lr 2.2949e-04 eta 0:00:27
epoch [80/100] batch [5/5] time 0.067 (0.255) data 0.000 (0.188) loss 1.2514 (1.4572) acc 77.3810 (72.6891) lr 2.0984e-04 eta 0:00:25
epoch [81/100] batch [5/5] time 0.056 (0.247) data 0.000 (0.179) loss 1.1902 (1.4419) acc 80.2632 (78.5189) lr 1.9098e-04 eta 0:00:23
>>> samples [97/160] noisy rate: 0.38 --> 0.21 --> 0.25 <<<
epoch [82/100] batch [5/5] time 0.056 (0.257) data 0.000 (0.192) loss 1.6807 (1.4645) acc 61.1111 (75.7976) lr 1.7292e-04 eta 0:00:23
epoch [83/100] batch [5/5] time 0.062 (0.247) data 0.000 (0.181) loss 1.6517 (1.4697) acc 72.5000 (75.7636) lr 1.5567e-04 eta 0:00:21
epoch [84/100] batch [5/5] time 0.065 (0.230) data 0.000 (0.164) loss 1.3094 (1.4630) acc 85.8696 (74.2902) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.060 (0.247) data 0.000 (0.184) loss 1.3741 (1.4564) acc 78.7500 (77.1556) lr 1.2369e-04 eta 0:00:18
epoch [86/100] batch [5/5] time 0.067 (0.257) data 0.000 (0.191) loss 1.3236 (1.4769) acc 80.9524 (76.6684) lr 1.0899e-04 eta 0:00:17
>>> samples [97/160] noisy rate: 0.38 --> 0.18 --> 0.25 <<<
epoch [87/100] batch [5/5] time 0.072 (0.260) data 0.000 (0.194) loss 1.3201 (1.4543) acc 77.1739 (76.4512) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.073 (0.259) data 0.000 (0.191) loss 1.5470 (1.4323) acc 77.1739 (78.5397) lr 8.2245e-05 eta 0:00:15
epoch [89/100] batch [5/5] time 0.060 (0.277) data 0.000 (0.211) loss 1.3533 (1.4406) acc 86.8421 (80.2371) lr 7.0224e-05 eta 0:00:15
epoch [90/100] batch [5/5] time 0.060 (0.259) data 0.001 (0.194) loss 1.3674 (1.4759) acc 75.0000 (75.5761) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.068 (0.250) data 0.000 (0.183) loss 1.5278 (1.4564) acc 77.3810 (75.5000) lr 4.8943e-05 eta 0:00:11
>>> samples [97/160] noisy rate: 0.38 --> 0.19 --> 0.25 <<<
epoch [92/100] batch [5/5] time 0.075 (0.257) data 0.000 (0.189) loss 1.4454 (1.4352) acc 73.0000 (77.7209) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.060 (0.269) data 0.001 (0.206) loss 1.4752 (1.4752) acc 85.0000 (76.1479) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.067 (0.248) data 0.001 (0.180) loss 1.4558 (1.4516) acc 69.0476 (76.5539) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.055 (0.284) data 0.000 (0.216) loss 1.3850 (1.4395) acc 70.3125 (77.7274) lr 1.7713e-05 eta 0:00:07
epoch [96/100] batch [5/5] time 0.055 (0.259) data 0.000 (0.194) loss 1.4675 (1.4510) acc 73.4375 (77.8542) lr 1.2312e-05 eta 0:00:05
>>> samples [100/160] noisy rate: 0.38 --> 0.19 --> 0.24 <<<
epoch [97/100] batch [5/5] time 0.053 (0.267) data 0.000 (0.201) loss 1.4246 (1.4618) acc 83.8235 (76.5742) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.069 (0.255) data 0.000 (0.189) loss 1.5719 (1.4217) acc 76.0870 (76.6262) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.052 (0.251) data 0.000 (0.183) loss 1.4180 (1.4232) acc 72.0588 (79.9293) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.057 (0.237) data 0.000 (0.168) loss 1.4464 (1.4430) acc 72.2222 (77.1598) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.37, 0.37, 0.36, 0.3, 0.29, 0.32, 0.26, 0.19, 0.21, 0.22, 0.2, 0.22, 0.16, 0.21, 0.19, 0.19, 0.21, 0.18, 0.19, 0.19]
* learned noise rate: [0.19, 0.21, 0.22, 0.22, 0.22, 0.24, 0.24, 0.23, 0.25, 0.24, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.24]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<01:29,  1.12s/it]  4%|▎         | 3/81 [00:01<00:27,  2.84it/s]  6%|▌         | 5/81 [00:01<00:16,  4.57it/s]  7%|▋         | 6/81 [00:01<00:14,  5.30it/s]  9%|▊         | 7/81 [00:01<00:12,  5.98it/s] 11%|█         | 9/81 [00:01<00:09,  7.32it/s] 12%|█▏        | 10/81 [00:01<00:09,  7.79it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.20it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.28it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.63it/s] 21%|██        | 17/81 [00:02<00:05, 12.47it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.38it/s] 26%|██▌       | 21/81 [00:02<00:04, 14.11it/s] 28%|██▊       | 23/81 [00:02<00:03, 14.60it/s] 31%|███       | 25/81 [00:03<00:03, 14.68it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.65it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.94it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.25it/s] 41%|████      | 33/81 [00:03<00:03, 14.68it/s] 43%|████▎     | 35/81 [00:03<00:03, 15.04it/s] 46%|████▌     | 37/81 [00:03<00:02, 15.00it/s] 48%|████▊     | 39/81 [00:03<00:02, 14.23it/s] 51%|█████     | 41/81 [00:04<00:02, 14.50it/s] 53%|█████▎    | 43/81 [00:04<00:02, 14.80it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.59it/s] 58%|█████▊    | 47/81 [00:04<00:02, 14.73it/s] 60%|██████    | 49/81 [00:04<00:02, 14.99it/s] 63%|██████▎   | 51/81 [00:04<00:02, 14.93it/s] 65%|██████▌   | 53/81 [00:04<00:01, 15.10it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.01it/s] 70%|███████   | 57/81 [00:05<00:01, 15.14it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.23it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.01it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.18it/s] 80%|████████  | 65/81 [00:05<00:01, 15.02it/s] 83%|████████▎ | 67/81 [00:05<00:00, 14.77it/s] 85%|████████▌ | 69/81 [00:05<00:00, 15.10it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.35it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.54it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.66it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.76it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.69it/s]100%|██████████| 81/81 [00:06<00:00, 15.64it/s]100%|██████████| 81/81 [00:06<00:00, 11.85it/s]
=> result
* total: 8,100
* correct: 5,268
* accuracy: 65.0%
* error: 35.0%
* macro_f1: 62.6%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 430	acc: 47.8%
* class: 1 (Forest)	total: 900	correct: 872	acc: 96.9%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 69	acc: 7.7%
* class: 3 (Highway or Road)	total: 750	correct: 368	acc: 49.1%
* class: 4 (Industrial Buildings)	total: 750	correct: 727	acc: 96.9%
* class: 5 (Pasture Land)	total: 600	correct: 379	acc: 63.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 618	acc: 82.4%
* class: 7 (Residential Buildings)	total: 900	correct: 676	acc: 75.1%
* class: 8 (River)	total: 750	correct: 359	acc: 47.9%
* class: 9 (Sea or Lake)	total: 900	correct: 770	acc: 85.6%
* average: 65.2%
Elapsed: 0:04:14
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_6-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.091 (0.793) data 0.000 (0.247) loss 1.1939 (1.1752) acc 18.7500 (14.2188) lr 2.0000e-03 eta 0:03:14
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [58/160] noisy rate: 0.38 --> 0.52 --> 0.19 <<<
epoch [2/50] batch [5/5] time 0.234 (0.458) data 0.000 (0.206) loss 2.3166 (2.1975) acc 13.8889 (22.3968) lr 1.9980e-03 eta 0:01:49
epoch [3/50] batch [5/5] time 0.035 (0.332) data 0.000 (0.186) loss 2.0656 (2.0932) acc 52.7778 (29.9116) lr 1.9921e-03 eta 0:01:17
epoch [4/50] batch [5/5] time 0.038 (0.258) data 0.000 (0.213) loss 2.0358 (2.0222) acc 27.5000 (38.8175) lr 1.9823e-03 eta 0:00:59
epoch [5/50] batch [5/5] time 0.037 (0.283) data 0.000 (0.183) loss 2.2684 (1.9953) acc 28.1250 (41.3045) lr 1.9686e-03 eta 0:01:03
epoch [6/50] batch [5/5] time 0.035 (0.232) data 0.000 (0.190) loss 1.9090 (1.9733) acc 38.8889 (40.3492) lr 1.9511e-03 eta 0:00:51
>>> samples [63/160] noisy rate: 0.38 --> 0.39 --> 0.21 <<<
epoch [7/50] batch [5/5] time 0.047 (0.239) data 0.000 (0.186) loss 1.9066 (2.0151) acc 46.4286 (37.3819) lr 1.9298e-03 eta 0:00:51
epoch [8/50] batch [5/5] time 0.043 (0.231) data 0.000 (0.177) loss 1.8788 (1.9357) acc 63.6364 (47.3823) lr 1.9048e-03 eta 0:00:48
epoch [9/50] batch [5/5] time 0.036 (0.234) data 0.000 (0.181) loss 1.9942 (1.8975) acc 40.0000 (44.1667) lr 1.8763e-03 eta 0:00:47
epoch [10/50] batch [5/5] time 0.050 (0.244) data 0.000 (0.195) loss 1.6197 (1.8235) acc 58.9286 (54.7723) lr 1.8443e-03 eta 0:00:48
epoch [11/50] batch [5/5] time 0.048 (0.307) data 0.000 (0.196) loss 1.8553 (1.8208) acc 44.2308 (50.2571) lr 1.8090e-03 eta 0:00:59
>>> samples [73/160] noisy rate: 0.38 --> 0.39 --> 0.21 <<<
epoch [12/50] batch [5/5] time 0.576 (0.362) data 0.000 (0.204) loss 1.7237 (1.7645) acc 56.5789 (56.3880) lr 1.7705e-03 eta 0:01:08
epoch [13/50] batch [5/5] time 0.046 (0.334) data 0.000 (0.215) loss 1.6294 (1.7836) acc 73.2143 (58.4958) lr 1.7290e-03 eta 0:01:01
epoch [14/50] batch [5/5] time 0.045 (0.326) data 0.000 (0.195) loss 1.6225 (1.6946) acc 75.0000 (60.0925) lr 1.6845e-03 eta 0:00:58
epoch [15/50] batch [5/5] time 0.050 (0.346) data 0.000 (0.215) loss 1.9395 (1.7575) acc 48.0769 (53.5302) lr 1.6374e-03 eta 0:01:00
epoch [16/50] batch [5/5] time 0.047 (0.289) data 0.000 (0.232) loss 1.9151 (1.7594) acc 52.7778 (54.6947) lr 1.5878e-03 eta 0:00:49
>>> samples [75/160] noisy rate: 0.38 --> 0.38 --> 0.20 <<<
epoch [17/50] batch [5/5] time 0.055 (0.323) data 0.000 (0.262) loss 1.5981 (1.7180) acc 48.2143 (52.5060) lr 1.5358e-03 eta 0:00:53
epoch [18/50] batch [5/5] time 0.060 (0.315) data 0.000 (0.257) loss 1.7134 (1.7348) acc 47.0588 (55.8439) lr 1.4818e-03 eta 0:00:50
epoch [19/50] batch [5/5] time 0.051 (0.260) data 0.000 (0.206) loss 1.5396 (1.6763) acc 59.3750 (60.6845) lr 1.4258e-03 eta 0:00:40
epoch [20/50] batch [5/5] time 0.066 (0.271) data 0.000 (0.207) loss 1.7121 (1.6528) acc 61.8421 (62.2692) lr 1.3681e-03 eta 0:00:40
epoch [21/50] batch [5/5] time 0.058 (0.359) data 0.000 (0.295) loss 1.7581 (1.6623) acc 64.0625 (62.5585) lr 1.3090e-03 eta 0:00:52
>>> samples [75/160] noisy rate: 0.38 --> 0.36 --> 0.20 <<<
epoch [22/50] batch [5/5] time 0.056 (0.303) data 0.000 (0.243) loss 1.5608 (1.6576) acc 69.4444 (66.7117) lr 1.2487e-03 eta 0:00:42
epoch [23/50] batch [5/5] time 0.039 (0.268) data 0.000 (0.210) loss 1.7621 (1.6869) acc 45.0000 (56.4463) lr 1.1874e-03 eta 0:00:36
epoch [24/50] batch [5/5] time 0.058 (0.249) data 0.000 (0.190) loss 1.5182 (1.6766) acc 61.7647 (61.7902) lr 1.1253e-03 eta 0:00:32
epoch [25/50] batch [5/5] time 0.052 (0.276) data 0.000 (0.219) loss 2.1273 (1.6653) acc 51.7857 (59.7395) lr 1.0628e-03 eta 0:00:34
epoch [26/50] batch [5/5] time 0.053 (0.309) data 0.000 (0.257) loss 1.6377 (1.6526) acc 56.6667 (62.8839) lr 1.0000e-03 eta 0:00:37
>>> samples [77/160] noisy rate: 0.38 --> 0.38 --> 0.22 <<<
epoch [27/50] batch [5/5] time 0.064 (0.266) data 0.000 (0.209) loss 1.6040 (1.6198) acc 76.3889 (67.5982) lr 9.3721e-04 eta 0:00:30
epoch [28/50] batch [5/5] time 0.054 (0.271) data 0.000 (0.210) loss 1.8200 (1.6201) acc 63.3333 (66.5523) lr 8.7467e-04 eta 0:00:29
epoch [29/50] batch [5/5] time 0.057 (0.271) data 0.001 (0.200) loss 1.6930 (1.5981) acc 56.9444 (65.6977) lr 8.1262e-04 eta 0:00:28
epoch [30/50] batch [5/5] time 0.049 (0.263) data 0.000 (0.195) loss 1.5745 (1.5799) acc 59.6154 (68.9419) lr 7.5131e-04 eta 0:00:26
epoch [31/50] batch [5/5] time 0.056 (0.278) data 0.000 (0.220) loss 1.6510 (1.5749) acc 70.0000 (67.1137) lr 6.9098e-04 eta 0:00:26
>>> samples [81/160] noisy rate: 0.38 --> 0.40 --> 0.25 <<<
epoch [32/50] batch [5/5] time 0.067 (0.274) data 0.000 (0.208) loss 1.7466 (1.6363) acc 55.8824 (63.2816) lr 6.3188e-04 eta 0:00:24
epoch [33/50] batch [5/5] time 0.051 (0.277) data 0.000 (0.216) loss 1.6387 (1.6006) acc 59.6154 (65.1286) lr 5.7422e-04 eta 0:00:23
epoch [34/50] batch [5/5] time 0.059 (0.389) data 0.000 (0.261) loss 1.5551 (1.6010) acc 85.0000 (66.8241) lr 5.1825e-04 eta 0:00:31
epoch [35/50] batch [5/5] time 0.057 (0.297) data 0.000 (0.229) loss 1.7068 (1.5793) acc 69.1176 (71.4902) lr 4.6417e-04 eta 0:00:22
epoch [36/50] batch [5/5] time 0.061 (0.290) data 0.000 (0.232) loss 1.5755 (1.5762) acc 59.2105 (67.4748) lr 4.1221e-04 eta 0:00:20
>>> samples [82/160] noisy rate: 0.38 --> 0.38 --> 0.26 <<<
epoch [37/50] batch [5/5] time 0.056 (0.255) data 0.000 (0.195) loss 1.6020 (1.5749) acc 63.2353 (67.6465) lr 3.6258e-04 eta 0:00:16
epoch [38/50] batch [5/5] time 0.066 (0.288) data 0.000 (0.228) loss 1.4758 (1.5817) acc 73.5294 (68.9877) lr 3.1545e-04 eta 0:00:17
epoch [39/50] batch [5/5] time 0.057 (0.269) data 0.000 (0.206) loss 1.3778 (1.5779) acc 70.5882 (68.9205) lr 2.7103e-04 eta 0:00:14
epoch [40/50] batch [5/5] time 0.466 (0.355) data 0.000 (0.206) loss 1.6449 (1.5535) acc 70.6522 (69.2350) lr 2.2949e-04 eta 0:00:17
epoch [41/50] batch [5/5] time 0.050 (0.264) data 0.000 (0.207) loss 1.3944 (1.5970) acc 71.8750 (68.4295) lr 1.9098e-04 eta 0:00:11
>>> samples [85/160] noisy rate: 0.38 --> 0.39 --> 0.28 <<<
epoch [42/50] batch [5/5] time 0.058 (0.369) data 0.000 (0.312) loss 1.4241 (1.5646) acc 79.1667 (70.7560) lr 1.5567e-04 eta 0:00:14
epoch [43/50] batch [5/5] time 0.054 (0.409) data 0.000 (0.350) loss 1.5621 (1.5695) acc 67.6471 (72.5103) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [5/5] time 0.065 (0.354) data 0.000 (0.290) loss 1.5655 (1.5752) acc 71.2500 (71.6478) lr 9.5173e-05 eta 0:00:10
epoch [45/50] batch [5/5] time 0.053 (0.335) data 0.000 (0.275) loss 1.4892 (1.5802) acc 71.8750 (72.0384) lr 7.0224e-05 eta 0:00:08
epoch [46/50] batch [5/5] time 0.063 (0.296) data 0.000 (0.239) loss 1.5846 (1.5963) acc 70.0000 (71.7569) lr 4.8943e-05 eta 0:00:05
>>> samples [86/160] noisy rate: 0.38 --> 0.39 --> 0.28 <<<
epoch [47/50] batch [5/5] time 0.051 (0.321) data 0.000 (0.254) loss 1.4412 (1.5884) acc 73.2143 (72.6875) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/5] time 0.059 (0.286) data 0.000 (0.224) loss 1.9218 (1.5738) acc 66.1765 (71.9524) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [5/5] time 0.050 (0.389) data 0.000 (0.246) loss 1.4401 (1.5774) acc 80.0000 (70.6644) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/5] time 0.070 (0.319) data 0.000 (0.259) loss 1.3626 (1.5735) acc 72.7273 (74.1155) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.52, 0.39, 0.39, 0.38, 0.36, 0.38, 0.4, 0.38, 0.39, 0.39]
* learned noise rate: [0.19, 0.21, 0.21, 0.2, 0.2, 0.22, 0.25, 0.26, 0.28, 0.28]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<03:01,  2.27s/it]  4%|▎         | 3/81 [00:02<00:49,  1.57it/s]  6%|▌         | 5/81 [00:02<00:26,  2.92it/s]  9%|▊         | 7/81 [00:02<00:16,  4.44it/s] 11%|█         | 9/81 [00:02<00:11,  6.04it/s] 14%|█▎        | 11/81 [00:02<00:09,  7.46it/s] 16%|█▌        | 13/81 [00:03<00:07,  8.86it/s] 19%|█▊        | 15/81 [00:03<00:06, 10.25it/s] 21%|██        | 17/81 [00:03<00:05, 11.45it/s] 23%|██▎       | 19/81 [00:03<00:05, 12.32it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.10it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.09it/s] 31%|███       | 25/81 [00:03<00:04, 13.59it/s] 33%|███▎      | 27/81 [00:04<00:04, 12.85it/s] 36%|███▌      | 29/81 [00:04<00:03, 13.01it/s] 38%|███▊      | 31/81 [00:04<00:03, 12.75it/s] 41%|████      | 33/81 [00:04<00:03, 13.38it/s] 43%|████▎     | 35/81 [00:04<00:03, 13.61it/s] 46%|████▌     | 37/81 [00:04<00:03, 13.99it/s] 48%|████▊     | 39/81 [00:04<00:03, 13.74it/s] 51%|█████     | 41/81 [00:05<00:02, 13.35it/s] 53%|█████▎    | 43/81 [00:05<00:02, 13.29it/s] 56%|█████▌    | 45/81 [00:05<00:02, 13.67it/s] 58%|█████▊    | 47/81 [00:05<00:02, 12.47it/s] 60%|██████    | 49/81 [00:05<00:02, 12.94it/s] 63%|██████▎   | 51/81 [00:05<00:02, 12.83it/s] 65%|██████▌   | 53/81 [00:06<00:02, 13.51it/s] 68%|██████▊   | 55/81 [00:06<00:01, 14.03it/s] 70%|███████   | 57/81 [00:06<00:01, 14.08it/s] 73%|███████▎  | 59/81 [00:06<00:01, 14.45it/s] 75%|███████▌  | 61/81 [00:06<00:01, 14.72it/s] 78%|███████▊  | 63/81 [00:06<00:01, 14.82it/s] 80%|████████  | 65/81 [00:06<00:01, 14.95it/s] 83%|████████▎ | 67/81 [00:06<00:00, 14.98it/s] 85%|████████▌ | 69/81 [00:07<00:00, 14.55it/s] 88%|████████▊ | 71/81 [00:07<00:00, 14.78it/s] 90%|█████████ | 73/81 [00:07<00:00, 14.81it/s] 93%|█████████▎| 75/81 [00:07<00:00, 15.01it/s] 95%|█████████▌| 77/81 [00:07<00:00, 15.21it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.29it/s]100%|██████████| 81/81 [00:07<00:00, 15.38it/s]100%|██████████| 81/81 [00:08<00:00, 10.10it/s]
=> result
* total: 8,100
* correct: 4,216
* accuracy: 52.0%
* error: 48.0%
* macro_f1: 47.7%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 181	acc: 20.1%
* class: 1 (Forest)	total: 900	correct: 885	acc: 98.3%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 82	acc: 9.1%
* class: 3 (Highway or Road)	total: 750	correct: 484	acc: 64.5%
* class: 4 (Industrial Buildings)	total: 750	correct: 628	acc: 83.7%
* class: 5 (Pasture Land)	total: 600	correct: 178	acc: 29.7%
* class: 6 (Permanent Crop Land)	total: 750	correct: 497	acc: 66.3%
* class: 7 (Residential Buildings)	total: 900	correct: 769	acc: 85.4%
* class: 8 (River)	total: 750	correct: 390	acc: 52.0%
* class: 9 (Sea or Lake)	total: 900	correct: 122	acc: 13.6%
* average: 52.3%
Elapsed: 0:02:30
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_8-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.092 (0.822) data 0.000 (0.304) loss 1.1614 (1.2000) acc 12.5000 (13.2812) lr 2.0000e-03 eta 0:03:21
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [42/160] noisy rate: 0.50 --> 0.49 --> 0.19 <<<
epoch [2/50] batch [5/5] time 0.213 (0.389) data 0.000 (0.220) loss 2.2684 (2.2013) acc 30.0000 (24.3018) lr 1.9980e-03 eta 0:01:33
epoch [3/50] batch [5/5] time 0.198 (0.489) data 0.000 (0.308) loss 1.8791 (2.0923) acc 45.8333 (34.2500) lr 1.9921e-03 eta 0:01:54
epoch [4/50] batch [5/5] time 0.037 (0.456) data 0.000 (0.250) loss 1.8192 (2.0157) acc 54.1667 (39.4405) lr 1.9823e-03 eta 0:01:44
epoch [5/50] batch [5/5] time 0.038 (0.307) data 0.000 (0.219) loss 2.0936 (2.0153) acc 35.7143 (38.3571) lr 1.9686e-03 eta 0:01:09
epoch [6/50] batch [5/5] time 0.039 (0.266) data 0.000 (0.221) loss 1.8492 (1.9245) acc 72.2222 (48.9380) lr 1.9511e-03 eta 0:00:58
>>> samples [45/160] noisy rate: 0.50 --> 0.49 --> 0.22 <<<
epoch [7/50] batch [5/5] time 0.048 (0.310) data 0.000 (0.210) loss 2.0703 (1.8280) acc 38.8889 (51.5919) lr 1.9298e-03 eta 0:01:06
epoch [8/50] batch [5/5] time 0.035 (0.275) data 0.000 (0.230) loss 1.9105 (1.7433) acc 52.7778 (50.4646) lr 1.9048e-03 eta 0:00:57
epoch [9/50] batch [5/5] time 0.041 (0.250) data 0.000 (0.213) loss 1.7269 (1.8098) acc 47.5000 (47.7778) lr 1.8763e-03 eta 0:00:51
epoch [10/50] batch [5/5] time 0.027 (0.253) data 0.000 (0.210) loss 1.6959 (1.7511) acc 60.0000 (52.5000) lr 1.8443e-03 eta 0:00:50
epoch [11/50] batch [5/5] time 0.037 (0.298) data 0.000 (0.250) loss 1.5936 (1.6841) acc 56.2500 (52.6410) lr 1.8090e-03 eta 0:00:58
>>> samples [48/160] noisy rate: 0.50 --> 0.46 --> 0.23 <<<
epoch [12/50] batch [5/5] time 0.036 (0.303) data 0.000 (0.249) loss 1.8818 (1.6228) acc 45.8333 (60.1623) lr 1.7705e-03 eta 0:00:57
epoch [13/50] batch [5/5] time 0.036 (0.284) data 0.000 (0.232) loss 1.3315 (1.5476) acc 62.5000 (62.7564) lr 1.7290e-03 eta 0:00:52
epoch [14/50] batch [5/5] time 0.033 (0.286) data 0.000 (0.240) loss 1.3895 (1.5628) acc 75.0000 (61.4659) lr 1.6845e-03 eta 0:00:51
epoch [15/50] batch [5/5] time 0.044 (0.244) data 0.000 (0.208) loss 1.4918 (1.5242) acc 75.0000 (63.7857) lr 1.6374e-03 eta 0:00:42
epoch [16/50] batch [5/5] time 0.035 (0.287) data 0.000 (0.178) loss 1.6169 (1.5353) acc 71.8750 (65.8829) lr 1.5878e-03 eta 0:00:48
>>> samples [49/160] noisy rate: 0.50 --> 0.44 --> 0.22 <<<
epoch [17/50] batch [5/5] time 0.040 (0.276) data 0.000 (0.228) loss 1.7170 (1.5533) acc 57.1429 (64.9382) lr 1.5358e-03 eta 0:00:45
epoch [18/50] batch [5/5] time 0.050 (0.370) data 0.000 (0.249) loss 1.8245 (1.3928) acc 58.3333 (73.4167) lr 1.4818e-03 eta 0:00:59
epoch [19/50] batch [5/5] time 0.036 (0.365) data 0.000 (0.318) loss 1.5528 (1.4972) acc 64.2857 (65.0655) lr 1.4258e-03 eta 0:00:56
epoch [20/50] batch [5/5] time 0.044 (0.383) data 0.000 (0.339) loss 1.5883 (1.4163) acc 67.3077 (70.7790) lr 1.3681e-03 eta 0:00:57
epoch [21/50] batch [5/5] time 0.049 (0.251) data 0.000 (0.198) loss 1.3456 (1.4865) acc 82.1429 (73.3575) lr 1.3090e-03 eta 0:00:36
>>> samples [53/160] noisy rate: 0.50 --> 0.40 --> 0.21 <<<
epoch [22/50] batch [5/5] time 0.039 (0.348) data 0.000 (0.290) loss 1.5012 (1.3792) acc 57.1429 (69.9989) lr 1.2487e-03 eta 0:00:48
epoch [23/50] batch [5/5] time 0.047 (0.325) data 0.000 (0.267) loss 0.9424 (1.3310) acc 93.7500 (74.2619) lr 1.1874e-03 eta 0:00:43
epoch [24/50] batch [5/5] time 0.042 (0.269) data 0.000 (0.226) loss 1.1400 (1.3575) acc 80.0000 (70.7990) lr 1.1253e-03 eta 0:00:34
epoch [25/50] batch [5/5] time 0.048 (0.315) data 0.000 (0.260) loss 1.2650 (1.3441) acc 66.6667 (72.3258) lr 1.0628e-03 eta 0:00:39
epoch [26/50] batch [5/5] time 0.056 (0.405) data 0.000 (0.350) loss 1.6102 (1.3527) acc 59.6154 (68.3661) lr 1.0000e-03 eta 0:00:48
>>> samples [57/160] noisy rate: 0.50 --> 0.44 --> 0.26 <<<
epoch [27/50] batch [5/5] time 0.054 (0.580) data 0.000 (0.507) loss 1.1905 (1.3116) acc 82.5000 (80.2351) lr 9.3721e-04 eta 0:01:06
epoch [28/50] batch [5/5] time 0.048 (0.319) data 0.000 (0.268) loss 1.4179 (1.3070) acc 89.5833 (78.0985) lr 8.7467e-04 eta 0:00:35
epoch [29/50] batch [5/5] time 0.041 (0.345) data 0.000 (0.296) loss 1.4687 (1.3326) acc 70.4545 (73.5227) lr 8.1262e-04 eta 0:00:36
epoch [30/50] batch [5/5] time 0.052 (0.340) data 0.000 (0.281) loss 1.3837 (1.3711) acc 78.3333 (75.7861) lr 7.5131e-04 eta 0:00:33
epoch [31/50] batch [5/5] time 0.087 (0.337) data 0.000 (0.261) loss 1.1497 (1.2538) acc 79.1667 (76.6685) lr 6.9098e-04 eta 0:00:32
>>> samples [71/160] noisy rate: 0.50 --> 0.38 --> 0.27 <<<
epoch [32/50] batch [5/5] time 0.086 (0.866) data 0.000 (0.635) loss 1.2920 (1.2474) acc 75.0000 (79.5119) lr 6.3188e-04 eta 0:01:17
epoch [33/50] batch [5/5] time 0.103 (0.348) data 0.000 (0.255) loss 0.9205 (1.2361) acc 82.6923 (80.7370) lr 5.7422e-04 eta 0:00:29
epoch [34/50] batch [5/5] time 0.082 (0.369) data 0.001 (0.269) loss 1.1982 (1.1892) acc 75.0000 (81.3655) lr 5.1825e-04 eta 0:00:29
epoch [35/50] batch [5/5] time 0.120 (0.352) data 0.000 (0.257) loss 1.3780 (1.2004) acc 72.0588 (84.1972) lr 4.6417e-04 eta 0:00:26
epoch [36/50] batch [5/5] time 0.082 (0.591) data 0.000 (0.367) loss 1.4118 (1.3208) acc 70.0000 (75.5161) lr 4.1221e-04 eta 0:00:41
>>> samples [83/160] noisy rate: 0.50 --> 0.38 --> 0.29 <<<
epoch [37/50] batch [5/5] time 0.092 (0.736) data 0.000 (0.455) loss 1.3641 (1.2255) acc 83.9286 (80.5401) lr 3.6258e-04 eta 0:00:47
epoch [38/50] batch [5/5] time 0.057 (0.449) data 0.000 (0.304) loss 1.5138 (1.2784) acc 76.9231 (78.3013) lr 3.1545e-04 eta 0:00:26
epoch [39/50] batch [5/5] time 0.141 (0.589) data 0.001 (0.447) loss 1.2341 (1.2536) acc 76.4706 (77.2628) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [5/5] time 0.765 (0.661) data 0.000 (0.419) loss 1.0732 (1.2461) acc 85.5263 (77.4857) lr 2.2949e-04 eta 0:00:33
epoch [41/50] batch [5/5] time 0.101 (0.370) data 0.000 (0.256) loss 1.3732 (1.2542) acc 73.4375 (78.7161) lr 1.9098e-04 eta 0:00:16
>>> samples [84/160] noisy rate: 0.50 --> 0.41 --> 0.29 <<<
epoch [42/50] batch [5/5] time 0.101 (0.393) data 0.000 (0.293) loss 1.2352 (1.2548) acc 79.4118 (79.6969) lr 1.5567e-04 eta 0:00:15
epoch [43/50] batch [5/5] time 0.106 (0.390) data 0.001 (0.281) loss 1.3454 (1.2312) acc 80.8824 (78.8066) lr 1.2369e-04 eta 0:00:13
epoch [44/50] batch [5/5] time 0.067 (0.438) data 0.000 (0.336) loss 1.2826 (1.2445) acc 78.1250 (75.8661) lr 9.5173e-05 eta 0:00:13
epoch [45/50] batch [5/5] time 0.066 (0.409) data 0.000 (0.256) loss 1.2910 (1.2774) acc 75.0000 (78.8974) lr 7.0224e-05 eta 0:00:10
epoch [46/50] batch [5/5] time 0.103 (0.393) data 0.000 (0.301) loss 1.3739 (1.2438) acc 73.6111 (80.0452) lr 4.8943e-05 eta 0:00:07
>>> samples [86/160] noisy rate: 0.50 --> 0.37 --> 0.29 <<<
epoch [47/50] batch [5/5] time 0.045 (0.332) data 0.000 (0.271) loss 1.0270 (1.2486) acc 87.5000 (78.0283) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/5] time 0.124 (0.362) data 0.000 (0.252) loss 1.4156 (1.2633) acc 72.3684 (75.3364) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [5/5] time 0.129 (0.341) data 0.000 (0.226) loss 1.0470 (1.2790) acc 84.5238 (76.5232) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/5] time 0.131 (0.395) data 0.000 (0.286) loss 1.0164 (1.2692) acc 78.4091 (73.8354) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.49, 0.49, 0.46, 0.44, 0.4, 0.44, 0.38, 0.38, 0.41, 0.37]
* learned noise rate: [0.19, 0.22, 0.23, 0.22, 0.21, 0.26, 0.27, 0.29, 0.29, 0.29]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:10,  3.13s/it]  2%|▏         | 2/81 [00:03<01:47,  1.36s/it]  4%|▎         | 3/81 [00:03<01:01,  1.27it/s]  5%|▍         | 4/81 [00:03<00:40,  1.91it/s]  6%|▌         | 5/81 [00:03<00:28,  2.62it/s]  7%|▋         | 6/81 [00:03<00:21,  3.41it/s]  9%|▊         | 7/81 [00:03<00:17,  4.31it/s] 10%|▉         | 8/81 [00:03<00:14,  5.19it/s] 11%|█         | 9/81 [00:04<00:11,  6.11it/s] 12%|█▏        | 10/81 [00:04<00:10,  6.56it/s] 15%|█▍        | 12/81 [00:04<00:09,  7.51it/s] 16%|█▌        | 13/81 [00:04<00:08,  7.62it/s] 17%|█▋        | 14/81 [00:04<00:08,  7.62it/s] 19%|█▊        | 15/81 [00:04<00:08,  7.70it/s] 20%|█▉        | 16/81 [00:04<00:08,  7.87it/s] 21%|██        | 17/81 [00:05<00:07,  8.12it/s] 22%|██▏       | 18/81 [00:05<00:07,  8.40it/s] 25%|██▍       | 20/81 [00:05<00:07,  8.58it/s] 26%|██▌       | 21/81 [00:05<00:07,  8.45it/s] 27%|██▋       | 22/81 [00:05<00:06,  8.63it/s] 28%|██▊       | 23/81 [00:05<00:06,  8.40it/s] 31%|███       | 25/81 [00:05<00:06,  8.58it/s] 32%|███▏      | 26/81 [00:06<00:06,  8.35it/s] 33%|███▎      | 27/81 [00:06<00:06,  8.67it/s] 35%|███▍      | 28/81 [00:06<00:06,  8.77it/s] 36%|███▌      | 29/81 [00:06<00:06,  8.27it/s] 37%|███▋      | 30/81 [00:06<00:05,  8.60it/s] 38%|███▊      | 31/81 [00:06<00:05,  8.41it/s] 40%|███▉      | 32/81 [00:06<00:05,  8.72it/s] 41%|████      | 33/81 [00:06<00:05,  8.39it/s] 42%|████▏     | 34/81 [00:06<00:05,  8.40it/s] 43%|████▎     | 35/81 [00:07<00:05,  8.37it/s] 44%|████▍     | 36/81 [00:07<00:05,  8.02it/s] 46%|████▌     | 37/81 [00:07<00:05,  8.08it/s] 47%|████▋     | 38/81 [00:07<00:05,  8.34it/s] 48%|████▊     | 39/81 [00:07<00:05,  8.30it/s] 49%|████▉     | 40/81 [00:07<00:04,  8.32it/s] 51%|█████     | 41/81 [00:07<00:04,  8.56it/s] 52%|█████▏    | 42/81 [00:07<00:04,  8.39it/s] 53%|█████▎    | 43/81 [00:08<00:04,  8.36it/s] 54%|█████▍    | 44/81 [00:08<00:04,  8.31it/s] 56%|█████▌    | 45/81 [00:08<00:04,  8.53it/s] 57%|█████▋    | 46/81 [00:08<00:03,  8.81it/s] 58%|█████▊    | 47/81 [00:08<00:03,  8.63it/s] 59%|█████▉    | 48/81 [00:08<00:03,  8.36it/s] 60%|██████    | 49/81 [00:08<00:03,  8.53it/s] 62%|██████▏   | 50/81 [00:08<00:03,  8.44it/s] 63%|██████▎   | 51/81 [00:09<00:03,  8.46it/s] 64%|██████▍   | 52/81 [00:09<00:03,  8.72it/s] 65%|██████▌   | 53/81 [00:09<00:03,  8.61it/s] 67%|██████▋   | 54/81 [00:09<00:03,  8.54it/s] 68%|██████▊   | 55/81 [00:09<00:03,  8.56it/s] 69%|██████▉   | 56/81 [00:09<00:02,  8.47it/s] 70%|███████   | 57/81 [00:09<00:02,  8.41it/s] 72%|███████▏  | 58/81 [00:09<00:02,  8.36it/s] 73%|███████▎  | 59/81 [00:09<00:02,  8.04it/s] 74%|███████▍  | 60/81 [00:10<00:02,  8.08it/s] 75%|███████▌  | 61/81 [00:10<00:02,  8.44it/s] 77%|███████▋  | 62/81 [00:10<00:02,  8.40it/s] 78%|███████▊  | 63/81 [00:10<00:02,  8.37it/s] 79%|███████▉  | 64/81 [00:10<00:02,  8.40it/s] 80%|████████  | 65/81 [00:10<00:01,  8.15it/s] 81%|████████▏ | 66/81 [00:10<00:01,  8.10it/s] 84%|████████▍ | 68/81 [00:10<00:01,  9.60it/s] 86%|████████▋ | 70/81 [00:11<00:00, 11.35it/s] 89%|████████▉ | 72/81 [00:11<00:00, 12.66it/s] 91%|█████████▏| 74/81 [00:11<00:00, 13.60it/s] 94%|█████████▍| 76/81 [00:11<00:00, 14.28it/s] 96%|█████████▋| 78/81 [00:11<00:00, 14.77it/s] 99%|█████████▉| 80/81 [00:11<00:00, 15.13it/s]100%|██████████| 81/81 [00:12<00:00,  6.73it/s]
=> result
* total: 8,100
* correct: 4,040
* accuracy: 49.9%
* error: 50.1%
* macro_f1: 45.7%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 239	acc: 26.6%
* class: 1 (Forest)	total: 900	correct: 883	acc: 98.1%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 18	acc: 2.0%
* class: 3 (Highway or Road)	total: 750	correct: 321	acc: 42.8%
* class: 4 (Industrial Buildings)	total: 750	correct: 431	acc: 57.5%
* class: 5 (Pasture Land)	total: 600	correct: 349	acc: 58.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 605	acc: 80.7%
* class: 7 (Residential Buildings)	total: 900	correct: 829	acc: 92.1%
* class: 8 (River)	total: 750	correct: 302	acc: 40.3%
* class: 9 (Sea or Lake)	total: 900	correct: 63	acc: 7.0%
* average: 50.5%
Elapsed: 0:03:13
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_8-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.143 (1.306) data 0.000 (0.376) loss 1.1855 (1.1942) acc 16.4062 (13.5938) lr 2.0000e-03 eta 0:05:19
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [71/160] noisy rate: 0.50 --> 0.47 --> 0.28 <<<
epoch [2/50] batch [5/5] time 0.332 (0.766) data 0.000 (0.301) loss 2.2490 (2.1806) acc 25.0000 (28.4581) lr 1.9980e-03 eta 0:03:03
epoch [3/50] batch [5/5] time 0.692 (1.024) data 0.000 (0.399) loss 1.9522 (2.1063) acc 42.1875 (31.4296) lr 1.9921e-03 eta 0:04:00
epoch [4/50] batch [5/5] time 0.092 (0.541) data 0.000 (0.312) loss 2.0760 (2.0399) acc 36.6667 (40.5476) lr 1.9823e-03 eta 0:02:04
epoch [5/50] batch [5/5] time 0.051 (0.405) data 0.000 (0.292) loss 2.2043 (2.0569) acc 28.1250 (36.5673) lr 1.9686e-03 eta 0:01:31
epoch [6/50] batch [5/5] time 0.107 (0.419) data 0.000 (0.320) loss 1.9910 (1.9831) acc 45.0000 (44.8801) lr 1.9511e-03 eta 0:01:32
>>> samples [81/160] noisy rate: 0.50 --> 0.41 --> 0.30 <<<
epoch [7/50] batch [5/5] time 0.083 (0.492) data 0.001 (0.257) loss 1.9214 (1.9622) acc 58.9286 (48.8739) lr 1.9298e-03 eta 0:01:45
epoch [8/50] batch [5/5] time 0.115 (0.520) data 0.000 (0.255) loss 1.8667 (1.9518) acc 43.4211 (48.0148) lr 1.9048e-03 eta 0:01:49
epoch [9/50] batch [5/5] time 0.663 (0.789) data 0.000 (0.591) loss 1.9114 (1.9683) acc 55.6818 (49.8973) lr 1.8763e-03 eta 0:02:41
epoch [10/50] batch [5/5] time 0.131 (0.401) data 0.000 (0.296) loss 1.9696 (1.8907) acc 54.5455 (54.9413) lr 1.8443e-03 eta 0:01:20
epoch [11/50] batch [5/5] time 0.121 (0.576) data 0.000 (0.454) loss 2.0165 (1.8945) acc 48.2143 (56.1073) lr 1.8090e-03 eta 0:01:52
>>> samples [81/160] noisy rate: 0.50 --> 0.43 --> 0.30 <<<
epoch [12/50] batch [5/5] time 0.071 (0.502) data 0.000 (0.428) loss 1.9066 (1.8985) acc 63.2353 (57.2272) lr 1.7705e-03 eta 0:01:35
epoch [13/50] batch [5/5] time 0.119 (0.832) data 0.000 (0.702) loss 2.0021 (1.8974) acc 59.3750 (56.1485) lr 1.7290e-03 eta 0:02:33
epoch [14/50] batch [5/5] time 0.093 (0.407) data 0.000 (0.298) loss 1.8419 (1.8803) acc 53.3333 (55.0037) lr 1.6845e-03 eta 0:01:13
epoch [15/50] batch [5/5] time 0.072 (0.564) data 0.000 (0.457) loss 1.8036 (1.8546) acc 73.0769 (56.2352) lr 1.6374e-03 eta 0:01:38
epoch [16/50] batch [5/5] time 0.124 (0.551) data 0.000 (0.448) loss 1.9335 (1.8667) acc 48.6842 (54.6854) lr 1.5878e-03 eta 0:01:33
>>> samples [81/160] noisy rate: 0.50 --> 0.41 --> 0.30 <<<
epoch [17/50] batch [5/5] time 0.092 (0.364) data 0.000 (0.260) loss 1.7533 (1.8599) acc 56.6667 (61.2500) lr 1.5358e-03 eta 0:01:00
epoch [18/50] batch [5/5] time 0.063 (0.475) data 0.000 (0.398) loss 1.8663 (1.8403) acc 55.5556 (57.1626) lr 1.4818e-03 eta 0:01:15
epoch [19/50] batch [5/5] time 0.132 (0.456) data 0.000 (0.350) loss 1.7588 (1.8086) acc 63.1579 (61.2732) lr 1.4258e-03 eta 0:01:10
epoch [20/50] batch [5/5] time 0.102 (0.357) data 0.000 (0.252) loss 1.7833 (1.8011) acc 63.2353 (65.3978) lr 1.3681e-03 eta 0:00:53
epoch [21/50] batch [5/5] time 0.097 (0.506) data 0.000 (0.252) loss 1.6337 (1.8053) acc 78.3333 (61.6696) lr 1.3090e-03 eta 0:01:13
>>> samples [81/160] noisy rate: 0.50 --> 0.41 --> 0.30 <<<
epoch [22/50] batch [5/5] time 0.069 (0.389) data 0.000 (0.293) loss 1.7990 (1.8071) acc 80.5555 (63.1740) lr 1.2487e-03 eta 0:00:54
epoch [23/50] batch [5/5] time 0.113 (0.323) data 0.000 (0.219) loss 1.8315 (1.7892) acc 67.5000 (62.8194) lr 1.1874e-03 eta 0:00:43
epoch [24/50] batch [5/5] time 0.115 (0.492) data 0.000 (0.246) loss 1.8665 (1.7886) acc 64.0625 (64.4550) lr 1.1253e-03 eta 0:01:03
epoch [25/50] batch [5/5] time 0.078 (0.429) data 0.000 (0.325) loss 1.7114 (1.7686) acc 83.9286 (65.5864) lr 1.0628e-03 eta 0:00:53
epoch [26/50] batch [5/5] time 0.061 (0.288) data 0.001 (0.214) loss 1.8411 (1.7460) acc 56.5789 (64.7674) lr 1.0000e-03 eta 0:00:34
>>> samples [81/160] noisy rate: 0.50 --> 0.38 --> 0.30 <<<
epoch [27/50] batch [5/5] time 0.114 (0.482) data 0.000 (0.378) loss 1.6636 (1.7573) acc 67.6471 (64.7687) lr 9.3721e-04 eta 0:00:55
epoch [28/50] batch [5/5] time 0.121 (0.580) data 0.000 (0.465) loss 1.7920 (1.7677) acc 76.1905 (66.5039) lr 8.7467e-04 eta 0:01:03
epoch [29/50] batch [5/5] time 0.058 (0.422) data 0.000 (0.362) loss 1.6660 (1.7782) acc 68.3333 (66.1935) lr 8.1262e-04 eta 0:00:44
epoch [30/50] batch [5/5] time 0.122 (0.426) data 0.001 (0.313) loss 1.5984 (1.7275) acc 63.8889 (69.9338) lr 7.5131e-04 eta 0:00:42
epoch [31/50] batch [5/5] time 0.098 (0.586) data 0.000 (0.479) loss 1.9156 (1.7718) acc 67.6471 (65.9575) lr 6.9098e-04 eta 0:00:55
>>> samples [87/160] noisy rate: 0.50 --> 0.31 --> 0.30 <<<
epoch [32/50] batch [5/5] time 0.054 (0.324) data 0.000 (0.259) loss 1.7098 (1.7261) acc 55.0000 (67.9134) lr 6.3188e-04 eta 0:00:29
epoch [33/50] batch [5/5] time 0.104 (0.466) data 0.000 (0.361) loss 1.7971 (1.7318) acc 62.5000 (69.5537) lr 5.7422e-04 eta 0:00:39
epoch [34/50] batch [5/5] time 0.100 (0.431) data 0.000 (0.313) loss 1.7206 (1.7159) acc 54.1667 (65.2922) lr 5.1825e-04 eta 0:00:34
epoch [35/50] batch [5/5] time 0.061 (0.319) data 0.000 (0.249) loss 1.9220 (1.7044) acc 61.1111 (70.5688) lr 4.6417e-04 eta 0:00:23
epoch [36/50] batch [5/5] time 0.123 (0.401) data 0.000 (0.296) loss 1.6556 (1.7045) acc 59.7222 (68.3183) lr 4.1221e-04 eta 0:00:28
>>> samples [90/160] noisy rate: 0.50 --> 0.27 --> 0.30 <<<
epoch [37/50] batch [5/5] time 0.115 (0.806) data 0.000 (0.694) loss 1.6675 (1.7104) acc 70.8333 (70.2861) lr 3.6258e-04 eta 0:00:52
epoch [38/50] batch [5/5] time 0.078 (0.404) data 0.000 (0.294) loss 1.6548 (1.7030) acc 71.8750 (69.9830) lr 3.1545e-04 eta 0:00:24
epoch [39/50] batch [5/5] time 0.124 (0.432) data 0.001 (0.320) loss 1.5025 (1.6923) acc 64.7059 (69.0379) lr 2.7103e-04 eta 0:00:23
epoch [40/50] batch [5/5] time 0.118 (0.474) data 0.000 (0.363) loss 1.5763 (1.7038) acc 72.6190 (70.7777) lr 2.2949e-04 eta 0:00:23
epoch [41/50] batch [5/5] time 0.056 (0.340) data 0.000 (0.279) loss 1.6419 (1.6730) acc 68.3333 (73.9618) lr 1.9098e-04 eta 0:00:15
>>> samples [91/160] noisy rate: 0.50 --> 0.26 --> 0.31 <<<
epoch [42/50] batch [5/5] time 0.096 (0.656) data 0.000 (0.553) loss 1.8814 (1.7225) acc 58.8235 (64.5528) lr 1.5567e-04 eta 0:00:26
epoch [43/50] batch [5/5] time 0.130 (0.734) data 0.000 (0.623) loss 1.5712 (1.6917) acc 70.4545 (71.1981) lr 1.2369e-04 eta 0:00:25
epoch [44/50] batch [5/5] time 0.104 (0.491) data 0.000 (0.386) loss 1.5439 (1.7034) acc 60.9375 (66.2407) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [5/5] time 0.093 (0.584) data 0.000 (0.472) loss 1.7404 (1.6928) acc 66.0714 (68.5150) lr 7.0224e-05 eta 0:00:14
epoch [46/50] batch [5/5] time 0.129 (0.336) data 0.000 (0.224) loss 1.6429 (1.6793) acc 70.8333 (72.1474) lr 4.8943e-05 eta 0:00:06
>>> samples [92/160] noisy rate: 0.50 --> 0.26 --> 0.30 <<<
epoch [47/50] batch [5/5] time 0.070 (0.537) data 0.000 (0.458) loss 1.6945 (1.6797) acc 65.2778 (70.9785) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [5/5] time 0.135 (0.496) data 0.000 (0.360) loss 1.8386 (1.6783) acc 61.9048 (69.8915) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/5] time 0.087 (0.447) data 0.000 (0.336) loss 1.4498 (1.6682) acc 84.6154 (73.6064) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/5] time 0.064 (0.299) data 0.000 (0.232) loss 1.6773 (1.6695) acc 66.6667 (70.1325) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.47, 0.41, 0.43, 0.41, 0.41, 0.38, 0.31, 0.27, 0.26, 0.26]
* learned noise rate: [0.28, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:13,  3.17s/it]  2%|▏         | 2/81 [00:03<01:49,  1.38s/it]  4%|▎         | 3/81 [00:03<01:02,  1.24it/s]  5%|▍         | 4/81 [00:03<00:40,  1.88it/s]  6%|▌         | 5/81 [00:03<00:29,  2.55it/s]  7%|▋         | 6/81 [00:03<00:22,  3.35it/s]  9%|▊         | 7/81 [00:03<00:18,  4.08it/s] 10%|▉         | 8/81 [00:04<00:15,  4.84it/s] 11%|█         | 9/81 [00:04<00:12,  5.68it/s] 12%|█▏        | 10/81 [00:04<00:11,  6.20it/s] 14%|█▎        | 11/81 [00:04<00:10,  6.70it/s] 15%|█▍        | 12/81 [00:04<00:09,  7.02it/s] 16%|█▌        | 13/81 [00:04<00:08,  7.60it/s] 17%|█▋        | 14/81 [00:04<00:08,  7.75it/s] 19%|█▊        | 15/81 [00:04<00:08,  7.92it/s] 20%|█▉        | 16/81 [00:05<00:08,  8.10it/s] 21%|██        | 17/81 [00:05<00:07,  8.04it/s] 22%|██▏       | 18/81 [00:05<00:07,  8.11it/s] 23%|██▎       | 19/81 [00:05<00:07,  8.15it/s] 25%|██▍       | 20/81 [00:05<00:07,  7.73it/s] 26%|██▌       | 21/81 [00:05<00:07,  8.29it/s] 27%|██▋       | 22/81 [00:05<00:07,  8.22it/s] 28%|██▊       | 23/81 [00:05<00:07,  8.19it/s] 30%|██▉       | 24/81 [00:05<00:06,  8.17it/s] 31%|███       | 25/81 [00:06<00:06,  8.17it/s] 32%|███▏      | 26/81 [00:06<00:06,  7.97it/s] 33%|███▎      | 27/81 [00:06<00:06,  8.13it/s] 35%|███▍      | 28/81 [00:06<00:06,  8.15it/s] 36%|███▌      | 29/81 [00:06<00:06,  7.92it/s] 37%|███▋      | 30/81 [00:06<00:06,  8.00it/s] 38%|███▊      | 31/81 [00:06<00:06,  8.07it/s] 40%|███▉      | 32/81 [00:06<00:06,  7.98it/s] 41%|████      | 33/81 [00:07<00:05,  8.30it/s] 42%|████▏     | 34/81 [00:07<00:05,  8.23it/s] 43%|████▎     | 35/81 [00:07<00:05,  8.27it/s] 44%|████▍     | 36/81 [00:07<00:05,  8.21it/s] 46%|████▌     | 37/81 [00:07<00:05,  8.09it/s] 47%|████▋     | 38/81 [00:07<00:05,  8.37it/s] 48%|████▊     | 39/81 [00:07<00:05,  8.29it/s] 49%|████▉     | 40/81 [00:07<00:04,  8.26it/s] 51%|█████     | 41/81 [00:08<00:04,  8.48it/s] 52%|█████▏    | 42/81 [00:08<00:04,  8.19it/s] 53%|█████▎    | 43/81 [00:08<00:04,  8.20it/s] 54%|█████▍    | 44/81 [00:08<00:04,  8.20it/s] 56%|█████▌    | 45/81 [00:08<00:04,  8.06it/s] 57%|█████▋    | 46/81 [00:08<00:04,  8.30it/s] 58%|█████▊    | 47/81 [00:08<00:04,  8.04it/s] 59%|█████▉    | 48/81 [00:08<00:04,  8.02it/s] 62%|██████▏   | 50/81 [00:09<00:03,  9.61it/s] 64%|██████▍   | 52/81 [00:09<00:02, 11.11it/s] 67%|██████▋   | 54/81 [00:09<00:02, 12.29it/s] 69%|██████▉   | 56/81 [00:09<00:01, 13.18it/s] 72%|███████▏  | 58/81 [00:09<00:01, 13.76it/s] 74%|███████▍  | 60/81 [00:09<00:01, 14.18it/s] 77%|███████▋  | 62/81 [00:09<00:01, 13.87it/s] 79%|███████▉  | 64/81 [00:10<00:01, 13.91it/s] 81%|████████▏ | 66/81 [00:10<00:01, 13.74it/s] 84%|████████▍ | 68/81 [00:10<00:00, 14.09it/s] 86%|████████▋ | 70/81 [00:10<00:00, 14.41it/s] 89%|████████▉ | 72/81 [00:10<00:00, 14.58it/s] 91%|█████████▏| 74/81 [00:10<00:00, 14.76it/s] 94%|█████████▍| 76/81 [00:10<00:00, 14.95it/s] 96%|█████████▋| 78/81 [00:10<00:00, 14.99it/s] 99%|█████████▉| 80/81 [00:11<00:00, 15.11it/s]100%|██████████| 81/81 [00:11<00:00,  6.99it/s]
=> result
* total: 8,100
* correct: 4,646
* accuracy: 57.4%
* error: 42.6%
* macro_f1: 53.8%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 208	acc: 23.1%
* class: 1 (Forest)	total: 900	correct: 830	acc: 92.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 58	acc: 6.4%
* class: 3 (Highway or Road)	total: 750	correct: 425	acc: 56.7%
* class: 4 (Industrial Buildings)	total: 750	correct: 684	acc: 91.2%
* class: 5 (Pasture Land)	total: 600	correct: 402	acc: 67.0%
* class: 6 (Permanent Crop Land)	total: 750	correct: 590	acc: 78.7%
* class: 7 (Residential Buildings)	total: 900	correct: 702	acc: 78.0%
* class: 8 (River)	total: 750	correct: 129	acc: 17.2%
* class: 9 (Sea or Lake)	total: 900	correct: 618	acc: 68.7%
* average: 57.9%
Elapsed: 0:04:05
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_symflip/fp_8-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.166 (1.295) data 0.000 (0.465) loss 1.2205 (1.1587) acc 17.1875 (13.2812) lr 2.0000e-03 eta 0:05:17
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [61/160] noisy rate: 0.50 --> 0.52 --> 0.25 <<<
epoch [2/50] batch [5/5] time 0.606 (1.124) data 0.000 (0.487) loss 2.3083 (2.1931) acc 19.4444 (20.5198) lr 1.9980e-03 eta 0:04:29
epoch [3/50] batch [5/5] time 0.047 (0.565) data 0.000 (0.272) loss 2.0467 (2.1290) acc 39.5833 (26.9327) lr 1.9921e-03 eta 0:02:12
epoch [4/50] batch [5/5] time 0.073 (0.717) data 0.000 (0.474) loss 2.0419 (2.0794) acc 25.0000 (32.5952) lr 1.9823e-03 eta 0:02:44
epoch [5/50] batch [5/5] time 0.074 (0.307) data 0.000 (0.221) loss 2.0655 (2.0203) acc 34.0909 (38.9641) lr 1.9686e-03 eta 0:01:09
epoch [6/50] batch [5/5] time 0.048 (0.278) data 0.000 (0.196) loss 1.9772 (1.9915) acc 45.0000 (38.1690) lr 1.9511e-03 eta 0:01:01
>>> samples [68/160] noisy rate: 0.50 --> 0.51 --> 0.25 <<<
epoch [7/50] batch [5/5] time 0.053 (0.370) data 0.000 (0.315) loss 1.9131 (2.0073) acc 46.1538 (37.9451) lr 1.9298e-03 eta 0:01:19
epoch [8/50] batch [5/5] time 0.101 (0.598) data 0.000 (0.360) loss 1.8571 (1.9564) acc 51.7857 (44.9278) lr 1.9048e-03 eta 0:02:05
epoch [9/50] batch [5/5] time 0.076 (0.368) data 0.000 (0.270) loss 1.9822 (1.9171) acc 29.5455 (47.3816) lr 1.8763e-03 eta 0:01:15
epoch [10/50] batch [5/5] time 0.099 (0.330) data 0.000 (0.235) loss 1.6708 (1.9074) acc 55.3571 (46.1667) lr 1.8443e-03 eta 0:01:06
epoch [11/50] batch [5/5] time 0.704 (0.443) data 0.000 (0.247) loss 1.8392 (1.8913) acc 54.6875 (48.6939) lr 1.8090e-03 eta 0:01:26
>>> samples [76/160] noisy rate: 0.50 --> 0.46 --> 0.25 <<<
epoch [12/50] batch [5/5] time 0.809 (0.539) data 0.000 (0.292) loss 1.8203 (1.9220) acc 52.3810 (49.8187) lr 1.7705e-03 eta 0:01:42
epoch [13/50] batch [5/5] time 0.115 (0.571) data 0.000 (0.455) loss 1.8841 (1.8811) acc 63.3333 (50.0694) lr 1.7290e-03 eta 0:01:45
epoch [14/50] batch [5/5] time 0.056 (0.352) data 0.000 (0.220) loss 1.8509 (1.8486) acc 54.4118 (57.9251) lr 1.6845e-03 eta 0:01:03
epoch [15/50] batch [5/5] time 0.084 (0.321) data 0.000 (0.224) loss 2.0823 (1.8462) acc 41.0714 (52.8539) lr 1.6374e-03 eta 0:00:56
epoch [16/50] batch [5/5] time 0.080 (0.383) data 0.000 (0.276) loss 1.9501 (1.8837) acc 60.0000 (55.7021) lr 1.5878e-03 eta 0:01:05
>>> samples [80/160] noisy rate: 0.50 --> 0.44 --> 0.26 <<<
epoch [17/50] batch [5/5] time 0.098 (0.300) data 0.000 (0.199) loss 1.6319 (1.8273) acc 60.2941 (55.7265) lr 1.5358e-03 eta 0:00:49
epoch [18/50] batch [5/5] time 0.119 (0.578) data 0.000 (0.314) loss 1.7550 (1.8108) acc 66.1765 (61.8734) lr 1.4818e-03 eta 0:01:32
epoch [19/50] batch [5/5] time 0.125 (0.546) data 0.000 (0.433) loss 1.6232 (1.7822) acc 72.0588 (62.8505) lr 1.4258e-03 eta 0:01:24
epoch [20/50] batch [5/5] time 0.056 (0.389) data 0.000 (0.317) loss 1.7133 (1.8172) acc 60.9375 (56.4236) lr 1.3681e-03 eta 0:00:58
epoch [21/50] batch [5/5] time 0.072 (0.444) data 0.000 (0.378) loss 1.9573 (1.7932) acc 58.3333 (55.7105) lr 1.3090e-03 eta 0:01:04
>>> samples [82/160] noisy rate: 0.50 --> 0.43 --> 0.28 <<<
epoch [22/50] batch [5/5] time 0.957 (0.643) data 0.000 (0.371) loss 1.7402 (1.7684) acc 66.3043 (62.9672) lr 1.2487e-03 eta 0:01:30
epoch [23/50] batch [5/5] time 0.075 (0.342) data 0.000 (0.237) loss 1.8499 (1.7988) acc 54.1667 (60.5421) lr 1.1874e-03 eta 0:00:46
epoch [24/50] batch [5/5] time 0.119 (0.381) data 0.000 (0.276) loss 1.6975 (1.7624) acc 67.6471 (68.4007) lr 1.1253e-03 eta 0:00:49
epoch [25/50] batch [5/5] time 0.118 (0.452) data 0.001 (0.332) loss 2.1808 (1.7868) acc 66.6667 (65.7054) lr 1.0628e-03 eta 0:00:56
epoch [26/50] batch [5/5] time 0.081 (0.489) data 0.000 (0.386) loss 1.6725 (1.7421) acc 57.1429 (64.9916) lr 1.0000e-03 eta 0:00:58
>>> samples [83/160] noisy rate: 0.50 --> 0.47 --> 0.28 <<<
epoch [27/50] batch [5/5] time 0.124 (0.320) data 0.000 (0.208) loss 1.6393 (1.7689) acc 64.7059 (64.9160) lr 9.3721e-04 eta 0:00:36
epoch [28/50] batch [5/5] time 0.044 (0.295) data 0.000 (0.220) loss 1.7850 (1.7768) acc 57.6923 (64.6817) lr 8.7467e-04 eta 0:00:32
epoch [29/50] batch [5/5] time 0.132 (0.328) data 0.000 (0.252) loss 1.6669 (1.7628) acc 78.7500 (67.1240) lr 8.1262e-04 eta 0:00:34
epoch [30/50] batch [5/5] time 0.125 (0.307) data 0.000 (0.195) loss 1.7284 (1.7452) acc 72.5000 (70.0882) lr 7.5131e-04 eta 0:00:30
epoch [31/50] batch [5/5] time 0.077 (0.282) data 0.000 (0.175) loss 1.7794 (1.7384) acc 75.0000 (69.6422) lr 6.9098e-04 eta 0:00:26
>>> samples [85/160] noisy rate: 0.50 --> 0.44 --> 0.28 <<<
epoch [32/50] batch [5/5] time 0.113 (0.331) data 0.000 (0.214) loss 1.8227 (1.7717) acc 55.2632 (64.6214) lr 6.3188e-04 eta 0:00:29
epoch [33/50] batch [5/5] time 0.051 (0.383) data 0.000 (0.310) loss 1.6654 (1.7554) acc 59.6154 (62.9394) lr 5.7422e-04 eta 0:00:32
epoch [34/50] batch [5/5] time 0.087 (0.547) data 0.000 (0.305) loss 1.5653 (1.7227) acc 80.3571 (68.5012) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [5/5] time 0.127 (0.441) data 0.000 (0.327) loss 2.0363 (1.7783) acc 47.2222 (68.4178) lr 4.6417e-04 eta 0:00:33
epoch [36/50] batch [5/5] time 0.127 (0.385) data 0.000 (0.271) loss 1.6569 (1.7066) acc 73.8095 (71.8660) lr 4.1221e-04 eta 0:00:26
>>> samples [87/160] noisy rate: 0.50 --> 0.41 --> 0.29 <<<
epoch [37/50] batch [5/5] time 0.063 (0.310) data 0.000 (0.205) loss 1.8032 (1.7298) acc 59.2105 (68.7401) lr 3.6258e-04 eta 0:00:20
epoch [38/50] batch [5/5] time 0.097 (0.361) data 0.000 (0.263) loss 1.5330 (1.7106) acc 77.6316 (69.5757) lr 3.1545e-04 eta 0:00:21
epoch [39/50] batch [5/5] time 0.110 (0.447) data 0.000 (0.336) loss 1.6149 (1.7128) acc 75.0000 (69.8611) lr 2.7103e-04 eta 0:00:24
epoch [40/50] batch [5/5] time 0.107 (0.419) data 0.000 (0.314) loss 1.7185 (1.7160) acc 73.5294 (68.6401) lr 2.2949e-04 eta 0:00:20
epoch [41/50] batch [5/5] time 0.121 (0.465) data 0.000 (0.348) loss 1.4985 (1.7288) acc 84.3750 (69.6250) lr 1.9098e-04 eta 0:00:20
>>> samples [89/160] noisy rate: 0.50 --> 0.42 --> 0.29 <<<
epoch [42/50] batch [5/5] time 0.105 (0.364) data 0.000 (0.249) loss 1.5895 (1.7025) acc 63.3333 (69.4720) lr 1.5567e-04 eta 0:00:14
epoch [43/50] batch [5/5] time 0.105 (0.381) data 0.000 (0.272) loss 1.7978 (1.7063) acc 57.3529 (67.5956) lr 1.2369e-04 eta 0:00:13
epoch [44/50] batch [5/5] time 0.145 (0.406) data 0.000 (0.275) loss 1.6486 (1.7286) acc 70.2381 (68.9782) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [5/5] time 0.050 (0.363) data 0.000 (0.299) loss 1.4914 (1.6935) acc 87.5000 (73.8116) lr 7.0224e-05 eta 0:00:09
epoch [46/50] batch [5/5] time 0.112 (0.381) data 0.000 (0.267) loss 1.7004 (1.7064) acc 76.3158 (73.4862) lr 4.8943e-05 eta 0:00:07
>>> samples [90/160] noisy rate: 0.50 --> 0.41 --> 0.30 <<<
epoch [47/50] batch [5/5] time 0.107 (0.499) data 0.000 (0.377) loss 1.5901 (1.7034) acc 77.9412 (74.6108) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/5] time 0.108 (0.477) data 0.000 (0.358) loss 1.8160 (1.7213) acc 63.2353 (71.0473) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/5] time 0.131 (0.475) data 0.001 (0.342) loss 1.6690 (1.7214) acc 76.3158 (71.6297) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/5] time 0.144 (0.449) data 0.000 (0.338) loss 1.6036 (1.7046) acc 64.2857 (69.0333) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.52, 0.51, 0.46, 0.44, 0.43, 0.47, 0.44, 0.41, 0.42, 0.41]
* learned noise rate: [0.25, 0.25, 0.25, 0.26, 0.28, 0.28, 0.28, 0.29, 0.29, 0.3]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<05:10,  3.88s/it]  4%|▎         | 3/81 [00:04<01:22,  1.05s/it]  6%|▌         | 5/81 [00:04<00:41,  1.83it/s]  9%|▊         | 7/81 [00:04<00:25,  2.91it/s] 11%|█         | 9/81 [00:04<00:17,  4.14it/s] 14%|█▎        | 11/81 [00:04<00:12,  5.53it/s] 16%|█▌        | 13/81 [00:04<00:10,  6.75it/s] 19%|█▊        | 15/81 [00:04<00:08,  7.90it/s] 21%|██        | 17/81 [00:05<00:08,  8.00it/s] 23%|██▎       | 19/81 [00:05<00:07,  8.54it/s] 26%|██▌       | 21/81 [00:05<00:07,  8.14it/s] 27%|██▋       | 22/81 [00:05<00:07,  8.25it/s] 28%|██▊       | 23/81 [00:05<00:07,  8.09it/s] 30%|██▉       | 24/81 [00:05<00:06,  8.30it/s] 31%|███       | 25/81 [00:06<00:06,  8.01it/s] 32%|███▏      | 26/81 [00:06<00:06,  8.07it/s] 33%|███▎      | 27/81 [00:06<00:06,  8.02it/s] 36%|███▌      | 29/81 [00:06<00:06,  8.28it/s] 37%|███▋      | 30/81 [00:06<00:06,  8.27it/s] 38%|███▊      | 31/81 [00:06<00:05,  8.34it/s] 40%|███▉      | 32/81 [00:06<00:06,  7.86it/s] 41%|████      | 33/81 [00:07<00:06,  7.67it/s] 42%|████▏     | 34/81 [00:07<00:05,  8.07it/s] 44%|████▍     | 36/81 [00:07<00:05,  8.43it/s] 46%|████▌     | 37/81 [00:07<00:05,  8.71it/s] 47%|████▋     | 38/81 [00:07<00:05,  8.44it/s] 48%|████▊     | 39/81 [00:07<00:04,  8.46it/s] 49%|████▉     | 40/81 [00:07<00:04,  8.56it/s] 51%|█████     | 41/81 [00:08<00:04,  8.42it/s] 52%|█████▏    | 42/81 [00:08<00:04,  8.33it/s] 53%|█████▎    | 43/81 [00:08<00:04,  7.63it/s] 56%|█████▌    | 45/81 [00:08<00:04,  8.35it/s] 57%|█████▋    | 46/81 [00:08<00:04,  8.18it/s] 58%|█████▊    | 47/81 [00:08<00:04,  7.67it/s] 59%|█████▉    | 48/81 [00:08<00:04,  8.03it/s] 60%|██████    | 49/81 [00:09<00:04,  7.60it/s] 62%|██████▏   | 50/81 [00:09<00:03,  7.80it/s] 63%|██████▎   | 51/81 [00:09<00:03,  7.76it/s] 64%|██████▍   | 52/81 [00:09<00:04,  7.08it/s] 65%|██████▌   | 53/81 [00:09<00:03,  7.13it/s] 67%|██████▋   | 54/81 [00:09<00:03,  7.34it/s] 68%|██████▊   | 55/81 [00:09<00:03,  6.66it/s] 69%|██████▉   | 56/81 [00:10<00:03,  6.71it/s] 70%|███████   | 57/81 [00:10<00:03,  7.34it/s] 72%|███████▏  | 58/81 [00:10<00:03,  7.14it/s] 73%|███████▎  | 59/81 [00:10<00:03,  7.17it/s] 74%|███████▍  | 60/81 [00:10<00:02,  7.37it/s] 75%|███████▌  | 61/81 [00:10<00:02,  7.94it/s] 77%|███████▋  | 62/81 [00:10<00:02,  7.68it/s] 78%|███████▊  | 63/81 [00:10<00:02,  7.89it/s] 79%|███████▉  | 64/81 [00:11<00:02,  7.63it/s] 80%|████████  | 65/81 [00:11<00:02,  7.27it/s] 81%|████████▏ | 66/81 [00:11<00:02,  6.68it/s] 83%|████████▎ | 67/81 [00:11<00:01,  7.20it/s] 84%|████████▍ | 68/81 [00:11<00:01,  7.04it/s] 85%|████████▌ | 69/81 [00:11<00:01,  7.31it/s] 86%|████████▋ | 70/81 [00:11<00:01,  7.41it/s] 88%|████████▊ | 71/81 [00:12<00:01,  7.57it/s] 89%|████████▉ | 72/81 [00:12<00:01,  7.89it/s] 90%|█████████ | 73/81 [00:12<00:00,  8.17it/s] 91%|█████████▏| 74/81 [00:12<00:00,  8.02it/s] 93%|█████████▎| 75/81 [00:12<00:00,  8.12it/s] 94%|█████████▍| 76/81 [00:12<00:00,  8.17it/s] 96%|█████████▋| 78/81 [00:12<00:00,  8.45it/s] 98%|█████████▊| 79/81 [00:13<00:00,  8.50it/s] 99%|█████████▉| 80/81 [00:13<00:00,  8.39it/s]100%|██████████| 81/81 [00:13<00:00,  6.00it/s]
=> result
* total: 8,100
* correct: 4,219
* accuracy: 52.1%
* error: 47.9%
* macro_f1: 46.6%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 3	acc: 0.3%
* class: 1 (Forest)	total: 900	correct: 850	acc: 94.4%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 21	acc: 2.3%
* class: 3 (Highway or Road)	total: 750	correct: 539	acc: 71.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 712	acc: 94.9%
* class: 5 (Pasture Land)	total: 600	correct: 236	acc: 39.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 576	acc: 76.8%
* class: 7 (Residential Buildings)	total: 900	correct: 770	acc: 85.6%
* class: 8 (River)	total: 750	correct: 386	acc: 51.5%
* class: 9 (Sea or Lake)	total: 900	correct: 126	acc: 14.0%
* average: 53.1%
Elapsed: 0:03:47
