nohup: ignoring input
Run this job and save the output to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_0-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.262 (1.149) data 0.000 (0.444) loss 3.3595 (3.4417) acc 18.7500 (21.2500) lr 1.0000e-05 eta 0:21:55
epoch [1/50] batch [10/23] time 0.263 (0.705) data 0.000 (0.222) loss 3.5178 (3.4424) acc 9.3750 (19.3750) lr 1.0000e-05 eta 0:13:23
epoch [1/50] batch [15/23] time 0.254 (0.559) data 0.000 (0.148) loss 3.2088 (3.4139) acc 25.0000 (19.5833) lr 1.0000e-05 eta 0:10:34
epoch [1/50] batch [20/23] time 0.262 (0.485) data 0.000 (0.111) loss 3.3061 (3.3639) acc 25.0000 (20.9375) lr 1.0000e-05 eta 0:09:07
epoch [2/50] batch [5/23] time 0.176 (0.488) data 0.000 (0.286) loss 2.5954 (3.0336) acc 21.8750 (18.1250) lr 2.0000e-03 eta 0:09:07
epoch [2/50] batch [10/23] time 0.198 (0.345) data 0.000 (0.144) loss 2.0477 (2.6055) acc 37.5000 (29.0625) lr 2.0000e-03 eta 0:06:25
epoch [2/50] batch [15/23] time 0.202 (0.296) data 0.000 (0.096) loss 2.4133 (2.5052) acc 40.6250 (32.0833) lr 2.0000e-03 eta 0:05:29
epoch [2/50] batch [20/23] time 0.228 (0.274) data 0.000 (0.072) loss 1.6339 (2.3547) acc 50.0000 (36.0938) lr 2.0000e-03 eta 0:05:03
epoch [3/50] batch [5/23] time 0.233 (0.497) data 0.000 (0.254) loss 1.6542 (1.7478) acc 46.8750 (53.1250) lr 1.9980e-03 eta 0:09:05
epoch [3/50] batch [10/23] time 0.205 (0.354) data 0.000 (0.127) loss 2.4105 (1.8006) acc 40.6250 (52.5000) lr 1.9980e-03 eta 0:06:27
epoch [3/50] batch [15/23] time 0.208 (0.304) data 0.000 (0.085) loss 1.2594 (1.6854) acc 62.5000 (53.7500) lr 1.9980e-03 eta 0:05:30
epoch [3/50] batch [20/23] time 0.192 (0.281) data 0.000 (0.064) loss 1.8712 (1.7555) acc 53.1250 (52.6562) lr 1.9980e-03 eta 0:05:04
epoch [4/50] batch [5/23] time 0.203 (0.492) data 0.000 (0.271) loss 0.8502 (1.6320) acc 78.1250 (52.5000) lr 1.9921e-03 eta 0:08:49
epoch [4/50] batch [10/23] time 0.203 (0.351) data 0.000 (0.136) loss 1.5201 (1.6181) acc 53.1250 (52.1875) lr 1.9921e-03 eta 0:06:16
epoch [4/50] batch [15/23] time 0.223 (0.303) data 0.000 (0.091) loss 1.0749 (1.5060) acc 78.1250 (57.0833) lr 1.9921e-03 eta 0:05:23
epoch [4/50] batch [20/23] time 0.192 (0.278) data 0.000 (0.068) loss 1.7967 (1.5080) acc 53.1250 (57.9688) lr 1.9921e-03 eta 0:04:54
epoch [5/50] batch [5/23] time 0.205 (0.561) data 0.000 (0.325) loss 1.3678 (1.4337) acc 75.0000 (63.1250) lr 1.9823e-03 eta 0:09:51
epoch [5/50] batch [10/23] time 0.220 (0.388) data 0.000 (0.163) loss 1.2436 (1.3689) acc 56.2500 (62.8125) lr 1.9823e-03 eta 0:06:46
epoch [5/50] batch [15/23] time 0.272 (0.332) data 0.000 (0.109) loss 1.1057 (1.3319) acc 68.7500 (63.5417) lr 1.9823e-03 eta 0:05:46
epoch [5/50] batch [20/23] time 0.192 (0.298) data 0.000 (0.081) loss 1.5086 (1.3788) acc 62.5000 (62.3438) lr 1.9823e-03 eta 0:05:09
epoch [6/50] batch [5/23] time 0.214 (0.521) data 0.000 (0.285) loss 1.4155 (1.4257) acc 59.3750 (61.2500) lr 1.9686e-03 eta 0:08:56
epoch [6/50] batch [10/23] time 0.214 (0.368) data 0.000 (0.143) loss 1.8744 (1.3860) acc 50.0000 (59.6875) lr 1.9686e-03 eta 0:06:17
epoch [6/50] batch [15/23] time 0.193 (0.315) data 0.000 (0.095) loss 1.2018 (1.3034) acc 65.6250 (62.5000) lr 1.9686e-03 eta 0:05:21
epoch [6/50] batch [20/23] time 0.198 (0.285) data 0.000 (0.071) loss 0.8678 (1.2911) acc 75.0000 (63.1250) lr 1.9686e-03 eta 0:04:49
epoch [7/50] batch [5/23] time 0.211 (0.493) data 0.000 (0.248) loss 1.0270 (1.1049) acc 62.5000 (67.5000) lr 1.9511e-03 eta 0:08:16
epoch [7/50] batch [10/23] time 0.199 (0.351) data 0.000 (0.124) loss 1.0414 (1.1901) acc 68.7500 (67.1875) lr 1.9511e-03 eta 0:05:51
epoch [7/50] batch [15/23] time 0.246 (0.302) data 0.000 (0.083) loss 1.2039 (1.1988) acc 68.7500 (67.0833) lr 1.9511e-03 eta 0:05:01
epoch [7/50] batch [20/23] time 0.201 (0.278) data 0.000 (0.062) loss 1.4711 (1.2012) acc 62.5000 (67.5000) lr 1.9511e-03 eta 0:04:35
epoch [8/50] batch [5/23] time 0.211 (0.470) data 0.000 (0.252) loss 1.5469 (1.2174) acc 56.2500 (61.8750) lr 1.9298e-03 eta 0:07:42
epoch [8/50] batch [10/23] time 0.203 (0.342) data 0.000 (0.126) loss 1.2175 (1.2156) acc 62.5000 (63.1250) lr 1.9298e-03 eta 0:05:35
epoch [8/50] batch [15/23] time 0.203 (0.301) data 0.000 (0.084) loss 0.9572 (1.1611) acc 78.1250 (66.6667) lr 1.9298e-03 eta 0:04:52
epoch [8/50] batch [20/23] time 0.197 (0.276) data 0.000 (0.063) loss 1.3653 (1.1559) acc 71.8750 (67.9688) lr 1.9298e-03 eta 0:04:26
epoch [9/50] batch [5/23] time 0.203 (0.486) data 0.000 (0.258) loss 0.9679 (1.3021) acc 75.0000 (65.0000) lr 1.9048e-03 eta 0:07:46
epoch [9/50] batch [10/23] time 0.198 (0.347) data 0.000 (0.129) loss 1.0727 (1.1527) acc 68.7500 (70.6250) lr 1.9048e-03 eta 0:05:31
epoch [9/50] batch [15/23] time 0.192 (0.301) data 0.000 (0.086) loss 0.5781 (1.1051) acc 81.2500 (70.4167) lr 1.9048e-03 eta 0:04:46
epoch [9/50] batch [20/23] time 0.196 (0.274) data 0.000 (0.065) loss 1.2275 (1.1286) acc 56.2500 (68.4375) lr 1.9048e-03 eta 0:04:19
epoch [10/50] batch [5/23] time 0.195 (0.519) data 0.000 (0.301) loss 1.4488 (1.0326) acc 59.3750 (70.6250) lr 1.8763e-03 eta 0:08:06
epoch [10/50] batch [10/23] time 0.198 (0.367) data 0.000 (0.150) loss 0.7959 (1.0694) acc 75.0000 (68.1250) lr 1.8763e-03 eta 0:05:42
epoch [10/50] batch [15/23] time 0.200 (0.316) data 0.000 (0.100) loss 0.6121 (1.0306) acc 87.5000 (70.4167) lr 1.8763e-03 eta 0:04:53
epoch [10/50] batch [20/23] time 0.201 (0.287) data 0.000 (0.075) loss 1.1209 (1.0736) acc 62.5000 (68.4375) lr 1.8763e-03 eta 0:04:24
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.383  alpha2: -0.090 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [11/50] batch [5/23] time 0.259 (1.613) data 0.001 (0.296) loss 0.8474 (0.8396) acc 73.5000 (77.0305) lr 1.8443e-03 eta 0:24:35
epoch [11/50] batch [10/23] time 0.143 (1.292) data 0.000 (0.148) loss 0.9043 (0.8496) acc 71.7391 (75.3580) lr 1.8443e-03 eta 0:19:35
epoch [11/50] batch [15/23] time 0.968 (0.964) data 0.001 (0.099) loss 0.7086 (0.8363) acc 75.9804 (75.3894) lr 1.8443e-03 eta 0:14:32
epoch [11/50] batch [20/23] time 0.234 (0.899) data 0.000 (0.074) loss 0.7391 (0.8217) acc 79.8077 (75.5647) lr 1.8443e-03 eta 0:13:29
>>> alpha1: 0.368  alpha2: -0.072 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/23] time 0.183 (0.645) data 0.000 (0.240) loss 0.7204 (0.6847) acc 82.8125 (78.8922) lr 1.8090e-03 eta 0:09:35
epoch [12/50] batch [10/23] time 0.178 (0.489) data 0.000 (0.120) loss 0.8181 (0.7132) acc 75.0000 (77.4414) lr 1.8090e-03 eta 0:07:13
epoch [12/50] batch [15/23] time 0.191 (0.390) data 0.000 (0.080) loss 0.7865 (0.6908) acc 77.5510 (78.6020) lr 1.8090e-03 eta 0:05:43
epoch [12/50] batch [20/23] time 0.180 (0.339) data 0.000 (0.060) loss 0.5327 (0.6843) acc 81.9149 (79.0290) lr 1.8090e-03 eta 0:04:57
>>> alpha1: 0.362  alpha2: -0.073 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.28 <<<
epoch [13/50] batch [5/23] time 0.184 (0.475) data 0.000 (0.287) loss 0.7911 (0.6493) acc 69.7917 (78.3212) lr 1.7705e-03 eta 0:06:53
epoch [13/50] batch [10/23] time 0.194 (0.443) data 0.000 (0.143) loss 0.4644 (0.6767) acc 89.7059 (79.5381) lr 1.7705e-03 eta 0:06:22
epoch [13/50] batch [15/23] time 0.187 (0.356) data 0.000 (0.096) loss 0.9903 (0.6631) acc 71.3542 (78.4604) lr 1.7705e-03 eta 0:05:05
epoch [13/50] batch [20/23] time 0.206 (0.313) data 0.000 (0.072) loss 0.9923 (0.6879) acc 62.7358 (77.3349) lr 1.7705e-03 eta 0:04:27
>>> alpha1: 0.353  alpha2: -0.053 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.25 <<<
epoch [14/50] batch [5/23] time 0.172 (0.576) data 0.000 (0.229) loss 0.6542 (0.6017) acc 79.5455 (77.6921) lr 1.7290e-03 eta 0:08:07
epoch [14/50] batch [10/23] time 0.176 (0.380) data 0.000 (0.115) loss 0.8621 (0.6132) acc 63.0682 (78.4160) lr 1.7290e-03 eta 0:05:19
epoch [14/50] batch [15/23] time 0.177 (0.319) data 0.000 (0.077) loss 0.4922 (0.6055) acc 86.1111 (80.8253) lr 1.7290e-03 eta 0:04:26
epoch [14/50] batch [20/23] time 0.193 (0.285) data 0.000 (0.057) loss 0.7224 (0.6293) acc 71.5000 (79.9286) lr 1.7290e-03 eta 0:03:57
>>> alpha1: 0.341  alpha2: -0.067 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.30 <<<
epoch [15/50] batch [5/23] time 0.191 (0.500) data 0.000 (0.294) loss 0.6577 (0.6340) acc 83.5000 (79.3668) lr 1.6845e-03 eta 0:06:51
epoch [15/50] batch [10/23] time 0.209 (0.350) data 0.000 (0.147) loss 0.4872 (0.6253) acc 83.0189 (80.2553) lr 1.6845e-03 eta 0:04:46
epoch [15/50] batch [15/23] time 0.202 (0.298) data 0.000 (0.098) loss 0.5647 (0.6332) acc 81.3636 (80.1634) lr 1.6845e-03 eta 0:04:02
epoch [15/50] batch [20/23] time 0.214 (0.275) data 0.000 (0.074) loss 0.6615 (0.6365) acc 76.5306 (80.0312) lr 1.6845e-03 eta 0:03:42
>>> alpha1: 0.310  alpha2: -0.085 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.26 <<<
epoch [16/50] batch [5/23] time 0.208 (0.487) data 0.001 (0.288) loss 0.5431 (0.5139) acc 81.3636 (81.8725) lr 1.6374e-03 eta 0:06:29
epoch [16/50] batch [10/23] time 0.169 (0.330) data 0.000 (0.144) loss 0.7189 (0.5629) acc 75.5814 (81.0817) lr 1.6374e-03 eta 0:04:22
epoch [16/50] batch [15/23] time 0.225 (0.282) data 0.000 (0.096) loss 0.7083 (0.5986) acc 75.0000 (79.5237) lr 1.6374e-03 eta 0:03:42
epoch [16/50] batch [20/23] time 0.187 (0.257) data 0.000 (0.072) loss 0.5148 (0.6251) acc 77.5510 (78.7873) lr 1.6374e-03 eta 0:03:21
>>> alpha1: 0.288  alpha2: -0.109 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [17/50] batch [5/23] time 0.181 (0.451) data 0.000 (0.257) loss 0.3764 (0.5028) acc 92.6136 (85.7374) lr 1.5878e-03 eta 0:05:50
epoch [17/50] batch [10/23] time 0.170 (0.318) data 0.000 (0.128) loss 0.6039 (0.5390) acc 83.1395 (84.5457) lr 1.5878e-03 eta 0:04:05
epoch [17/50] batch [15/23] time 0.174 (0.271) data 0.000 (0.086) loss 0.4174 (0.5822) acc 84.6591 (82.4246) lr 1.5878e-03 eta 0:03:27
epoch [17/50] batch [20/23] time 0.191 (0.253) data 0.000 (0.064) loss 0.6077 (0.5863) acc 78.5714 (81.6338) lr 1.5878e-03 eta 0:03:12
>>> alpha1: 0.277  alpha2: -0.111 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [18/50] batch [5/23] time 0.164 (0.447) data 0.001 (0.261) loss 0.7137 (0.5506) acc 78.5714 (82.4706) lr 1.5358e-03 eta 0:05:36
epoch [18/50] batch [10/23] time 0.176 (0.314) data 0.000 (0.130) loss 0.7199 (0.5785) acc 79.7872 (83.3440) lr 1.5358e-03 eta 0:03:55
epoch [18/50] batch [15/23] time 0.129 (0.264) data 0.000 (0.087) loss 0.7607 (0.5829) acc 75.5814 (82.5661) lr 1.5358e-03 eta 0:03:16
epoch [18/50] batch [20/23] time 0.133 (0.232) data 0.000 (0.065) loss 0.6745 (0.5512) acc 82.0652 (83.2624) lr 1.5358e-03 eta 0:02:51
>>> alpha1: 0.272  alpha2: -0.123 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.29 <<<
epoch [19/50] batch [5/23] time 0.186 (0.503) data 0.000 (0.307) loss 0.4713 (0.4349) acc 90.5000 (88.9221) lr 1.4818e-03 eta 0:06:07
epoch [19/50] batch [10/23] time 0.201 (0.352) data 0.000 (0.154) loss 0.8050 (0.5066) acc 72.9167 (85.2081) lr 1.4818e-03 eta 0:04:15
epoch [19/50] batch [15/23] time 0.181 (0.297) data 0.000 (0.103) loss 0.4189 (0.4927) acc 88.0435 (85.2162) lr 1.4818e-03 eta 0:03:34
epoch [19/50] batch [20/23] time 0.190 (0.273) data 0.000 (0.077) loss 0.5552 (0.5250) acc 82.8431 (83.6390) lr 1.4818e-03 eta 0:03:15
>>> alpha1: 0.264  alpha2: -0.135 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [20/50] batch [5/23] time 0.181 (0.710) data 0.000 (0.291) loss 0.5603 (0.4914) acc 84.8958 (84.1768) lr 1.4258e-03 eta 0:08:22
epoch [20/50] batch [10/23] time 0.257 (0.458) data 0.000 (0.146) loss 0.6975 (0.5063) acc 82.6923 (84.1958) lr 1.4258e-03 eta 0:05:22
epoch [20/50] batch [15/23] time 0.189 (0.368) data 0.000 (0.097) loss 0.6026 (0.5285) acc 84.8039 (83.7946) lr 1.4258e-03 eta 0:04:16
epoch [20/50] batch [20/23] time 0.195 (0.325) data 0.000 (0.073) loss 0.5134 (0.5249) acc 84.8039 (83.7799) lr 1.4258e-03 eta 0:03:45
>>> alpha1: 0.251  alpha2: -0.140 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [21/50] batch [5/23] time 0.200 (0.460) data 0.001 (0.263) loss 0.3606 (0.4401) acc 84.6154 (88.0581) lr 1.3681e-03 eta 0:05:15
epoch [21/50] batch [10/23] time 0.198 (0.329) data 0.000 (0.132) loss 0.6045 (0.5031) acc 78.8462 (84.5267) lr 1.3681e-03 eta 0:03:43
epoch [21/50] batch [15/23] time 0.247 (0.289) data 0.000 (0.088) loss 0.3402 (0.4914) acc 92.2727 (84.4529) lr 1.3681e-03 eta 0:03:15
epoch [21/50] batch [20/23] time 0.189 (0.268) data 0.000 (0.066) loss 0.4452 (0.4941) acc 90.8163 (84.4833) lr 1.3681e-03 eta 0:02:59
>>> alpha1: 0.245  alpha2: -0.143 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.28 <<<
epoch [22/50] batch [5/23] time 0.185 (0.433) data 0.000 (0.265) loss 0.2961 (0.4864) acc 88.4259 (85.2583) lr 1.3090e-03 eta 0:04:46
epoch [22/50] batch [10/23] time 0.282 (0.326) data 0.000 (0.133) loss 0.5187 (0.4795) acc 75.0000 (84.2145) lr 1.3090e-03 eta 0:03:33
epoch [22/50] batch [15/23] time 0.275 (0.313) data 0.000 (0.088) loss 0.5156 (0.4868) acc 82.8125 (83.7781) lr 1.3090e-03 eta 0:03:24
epoch [22/50] batch [20/23] time 0.305 (0.309) data 0.000 (0.066) loss 0.5378 (0.4906) acc 85.5769 (84.2052) lr 1.3090e-03 eta 0:03:20
>>> alpha1: 0.243  alpha2: -0.143 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [23/50] batch [5/23] time 0.230 (0.511) data 0.000 (0.296) loss 0.4185 (0.5407) acc 89.0909 (84.4622) lr 1.2487e-03 eta 0:05:26
epoch [23/50] batch [10/23] time 0.201 (0.363) data 0.000 (0.148) loss 0.4135 (0.5091) acc 82.2917 (83.5031) lr 1.2487e-03 eta 0:03:50
epoch [23/50] batch [15/23] time 0.208 (0.308) data 0.000 (0.099) loss 0.4895 (0.4920) acc 77.7273 (83.2916) lr 1.2487e-03 eta 0:03:13
epoch [23/50] batch [20/23] time 0.196 (0.281) data 0.000 (0.074) loss 0.5456 (0.4819) acc 84.8039 (84.0721) lr 1.2487e-03 eta 0:02:55
>>> alpha1: 0.234  alpha2: -0.147 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [24/50] batch [5/23] time 0.201 (0.474) data 0.000 (0.266) loss 0.3895 (0.4252) acc 85.7843 (85.5824) lr 1.1874e-03 eta 0:04:52
epoch [24/50] batch [10/23] time 0.195 (0.341) data 0.000 (0.133) loss 0.5132 (0.4780) acc 80.8824 (84.2905) lr 1.1874e-03 eta 0:03:28
epoch [24/50] batch [15/23] time 0.196 (0.294) data 0.000 (0.089) loss 0.3177 (0.4694) acc 89.7059 (85.1420) lr 1.1874e-03 eta 0:02:58
epoch [24/50] batch [20/23] time 0.218 (0.272) data 0.000 (0.067) loss 0.4961 (0.4810) acc 83.3333 (84.9353) lr 1.1874e-03 eta 0:02:43
>>> alpha1: 0.229  alpha2: -0.140 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [25/50] batch [5/23] time 0.202 (0.489) data 0.000 (0.281) loss 0.4105 (0.4348) acc 86.7647 (87.3810) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [10/23] time 0.194 (0.347) data 0.000 (0.142) loss 0.4660 (0.4491) acc 87.0000 (87.3304) lr 1.1253e-03 eta 0:03:23
epoch [25/50] batch [15/23] time 0.199 (0.302) data 0.000 (0.095) loss 0.5397 (0.4504) acc 84.6154 (85.9726) lr 1.1253e-03 eta 0:02:56
epoch [25/50] batch [20/23] time 0.208 (0.276) data 0.000 (0.071) loss 0.4938 (0.4518) acc 81.6038 (86.0236) lr 1.1253e-03 eta 0:02:39
>>> alpha1: 0.229  alpha2: -0.133 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [26/50] batch [5/23] time 0.308 (0.569) data 0.000 (0.258) loss 0.3590 (0.4574) acc 88.2075 (83.2431) lr 1.0628e-03 eta 0:05:24
epoch [26/50] batch [10/23] time 0.316 (0.442) data 0.000 (0.129) loss 0.5604 (0.4649) acc 80.8036 (84.4142) lr 1.0628e-03 eta 0:04:09
epoch [26/50] batch [15/23] time 0.298 (0.394) data 0.000 (0.086) loss 0.4678 (0.4620) acc 81.2500 (84.2596) lr 1.0628e-03 eta 0:03:40
epoch [26/50] batch [20/23] time 0.150 (0.332) data 0.000 (0.065) loss 0.5602 (0.4675) acc 80.2885 (84.2146) lr 1.0628e-03 eta 0:03:04
>>> alpha1: 0.230  alpha2: -0.126 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [27/50] batch [5/23] time 0.202 (0.449) data 0.000 (0.251) loss 0.5671 (0.4457) acc 79.1667 (86.3682) lr 1.0000e-03 eta 0:04:05
epoch [27/50] batch [10/23] time 0.201 (0.324) data 0.000 (0.126) loss 0.4191 (0.4281) acc 82.0755 (86.9581) lr 1.0000e-03 eta 0:02:55
epoch [27/50] batch [15/23] time 0.186 (0.360) data 0.000 (0.084) loss 0.3569 (0.4193) acc 91.8367 (87.7962) lr 1.0000e-03 eta 0:03:13
epoch [27/50] batch [20/23] time 0.195 (0.318) data 0.000 (0.063) loss 0.4005 (0.4183) acc 89.7059 (87.6234) lr 1.0000e-03 eta 0:02:49
>>> alpha1: 0.216  alpha2: -0.128 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [28/50] batch [5/23] time 0.223 (0.457) data 0.001 (0.253) loss 0.3033 (0.4329) acc 91.2037 (87.9802) lr 9.3721e-04 eta 0:03:59
epoch [28/50] batch [10/23] time 0.211 (0.337) data 0.000 (0.126) loss 0.2364 (0.4019) acc 92.2727 (88.4707) lr 9.3721e-04 eta 0:02:55
epoch [28/50] batch [15/23] time 0.201 (0.293) data 0.000 (0.084) loss 0.3728 (0.4018) acc 90.8654 (87.7783) lr 9.3721e-04 eta 0:02:30
epoch [28/50] batch [20/23] time 0.194 (0.268) data 0.000 (0.063) loss 0.5940 (0.4250) acc 83.0000 (87.6714) lr 9.3721e-04 eta 0:02:16
>>> alpha1: 0.217  alpha2: -0.140 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [29/50] batch [5/23] time 0.203 (0.483) data 0.000 (0.283) loss 0.4261 (0.3900) acc 86.0000 (86.3895) lr 8.7467e-04 eta 0:04:02
epoch [29/50] batch [10/23] time 0.186 (0.342) data 0.000 (0.141) loss 0.3816 (0.3791) acc 87.2340 (87.6272) lr 8.7467e-04 eta 0:02:49
epoch [29/50] batch [15/23] time 0.198 (0.299) data 0.000 (0.094) loss 0.4067 (0.4043) acc 89.4231 (87.3331) lr 8.7467e-04 eta 0:02:26
epoch [29/50] batch [20/23] time 0.203 (0.274) data 0.000 (0.071) loss 0.4339 (0.4017) acc 87.2642 (87.3319) lr 8.7467e-04 eta 0:02:13
>>> alpha1: 0.218  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [30/50] batch [5/23] time 0.157 (0.465) data 0.000 (0.304) loss 0.3418 (0.4675) acc 86.9318 (85.7185) lr 8.1262e-04 eta 0:03:42
epoch [30/50] batch [10/23] time 0.184 (0.324) data 0.000 (0.154) loss 0.4495 (0.4241) acc 86.6667 (87.6119) lr 8.1262e-04 eta 0:02:33
epoch [30/50] batch [15/23] time 0.176 (0.272) data 0.000 (0.103) loss 0.3743 (0.4170) acc 92.3469 (88.4152) lr 8.1262e-04 eta 0:02:07
epoch [30/50] batch [20/23] time 0.180 (0.247) data 0.000 (0.077) loss 0.3201 (0.4226) acc 89.2857 (87.3417) lr 8.1262e-04 eta 0:01:54
>>> alpha1: 0.212  alpha2: -0.135 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [31/50] batch [5/23] time 0.183 (0.483) data 0.000 (0.286) loss 0.4071 (0.4074) acc 88.8889 (88.2710) lr 7.5131e-04 eta 0:03:39
epoch [31/50] batch [10/23] time 0.185 (0.337) data 0.000 (0.143) loss 0.4089 (0.3997) acc 84.5745 (87.0483) lr 7.5131e-04 eta 0:02:31
epoch [31/50] batch [15/23] time 0.177 (0.285) data 0.000 (0.096) loss 0.3523 (0.3892) acc 88.3333 (87.9324) lr 7.5131e-04 eta 0:02:06
epoch [31/50] batch [20/23] time 0.183 (0.258) data 0.000 (0.072) loss 0.3981 (0.3993) acc 81.5217 (87.5644) lr 7.5131e-04 eta 0:01:53
>>> alpha1: 0.209  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [32/50] batch [5/23] time 0.177 (0.455) data 0.000 (0.257) loss 0.5259 (0.4348) acc 85.2273 (85.1271) lr 6.9098e-04 eta 0:03:16
epoch [32/50] batch [10/23] time 0.184 (0.322) data 0.000 (0.129) loss 0.3308 (0.4175) acc 86.9792 (86.4869) lr 6.9098e-04 eta 0:02:17
epoch [32/50] batch [15/23] time 0.191 (0.279) data 0.000 (0.086) loss 0.3589 (0.3904) acc 87.2449 (87.5123) lr 6.9098e-04 eta 0:01:57
epoch [32/50] batch [20/23] time 0.171 (0.253) data 0.000 (0.065) loss 0.3815 (0.4022) acc 87.7907 (87.6754) lr 6.9098e-04 eta 0:01:45
>>> alpha1: 0.211  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [33/50] batch [5/23] time 0.206 (0.458) data 0.000 (0.269) loss 0.4079 (0.3911) acc 88.2653 (89.5241) lr 6.3188e-04 eta 0:03:07
epoch [33/50] batch [10/23] time 0.173 (0.323) data 0.000 (0.135) loss 0.4702 (0.3685) acc 85.4651 (90.1431) lr 6.3188e-04 eta 0:02:10
epoch [33/50] batch [15/23] time 0.196 (0.334) data 0.000 (0.090) loss 0.2154 (0.3600) acc 93.6274 (89.7820) lr 6.3188e-04 eta 0:02:13
epoch [33/50] batch [20/23] time 0.185 (0.297) data 0.000 (0.067) loss 0.4346 (0.3692) acc 83.5106 (89.5604) lr 6.3188e-04 eta 0:01:56
>>> alpha1: 0.203  alpha2: -0.118 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.28 <<<
epoch [34/50] batch [5/23] time 0.206 (0.500) data 0.000 (0.299) loss 0.3623 (0.3931) acc 89.0000 (88.5908) lr 5.7422e-04 eta 0:03:13
epoch [34/50] batch [10/23] time 0.193 (0.346) data 0.000 (0.150) loss 0.3941 (0.3765) acc 87.5000 (88.9542) lr 5.7422e-04 eta 0:02:11
epoch [34/50] batch [15/23] time 0.195 (0.299) data 0.000 (0.100) loss 0.4762 (0.3941) acc 83.3333 (88.2581) lr 5.7422e-04 eta 0:01:52
epoch [34/50] batch [20/23] time 0.178 (0.273) data 0.000 (0.075) loss 0.3465 (0.3949) acc 88.0435 (88.2233) lr 5.7422e-04 eta 0:01:41
>>> alpha1: 0.201  alpha2: -0.114 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [35/50] batch [5/23] time 0.186 (0.455) data 0.000 (0.264) loss 0.3251 (0.3698) acc 86.7347 (89.9569) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [10/23] time 0.236 (0.326) data 0.000 (0.132) loss 0.3198 (0.3542) acc 90.1163 (90.6200) lr 5.1825e-04 eta 0:01:56
epoch [35/50] batch [15/23] time 0.174 (0.278) data 0.000 (0.088) loss 0.3580 (0.3553) acc 92.0455 (90.3746) lr 5.1825e-04 eta 0:01:38
epoch [35/50] batch [20/23] time 0.183 (0.254) data 0.000 (0.066) loss 0.3741 (0.3592) acc 86.7021 (89.7302) lr 5.1825e-04 eta 0:01:28
>>> alpha1: 0.197  alpha2: -0.113 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [36/50] batch [5/23] time 0.189 (0.459) data 0.000 (0.255) loss 0.5005 (0.3351) acc 87.0000 (89.2509) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [10/23] time 0.192 (0.327) data 0.000 (0.128) loss 0.4241 (0.3762) acc 87.7451 (88.9155) lr 4.6417e-04 eta 0:01:49
epoch [36/50] batch [15/23] time 0.212 (0.288) data 0.000 (0.085) loss 0.3594 (0.3786) acc 90.1786 (88.6510) lr 4.6417e-04 eta 0:01:35
epoch [36/50] batch [20/23] time 0.187 (0.264) data 0.000 (0.064) loss 0.3792 (0.3821) acc 89.5833 (88.2699) lr 4.6417e-04 eta 0:01:25
>>> alpha1: 0.197  alpha2: -0.112 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [37/50] batch [5/23] time 0.151 (0.455) data 0.000 (0.269) loss 0.3980 (0.3940) acc 88.4615 (87.9939) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [10/23] time 0.148 (0.302) data 0.000 (0.135) loss 0.3634 (0.3654) acc 88.7255 (87.4293) lr 4.1221e-04 eta 0:01:34
epoch [37/50] batch [15/23] time 0.314 (0.263) data 0.000 (0.090) loss 0.3603 (0.3785) acc 90.4546 (87.4661) lr 4.1221e-04 eta 0:01:20
epoch [37/50] batch [20/23] time 0.312 (0.272) data 0.000 (0.067) loss 0.2959 (0.3696) acc 87.7358 (87.9925) lr 4.1221e-04 eta 0:01:22
>>> alpha1: 0.195  alpha2: -0.117 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [38/50] batch [5/23] time 0.207 (0.457) data 0.000 (0.251) loss 0.3653 (0.5213) acc 94.1964 (87.0994) lr 3.6258e-04 eta 0:02:14
epoch [38/50] batch [10/23] time 0.197 (0.335) data 0.000 (0.126) loss 0.2902 (0.4436) acc 94.7115 (88.3327) lr 3.6258e-04 eta 0:01:36
epoch [38/50] batch [15/23] time 0.194 (0.288) data 0.000 (0.084) loss 0.5802 (0.4189) acc 84.5000 (88.7900) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [20/23] time 0.208 (0.266) data 0.000 (0.063) loss 0.3304 (0.4003) acc 92.1296 (89.1085) lr 3.6258e-04 eta 0:01:14
>>> alpha1: 0.194  alpha2: -0.123 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [39/50] batch [5/23] time 0.191 (0.461) data 0.000 (0.255) loss 0.4145 (0.3473) acc 91.1765 (90.9351) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [10/23] time 0.191 (0.326) data 0.001 (0.128) loss 0.3362 (0.3317) acc 90.8163 (91.6548) lr 3.1545e-04 eta 0:01:26
epoch [39/50] batch [15/23] time 0.207 (0.285) data 0.000 (0.085) loss 0.2841 (0.3620) acc 90.2778 (90.9265) lr 3.1545e-04 eta 0:01:14
epoch [39/50] batch [20/23] time 0.212 (0.266) data 0.000 (0.064) loss 0.3046 (0.3573) acc 93.1818 (90.2538) lr 3.1545e-04 eta 0:01:08
>>> alpha1: 0.195  alpha2: -0.120 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [40/50] batch [5/23] time 0.193 (0.442) data 0.000 (0.242) loss 0.5426 (0.4117) acc 89.3617 (87.2783) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [10/23] time 0.211 (0.322) data 0.000 (0.121) loss 0.2897 (0.3877) acc 93.0556 (88.4990) lr 2.7103e-04 eta 0:01:18
epoch [40/50] batch [15/23] time 0.192 (0.286) data 0.000 (0.081) loss 0.3147 (0.3742) acc 86.2245 (88.7618) lr 2.7103e-04 eta 0:01:08
epoch [40/50] batch [20/23] time 0.204 (0.265) data 0.000 (0.061) loss 0.3091 (0.3600) acc 88.9423 (89.5314) lr 2.7103e-04 eta 0:01:01
>>> alpha1: 0.194  alpha2: -0.119 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [41/50] batch [5/23] time 0.321 (0.577) data 0.000 (0.280) loss 0.2418 (0.3405) acc 85.1852 (87.1878) lr 2.2949e-04 eta 0:02:09
epoch [41/50] batch [10/23] time 0.156 (0.368) data 0.001 (0.140) loss 0.2592 (0.3447) acc 87.0370 (88.1355) lr 2.2949e-04 eta 0:01:20
epoch [41/50] batch [15/23] time 0.155 (0.295) data 0.000 (0.094) loss 0.4538 (0.3674) acc 86.5741 (88.7481) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [20/23] time 0.284 (0.281) data 0.000 (0.070) loss 0.3521 (0.3738) acc 93.5185 (89.0175) lr 2.2949e-04 eta 0:00:58
>>> alpha1: 0.190  alpha2: -0.122 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [42/50] batch [5/23] time 0.208 (0.450) data 0.000 (0.240) loss 0.3310 (0.3863) acc 92.4528 (88.9644) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [10/23] time 0.274 (0.332) data 0.000 (0.120) loss 0.4269 (0.3819) acc 88.6792 (88.6220) lr 1.9098e-04 eta 0:01:05
epoch [42/50] batch [15/23] time 0.211 (0.287) data 0.000 (0.080) loss 0.3010 (0.3946) acc 91.3636 (88.9800) lr 1.9098e-04 eta 0:00:55
epoch [42/50] batch [20/23] time 0.196 (0.266) data 0.000 (0.060) loss 0.2760 (0.3736) acc 85.5000 (89.3879) lr 1.9098e-04 eta 0:00:49
>>> alpha1: 0.190  alpha2: -0.115 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [43/50] batch [5/23] time 0.204 (0.499) data 0.000 (0.285) loss 0.3399 (0.3526) acc 91.5094 (91.1426) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [10/23] time 0.196 (0.352) data 0.000 (0.144) loss 0.4027 (0.3676) acc 90.1961 (90.7518) lr 1.5567e-04 eta 0:01:01
epoch [43/50] batch [15/23] time 0.194 (0.302) data 0.000 (0.096) loss 0.3519 (0.3549) acc 88.5000 (89.9356) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [20/23] time 0.209 (0.279) data 0.000 (0.072) loss 0.4629 (0.3593) acc 81.0000 (89.3779) lr 1.5567e-04 eta 0:00:45
>>> alpha1: 0.188  alpha2: -0.120 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [44/50] batch [5/23] time 0.205 (0.441) data 0.000 (0.239) loss 0.2939 (0.5496) acc 92.5926 (88.4826) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [10/23] time 0.199 (0.322) data 0.000 (0.120) loss 0.2877 (0.4414) acc 94.7115 (89.9271) lr 1.2369e-04 eta 0:00:48
epoch [44/50] batch [15/23] time 1.399 (0.366) data 0.000 (0.080) loss 0.3897 (0.4218) acc 89.5833 (89.8232) lr 1.2369e-04 eta 0:00:53
epoch [44/50] batch [20/23] time 0.201 (0.323) data 0.000 (0.060) loss 0.3998 (0.4203) acc 91.6667 (89.0893) lr 1.2369e-04 eta 0:00:45
>>> alpha1: 0.183  alpha2: -0.120 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [45/50] batch [5/23] time 0.206 (0.524) data 0.000 (0.322) loss 0.2829 (0.2953) acc 87.5000 (90.6437) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [10/23] time 0.203 (0.357) data 0.000 (0.161) loss 0.3350 (0.3492) acc 90.9091 (90.1397) lr 9.5173e-05 eta 0:00:45
epoch [45/50] batch [15/23] time 0.204 (0.303) data 0.000 (0.108) loss 0.3301 (0.3391) acc 92.1296 (90.2336) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [20/23] time 0.219 (0.278) data 0.000 (0.081) loss 0.1795 (0.3388) acc 95.0000 (90.1055) lr 9.5173e-05 eta 0:00:32
>>> alpha1: 0.181  alpha2: -0.127 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [46/50] batch [5/23] time 0.188 (0.483) data 0.000 (0.281) loss 0.3488 (0.3272) acc 88.2653 (90.2420) lr 7.0224e-05 eta 0:00:53
epoch [46/50] batch [10/23] time 0.201 (0.341) data 0.000 (0.141) loss 0.3023 (0.3382) acc 91.6667 (89.2208) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [15/23] time 0.205 (0.295) data 0.000 (0.094) loss 0.3260 (0.3376) acc 92.4528 (89.8876) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [20/23] time 0.209 (0.276) data 0.000 (0.070) loss 0.2215 (0.3391) acc 92.4528 (90.1637) lr 7.0224e-05 eta 0:00:26
>>> alpha1: 0.183  alpha2: -0.120 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [47/50] batch [5/23] time 0.178 (0.504) data 0.000 (0.311) loss 0.2221 (0.2904) acc 90.6250 (90.5916) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [10/23] time 0.253 (0.355) data 0.000 (0.156) loss 0.3871 (0.3460) acc 90.1042 (89.0484) lr 4.8943e-05 eta 0:00:29
epoch [47/50] batch [15/23] time 0.197 (0.298) data 0.000 (0.104) loss 0.2696 (0.3382) acc 94.6078 (90.7709) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [20/23] time 0.192 (0.271) data 0.000 (0.078) loss 0.3011 (0.3223) acc 89.7059 (91.0560) lr 4.8943e-05 eta 0:00:19
>>> alpha1: 0.187  alpha2: -0.118 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [48/50] batch [5/23] time 0.185 (0.471) data 0.000 (0.271) loss 0.4410 (0.3803) acc 88.0208 (88.5033) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [10/23] time 0.190 (0.334) data 0.001 (0.136) loss 0.4005 (0.3360) acc 84.3137 (89.3114) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [15/23] time 0.189 (0.287) data 0.000 (0.091) loss 0.2612 (0.3384) acc 92.6471 (89.9265) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.152 (0.257) data 0.000 (0.068) loss 0.3226 (0.3467) acc 89.1509 (89.7016) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.185  alpha2: -0.115 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [49/50] batch [5/23] time 0.184 (0.479) data 0.000 (0.267) loss 0.4623 (0.3916) acc 87.2449 (89.3023) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [10/23] time 0.188 (0.338) data 0.000 (0.134) loss 0.3477 (0.3546) acc 84.8958 (89.6510) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [15/23] time 0.195 (0.290) data 0.000 (0.089) loss 0.3313 (0.3448) acc 91.6667 (90.3485) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.263 (0.270) data 0.000 (0.067) loss 0.2715 (0.3428) acc 90.1961 (90.3387) lr 1.7713e-05 eta 0:00:07
>>> alpha1: 0.187  alpha2: -0.127 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [50/50] batch [5/23] time 0.229 (0.465) data 0.000 (0.259) loss 0.2909 (0.3125) acc 92.2727 (91.9986) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.211 (0.333) data 0.000 (0.130) loss 0.2395 (0.2888) acc 92.2727 (92.6229) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.194 (0.289) data 0.000 (0.086) loss 0.4265 (0.3227) acc 91.0000 (92.1559) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.204 (0.267) data 0.000 (0.065) loss 0.2049 (0.3316) acc 91.0377 (91.8123) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.19, 0.16, 0.17, 0.16, 0.17, 0.16, 0.18, 0.17, 0.17, 0.17, 0.17, 0.16, 0.16, 0.17, 0.17, 0.17, 0.16, 0.17, 0.17, 0.16, 0.17, 0.16, 0.16, 0.17, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17]
* matched noise rate: [0.1, 0.06, 0.05, 0.05, 0.08, 0.05, 0.07, 0.07, 0.08, 0.09, 0.1, 0.08, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.08, 0.08, 0.09, 0.08, 0.08, 0.08, 0.09, 0.1, 0.09, 0.09, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09, 0.09, 0.09, 0.09]
* unmatched noise rate: [0.31, 0.28, 0.28, 0.25, 0.3, 0.26, 0.27, 0.25, 0.29, 0.31, 0.27, 0.28, 0.29, 0.29, 0.3, 0.29, 0.28, 0.29, 0.3, 0.24, 0.25, 0.23, 0.24, 0.28, 0.24, 0.29, 0.29, 0.29, 0.29, 0.28, 0.27, 0.27, 0.28, 0.28, 0.29, 0.29, 0.26, 0.29, 0.28, 0.28]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:46,  2.89s/it] 18%|█▊        | 3/17 [00:03<00:11,  1.23it/s] 29%|██▉       | 5/17 [00:03<00:05,  2.26it/s] 41%|████      | 7/17 [00:03<00:02,  3.41it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.61it/s] 65%|██████▍   | 11/17 [00:03<00:01,  5.57it/s] 71%|███████   | 12/17 [00:03<00:00,  5.95it/s] 82%|████████▏ | 14/17 [00:04<00:00,  7.15it/s] 94%|█████████▍| 16/17 [00:04<00:00,  8.14it/s]100%|██████████| 17/17 [00:05<00:00,  3.28it/s]
=> result
* total: 1,692
* correct: 1,087
* accuracy: 64.2%
* error: 35.8%
* macro_f1: 63.5%
=> per-class result
* class: 0 (banded)	total: 36	correct: 26	acc: 72.2%
* class: 1 (blotchy)	total: 36	correct: 13	acc: 36.1%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 33	acc: 91.7%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 34	acc: 94.4%
* class: 6 (cobwebbed)	total: 36	correct: 28	acc: 77.8%
* class: 7 (cracked)	total: 36	correct: 29	acc: 80.6%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 12	acc: 33.3%
* class: 11 (fibrous)	total: 36	correct: 29	acc: 80.6%
* class: 12 (flecked)	total: 36	correct: 27	acc: 75.0%
* class: 13 (freckled)	total: 36	correct: 31	acc: 86.1%
* class: 14 (frilly)	total: 36	correct: 27	acc: 75.0%
* class: 15 (gauzy)	total: 36	correct: 22	acc: 61.1%
* class: 16 (grid)	total: 36	correct: 17	acc: 47.2%
* class: 17 (grooved)	total: 36	correct: 20	acc: 55.6%
* class: 18 (honeycombed)	total: 36	correct: 26	acc: 72.2%
* class: 19 (interlaced)	total: 36	correct: 28	acc: 77.8%
* class: 20 (knitted)	total: 36	correct: 31	acc: 86.1%
* class: 21 (lacelike)	total: 36	correct: 34	acc: 94.4%
* class: 22 (lined)	total: 36	correct: 16	acc: 44.4%
* class: 23 (marbled)	total: 36	correct: 22	acc: 61.1%
* class: 24 (matted)	total: 36	correct: 18	acc: 50.0%
* class: 25 (meshed)	total: 36	correct: 19	acc: 52.8%
* class: 26 (paisley)	total: 36	correct: 34	acc: 94.4%
* class: 27 (perforated)	total: 36	correct: 21	acc: 58.3%
* class: 28 (pitted)	total: 36	correct: 17	acc: 47.2%
* class: 29 (pleated)	total: 36	correct: 23	acc: 63.9%
* class: 30 (polka-dotted)	total: 36	correct: 26	acc: 72.2%
* class: 31 (porous)	total: 36	correct: 16	acc: 44.4%
* class: 32 (potholed)	total: 36	correct: 33	acc: 91.7%
* class: 33 (scaly)	total: 36	correct: 23	acc: 63.9%
* class: 34 (smeared)	total: 36	correct: 17	acc: 47.2%
* class: 35 (spiralled)	total: 36	correct: 23	acc: 63.9%
* class: 36 (sprinkled)	total: 36	correct: 18	acc: 50.0%
* class: 37 (stained)	total: 36	correct: 9	acc: 25.0%
* class: 38 (stratified)	total: 36	correct: 25	acc: 69.4%
* class: 39 (striped)	total: 36	correct: 29	acc: 80.6%
* class: 40 (studded)	total: 36	correct: 26	acc: 72.2%
* class: 41 (swirly)	total: 36	correct: 21	acc: 58.3%
* class: 42 (veined)	total: 36	correct: 21	acc: 58.3%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 18	acc: 50.0%
* class: 45 (wrinkled)	total: 36	correct: 23	acc: 63.9%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 64.2%
Elapsed: 0:16:05
Run this job and save the output to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_0-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.219 (0.952) data 0.000 (0.255) loss 3.3292 (3.3905) acc 34.3750 (25.6250) lr 1.0000e-05 eta 0:18:09
epoch [1/50] batch [10/23] time 0.154 (0.576) data 0.000 (0.128) loss 3.2104 (3.3824) acc 28.1250 (24.3750) lr 1.0000e-05 eta 0:10:56
epoch [1/50] batch [15/23] time 0.156 (0.435) data 0.000 (0.085) loss 3.3917 (3.3654) acc 25.0000 (24.7917) lr 1.0000e-05 eta 0:08:13
epoch [1/50] batch [20/23] time 0.153 (0.364) data 0.000 (0.064) loss 3.4406 (3.3520) acc 18.7500 (24.6875) lr 1.0000e-05 eta 0:06:51
epoch [2/50] batch [5/23] time 0.312 (0.574) data 0.000 (0.242) loss 2.2943 (2.8749) acc 37.5000 (29.3750) lr 2.0000e-03 eta 0:10:43
epoch [2/50] batch [10/23] time 0.309 (0.446) data 0.000 (0.121) loss 2.1542 (2.5801) acc 40.6250 (35.3125) lr 2.0000e-03 eta 0:08:18
epoch [2/50] batch [15/23] time 0.151 (0.367) data 0.000 (0.081) loss 2.1808 (2.4428) acc 34.3750 (37.0833) lr 2.0000e-03 eta 0:06:47
epoch [2/50] batch [20/23] time 0.152 (0.312) data 0.000 (0.061) loss 2.1007 (2.3596) acc 40.6250 (38.7500) lr 2.0000e-03 eta 0:05:45
epoch [3/50] batch [5/23] time 0.269 (0.602) data 0.000 (0.303) loss 1.9508 (1.9372) acc 43.7500 (44.3750) lr 1.9980e-03 eta 0:11:01
epoch [3/50] batch [10/23] time 0.162 (0.432) data 0.000 (0.151) loss 1.5093 (1.7747) acc 62.5000 (50.0000) lr 1.9980e-03 eta 0:07:52
epoch [3/50] batch [15/23] time 0.149 (0.338) data 0.000 (0.101) loss 1.8556 (1.7020) acc 50.0000 (51.8750) lr 1.9980e-03 eta 0:06:08
epoch [3/50] batch [20/23] time 0.149 (0.291) data 0.000 (0.076) loss 1.9602 (1.7885) acc 37.5000 (47.8125) lr 1.9980e-03 eta 0:05:14
epoch [4/50] batch [5/23] time 0.223 (0.455) data 0.000 (0.224) loss 1.6500 (1.4447) acc 56.2500 (60.6250) lr 1.9921e-03 eta 0:08:09
epoch [4/50] batch [10/23] time 0.251 (0.343) data 0.000 (0.112) loss 1.2076 (1.4460) acc 62.5000 (57.8125) lr 1.9921e-03 eta 0:06:07
epoch [4/50] batch [15/23] time 0.205 (0.296) data 0.000 (0.075) loss 2.4019 (1.5239) acc 28.1250 (54.5833) lr 1.9921e-03 eta 0:05:15
epoch [4/50] batch [20/23] time 0.207 (0.274) data 0.000 (0.056) loss 1.4951 (1.5579) acc 59.3750 (53.9062) lr 1.9921e-03 eta 0:04:50
epoch [5/50] batch [5/23] time 0.209 (0.461) data 0.000 (0.243) loss 1.1012 (1.3181) acc 71.8750 (63.7500) lr 1.9823e-03 eta 0:08:05
epoch [5/50] batch [10/23] time 0.199 (0.340) data 0.000 (0.122) loss 1.4031 (1.3494) acc 59.3750 (63.1250) lr 1.9823e-03 eta 0:05:56
epoch [5/50] batch [15/23] time 0.197 (0.292) data 0.000 (0.081) loss 1.0358 (1.3346) acc 59.3750 (62.0833) lr 1.9823e-03 eta 0:05:04
epoch [5/50] batch [20/23] time 0.205 (0.270) data 0.000 (0.061) loss 1.2912 (1.3717) acc 53.1250 (60.1562) lr 1.9823e-03 eta 0:04:40
epoch [6/50] batch [5/23] time 0.216 (0.452) data 0.000 (0.237) loss 1.7824 (1.3916) acc 50.0000 (60.0000) lr 1.9686e-03 eta 0:07:45
epoch [6/50] batch [10/23] time 0.198 (0.339) data 0.000 (0.119) loss 1.3664 (1.3057) acc 62.5000 (62.8125) lr 1.9686e-03 eta 0:05:47
epoch [6/50] batch [15/23] time 0.199 (0.292) data 0.000 (0.079) loss 1.3139 (1.3314) acc 59.3750 (62.2917) lr 1.9686e-03 eta 0:04:58
epoch [6/50] batch [20/23] time 0.201 (0.269) data 0.000 (0.059) loss 1.2138 (1.3166) acc 71.8750 (62.1875) lr 1.9686e-03 eta 0:04:33
epoch [7/50] batch [5/23] time 0.212 (0.474) data 0.000 (0.240) loss 1.1020 (1.1242) acc 65.6250 (68.1250) lr 1.9511e-03 eta 0:07:57
epoch [7/50] batch [10/23] time 0.203 (0.343) data 0.000 (0.121) loss 1.0133 (1.1930) acc 71.8750 (64.3750) lr 1.9511e-03 eta 0:05:43
epoch [7/50] batch [15/23] time 0.204 (0.301) data 0.000 (0.081) loss 1.4881 (1.2274) acc 50.0000 (63.7500) lr 1.9511e-03 eta 0:04:59
epoch [7/50] batch [20/23] time 0.205 (0.277) data 0.000 (0.061) loss 1.0213 (1.1915) acc 65.6250 (64.3750) lr 1.9511e-03 eta 0:04:34
epoch [8/50] batch [5/23] time 0.223 (0.441) data 0.000 (0.204) loss 1.0554 (1.1243) acc 65.6250 (68.1250) lr 1.9298e-03 eta 0:07:13
epoch [8/50] batch [10/23] time 0.208 (0.330) data 0.000 (0.102) loss 1.1736 (1.1124) acc 68.7500 (68.4375) lr 1.9298e-03 eta 0:05:23
epoch [8/50] batch [15/23] time 0.213 (0.294) data 0.000 (0.068) loss 1.0734 (1.0977) acc 65.6250 (68.3333) lr 1.9298e-03 eta 0:04:46
epoch [8/50] batch [20/23] time 0.209 (0.273) data 0.000 (0.051) loss 1.0098 (1.0999) acc 71.8750 (67.9688) lr 1.9298e-03 eta 0:04:24
epoch [9/50] batch [5/23] time 0.204 (0.444) data 0.000 (0.218) loss 1.1299 (1.0631) acc 68.7500 (68.1250) lr 1.9048e-03 eta 0:07:06
epoch [9/50] batch [10/23] time 0.249 (0.334) data 0.000 (0.109) loss 1.3690 (1.0510) acc 59.3750 (68.1250) lr 1.9048e-03 eta 0:05:19
epoch [9/50] batch [15/23] time 0.199 (0.292) data 0.000 (0.073) loss 1.0620 (1.0206) acc 68.7500 (68.7500) lr 1.9048e-03 eta 0:04:37
epoch [9/50] batch [20/23] time 0.203 (0.271) data 0.000 (0.055) loss 1.2848 (1.0109) acc 62.5000 (68.5938) lr 1.9048e-03 eta 0:04:16
epoch [10/50] batch [5/23] time 0.226 (0.450) data 0.000 (0.224) loss 1.4032 (1.1241) acc 65.6250 (65.6250) lr 1.8763e-03 eta 0:07:01
epoch [10/50] batch [10/23] time 0.274 (0.335) data 0.000 (0.112) loss 0.6453 (1.0145) acc 78.1250 (70.3125) lr 1.8763e-03 eta 0:05:12
epoch [10/50] batch [15/23] time 0.206 (0.292) data 0.000 (0.075) loss 0.9218 (1.0292) acc 75.0000 (70.4167) lr 1.8763e-03 eta 0:04:31
epoch [10/50] batch [20/23] time 0.206 (0.271) data 0.000 (0.056) loss 0.7573 (0.9673) acc 75.0000 (72.9688) lr 1.8763e-03 eta 0:04:09
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.357  alpha2: -0.114 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [11/50] batch [5/23] time 1.310 (1.299) data 0.000 (0.253) loss 0.9109 (0.7637) acc 74.0000 (81.2706) lr 1.8443e-03 eta 0:19:49
epoch [11/50] batch [10/23] time 0.161 (0.914) data 0.000 (0.127) loss 0.5750 (0.7938) acc 85.0962 (79.3805) lr 1.8443e-03 eta 0:13:51
epoch [11/50] batch [15/23] time 1.117 (0.765) data 0.000 (0.085) loss 0.7172 (0.7679) acc 69.1489 (79.4651) lr 1.8443e-03 eta 0:11:32
epoch [11/50] batch [20/23] time 1.515 (0.782) data 0.000 (0.064) loss 0.8337 (0.7867) acc 75.5435 (78.9208) lr 1.8443e-03 eta 0:11:43
>>> alpha1: 0.369  alpha2: -0.083 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/23] time 0.204 (0.431) data 0.000 (0.237) loss 0.6636 (0.6783) acc 79.5918 (79.4988) lr 1.8090e-03 eta 0:06:24
epoch [12/50] batch [10/23] time 0.186 (0.314) data 0.001 (0.119) loss 0.7911 (0.6580) acc 84.1837 (81.4611) lr 1.8090e-03 eta 0:04:38
epoch [12/50] batch [15/23] time 0.176 (0.274) data 0.000 (0.079) loss 0.5832 (0.7128) acc 80.8511 (79.1006) lr 1.8090e-03 eta 0:04:01
epoch [12/50] batch [20/23] time 0.188 (0.250) data 0.000 (0.060) loss 0.7216 (0.7030) acc 83.0000 (79.0586) lr 1.8090e-03 eta 0:03:39
>>> alpha1: 0.339  alpha2: -0.101 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [13/50] batch [5/23] time 0.188 (0.437) data 0.000 (0.246) loss 0.4701 (0.6075) acc 86.7647 (83.8785) lr 1.7705e-03 eta 0:06:19
epoch [13/50] batch [10/23] time 0.174 (0.308) data 0.000 (0.123) loss 0.6817 (0.6348) acc 82.0652 (81.3909) lr 1.7705e-03 eta 0:04:26
epoch [13/50] batch [15/23] time 0.166 (0.264) data 0.000 (0.082) loss 0.5645 (0.6236) acc 81.8182 (81.9854) lr 1.7705e-03 eta 0:03:46
epoch [13/50] batch [20/23] time 0.176 (0.245) data 0.000 (0.062) loss 0.6758 (0.6363) acc 79.3478 (81.1278) lr 1.7705e-03 eta 0:03:29
>>> alpha1: 0.322  alpha2: -0.110 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [14/50] batch [5/23] time 0.180 (0.462) data 0.000 (0.274) loss 0.7094 (0.6855) acc 75.5319 (79.8533) lr 1.7290e-03 eta 0:06:30
epoch [14/50] batch [10/23] time 0.225 (0.330) data 0.000 (0.137) loss 0.6163 (0.8670) acc 83.8889 (78.2890) lr 1.7290e-03 eta 0:04:37
epoch [14/50] batch [15/23] time 0.194 (0.330) data 0.000 (0.092) loss 0.3972 (0.8045) acc 89.5000 (78.4069) lr 1.7290e-03 eta 0:04:35
epoch [14/50] batch [20/23] time 0.181 (0.293) data 0.000 (0.069) loss 0.4978 (0.7422) acc 85.6383 (79.4340) lr 1.7290e-03 eta 0:04:03
>>> alpha1: 0.315  alpha2: -0.115 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [15/50] batch [5/23] time 0.144 (0.598) data 0.001 (0.265) loss 0.4703 (0.5003) acc 86.7647 (86.7381) lr 1.6845e-03 eta 0:08:12
epoch [15/50] batch [10/23] time 0.260 (0.572) data 0.000 (0.133) loss 0.5282 (0.5407) acc 78.7736 (83.2887) lr 1.6845e-03 eta 0:07:47
epoch [15/50] batch [15/23] time 0.144 (0.550) data 0.000 (0.089) loss 0.5407 (0.5559) acc 82.5000 (82.2245) lr 1.6845e-03 eta 0:07:27
epoch [15/50] batch [20/23] time 0.153 (0.451) data 0.001 (0.067) loss 0.4964 (0.5470) acc 89.4231 (82.9912) lr 1.6845e-03 eta 0:06:04
>>> alpha1: 0.289  alpha2: -0.116 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [16/50] batch [5/23] time 0.188 (0.478) data 0.000 (0.258) loss 0.6608 (0.5519) acc 89.2857 (84.2722) lr 1.6374e-03 eta 0:06:22
epoch [16/50] batch [10/23] time 0.193 (0.339) data 0.000 (0.129) loss 0.5028 (0.5346) acc 88.7255 (84.6543) lr 1.6374e-03 eta 0:04:29
epoch [16/50] batch [15/23] time 0.203 (0.292) data 0.000 (0.086) loss 0.8110 (0.5591) acc 80.0926 (83.0037) lr 1.6374e-03 eta 0:03:50
epoch [16/50] batch [20/23] time 0.185 (0.272) data 0.000 (0.065) loss 0.6355 (0.5666) acc 86.2245 (83.0281) lr 1.6374e-03 eta 0:03:33
>>> alpha1: 0.268  alpha2: -0.129 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.31 <<<
epoch [17/50] batch [5/23] time 0.201 (0.438) data 0.000 (0.230) loss 1.1483 (0.6353) acc 78.7037 (85.0699) lr 1.5878e-03 eta 0:05:40
epoch [17/50] batch [10/23] time 0.245 (0.320) data 0.000 (0.115) loss 0.4192 (0.6116) acc 87.0000 (83.6558) lr 1.5878e-03 eta 0:04:06
epoch [17/50] batch [15/23] time 0.191 (0.279) data 0.000 (0.077) loss 0.5907 (0.5911) acc 82.0000 (84.3543) lr 1.5878e-03 eta 0:03:33
epoch [17/50] batch [20/23] time 0.205 (0.259) data 0.000 (0.058) loss 0.4470 (0.5641) acc 85.3774 (85.1046) lr 1.5878e-03 eta 0:03:17
>>> alpha1: 0.249  alpha2: -0.145 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.32 <<<
epoch [18/50] batch [5/23] time 0.190 (0.489) data 0.000 (0.287) loss 0.3845 (0.4830) acc 89.9038 (83.8631) lr 1.5358e-03 eta 0:06:08
epoch [18/50] batch [10/23] time 0.194 (0.338) data 0.000 (0.144) loss 0.3974 (0.5038) acc 90.8654 (84.9654) lr 1.5358e-03 eta 0:04:13
epoch [18/50] batch [15/23] time 0.196 (0.288) data 0.000 (0.096) loss 0.6147 (0.4982) acc 80.8824 (85.2861) lr 1.5358e-03 eta 0:03:34
epoch [18/50] batch [20/23] time 0.183 (0.266) data 0.000 (0.072) loss 0.3974 (0.5082) acc 86.4583 (84.6685) lr 1.5358e-03 eta 0:03:16
>>> alpha1: 0.244  alpha2: -0.144 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [19/50] batch [5/23] time 0.190 (0.438) data 0.000 (0.248) loss 0.3703 (0.4397) acc 90.7609 (86.5039) lr 1.4818e-03 eta 0:05:20
epoch [19/50] batch [10/23] time 0.186 (0.311) data 0.000 (0.124) loss 0.6423 (0.5055) acc 82.6531 (83.0740) lr 1.4818e-03 eta 0:03:45
epoch [19/50] batch [15/23] time 0.181 (0.272) data 0.000 (0.083) loss 0.6210 (0.5042) acc 86.1702 (83.8506) lr 1.4818e-03 eta 0:03:15
epoch [19/50] batch [20/23] time 0.181 (0.247) data 0.000 (0.062) loss 0.3939 (0.5130) acc 82.8125 (83.9246) lr 1.4818e-03 eta 0:02:57
>>> alpha1: 0.228  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [20/50] batch [5/23] time 0.200 (0.499) data 0.000 (0.290) loss 0.4511 (0.4621) acc 90.9091 (87.7349) lr 1.4258e-03 eta 0:05:53
epoch [20/50] batch [10/23] time 0.174 (0.339) data 0.000 (0.145) loss 0.5286 (0.4875) acc 81.2500 (85.8971) lr 1.4258e-03 eta 0:03:58
epoch [20/50] batch [15/23] time 0.168 (0.285) data 0.000 (0.097) loss 0.4274 (0.4866) acc 83.3333 (85.7958) lr 1.4258e-03 eta 0:03:18
epoch [20/50] batch [20/23] time 0.198 (0.260) data 0.000 (0.073) loss 0.4311 (0.4785) acc 86.0577 (85.8690) lr 1.4258e-03 eta 0:03:00
>>> alpha1: 0.217  alpha2: -0.157 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [21/50] batch [5/23] time 0.187 (0.452) data 0.000 (0.262) loss 0.4778 (0.4335) acc 85.2273 (88.3371) lr 1.3681e-03 eta 0:05:09
epoch [21/50] batch [10/23] time 0.180 (0.317) data 0.000 (0.131) loss 0.5644 (0.4760) acc 83.5106 (87.3594) lr 1.3681e-03 eta 0:03:35
epoch [21/50] batch [15/23] time 0.177 (0.270) data 0.000 (0.088) loss 0.6071 (0.4839) acc 83.8889 (86.7087) lr 1.3681e-03 eta 0:03:02
epoch [21/50] batch [20/23] time 0.175 (0.287) data 0.000 (0.066) loss 0.4983 (0.4875) acc 89.6739 (86.2548) lr 1.3681e-03 eta 0:03:12
>>> alpha1: 0.212  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [22/50] batch [5/23] time 0.202 (0.725) data 0.000 (0.261) loss 0.3878 (0.3743) acc 90.0943 (88.7671) lr 1.3090e-03 eta 0:07:59
epoch [22/50] batch [10/23] time 0.197 (0.463) data 0.001 (0.131) loss 0.5940 (0.4654) acc 82.6923 (85.6661) lr 1.3090e-03 eta 0:05:04
epoch [22/50] batch [15/23] time 0.193 (0.374) data 0.001 (0.087) loss 0.3656 (0.4277) acc 90.1961 (87.4339) lr 1.3090e-03 eta 0:04:03
epoch [22/50] batch [20/23] time 0.143 (0.318) data 0.000 (0.066) loss 0.5123 (0.4388) acc 88.2653 (86.6172) lr 1.3090e-03 eta 0:03:25
>>> alpha1: 0.204  alpha2: -0.158 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [23/50] batch [5/23] time 0.209 (0.439) data 0.000 (0.229) loss 0.2657 (0.4069) acc 90.8654 (90.7989) lr 1.2487e-03 eta 0:04:40
epoch [23/50] batch [10/23] time 0.185 (0.321) data 0.000 (0.115) loss 0.5128 (0.4398) acc 85.0000 (89.2222) lr 1.2487e-03 eta 0:03:23
epoch [23/50] batch [15/23] time 0.212 (0.280) data 0.000 (0.077) loss 0.4141 (0.4410) acc 87.7193 (88.8983) lr 1.2487e-03 eta 0:02:56
epoch [23/50] batch [20/23] time 0.187 (0.257) data 0.001 (0.058) loss 0.4185 (0.4419) acc 87.2449 (88.0887) lr 1.2487e-03 eta 0:02:40
>>> alpha1: 0.197  alpha2: -0.153 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.34 <<<
epoch [24/50] batch [5/23] time 0.215 (0.492) data 0.000 (0.295) loss 0.3804 (0.4302) acc 93.8596 (87.9065) lr 1.1874e-03 eta 0:05:03
epoch [24/50] batch [10/23] time 0.193 (0.346) data 0.000 (0.148) loss 0.5290 (0.4285) acc 86.0577 (86.9846) lr 1.1874e-03 eta 0:03:31
epoch [24/50] batch [15/23] time 0.211 (0.296) data 0.000 (0.099) loss 0.3535 (0.4087) acc 91.3636 (88.0386) lr 1.1874e-03 eta 0:02:59
epoch [24/50] batch [20/23] time 0.211 (0.271) data 0.000 (0.074) loss 0.5979 (0.4330) acc 90.4546 (88.0794) lr 1.1874e-03 eta 0:02:42
>>> alpha1: 0.193  alpha2: -0.154 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.31 <<<
epoch [25/50] batch [5/23] time 0.202 (0.439) data 0.000 (0.243) loss 0.3996 (0.3664) acc 92.0000 (89.5571) lr 1.1253e-03 eta 0:04:20
epoch [25/50] batch [10/23] time 0.184 (0.315) data 0.000 (0.122) loss 0.4298 (0.3986) acc 85.7143 (87.9804) lr 1.1253e-03 eta 0:03:05
epoch [25/50] batch [15/23] time 0.182 (0.271) data 0.000 (0.081) loss 0.3810 (0.4130) acc 91.6667 (87.4857) lr 1.1253e-03 eta 0:02:38
epoch [25/50] batch [20/23] time 0.182 (0.251) data 0.000 (0.061) loss 0.4571 (0.4225) acc 91.1458 (87.6204) lr 1.1253e-03 eta 0:02:24
>>> alpha1: 0.187  alpha2: -0.150 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.32 <<<
epoch [26/50] batch [5/23] time 0.180 (0.446) data 0.000 (0.250) loss 0.4825 (0.3780) acc 82.9787 (89.7802) lr 1.0628e-03 eta 0:04:14
epoch [26/50] batch [10/23] time 0.134 (0.309) data 0.000 (0.125) loss 0.5136 (0.3762) acc 85.8696 (89.4264) lr 1.0628e-03 eta 0:02:54
epoch [26/50] batch [15/23] time 0.139 (0.253) data 0.000 (0.084) loss 0.3414 (0.3946) acc 95.9184 (89.5030) lr 1.0628e-03 eta 0:02:21
epoch [26/50] batch [20/23] time 0.139 (0.223) data 0.000 (0.063) loss 0.5139 (0.4290) acc 83.6735 (88.5105) lr 1.0628e-03 eta 0:02:03
>>> alpha1: 0.183  alpha2: -0.142 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [27/50] batch [5/23] time 0.186 (0.462) data 0.000 (0.280) loss 0.4686 (0.4752) acc 89.7959 (88.4418) lr 1.0000e-03 eta 0:04:12
epoch [27/50] batch [10/23] time 0.171 (0.319) data 0.000 (0.140) loss 0.3051 (0.4615) acc 90.7609 (87.5350) lr 1.0000e-03 eta 0:02:53
epoch [27/50] batch [15/23] time 0.182 (0.273) data 0.000 (0.094) loss 0.3433 (0.4328) acc 89.0625 (87.9792) lr 1.0000e-03 eta 0:02:26
epoch [27/50] batch [20/23] time 0.179 (0.254) data 0.000 (0.070) loss 0.4538 (0.4186) acc 85.8696 (88.6951) lr 1.0000e-03 eta 0:02:15
>>> alpha1: 0.180  alpha2: -0.136 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.33 <<<
epoch [28/50] batch [5/23] time 0.213 (0.458) data 0.000 (0.258) loss 0.3364 (0.3472) acc 87.0536 (88.2117) lr 9.3721e-04 eta 0:04:00
epoch [28/50] batch [10/23] time 0.191 (0.325) data 0.000 (0.129) loss 0.5643 (0.3836) acc 83.1633 (89.4610) lr 9.3721e-04 eta 0:02:48
epoch [28/50] batch [15/23] time 0.187 (0.280) data 0.000 (0.086) loss 0.5326 (0.4007) acc 83.5000 (89.3641) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [20/23] time 0.188 (0.264) data 0.000 (0.065) loss 0.2484 (0.3873) acc 86.5000 (89.3528) lr 9.3721e-04 eta 0:02:14
>>> alpha1: 0.178  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [29/50] batch [5/23] time 0.180 (0.457) data 0.001 (0.266) loss 0.3655 (0.3392) acc 88.2653 (89.3206) lr 8.7467e-04 eta 0:03:48
epoch [29/50] batch [10/23] time 0.204 (0.325) data 0.000 (0.133) loss 0.3392 (0.3596) acc 87.0370 (88.2704) lr 8.7467e-04 eta 0:02:41
epoch [29/50] batch [15/23] time 0.250 (0.284) data 0.000 (0.089) loss 0.5032 (0.3793) acc 89.3617 (87.8827) lr 8.7467e-04 eta 0:02:19
epoch [29/50] batch [20/23] time 0.201 (0.260) data 0.000 (0.067) loss 0.4570 (0.3900) acc 88.4259 (88.2631) lr 8.7467e-04 eta 0:02:06
>>> alpha1: 0.174  alpha2: -0.143 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [30/50] batch [5/23] time 0.247 (0.480) data 0.000 (0.303) loss 0.5337 (0.3217) acc 87.7778 (92.7231) lr 8.1262e-04 eta 0:03:49
epoch [30/50] batch [10/23] time 0.266 (0.381) data 0.000 (0.152) loss 0.3339 (0.3644) acc 87.5000 (90.3470) lr 8.1262e-04 eta 0:03:00
epoch [30/50] batch [15/23] time 0.270 (0.353) data 0.000 (0.101) loss 0.4228 (0.3696) acc 90.1042 (90.3076) lr 8.1262e-04 eta 0:02:45
epoch [30/50] batch [20/23] time 0.297 (0.337) data 0.001 (0.076) loss 0.3938 (0.3767) acc 90.8163 (89.8480) lr 8.1262e-04 eta 0:02:36
>>> alpha1: 0.172  alpha2: -0.157 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [31/50] batch [5/23] time 0.203 (0.468) data 0.000 (0.246) loss 0.2958 (0.3221) acc 95.9821 (91.3148) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [10/23] time 0.210 (0.333) data 0.000 (0.123) loss 0.5946 (0.3790) acc 85.3774 (89.7363) lr 7.5131e-04 eta 0:02:29
epoch [31/50] batch [15/23] time 0.198 (0.287) data 0.000 (0.082) loss 0.2570 (0.3768) acc 93.8679 (90.0288) lr 7.5131e-04 eta 0:02:07
epoch [31/50] batch [20/23] time 0.225 (0.263) data 0.000 (0.062) loss 0.3498 (0.3790) acc 92.1569 (89.8065) lr 7.5131e-04 eta 0:01:55
>>> alpha1: 0.168  alpha2: -0.144 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [32/50] batch [5/23] time 0.173 (0.435) data 0.000 (0.247) loss 0.3906 (0.3314) acc 91.1111 (91.0552) lr 6.9098e-04 eta 0:03:07
epoch [32/50] batch [10/23] time 0.173 (0.310) data 0.000 (0.124) loss 0.3821 (0.3707) acc 91.1111 (89.3953) lr 6.9098e-04 eta 0:02:12
epoch [32/50] batch [15/23] time 0.182 (0.266) data 0.000 (0.083) loss 0.2933 (0.3666) acc 93.3673 (90.3147) lr 6.9098e-04 eta 0:01:52
epoch [32/50] batch [20/23] time 0.169 (0.246) data 0.000 (0.062) loss 0.4767 (0.3740) acc 82.9545 (90.2446) lr 6.9098e-04 eta 0:01:42
>>> alpha1: 0.166  alpha2: -0.138 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.36 <<<
epoch [33/50] batch [5/23] time 0.182 (0.448) data 0.000 (0.239) loss 0.6095 (0.3907) acc 82.0652 (86.9833) lr 6.3188e-04 eta 0:03:03
epoch [33/50] batch [10/23] time 0.198 (0.320) data 0.000 (0.120) loss 0.2537 (0.3553) acc 93.5185 (88.2248) lr 6.3188e-04 eta 0:02:09
epoch [33/50] batch [15/23] time 0.187 (0.277) data 0.000 (0.080) loss 0.3670 (0.3600) acc 94.5000 (88.8055) lr 6.3188e-04 eta 0:01:50
epoch [33/50] batch [20/23] time 0.213 (0.258) data 0.000 (0.060) loss 0.1944 (0.3474) acc 94.2982 (89.4508) lr 6.3188e-04 eta 0:01:41
>>> alpha1: 0.164  alpha2: -0.144 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.37 <<<
epoch [34/50] batch [5/23] time 0.290 (0.607) data 0.000 (0.298) loss 0.4061 (0.3801) acc 92.1569 (87.6433) lr 5.7422e-04 eta 0:03:54
epoch [34/50] batch [10/23] time 0.305 (0.458) data 0.000 (0.149) loss 0.3426 (0.3746) acc 85.0962 (88.2414) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [15/23] time 0.287 (0.405) data 0.000 (0.100) loss 0.2294 (0.3705) acc 96.0000 (89.6139) lr 5.7422e-04 eta 0:02:32
epoch [34/50] batch [20/23] time 0.137 (0.341) data 0.000 (0.075) loss 0.4621 (0.3721) acc 83.3333 (89.5778) lr 5.7422e-04 eta 0:02:06
>>> alpha1: 0.161  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.36 <<<
epoch [35/50] batch [5/23] time 0.184 (0.497) data 0.000 (0.280) loss 0.3218 (0.3513) acc 94.2708 (89.8046) lr 5.1825e-04 eta 0:03:00
epoch [35/50] batch [10/23] time 0.197 (0.346) data 0.000 (0.140) loss 0.4181 (0.3447) acc 87.2549 (89.6715) lr 5.1825e-04 eta 0:02:03
epoch [35/50] batch [15/23] time 0.187 (0.297) data 0.000 (0.094) loss 0.4113 (0.3546) acc 89.0625 (89.6910) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [20/23] time 0.273 (0.275) data 0.000 (0.070) loss 0.2196 (0.3478) acc 93.9815 (90.2403) lr 5.1825e-04 eta 0:01:35
>>> alpha1: 0.158  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [36/50] batch [5/23] time 0.275 (0.440) data 0.000 (0.230) loss 0.3442 (0.3600) acc 89.2157 (90.4177) lr 4.6417e-04 eta 0:02:29
epoch [36/50] batch [10/23] time 0.194 (0.317) data 0.000 (0.115) loss 0.2171 (0.5600) acc 96.1538 (88.4837) lr 4.6417e-04 eta 0:01:46
epoch [36/50] batch [15/23] time 0.203 (0.275) data 0.000 (0.077) loss 0.5183 (0.5015) acc 86.5741 (88.9274) lr 4.6417e-04 eta 0:01:30
epoch [36/50] batch [20/23] time 0.188 (0.256) data 0.000 (0.058) loss 0.3508 (0.4698) acc 93.6274 (88.8560) lr 4.6417e-04 eta 0:01:23
>>> alpha1: 0.159  alpha2: -0.142 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [37/50] batch [5/23] time 0.173 (0.465) data 0.000 (0.254) loss 0.4602 (0.3100) acc 89.7727 (91.7282) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [10/23] time 0.198 (0.335) data 0.001 (0.127) loss 0.4854 (0.3517) acc 85.5769 (89.5292) lr 4.1221e-04 eta 0:01:44
epoch [37/50] batch [15/23] time 0.212 (0.291) data 0.000 (0.085) loss 0.2694 (0.3464) acc 95.1754 (89.2709) lr 4.1221e-04 eta 0:01:29
epoch [37/50] batch [20/23] time 0.222 (0.268) data 0.000 (0.064) loss 0.5808 (0.3537) acc 84.5000 (89.4702) lr 4.1221e-04 eta 0:01:21
>>> alpha1: 0.158  alpha2: -0.144 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [38/50] batch [5/23] time 0.153 (0.478) data 0.000 (0.323) loss 0.3257 (0.2901) acc 89.8148 (89.1281) lr 3.6258e-04 eta 0:02:20
epoch [38/50] batch [10/23] time 0.276 (0.338) data 0.000 (0.162) loss 0.3306 (0.3176) acc 91.2037 (89.9682) lr 3.6258e-04 eta 0:01:37
epoch [38/50] batch [15/23] time 0.247 (0.310) data 0.000 (0.108) loss 0.3105 (0.3341) acc 94.8980 (89.6421) lr 3.6258e-04 eta 0:01:28
epoch [38/50] batch [20/23] time 0.248 (0.296) data 0.000 (0.081) loss 0.3283 (0.3463) acc 96.4286 (90.4625) lr 3.6258e-04 eta 0:01:22
>>> alpha1: 0.154  alpha2: -0.133 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.36 <<<
epoch [39/50] batch [5/23] time 0.208 (0.484) data 0.000 (0.282) loss 0.4421 (0.3652) acc 90.9091 (89.0589) lr 3.1545e-04 eta 0:02:11
epoch [39/50] batch [10/23] time 0.249 (0.347) data 0.000 (0.141) loss 0.3580 (0.3555) acc 90.0000 (89.8109) lr 3.1545e-04 eta 0:01:32
epoch [39/50] batch [15/23] time 0.199 (0.297) data 0.000 (0.094) loss 0.3801 (0.3535) acc 87.9630 (89.9375) lr 3.1545e-04 eta 0:01:17
epoch [39/50] batch [20/23] time 0.186 (0.270) data 0.000 (0.071) loss 0.4427 (0.3526) acc 90.0000 (89.4961) lr 3.1545e-04 eta 0:01:09
>>> alpha1: 0.153  alpha2: -0.132 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [40/50] batch [5/23] time 0.205 (0.402) data 0.000 (0.200) loss 0.3926 (0.3524) acc 92.2727 (91.3022) lr 2.7103e-04 eta 0:01:39
epoch [40/50] batch [10/23] time 0.239 (0.302) data 0.000 (0.100) loss 0.2291 (0.3048) acc 94.2982 (91.5038) lr 2.7103e-04 eta 0:01:13
epoch [40/50] batch [15/23] time 0.198 (0.270) data 0.000 (0.067) loss 0.2850 (0.3273) acc 95.6731 (90.7534) lr 2.7103e-04 eta 0:01:04
epoch [40/50] batch [20/23] time 0.204 (0.252) data 0.000 (0.050) loss 0.3391 (0.3473) acc 89.1509 (90.2601) lr 2.7103e-04 eta 0:00:58
>>> alpha1: 0.156  alpha2: -0.128 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [41/50] batch [5/23] time 0.199 (0.444) data 0.000 (0.235) loss 0.3252 (0.3284) acc 86.5385 (89.2476) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [10/23] time 0.203 (0.321) data 0.000 (0.117) loss 0.2676 (0.3171) acc 93.7500 (89.8941) lr 2.2949e-04 eta 0:01:10
epoch [41/50] batch [15/23] time 0.193 (0.279) data 0.000 (0.078) loss 0.4543 (0.3322) acc 94.2308 (90.2498) lr 2.2949e-04 eta 0:00:59
epoch [41/50] batch [20/23] time 0.171 (0.262) data 0.000 (0.059) loss 0.4531 (0.3310) acc 89.7727 (90.7827) lr 2.2949e-04 eta 0:00:55
>>> alpha1: 0.156  alpha2: -0.124 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.35 <<<
epoch [42/50] batch [5/23] time 0.138 (0.701) data 0.000 (0.262) loss 0.3656 (0.3248) acc 94.0217 (93.0857) lr 1.9098e-04 eta 0:02:21
epoch [42/50] batch [10/23] time 0.166 (0.426) data 0.000 (0.131) loss 0.2586 (0.3172) acc 94.8113 (93.2532) lr 1.9098e-04 eta 0:01:23
epoch [42/50] batch [15/23] time 0.187 (0.350) data 0.000 (0.088) loss 0.2480 (0.3210) acc 92.1569 (91.6585) lr 1.9098e-04 eta 0:01:07
epoch [42/50] batch [20/23] time 0.201 (0.312) data 0.000 (0.066) loss 0.2897 (0.3025) acc 91.8182 (91.7319) lr 1.9098e-04 eta 0:00:58
>>> alpha1: 0.154  alpha2: -0.120 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [43/50] batch [5/23] time 0.205 (0.448) data 0.000 (0.239) loss 0.1844 (0.2806) acc 95.9091 (93.0780) lr 1.5567e-04 eta 0:01:20
epoch [43/50] batch [10/23] time 0.202 (0.327) data 0.000 (0.120) loss 0.3120 (0.3058) acc 90.8654 (91.4973) lr 1.5567e-04 eta 0:00:56
epoch [43/50] batch [15/23] time 0.198 (0.288) data 0.000 (0.080) loss 0.3795 (0.3184) acc 88.4615 (91.1532) lr 1.5567e-04 eta 0:00:48
epoch [43/50] batch [20/23] time 0.199 (0.263) data 0.000 (0.060) loss 0.3501 (0.3410) acc 92.3077 (90.8854) lr 1.5567e-04 eta 0:00:43
>>> alpha1: 0.154  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.36 <<<
epoch [44/50] batch [5/23] time 0.194 (0.437) data 0.000 (0.232) loss 0.2566 (0.3957) acc 92.6471 (89.5974) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [10/23] time 0.192 (0.318) data 0.000 (0.116) loss 0.2506 (0.5819) acc 93.8775 (86.9029) lr 1.2369e-04 eta 0:00:47
epoch [44/50] batch [15/23] time 0.208 (0.285) data 0.000 (0.078) loss 0.3309 (0.4925) acc 91.0377 (87.9059) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [20/23] time 0.198 (0.263) data 0.000 (0.058) loss 0.2779 (0.4411) acc 92.3077 (89.2266) lr 1.2369e-04 eta 0:00:37
>>> alpha1: 0.152  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.37 <<<
epoch [45/50] batch [5/23] time 0.216 (0.484) data 0.000 (0.270) loss 0.3688 (0.3395) acc 92.7273 (89.2285) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [10/23] time 0.196 (0.344) data 0.000 (0.135) loss 0.4189 (0.3456) acc 90.6863 (90.0596) lr 9.5173e-05 eta 0:00:44
epoch [45/50] batch [15/23] time 0.203 (0.301) data 0.000 (0.090) loss 0.4146 (0.3216) acc 89.1509 (90.8400) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [20/23] time 0.185 (0.280) data 0.000 (0.068) loss 0.3380 (0.3248) acc 95.3125 (91.5838) lr 9.5173e-05 eta 0:00:33
>>> alpha1: 0.151  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [46/50] batch [5/23] time 0.282 (0.448) data 0.000 (0.222) loss 0.2643 (0.2913) acc 89.9123 (91.6613) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [10/23] time 0.213 (0.325) data 0.000 (0.111) loss 0.4135 (0.3147) acc 88.8393 (91.2906) lr 7.0224e-05 eta 0:00:34
epoch [46/50] batch [15/23] time 0.192 (0.281) data 0.000 (0.074) loss 0.2846 (0.3197) acc 91.5000 (91.5342) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.202 (0.260) data 0.000 (0.056) loss 0.2010 (0.3086) acc 90.0000 (91.6083) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.150  alpha2: -0.133 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [47/50] batch [5/23] time 0.206 (0.468) data 0.001 (0.265) loss 0.2775 (0.3356) acc 92.5926 (90.7901) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [10/23] time 0.200 (0.332) data 0.000 (0.133) loss 0.2365 (0.3017) acc 92.1569 (91.7005) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [15/23] time 0.241 (0.290) data 0.000 (0.089) loss 0.3800 (0.3239) acc 88.5000 (91.1593) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [20/23] time 0.195 (0.266) data 0.000 (0.067) loss 0.2305 (0.3245) acc 94.6078 (91.5430) lr 4.8943e-05 eta 0:00:19
>>> alpha1: 0.150  alpha2: -0.133 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [48/50] batch [5/23] time 0.196 (0.419) data 0.000 (0.230) loss 0.3341 (0.3461) acc 90.5556 (90.7754) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [10/23] time 0.167 (0.301) data 0.000 (0.115) loss 0.4175 (0.3539) acc 89.5349 (91.0125) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.262 (0.265) data 0.000 (0.077) loss 0.4014 (0.3594) acc 90.5000 (90.1451) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [20/23] time 0.189 (0.245) data 0.000 (0.058) loss 0.4174 (0.3481) acc 87.0000 (90.6613) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.150  alpha2: -0.135 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [49/50] batch [5/23] time 0.185 (0.428) data 0.000 (0.227) loss 0.2549 (0.3242) acc 90.0000 (91.2342) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.211 (0.316) data 0.000 (0.114) loss 0.3081 (0.3280) acc 93.4783 (91.4322) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.206 (0.276) data 0.000 (0.076) loss 0.2563 (0.3291) acc 94.6429 (90.7964) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.161 (0.246) data 0.000 (0.057) loss 0.2626 (0.3178) acc 94.7368 (90.7753) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.151  alpha2: -0.128 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.37 <<<
epoch [50/50] batch [5/23] time 0.193 (0.462) data 0.000 (0.257) loss 0.2372 (0.2851) acc 97.5962 (93.1389) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.207 (0.336) data 0.000 (0.129) loss 0.2681 (0.3150) acc 92.4528 (91.8479) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.191 (0.289) data 0.000 (0.086) loss 0.3695 (0.3275) acc 93.0000 (91.6705) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.209 (0.266) data 0.000 (0.065) loss 0.2673 (0.3108) acc 95.0000 (91.8189) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.2, 0.18, 0.18, 0.18, 0.18, 0.19, 0.2, 0.2, 0.21, 0.21, 0.22, 0.21, 0.22, 0.21, 0.22, 0.22, 0.21, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.23, 0.22, 0.22, 0.23, 0.22, 0.21, 0.22, 0.22, 0.22, 0.22, 0.22, 0.23, 0.22, 0.21, 0.23]
* matched noise rate: [0.12, 0.09, 0.08, 0.08, 0.12, 0.12, 0.13, 0.12, 0.1, 0.1, 0.11, 0.13, 0.13, 0.12, 0.13, 0.13, 0.11, 0.14, 0.13, 0.14, 0.14, 0.11, 0.13, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.13, 0.15, 0.14, 0.13, 0.15, 0.14, 0.14, 0.11, 0.13, 0.15]
* unmatched noise rate: [0.3, 0.28, 0.27, 0.26, 0.29, 0.31, 0.31, 0.32, 0.29, 0.31, 0.31, 0.34, 0.35, 0.34, 0.31, 0.32, 0.31, 0.33, 0.33, 0.35, 0.34, 0.32, 0.36, 0.37, 0.36, 0.37, 0.37, 0.35, 0.36, 0.35, 0.33, 0.35, 0.34, 0.36, 0.37, 0.37, 0.35, 0.31, 0.34, 0.37]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:38,  2.43s/it] 18%|█▊        | 3/17 [00:02<00:09,  1.43it/s] 24%|██▎       | 4/17 [00:02<00:06,  2.04it/s] 35%|███▌      | 6/17 [00:02<00:03,  3.44it/s] 47%|████▋     | 8/17 [00:03<00:01,  4.79it/s] 59%|█████▉    | 10/17 [00:03<00:01,  6.03it/s] 71%|███████   | 12/17 [00:03<00:00,  7.09it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.97it/s] 94%|█████████▍| 16/17 [00:03<00:00,  8.66it/s]100%|██████████| 17/17 [00:04<00:00,  3.61it/s]
=> result
* total: 1,692
* correct: 1,057
* accuracy: 62.5%
* error: 37.5%
* macro_f1: 61.6%
=> per-class result
* class: 0 (banded)	total: 36	correct: 20	acc: 55.6%
* class: 1 (blotchy)	total: 36	correct: 2	acc: 5.6%
* class: 2 (braided)	total: 36	correct: 13	acc: 36.1%
* class: 3 (bubbly)	total: 36	correct: 30	acc: 83.3%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 32	acc: 88.9%
* class: 6 (cobwebbed)	total: 36	correct: 30	acc: 83.3%
* class: 7 (cracked)	total: 36	correct: 22	acc: 61.1%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 20	acc: 55.6%
* class: 11 (fibrous)	total: 36	correct: 28	acc: 77.8%
* class: 12 (flecked)	total: 36	correct: 12	acc: 33.3%
* class: 13 (freckled)	total: 36	correct: 26	acc: 72.2%
* class: 14 (frilly)	total: 36	correct: 30	acc: 83.3%
* class: 15 (gauzy)	total: 36	correct: 19	acc: 52.8%
* class: 16 (grid)	total: 36	correct: 15	acc: 41.7%
* class: 17 (grooved)	total: 36	correct: 23	acc: 63.9%
* class: 18 (honeycombed)	total: 36	correct: 27	acc: 75.0%
* class: 19 (interlaced)	total: 36	correct: 27	acc: 75.0%
* class: 20 (knitted)	total: 36	correct: 30	acc: 83.3%
* class: 21 (lacelike)	total: 36	correct: 33	acc: 91.7%
* class: 22 (lined)	total: 36	correct: 17	acc: 47.2%
* class: 23 (marbled)	total: 36	correct: 27	acc: 75.0%
* class: 24 (matted)	total: 36	correct: 26	acc: 72.2%
* class: 25 (meshed)	total: 36	correct: 21	acc: 58.3%
* class: 26 (paisley)	total: 36	correct: 33	acc: 91.7%
* class: 27 (perforated)	total: 36	correct: 21	acc: 58.3%
* class: 28 (pitted)	total: 36	correct: 19	acc: 52.8%
* class: 29 (pleated)	total: 36	correct: 15	acc: 41.7%
* class: 30 (polka-dotted)	total: 36	correct: 24	acc: 66.7%
* class: 31 (porous)	total: 36	correct: 9	acc: 25.0%
* class: 32 (potholed)	total: 36	correct: 30	acc: 83.3%
* class: 33 (scaly)	total: 36	correct: 24	acc: 66.7%
* class: 34 (smeared)	total: 36	correct: 17	acc: 47.2%
* class: 35 (spiralled)	total: 36	correct: 23	acc: 63.9%
* class: 36 (sprinkled)	total: 36	correct: 15	acc: 41.7%
* class: 37 (stained)	total: 36	correct: 19	acc: 52.8%
* class: 38 (stratified)	total: 36	correct: 27	acc: 75.0%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 24	acc: 66.7%
* class: 42 (veined)	total: 36	correct: 22	acc: 61.1%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 19	acc: 52.8%
* class: 45 (wrinkled)	total: 36	correct: 20	acc: 55.6%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 62.5%
Elapsed: 0:15:52
Run this job and save the output to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_0-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.212 (1.019) data 0.000 (0.321) loss 3.4071 (3.4241) acc 31.2500 (28.1250) lr 1.0000e-05 eta 0:19:26
epoch [1/50] batch [10/23] time 0.206 (0.615) data 0.000 (0.161) loss 3.2746 (3.4344) acc 25.0000 (23.7500) lr 1.0000e-05 eta 0:11:41
epoch [1/50] batch [15/23] time 0.206 (0.483) data 0.000 (0.107) loss 3.2534 (3.4103) acc 34.3750 (23.3333) lr 1.0000e-05 eta 0:09:08
epoch [1/50] batch [20/23] time 0.200 (0.414) data 0.000 (0.080) loss 3.2840 (3.3891) acc 18.7500 (22.5000) lr 1.0000e-05 eta 0:07:47
epoch [2/50] batch [5/23] time 0.205 (0.460) data 0.000 (0.231) loss 2.3891 (2.7758) acc 18.7500 (21.2500) lr 2.0000e-03 eta 0:08:35
epoch [2/50] batch [10/23] time 0.209 (0.335) data 0.000 (0.116) loss 2.1577 (2.6232) acc 40.6250 (25.9375) lr 2.0000e-03 eta 0:06:13
epoch [2/50] batch [15/23] time 0.205 (0.297) data 0.000 (0.077) loss 1.9329 (2.3925) acc 50.0000 (32.7083) lr 2.0000e-03 eta 0:05:29
epoch [2/50] batch [20/23] time 0.195 (0.273) data 0.000 (0.058) loss 2.0240 (2.2935) acc 34.3750 (35.0000) lr 2.0000e-03 eta 0:05:01
epoch [3/50] batch [5/23] time 0.218 (0.468) data 0.000 (0.245) loss 2.2790 (1.9348) acc 31.2500 (42.5000) lr 1.9980e-03 eta 0:08:34
epoch [3/50] batch [10/23] time 0.205 (0.340) data 0.000 (0.123) loss 2.0792 (1.9550) acc 40.6250 (44.0625) lr 1.9980e-03 eta 0:06:11
epoch [3/50] batch [15/23] time 0.205 (0.300) data 0.000 (0.082) loss 1.9320 (1.9090) acc 50.0000 (46.6667) lr 1.9980e-03 eta 0:05:26
epoch [3/50] batch [20/23] time 0.205 (0.276) data 0.000 (0.061) loss 1.6247 (1.8545) acc 50.0000 (47.3438) lr 1.9980e-03 eta 0:04:59
epoch [4/50] batch [5/23] time 0.201 (0.447) data 0.000 (0.238) loss 1.4576 (1.6102) acc 56.2500 (50.0000) lr 1.9921e-03 eta 0:08:01
epoch [4/50] batch [10/23] time 0.275 (0.336) data 0.000 (0.119) loss 1.2453 (1.6143) acc 56.2500 (50.6250) lr 1.9921e-03 eta 0:05:59
epoch [4/50] batch [15/23] time 0.208 (0.292) data 0.000 (0.079) loss 1.7383 (1.6385) acc 43.7500 (50.2083) lr 1.9921e-03 eta 0:05:11
epoch [4/50] batch [20/23] time 0.207 (0.270) data 0.000 (0.060) loss 1.6490 (1.6268) acc 56.2500 (51.0938) lr 1.9921e-03 eta 0:04:46
epoch [5/50] batch [5/23] time 0.231 (0.501) data 0.000 (0.279) loss 1.6422 (1.4539) acc 46.8750 (57.5000) lr 1.9823e-03 eta 0:08:47
epoch [5/50] batch [10/23] time 0.150 (0.343) data 0.000 (0.140) loss 1.2235 (1.4130) acc 68.7500 (57.1875) lr 1.9823e-03 eta 0:05:59
epoch [5/50] batch [15/23] time 0.149 (0.279) data 0.000 (0.093) loss 1.2939 (1.4211) acc 59.3750 (56.2500) lr 1.9823e-03 eta 0:04:50
epoch [5/50] batch [20/23] time 0.153 (0.247) data 0.000 (0.070) loss 1.3114 (1.4173) acc 65.6250 (56.7188) lr 1.9823e-03 eta 0:04:16
epoch [6/50] batch [5/23] time 0.309 (0.609) data 0.000 (0.284) loss 1.3923 (1.2929) acc 53.1250 (63.1250) lr 1.9686e-03 eta 0:10:27
epoch [6/50] batch [10/23] time 0.150 (0.440) data 0.000 (0.144) loss 1.3950 (1.2440) acc 65.6250 (61.8750) lr 1.9686e-03 eta 0:07:30
epoch [6/50] batch [15/23] time 0.153 (0.344) data 0.000 (0.096) loss 1.2454 (1.3490) acc 62.5000 (59.7917) lr 1.9686e-03 eta 0:05:50
epoch [6/50] batch [20/23] time 0.157 (0.296) data 0.000 (0.072) loss 1.2348 (1.3321) acc 59.3750 (59.3750) lr 1.9686e-03 eta 0:05:00
epoch [7/50] batch [5/23] time 0.170 (0.545) data 0.000 (0.275) loss 1.3232 (1.2016) acc 59.3750 (63.7500) lr 1.9511e-03 eta 0:09:08
epoch [7/50] batch [10/23] time 0.154 (0.353) data 0.000 (0.138) loss 1.4405 (1.2741) acc 56.2500 (62.1875) lr 1.9511e-03 eta 0:05:53
epoch [7/50] batch [15/23] time 0.154 (0.286) data 0.000 (0.092) loss 1.3476 (1.2848) acc 56.2500 (61.0417) lr 1.9511e-03 eta 0:04:45
epoch [7/50] batch [20/23] time 0.181 (0.261) data 0.000 (0.069) loss 1.4742 (1.2924) acc 62.5000 (60.6250) lr 1.9511e-03 eta 0:04:19
epoch [8/50] batch [5/23] time 0.201 (0.462) data 0.000 (0.244) loss 1.4226 (1.3500) acc 50.0000 (56.2500) lr 1.9298e-03 eta 0:07:34
epoch [8/50] batch [10/23] time 0.195 (0.338) data 0.000 (0.122) loss 1.1787 (1.2005) acc 65.6250 (60.6250) lr 1.9298e-03 eta 0:05:31
epoch [8/50] batch [15/23] time 0.195 (0.291) data 0.000 (0.082) loss 1.2223 (1.1823) acc 59.3750 (60.6250) lr 1.9298e-03 eta 0:04:43
epoch [8/50] batch [20/23] time 0.194 (0.267) data 0.000 (0.061) loss 1.0932 (1.1737) acc 65.6250 (62.1875) lr 1.9298e-03 eta 0:04:18
epoch [9/50] batch [5/23] time 0.219 (0.485) data 0.000 (0.266) loss 1.0194 (0.9579) acc 71.8750 (71.8750) lr 1.9048e-03 eta 0:07:45
epoch [9/50] batch [10/23] time 0.273 (0.355) data 0.000 (0.133) loss 0.9996 (0.9893) acc 68.7500 (70.6250) lr 1.9048e-03 eta 0:05:38
epoch [9/50] batch [15/23] time 0.200 (0.303) data 0.000 (0.089) loss 1.0781 (1.0459) acc 71.8750 (69.5833) lr 1.9048e-03 eta 0:04:47
epoch [9/50] batch [20/23] time 0.209 (0.278) data 0.000 (0.067) loss 1.2497 (1.0864) acc 59.3750 (68.1250) lr 1.9048e-03 eta 0:04:23
epoch [10/50] batch [5/23] time 0.198 (0.484) data 0.000 (0.265) loss 1.2538 (1.0288) acc 62.5000 (66.8750) lr 1.8763e-03 eta 0:07:34
epoch [10/50] batch [10/23] time 0.270 (0.352) data 0.000 (0.133) loss 1.1302 (1.0435) acc 62.5000 (66.8750) lr 1.8763e-03 eta 0:05:28
epoch [10/50] batch [15/23] time 0.205 (0.302) data 0.000 (0.089) loss 1.2263 (1.0945) acc 59.3750 (65.4167) lr 1.8763e-03 eta 0:04:40
epoch [10/50] batch [20/23] time 0.206 (0.278) data 0.000 (0.066) loss 1.3441 (1.0410) acc 59.3750 (67.9688) lr 1.8763e-03 eta 0:04:16
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.392  alpha2: -0.069 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [11/50] batch [5/23] time 1.032 (0.993) data 0.000 (0.224) loss 1.0146 (0.8371) acc 73.8372 (76.7045) lr 1.8443e-03 eta 0:15:09
epoch [11/50] batch [10/23] time 0.184 (0.984) data 0.000 (0.112) loss 0.9907 (0.8600) acc 71.8085 (73.9266) lr 1.8443e-03 eta 0:14:55
epoch [11/50] batch [15/23] time 0.187 (0.794) data 0.000 (0.075) loss 0.8227 (0.8886) acc 68.2292 (73.2321) lr 1.8443e-03 eta 0:11:58
epoch [11/50] batch [20/23] time 1.312 (0.754) data 0.000 (0.056) loss 0.9373 (0.8841) acc 72.4490 (73.3860) lr 1.8443e-03 eta 0:11:19
>>> alpha1: 0.409  alpha2: -0.031 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [12/50] batch [5/23] time 0.310 (0.595) data 0.000 (0.299) loss 0.7592 (0.7589) acc 76.0000 (79.4983) lr 1.8090e-03 eta 0:08:50
epoch [12/50] batch [10/23] time 0.294 (0.443) data 0.000 (0.150) loss 0.6653 (0.7471) acc 83.6735 (79.1864) lr 1.8090e-03 eta 0:06:33
epoch [12/50] batch [15/23] time 0.305 (0.392) data 0.001 (0.100) loss 0.7451 (0.7457) acc 81.1321 (79.2268) lr 1.8090e-03 eta 0:05:46
epoch [12/50] batch [20/23] time 0.131 (0.351) data 0.000 (0.075) loss 0.9040 (0.7582) acc 75.0000 (78.2295) lr 1.8090e-03 eta 0:05:07
>>> alpha1: 0.383  alpha2: -0.048 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [13/50] batch [5/23] time 0.210 (0.432) data 0.000 (0.239) loss 0.7325 (0.7216) acc 82.1429 (78.7243) lr 1.7705e-03 eta 0:06:15
epoch [13/50] batch [10/23] time 0.193 (0.430) data 0.000 (0.120) loss 0.5083 (0.7286) acc 86.5000 (79.6823) lr 1.7705e-03 eta 0:06:11
epoch [13/50] batch [15/23] time 1.336 (0.431) data 0.000 (0.080) loss 0.6251 (0.7431) acc 86.8421 (78.8796) lr 1.7705e-03 eta 0:06:09
epoch [13/50] batch [20/23] time 0.187 (0.370) data 0.000 (0.060) loss 0.4840 (0.7210) acc 77.6042 (78.1760) lr 1.7705e-03 eta 0:05:15
>>> alpha1: 0.379  alpha2: -0.068 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [14/50] batch [5/23] time 0.238 (0.500) data 0.000 (0.296) loss 0.5680 (0.6533) acc 81.2500 (83.1288) lr 1.7290e-03 eta 0:07:03
epoch [14/50] batch [10/23] time 0.201 (0.349) data 0.000 (0.148) loss 0.6252 (0.6462) acc 80.4348 (81.5761) lr 1.7290e-03 eta 0:04:53
epoch [14/50] batch [15/23] time 0.203 (0.294) data 0.000 (0.099) loss 0.7257 (0.6807) acc 76.9231 (79.9287) lr 1.7290e-03 eta 0:04:05
epoch [14/50] batch [20/23] time 0.196 (0.267) data 0.000 (0.074) loss 0.6336 (0.6588) acc 88.5000 (80.6213) lr 1.7290e-03 eta 0:03:41
>>> alpha1: 0.362  alpha2: -0.085 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [15/50] batch [5/23] time 0.187 (0.481) data 0.000 (0.271) loss 0.5530 (0.6544) acc 84.6939 (80.5749) lr 1.6845e-03 eta 0:06:36
epoch [15/50] batch [10/23] time 0.186 (0.443) data 0.000 (0.136) loss 0.6271 (0.5982) acc 77.6596 (82.2007) lr 1.6845e-03 eta 0:06:02
epoch [15/50] batch [15/23] time 0.208 (0.365) data 0.000 (0.091) loss 0.4573 (0.6126) acc 83.4906 (82.2742) lr 1.6845e-03 eta 0:04:56
epoch [15/50] batch [20/23] time 0.842 (0.412) data 0.001 (0.068) loss 0.6466 (0.5978) acc 77.3810 (82.2772) lr 1.6845e-03 eta 0:05:33
>>> alpha1: 0.332  alpha2: -0.110 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [16/50] batch [5/23] time 0.199 (0.478) data 0.000 (0.288) loss 0.3574 (0.5307) acc 90.0943 (85.5753) lr 1.6374e-03 eta 0:06:22
epoch [16/50] batch [10/23] time 0.174 (0.331) data 0.001 (0.144) loss 0.5055 (0.5621) acc 86.1702 (83.2938) lr 1.6374e-03 eta 0:04:23
epoch [16/50] batch [15/23] time 0.192 (0.282) data 0.000 (0.096) loss 0.6280 (0.5749) acc 79.8913 (82.9823) lr 1.6374e-03 eta 0:03:43
epoch [16/50] batch [20/23] time 0.184 (0.261) data 0.000 (0.072) loss 0.6087 (0.5935) acc 72.3404 (81.8572) lr 1.6374e-03 eta 0:03:25
>>> alpha1: 0.310  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [17/50] batch [5/23] time 0.174 (0.444) data 0.000 (0.267) loss 0.5980 (0.5382) acc 79.3478 (85.4215) lr 1.5878e-03 eta 0:05:45
epoch [17/50] batch [10/23] time 0.174 (0.310) data 0.001 (0.134) loss 0.5748 (0.5623) acc 79.3478 (83.2490) lr 1.5878e-03 eta 0:03:59
epoch [17/50] batch [15/23] time 0.170 (0.265) data 0.000 (0.089) loss 0.4729 (0.5744) acc 88.0435 (82.7321) lr 1.5878e-03 eta 0:03:23
epoch [17/50] batch [20/23] time 0.260 (0.248) data 0.000 (0.067) loss 0.6003 (0.5670) acc 83.1633 (83.0514) lr 1.5878e-03 eta 0:03:08
>>> alpha1: 0.292  alpha2: -0.138 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [18/50] batch [5/23] time 0.203 (0.452) data 0.000 (0.268) loss 0.6199 (1.0554) acc 81.7308 (74.7342) lr 1.5358e-03 eta 0:05:40
epoch [18/50] batch [10/23] time 0.169 (0.324) data 0.000 (0.134) loss 0.5433 (0.8045) acc 90.0000 (79.6074) lr 1.5358e-03 eta 0:04:02
epoch [18/50] batch [15/23] time 0.164 (0.274) data 0.000 (0.090) loss 0.6480 (0.7332) acc 77.2727 (81.0197) lr 1.5358e-03 eta 0:03:23
epoch [18/50] batch [20/23] time 1.056 (0.293) data 0.000 (0.067) loss 0.7941 (0.7082) acc 81.0976 (81.1503) lr 1.5358e-03 eta 0:03:36
>>> alpha1: 0.274  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [19/50] batch [5/23] time 0.175 (0.450) data 0.000 (0.293) loss 0.3952 (0.4850) acc 89.7959 (86.3372) lr 1.4818e-03 eta 0:05:29
epoch [19/50] batch [10/23] time 0.219 (0.338) data 0.000 (0.147) loss 0.5428 (0.5365) acc 82.7381 (84.2832) lr 1.4818e-03 eta 0:04:05
epoch [19/50] batch [15/23] time 0.237 (0.306) data 0.000 (0.098) loss 0.7010 (0.5448) acc 77.2222 (84.0042) lr 1.4818e-03 eta 0:03:40
epoch [19/50] batch [20/23] time 0.245 (0.291) data 0.000 (0.073) loss 0.6114 (0.5474) acc 81.2500 (83.8249) lr 1.4818e-03 eta 0:03:28
>>> alpha1: 0.253  alpha2: -0.148 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [20/50] batch [5/23] time 0.200 (0.443) data 0.001 (0.239) loss 0.2546 (0.4787) acc 86.9792 (85.7899) lr 1.4258e-03 eta 0:05:13
epoch [20/50] batch [10/23] time 0.170 (0.310) data 0.000 (0.120) loss 0.5319 (0.5088) acc 88.8889 (85.1587) lr 1.4258e-03 eta 0:03:37
epoch [20/50] batch [15/23] time 0.189 (0.268) data 0.000 (0.080) loss 0.4238 (0.5057) acc 85.0000 (84.4787) lr 1.4258e-03 eta 0:03:07
epoch [20/50] batch [20/23] time 0.179 (0.246) data 0.000 (0.060) loss 0.4180 (0.5147) acc 86.9792 (84.2298) lr 1.4258e-03 eta 0:02:50
>>> alpha1: 0.234  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [21/50] batch [5/23] time 0.182 (0.468) data 0.000 (0.282) loss 0.5145 (0.4621) acc 86.7021 (89.0598) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [10/23] time 0.192 (0.325) data 0.000 (0.141) loss 0.4382 (0.4741) acc 83.6735 (86.7029) lr 1.3681e-03 eta 0:03:41
epoch [21/50] batch [15/23] time 0.185 (0.278) data 0.000 (0.094) loss 0.3653 (0.4620) acc 94.2708 (86.9417) lr 1.3681e-03 eta 0:03:07
epoch [21/50] batch [20/23] time 0.184 (0.258) data 0.000 (0.071) loss 0.6632 (0.5163) acc 80.8511 (84.4741) lr 1.3681e-03 eta 0:02:52
>>> alpha1: 0.221  alpha2: -0.148 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [22/50] batch [5/23] time 0.192 (0.455) data 0.000 (0.247) loss 0.4774 (0.5146) acc 82.3864 (83.9344) lr 1.3090e-03 eta 0:05:01
epoch [22/50] batch [10/23] time 0.176 (0.323) data 0.000 (0.124) loss 0.7774 (0.5343) acc 74.4445 (83.5921) lr 1.3090e-03 eta 0:03:32
epoch [22/50] batch [15/23] time 0.167 (0.275) data 0.000 (0.083) loss 0.5928 (0.5356) acc 81.5476 (83.1039) lr 1.3090e-03 eta 0:02:59
epoch [22/50] batch [20/23] time 0.168 (0.255) data 0.000 (0.062) loss 0.6804 (0.5305) acc 75.5952 (83.3693) lr 1.3090e-03 eta 0:02:44
>>> alpha1: 0.212  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [23/50] batch [5/23] time 0.187 (0.447) data 0.001 (0.262) loss 0.4082 (0.5268) acc 85.5000 (83.2798) lr 1.2487e-03 eta 0:04:45
epoch [23/50] batch [10/23] time 0.176 (0.314) data 0.000 (0.131) loss 0.5407 (0.5089) acc 89.1304 (85.2950) lr 1.2487e-03 eta 0:03:19
epoch [23/50] batch [15/23] time 0.180 (0.270) data 0.000 (0.088) loss 0.4734 (0.5141) acc 76.0638 (84.2409) lr 1.2487e-03 eta 0:02:49
epoch [23/50] batch [20/23] time 0.188 (0.252) data 0.000 (0.066) loss 0.5034 (0.5156) acc 85.9375 (84.6315) lr 1.2487e-03 eta 0:02:37
>>> alpha1: 0.200  alpha2: -0.148 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [24/50] batch [5/23] time 0.200 (0.468) data 0.000 (0.254) loss 0.4271 (0.4534) acc 89.8936 (86.9938) lr 1.1874e-03 eta 0:04:48
epoch [24/50] batch [10/23] time 0.190 (0.326) data 0.000 (0.127) loss 0.3497 (0.4520) acc 87.7551 (87.1040) lr 1.1874e-03 eta 0:03:19
epoch [24/50] batch [15/23] time 0.178 (0.276) data 0.000 (0.085) loss 0.5832 (0.4891) acc 84.0425 (85.8334) lr 1.1874e-03 eta 0:02:47
epoch [24/50] batch [20/23] time 0.178 (0.252) data 0.000 (0.064) loss 0.3825 (0.4815) acc 88.8889 (85.8133) lr 1.1874e-03 eta 0:02:31
>>> alpha1: 0.198  alpha2: -0.142 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [25/50] batch [5/23] time 0.174 (0.441) data 0.000 (0.245) loss 0.4742 (0.4858) acc 82.9545 (84.3239) lr 1.1253e-03 eta 0:04:21
epoch [25/50] batch [10/23] time 0.244 (0.317) data 0.000 (0.123) loss 0.4539 (0.4836) acc 90.0000 (85.5896) lr 1.1253e-03 eta 0:03:06
epoch [25/50] batch [15/23] time 0.179 (0.270) data 0.000 (0.082) loss 0.4830 (0.4784) acc 80.8511 (85.1027) lr 1.1253e-03 eta 0:02:37
epoch [25/50] batch [20/23] time 0.181 (0.249) data 0.000 (0.061) loss 0.3510 (0.4668) acc 90.2174 (85.5766) lr 1.1253e-03 eta 0:02:23
>>> alpha1: 0.194  alpha2: -0.146 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.35 <<<
epoch [26/50] batch [5/23] time 0.171 (0.425) data 0.000 (0.267) loss 0.6761 (0.5065) acc 82.0652 (85.0620) lr 1.0628e-03 eta 0:04:02
epoch [26/50] batch [10/23] time 0.299 (0.345) data 0.000 (0.135) loss 0.3444 (0.4660) acc 94.5455 (86.6004) lr 1.0628e-03 eta 0:03:15
epoch [26/50] batch [15/23] time 0.276 (0.330) data 0.000 (0.090) loss 0.5290 (0.4519) acc 79.2553 (86.6420) lr 1.0628e-03 eta 0:03:04
epoch [26/50] batch [20/23] time 0.273 (0.396) data 0.001 (0.068) loss 0.4750 (0.4473) acc 88.8298 (86.7280) lr 1.0628e-03 eta 0:03:39
>>> alpha1: 0.187  alpha2: -0.147 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [27/50] batch [5/23] time 0.179 (0.399) data 0.000 (0.218) loss 0.3783 (0.4404) acc 91.6667 (89.1071) lr 1.0000e-03 eta 0:03:38
epoch [27/50] batch [10/23] time 0.923 (0.365) data 0.000 (0.109) loss 0.5705 (0.4440) acc 88.1579 (88.0909) lr 1.0000e-03 eta 0:03:18
epoch [27/50] batch [15/23] time 0.176 (0.309) data 0.000 (0.073) loss 0.4529 (0.4799) acc 88.0682 (86.8850) lr 1.0000e-03 eta 0:02:46
epoch [27/50] batch [20/23] time 0.184 (0.278) data 0.000 (0.055) loss 0.3715 (0.4649) acc 91.4894 (87.6091) lr 1.0000e-03 eta 0:02:28
>>> alpha1: 0.184  alpha2: -0.147 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.28 <<<
epoch [28/50] batch [5/23] time 0.173 (0.470) data 0.000 (0.279) loss 2.8058 (0.8725) acc 57.2222 (80.2739) lr 9.3721e-04 eta 0:04:06
epoch [28/50] batch [10/23] time 0.184 (0.325) data 0.000 (0.140) loss 0.4943 (0.8224) acc 80.5556 (82.3887) lr 9.3721e-04 eta 0:02:48
epoch [28/50] batch [15/23] time 0.197 (0.278) data 0.000 (0.093) loss 0.4751 (0.7241) acc 87.2549 (83.7004) lr 9.3721e-04 eta 0:02:22
epoch [28/50] batch [20/23] time 0.185 (0.255) data 0.000 (0.070) loss 0.3391 (0.6572) acc 95.3125 (84.2951) lr 9.3721e-04 eta 0:02:09
>>> alpha1: 0.184  alpha2: -0.159 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [29/50] batch [5/23] time 0.205 (0.491) data 0.015 (0.292) loss 0.3335 (0.3823) acc 90.8654 (88.2559) lr 8.7467e-04 eta 0:04:06
epoch [29/50] batch [10/23] time 0.215 (0.342) data 0.001 (0.147) loss 0.4026 (0.3979) acc 91.1458 (88.3446) lr 8.7467e-04 eta 0:02:49
epoch [29/50] batch [15/23] time 0.191 (0.292) data 0.000 (0.098) loss 0.3239 (0.4133) acc 86.0000 (88.3647) lr 8.7467e-04 eta 0:02:23
epoch [29/50] batch [20/23] time 0.258 (0.270) data 0.000 (0.073) loss 0.6591 (0.4332) acc 82.0000 (87.6824) lr 8.7467e-04 eta 0:02:10
>>> alpha1: 0.176  alpha2: -0.158 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [30/50] batch [5/23] time 0.153 (0.421) data 0.000 (0.269) loss 0.6199 (0.4107) acc 80.3571 (88.4896) lr 8.1262e-04 eta 0:03:21
epoch [30/50] batch [10/23] time 0.220 (0.305) data 0.000 (0.135) loss 0.4745 (0.4208) acc 80.9783 (86.9455) lr 8.1262e-04 eta 0:02:24
epoch [30/50] batch [15/23] time 0.244 (0.286) data 0.000 (0.090) loss 0.6774 (0.4250) acc 76.1628 (86.8113) lr 8.1262e-04 eta 0:02:13
epoch [30/50] batch [20/23] time 0.253 (0.275) data 0.000 (0.068) loss 0.5628 (0.4228) acc 80.6122 (86.8874) lr 8.1262e-04 eta 0:02:07
>>> alpha1: 0.174  alpha2: -0.148 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [31/50] batch [5/23] time 0.189 (0.495) data 0.000 (0.307) loss 0.4809 (0.3597) acc 86.1702 (89.2519) lr 7.5131e-04 eta 0:03:45
epoch [31/50] batch [10/23] time 0.181 (0.345) data 0.001 (0.154) loss 0.2591 (0.3629) acc 88.9535 (89.2736) lr 7.5131e-04 eta 0:02:35
epoch [31/50] batch [15/23] time 0.185 (0.291) data 0.000 (0.103) loss 0.4191 (0.3668) acc 87.7660 (89.3434) lr 7.5131e-04 eta 0:02:09
epoch [31/50] batch [20/23] time 0.184 (0.264) data 0.000 (0.077) loss 0.3671 (0.3870) acc 92.7083 (88.9489) lr 7.5131e-04 eta 0:01:56
>>> alpha1: 0.171  alpha2: -0.154 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [32/50] batch [5/23] time 0.216 (0.494) data 0.000 (0.290) loss 0.3985 (0.3559) acc 87.2222 (88.9412) lr 6.9098e-04 eta 0:03:33
epoch [32/50] batch [10/23] time 0.183 (0.341) data 0.000 (0.145) loss 0.3705 (0.3516) acc 90.4255 (89.8023) lr 6.9098e-04 eta 0:02:25
epoch [32/50] batch [15/23] time 0.178 (0.288) data 0.001 (0.097) loss 0.4600 (0.3767) acc 79.3478 (88.9639) lr 6.9098e-04 eta 0:02:01
epoch [32/50] batch [20/23] time 0.174 (0.305) data 0.000 (0.073) loss 0.3368 (0.3977) acc 87.5000 (88.3154) lr 6.9098e-04 eta 0:02:07
>>> alpha1: 0.165  alpha2: -0.146 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [33/50] batch [5/23] time 0.273 (0.476) data 0.000 (0.266) loss 0.3750 (0.3841) acc 88.2653 (89.2429) lr 6.3188e-04 eta 0:03:14
epoch [33/50] batch [10/23] time 0.168 (0.333) data 0.000 (0.133) loss 0.3859 (0.4152) acc 88.6905 (88.6176) lr 6.3188e-04 eta 0:02:14
epoch [33/50] batch [15/23] time 0.193 (0.281) data 0.000 (0.089) loss 0.4942 (0.4360) acc 86.5000 (88.0732) lr 6.3188e-04 eta 0:01:51
epoch [33/50] batch [20/23] time 0.181 (0.256) data 0.000 (0.067) loss 0.3888 (0.4146) acc 89.1304 (88.7861) lr 6.3188e-04 eta 0:01:40
>>> alpha1: 0.160  alpha2: -0.141 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [34/50] batch [5/23] time 0.215 (0.442) data 0.000 (0.234) loss 0.4205 (0.4047) acc 84.8039 (89.9461) lr 5.7422e-04 eta 0:02:50
epoch [34/50] batch [10/23] time 0.193 (0.313) data 0.000 (0.117) loss 0.4650 (0.3942) acc 89.7959 (89.3029) lr 5.7422e-04 eta 0:01:59
epoch [34/50] batch [15/23] time 0.182 (0.270) data 0.000 (0.078) loss 0.3720 (0.4133) acc 84.2391 (88.6591) lr 5.7422e-04 eta 0:01:41
epoch [34/50] batch [20/23] time 0.184 (0.249) data 0.000 (0.059) loss 0.4558 (0.4051) acc 89.8936 (89.0220) lr 5.7422e-04 eta 0:01:32
>>> alpha1: 0.159  alpha2: -0.145 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [35/50] batch [5/23] time 0.163 (0.475) data 0.000 (0.276) loss 0.4472 (0.4426) acc 87.5000 (86.2642) lr 5.1825e-04 eta 0:02:52
epoch [35/50] batch [10/23] time 0.181 (0.329) data 0.000 (0.138) loss 0.2849 (0.4183) acc 92.3913 (87.3490) lr 5.1825e-04 eta 0:01:57
epoch [35/50] batch [15/23] time 0.191 (0.281) data 0.000 (0.092) loss 0.2958 (0.3940) acc 93.6274 (88.1901) lr 5.1825e-04 eta 0:01:39
epoch [35/50] batch [20/23] time 0.211 (0.260) data 0.000 (0.069) loss 0.2979 (0.4017) acc 90.4255 (87.9146) lr 5.1825e-04 eta 0:01:30
>>> alpha1: 0.157  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [36/50] batch [5/23] time 0.191 (0.428) data 0.000 (0.243) loss 0.2823 (0.3930) acc 95.4082 (89.1397) lr 4.6417e-04 eta 0:02:25
epoch [36/50] batch [10/23] time 0.219 (0.314) data 0.000 (0.121) loss 0.3722 (0.4372) acc 91.5000 (87.9602) lr 4.6417e-04 eta 0:01:45
epoch [36/50] batch [15/23] time 0.181 (0.270) data 0.000 (0.081) loss 0.5111 (0.5773) acc 87.5000 (86.1341) lr 4.6417e-04 eta 0:01:29
epoch [36/50] batch [20/23] time 0.178 (0.248) data 0.000 (0.061) loss 0.2843 (0.5199) acc 94.4445 (87.6263) lr 4.6417e-04 eta 0:01:20
>>> alpha1: 0.158  alpha2: -0.157 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [37/50] batch [5/23] time 0.261 (0.599) data 0.000 (0.341) loss 0.3085 (0.3657) acc 91.4894 (90.7837) lr 4.1221e-04 eta 0:03:10
epoch [37/50] batch [10/23] time 0.236 (0.418) data 0.000 (0.171) loss 0.3017 (0.3557) acc 88.8889 (90.0093) lr 4.1221e-04 eta 0:02:10
epoch [37/50] batch [15/23] time 0.249 (0.360) data 0.000 (0.114) loss 0.3725 (0.3609) acc 93.7500 (89.8099) lr 4.1221e-04 eta 0:01:50
epoch [37/50] batch [20/23] time 0.127 (0.325) data 0.000 (0.086) loss 0.3476 (0.3654) acc 89.8810 (89.6798) lr 4.1221e-04 eta 0:01:38
>>> alpha1: 0.158  alpha2: -0.162 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [38/50] batch [5/23] time 0.193 (0.451) data 0.000 (0.258) loss 0.3170 (0.3360) acc 91.6667 (90.3852) lr 3.6258e-04 eta 0:02:12
epoch [38/50] batch [10/23] time 0.172 (0.322) data 0.000 (0.129) loss 0.1893 (0.3746) acc 97.8261 (89.9680) lr 3.6258e-04 eta 0:01:32
epoch [38/50] batch [15/23] time 0.166 (0.276) data 0.000 (0.086) loss 0.5236 (0.3728) acc 85.9756 (89.6242) lr 3.6258e-04 eta 0:01:18
epoch [38/50] batch [20/23] time 0.181 (0.253) data 0.000 (0.065) loss 0.4198 (0.3537) acc 89.6739 (90.0074) lr 3.6258e-04 eta 0:01:10
>>> alpha1: 0.158  alpha2: -0.165 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [39/50] batch [5/23] time 0.176 (0.495) data 0.001 (0.294) loss 0.2386 (0.2961) acc 92.5532 (92.5890) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [10/23] time 0.167 (0.334) data 0.000 (0.147) loss 0.3764 (0.3293) acc 95.3488 (91.1288) lr 3.1545e-04 eta 0:01:28
epoch [39/50] batch [15/23] time 0.188 (0.281) data 0.000 (0.098) loss 0.3877 (0.3468) acc 93.8775 (90.8428) lr 3.1545e-04 eta 0:01:13
epoch [39/50] batch [20/23] time 0.176 (0.255) data 0.000 (0.074) loss 0.4710 (0.3490) acc 89.1304 (90.5812) lr 3.1545e-04 eta 0:01:05
>>> alpha1: 0.158  alpha2: -0.157 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [40/50] batch [5/23] time 0.178 (0.488) data 0.001 (0.300) loss 0.3186 (0.3381) acc 92.9348 (91.3383) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [10/23] time 0.174 (0.336) data 0.000 (0.150) loss 0.4064 (0.3431) acc 89.1304 (90.7690) lr 2.7103e-04 eta 0:01:21
epoch [40/50] batch [15/23] time 0.228 (0.286) data 0.000 (0.100) loss 0.4342 (0.3486) acc 83.5106 (89.9476) lr 2.7103e-04 eta 0:01:08
epoch [40/50] batch [20/23] time 0.172 (0.260) data 0.000 (0.075) loss 0.4630 (0.3514) acc 89.2045 (89.6709) lr 2.7103e-04 eta 0:01:00
>>> alpha1: 0.157  alpha2: -0.155 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [41/50] batch [5/23] time 0.168 (0.467) data 0.000 (0.291) loss 0.3621 (0.3519) acc 86.9565 (89.8715) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [10/23] time 0.166 (0.319) data 0.000 (0.145) loss 0.4243 (0.3585) acc 87.2222 (88.8199) lr 2.2949e-04 eta 0:01:10
epoch [41/50] batch [15/23] time 0.192 (0.274) data 0.000 (0.097) loss 0.3298 (0.4681) acc 89.7959 (87.9787) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [20/23] time 0.184 (0.254) data 0.000 (0.073) loss 0.6728 (0.4431) acc 80.3191 (88.8568) lr 2.2949e-04 eta 0:00:53
>>> alpha1: 0.155  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [42/50] batch [5/23] time 0.209 (0.406) data 0.000 (0.210) loss 0.4845 (0.3297) acc 85.9375 (89.9653) lr 1.9098e-04 eta 0:01:22
epoch [42/50] batch [10/23] time 0.181 (0.301) data 0.001 (0.105) loss 0.3156 (0.3385) acc 96.1956 (90.8502) lr 1.9098e-04 eta 0:00:59
epoch [42/50] batch [15/23] time 0.184 (0.263) data 0.000 (0.070) loss 0.2297 (0.3281) acc 95.2128 (91.1415) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [20/23] time 0.190 (0.243) data 0.000 (0.053) loss 0.3773 (0.3427) acc 79.0816 (90.4587) lr 1.9098e-04 eta 0:00:45
>>> alpha1: 0.153  alpha2: -0.154 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [43/50] batch [5/23] time 0.194 (0.417) data 0.000 (0.224) loss 0.3748 (0.3652) acc 85.6383 (91.2069) lr 1.5567e-04 eta 0:01:14
epoch [43/50] batch [10/23] time 0.202 (0.304) data 0.000 (0.112) loss 0.4630 (0.3391) acc 89.4231 (91.6152) lr 1.5567e-04 eta 0:00:52
epoch [43/50] batch [15/23] time 0.182 (0.267) data 0.000 (0.075) loss 0.3559 (0.3369) acc 87.7660 (90.9755) lr 1.5567e-04 eta 0:00:45
epoch [43/50] batch [20/23] time 0.200 (0.247) data 0.000 (0.056) loss 0.1701 (0.3401) acc 96.5686 (90.6096) lr 1.5567e-04 eta 0:00:40
>>> alpha1: 0.152  alpha2: -0.155 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [44/50] batch [5/23] time 0.161 (0.431) data 0.000 (0.281) loss 0.5082 (0.3676) acc 83.1522 (87.8300) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [10/23] time 0.248 (0.303) data 0.000 (0.141) loss 0.3512 (0.4070) acc 86.4130 (89.2046) lr 1.2369e-04 eta 0:00:45
epoch [44/50] batch [15/23] time 0.273 (0.290) data 0.000 (0.094) loss 0.2605 (0.3669) acc 95.7447 (90.0476) lr 1.2369e-04 eta 0:00:42
epoch [44/50] batch [20/23] time 0.277 (0.286) data 0.000 (0.070) loss 0.5422 (0.3738) acc 86.1702 (89.7514) lr 1.2369e-04 eta 0:00:40
>>> alpha1: 0.150  alpha2: -0.156 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [45/50] batch [5/23] time 0.199 (0.461) data 0.000 (0.266) loss 0.2448 (0.3430) acc 96.0784 (90.4634) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [10/23] time 0.178 (0.323) data 0.000 (0.133) loss 0.3921 (0.3319) acc 90.2174 (90.9866) lr 9.5173e-05 eta 0:00:41
epoch [45/50] batch [15/23] time 0.181 (0.278) data 0.000 (0.089) loss 0.4806 (0.3354) acc 84.4445 (90.6529) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [20/23] time 0.179 (0.255) data 0.000 (0.067) loss 0.3553 (0.3419) acc 91.1111 (90.8186) lr 9.5173e-05 eta 0:00:30
>>> alpha1: 0.149  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [46/50] batch [5/23] time 0.189 (0.451) data 0.000 (0.257) loss 0.3176 (0.3982) acc 91.8367 (88.5541) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [10/23] time 0.178 (0.320) data 0.000 (0.129) loss 0.5602 (0.3787) acc 87.5000 (89.7756) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [15/23] time 0.205 (0.281) data 0.000 (0.086) loss 0.3616 (0.3494) acc 89.2857 (90.7594) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.176 (0.256) data 0.000 (0.065) loss 0.3571 (0.3367) acc 90.0000 (90.9433) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.148  alpha2: -0.150 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [47/50] batch [5/23] time 0.190 (0.427) data 0.000 (0.214) loss 0.4232 (0.3257) acc 88.3333 (92.2111) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [10/23] time 0.186 (0.302) data 0.000 (0.107) loss 0.3607 (0.3810) acc 93.8889 (90.2363) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [15/23] time 0.175 (0.263) data 0.000 (0.072) loss 0.1993 (0.3524) acc 90.5556 (91.0312) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [20/23] time 0.252 (0.248) data 0.000 (0.054) loss 0.2836 (0.3395) acc 93.0000 (91.1683) lr 4.8943e-05 eta 0:00:17
>>> alpha1: 0.147  alpha2: -0.151 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [48/50] batch [5/23] time 0.151 (0.419) data 0.000 (0.268) loss 0.4059 (0.3501) acc 89.5000 (88.9054) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [10/23] time 0.268 (0.292) data 0.000 (0.134) loss 0.3575 (0.3474) acc 88.5000 (89.9333) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.165 (0.254) data 0.001 (0.090) loss 0.4373 (0.3459) acc 85.3659 (90.3246) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.189 (0.236) data 0.001 (0.067) loss 0.3219 (0.3454) acc 90.5000 (90.1762) lr 3.1417e-05 eta 0:00:11
>>> alpha1: 0.145  alpha2: -0.147 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [49/50] batch [5/23] time 0.189 (0.443) data 0.000 (0.234) loss 0.2887 (0.3270) acc 85.4167 (89.4859) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.181 (0.314) data 0.000 (0.117) loss 0.3184 (0.3177) acc 92.9348 (91.2425) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.168 (0.269) data 0.000 (0.078) loss 0.4509 (0.3435) acc 80.8139 (90.7819) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.239 (0.249) data 0.000 (0.059) loss 0.4026 (0.3390) acc 87.2093 (90.8971) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.148  alpha2: -0.146 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [50/50] batch [5/23] time 0.184 (0.499) data 0.000 (0.292) loss 0.4421 (0.3718) acc 87.5000 (90.2176) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.173 (0.346) data 0.000 (0.146) loss 0.3525 (0.3356) acc 88.6364 (91.2416) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.193 (0.295) data 0.001 (0.098) loss 0.3298 (0.3346) acc 91.8367 (90.9882) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.195 (0.271) data 0.000 (0.073) loss 0.2499 (0.3262) acc 94.0000 (90.9039) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.21, 0.2, 0.2, 0.21, 0.2, 0.21, 0.2, 0.21, 0.2, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.2, 0.21, 0.21, 0.22, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.2, 0.21, 0.21, 0.21, 0.2, 0.21, 0.21, 0.2, 0.21, 0.21, 0.21, 0.21, 0.21]
* matched noise rate: [0.12, 0.11, 0.1, 0.1, 0.11, 0.11, 0.09, 0.1, 0.09, 0.1, 0.11, 0.1, 0.11, 0.11, 0.11, 0.11, 0.12, 0.11, 0.12, 0.12, 0.11, 0.11, 0.12, 0.12, 0.12, 0.12, 0.11, 0.12, 0.11, 0.11, 0.11, 0.12, 0.11, 0.12, 0.12, 0.12, 0.11, 0.12, 0.11, 0.11]
* unmatched noise rate: [0.3, 0.3, 0.31, 0.32, 0.32, 0.32, 0.29, 0.29, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.35, 0.29, 0.28, 0.33, 0.28, 0.3, 0.29, 0.29, 0.28, 0.28, 0.29, 0.29, 0.28, 0.29, 0.3, 0.29, 0.29, 0.29, 0.3, 0.28, 0.29, 0.3, 0.3, 0.31, 0.31]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:40,  2.53s/it] 12%|█▏        | 2/17 [00:02<00:16,  1.10s/it] 24%|██▎       | 4/17 [00:02<00:06,  2.13it/s] 35%|███▌      | 6/17 [00:03<00:03,  3.43it/s] 47%|████▋     | 8/17 [00:03<00:01,  4.73it/s] 59%|█████▉    | 10/17 [00:03<00:01,  5.95it/s] 71%|███████   | 12/17 [00:03<00:00,  7.02it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.94it/s] 94%|█████████▍| 16/17 [00:03<00:00,  8.67it/s]100%|██████████| 17/17 [00:04<00:00,  3.55it/s]
=> result
* total: 1,692
* correct: 1,048
* accuracy: 61.9%
* error: 38.1%
* macro_f1: 60.6%
=> per-class result
* class: 0 (banded)	total: 36	correct: 30	acc: 83.3%
* class: 1 (blotchy)	total: 36	correct: 1	acc: 2.8%
* class: 2 (braided)	total: 36	correct: 11	acc: 30.6%
* class: 3 (bubbly)	total: 36	correct: 34	acc: 94.4%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 31	acc: 86.1%
* class: 7 (cracked)	total: 36	correct: 22	acc: 61.1%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 32	acc: 88.9%
* class: 10 (dotted)	total: 36	correct: 10	acc: 27.8%
* class: 11 (fibrous)	total: 36	correct: 27	acc: 75.0%
* class: 12 (flecked)	total: 36	correct: 20	acc: 55.6%
* class: 13 (freckled)	total: 36	correct: 31	acc: 86.1%
* class: 14 (frilly)	total: 36	correct: 31	acc: 86.1%
* class: 15 (gauzy)	total: 36	correct: 19	acc: 52.8%
* class: 16 (grid)	total: 36	correct: 18	acc: 50.0%
* class: 17 (grooved)	total: 36	correct: 21	acc: 58.3%
* class: 18 (honeycombed)	total: 36	correct: 24	acc: 66.7%
* class: 19 (interlaced)	total: 36	correct: 25	acc: 69.4%
* class: 20 (knitted)	total: 36	correct: 25	acc: 69.4%
* class: 21 (lacelike)	total: 36	correct: 35	acc: 97.2%
* class: 22 (lined)	total: 36	correct: 7	acc: 19.4%
* class: 23 (marbled)	total: 36	correct: 17	acc: 47.2%
* class: 24 (matted)	total: 36	correct: 22	acc: 61.1%
* class: 25 (meshed)	total: 36	correct: 13	acc: 36.1%
* class: 26 (paisley)	total: 36	correct: 31	acc: 86.1%
* class: 27 (perforated)	total: 36	correct: 26	acc: 72.2%
* class: 28 (pitted)	total: 36	correct: 12	acc: 33.3%
* class: 29 (pleated)	total: 36	correct: 21	acc: 58.3%
* class: 30 (polka-dotted)	total: 36	correct: 28	acc: 77.8%
* class: 31 (porous)	total: 36	correct: 11	acc: 30.6%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 29	acc: 80.6%
* class: 34 (smeared)	total: 36	correct: 20	acc: 55.6%
* class: 35 (spiralled)	total: 36	correct: 25	acc: 69.4%
* class: 36 (sprinkled)	total: 36	correct: 17	acc: 47.2%
* class: 37 (stained)	total: 36	correct: 13	acc: 36.1%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 26	acc: 72.2%
* class: 40 (studded)	total: 36	correct: 28	acc: 77.8%
* class: 41 (swirly)	total: 36	correct: 27	acc: 75.0%
* class: 42 (veined)	total: 36	correct: 17	acc: 47.2%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 18	acc: 50.0%
* class: 45 (wrinkled)	total: 36	correct: 25	acc: 69.4%
* class: 46 (zigzagged)	total: 36	correct: 29	acc: 80.6%
* average: 61.9%
Elapsed: 0:16:09
Run this job and save the output to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_2-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.201 (1.195) data 0.000 (0.511) loss 3.3963 (3.4734) acc 18.7500 (21.2500) lr 1.0000e-05 eta 0:22:48
epoch [1/50] batch [10/23] time 0.208 (0.701) data 0.000 (0.256) loss 3.5391 (3.5088) acc 9.3750 (18.1250) lr 1.0000e-05 eta 0:13:18
epoch [1/50] batch [15/23] time 0.210 (0.537) data 0.000 (0.170) loss 3.2683 (3.4954) acc 25.0000 (17.2917) lr 1.0000e-05 eta 0:10:09
epoch [1/50] batch [20/23] time 0.202 (0.458) data 0.000 (0.128) loss 3.3735 (3.4573) acc 18.7500 (19.2188) lr 1.0000e-05 eta 0:08:37
epoch [2/50] batch [5/23] time 0.215 (0.495) data 0.000 (0.275) loss 3.0125 (3.2425) acc 25.0000 (18.1250) lr 2.0000e-03 eta 0:09:15
epoch [2/50] batch [10/23] time 0.207 (0.353) data 0.000 (0.138) loss 2.4671 (2.9878) acc 28.1250 (24.3750) lr 2.0000e-03 eta 0:06:34
epoch [2/50] batch [15/23] time 0.226 (0.310) data 0.000 (0.092) loss 3.1630 (2.8978) acc 12.5000 (27.2917) lr 2.0000e-03 eta 0:05:44
epoch [2/50] batch [20/23] time 0.213 (0.285) data 0.000 (0.069) loss 2.3201 (2.8180) acc 43.7500 (30.1562) lr 2.0000e-03 eta 0:05:15
epoch [3/50] batch [5/23] time 0.214 (0.458) data 0.000 (0.234) loss 1.8716 (2.2259) acc 43.7500 (42.5000) lr 1.9980e-03 eta 0:08:23
epoch [3/50] batch [10/23] time 0.233 (0.340) data 0.000 (0.117) loss 2.7149 (2.3740) acc 28.1250 (40.9375) lr 1.9980e-03 eta 0:06:11
epoch [3/50] batch [15/23] time 0.211 (0.299) data 0.000 (0.078) loss 1.9347 (2.3276) acc 46.8750 (42.5000) lr 1.9980e-03 eta 0:05:25
epoch [3/50] batch [20/23] time 0.204 (0.276) data 0.000 (0.059) loss 3.2557 (2.4316) acc 31.2500 (40.6250) lr 1.9980e-03 eta 0:04:59
epoch [4/50] batch [5/23] time 0.210 (0.502) data 0.000 (0.274) loss 1.8474 (2.3193) acc 59.3750 (43.7500) lr 1.9921e-03 eta 0:08:59
epoch [4/50] batch [10/23] time 0.210 (0.367) data 0.000 (0.137) loss 2.6590 (2.3083) acc 50.0000 (43.1250) lr 1.9921e-03 eta 0:06:33
epoch [4/50] batch [15/23] time 0.210 (0.315) data 0.000 (0.092) loss 1.7244 (2.2727) acc 62.5000 (44.5833) lr 1.9921e-03 eta 0:05:35
epoch [4/50] batch [20/23] time 0.210 (0.289) data 0.000 (0.069) loss 2.2410 (2.3116) acc 46.8750 (45.3125) lr 1.9921e-03 eta 0:05:06
epoch [5/50] batch [5/23] time 0.231 (0.497) data 0.000 (0.259) loss 1.8309 (2.1499) acc 62.5000 (48.7500) lr 1.9823e-03 eta 0:08:43
epoch [5/50] batch [10/23] time 0.213 (0.358) data 0.000 (0.130) loss 1.9126 (2.1425) acc 65.6250 (51.5625) lr 1.9823e-03 eta 0:06:14
epoch [5/50] batch [15/23] time 0.215 (0.309) data 0.000 (0.087) loss 1.6682 (2.1249) acc 50.0000 (50.0000) lr 1.9823e-03 eta 0:05:22
epoch [5/50] batch [20/23] time 0.212 (0.289) data 0.000 (0.065) loss 2.4081 (2.1996) acc 50.0000 (49.0625) lr 1.9823e-03 eta 0:04:59
epoch [6/50] batch [5/23] time 0.214 (0.483) data 0.000 (0.253) loss 1.9723 (2.2519) acc 56.2500 (44.3750) lr 1.9686e-03 eta 0:08:17
epoch [6/50] batch [10/23] time 0.225 (0.352) data 0.000 (0.127) loss 2.3462 (2.2105) acc 40.6250 (46.2500) lr 1.9686e-03 eta 0:06:01
epoch [6/50] batch [15/23] time 0.210 (0.309) data 0.000 (0.085) loss 1.9664 (2.1234) acc 50.0000 (48.3333) lr 1.9686e-03 eta 0:05:15
epoch [6/50] batch [20/23] time 0.208 (0.285) data 0.000 (0.063) loss 2.0433 (2.1201) acc 53.1250 (49.3750) lr 1.9686e-03 eta 0:04:48
epoch [7/50] batch [5/23] time 0.218 (0.488) data 0.000 (0.258) loss 1.6227 (2.1015) acc 59.3750 (51.8750) lr 1.9511e-03 eta 0:08:11
epoch [7/50] batch [10/23] time 0.210 (0.361) data 0.000 (0.129) loss 1.9353 (2.0432) acc 50.0000 (52.8125) lr 1.9511e-03 eta 0:06:02
epoch [7/50] batch [15/23] time 0.210 (0.312) data 0.000 (0.086) loss 1.9573 (2.0553) acc 53.1250 (51.2500) lr 1.9511e-03 eta 0:05:10
epoch [7/50] batch [20/23] time 0.210 (0.286) data 0.000 (0.065) loss 2.4029 (2.1039) acc 46.8750 (50.7812) lr 1.9511e-03 eta 0:04:44
epoch [8/50] batch [5/23] time 0.225 (0.484) data 0.000 (0.246) loss 2.4854 (2.1803) acc 43.7500 (48.7500) lr 1.9298e-03 eta 0:07:55
epoch [8/50] batch [10/23] time 0.213 (0.350) data 0.000 (0.123) loss 2.0696 (2.2312) acc 40.6250 (48.1250) lr 1.9298e-03 eta 0:05:42
epoch [8/50] batch [15/23] time 0.213 (0.304) data 0.000 (0.082) loss 2.0579 (2.0935) acc 53.1250 (51.0417) lr 1.9298e-03 eta 0:04:55
epoch [8/50] batch [20/23] time 0.211 (0.284) data 0.000 (0.062) loss 1.9873 (2.0345) acc 59.3750 (52.5000) lr 1.9298e-03 eta 0:04:35
epoch [9/50] batch [5/23] time 0.212 (0.457) data 0.000 (0.215) loss 1.6323 (1.9202) acc 65.6250 (56.8750) lr 1.9048e-03 eta 0:07:18
epoch [9/50] batch [10/23] time 0.213 (0.335) data 0.000 (0.108) loss 1.7565 (1.9512) acc 59.3750 (55.9375) lr 1.9048e-03 eta 0:05:20
epoch [9/50] batch [15/23] time 0.278 (0.299) data 0.000 (0.072) loss 1.5320 (1.9110) acc 75.0000 (58.3333) lr 1.9048e-03 eta 0:04:43
epoch [9/50] batch [20/23] time 0.210 (0.277) data 0.000 (0.054) loss 2.0772 (1.9129) acc 53.1250 (57.0312) lr 1.9048e-03 eta 0:04:22
epoch [10/50] batch [5/23] time 0.229 (0.472) data 0.000 (0.242) loss 2.2487 (1.8900) acc 53.1250 (58.7500) lr 1.8763e-03 eta 0:07:22
epoch [10/50] batch [10/23] time 0.226 (0.347) data 0.000 (0.121) loss 1.8873 (1.9849) acc 59.3750 (55.0000) lr 1.8763e-03 eta 0:05:23
epoch [10/50] batch [15/23] time 0.215 (0.306) data 0.000 (0.081) loss 1.7635 (1.9052) acc 59.3750 (56.4583) lr 1.8763e-03 eta 0:04:43
epoch [10/50] batch [20/23] time 0.214 (0.283) data 0.000 (0.061) loss 2.1156 (1.9383) acc 56.2500 (56.2500) lr 1.8763e-03 eta 0:04:20
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.529  alpha2: -0.066 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.18 <<<
epoch [11/50] batch [5/23] time 1.210 (1.518) data 0.000 (0.287) loss 1.1486 (1.1553) acc 70.6731 (68.6259) lr 1.8443e-03 eta 0:23:08
epoch [11/50] batch [10/23] time 0.188 (1.245) data 0.000 (0.143) loss 0.9316 (1.1827) acc 76.5625 (65.5071) lr 1.8443e-03 eta 0:18:53
epoch [11/50] batch [15/23] time 0.201 (0.898) data 0.000 (0.096) loss 1.0095 (1.1700) acc 58.6538 (63.5658) lr 1.8443e-03 eta 0:13:32
epoch [11/50] batch [20/23] time 0.200 (0.833) data 0.000 (0.072) loss 1.0693 (1.1420) acc 70.5882 (65.1502) lr 1.8443e-03 eta 0:12:29
>>> alpha1: 0.448  alpha2: -0.037 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.17 <<<
epoch [12/50] batch [5/23] time 0.196 (0.500) data 0.001 (0.290) loss 1.0701 (1.0172) acc 60.5000 (65.1927) lr 1.8090e-03 eta 0:07:26
epoch [12/50] batch [10/23] time 0.193 (0.537) data 0.000 (0.145) loss 1.0841 (1.0169) acc 64.2857 (67.2714) lr 1.8090e-03 eta 0:07:56
epoch [12/50] batch [15/23] time 0.191 (0.423) data 0.000 (0.097) loss 1.0432 (1.0220) acc 69.8980 (67.7608) lr 1.8090e-03 eta 0:06:13
epoch [12/50] batch [20/23] time 0.187 (0.369) data 0.000 (0.073) loss 0.9093 (1.0139) acc 73.4375 (68.5636) lr 1.8090e-03 eta 0:05:23
>>> alpha1: 0.398  alpha2: -0.044 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.18 <<<
epoch [13/50] batch [5/23] time 0.282 (0.602) data 0.000 (0.311) loss 1.0005 (0.9163) acc 76.1111 (72.2409) lr 1.7705e-03 eta 0:08:43
epoch [13/50] batch [10/23] time 0.162 (0.434) data 0.000 (0.156) loss 0.8918 (0.9706) acc 73.1481 (71.5880) lr 1.7705e-03 eta 0:06:14
epoch [13/50] batch [15/23] time 0.141 (0.335) data 0.000 (0.104) loss 1.2064 (0.9451) acc 63.7755 (71.0384) lr 1.7705e-03 eta 0:04:47
epoch [13/50] batch [20/23] time 0.239 (0.292) data 0.000 (0.078) loss 0.9782 (0.9624) acc 58.5000 (69.5697) lr 1.7705e-03 eta 0:04:09
>>> alpha1: 0.375  alpha2: -0.028 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.19 <<<
epoch [14/50] batch [5/23] time 0.178 (0.623) data 0.000 (0.283) loss 0.7886 (0.8001) acc 75.0000 (74.7783) lr 1.7290e-03 eta 0:08:47
epoch [14/50] batch [10/23] time 0.175 (0.403) data 0.000 (0.142) loss 0.8640 (0.7853) acc 73.3696 (76.4443) lr 1.7290e-03 eta 0:05:38
epoch [14/50] batch [15/23] time 0.204 (0.334) data 0.000 (0.095) loss 0.8394 (0.7969) acc 68.0233 (75.8056) lr 1.7290e-03 eta 0:04:39
epoch [14/50] batch [20/23] time 0.193 (0.297) data 0.000 (0.071) loss 0.9164 (0.8479) acc 66.0000 (73.7481) lr 1.7290e-03 eta 0:04:07
>>> alpha1: 0.356  alpha2: -0.027 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.23 <<<
epoch [15/50] batch [5/23] time 0.197 (0.492) data 0.000 (0.287) loss 0.8031 (0.8023) acc 75.0000 (74.4658) lr 1.6845e-03 eta 0:06:44
epoch [15/50] batch [10/23] time 0.212 (0.348) data 0.000 (0.144) loss 0.8603 (0.8230) acc 76.3636 (72.9429) lr 1.6845e-03 eta 0:04:44
epoch [15/50] batch [15/23] time 0.214 (0.303) data 0.000 (0.096) loss 0.8345 (0.8556) acc 73.2143 (71.2508) lr 1.6845e-03 eta 0:04:06
epoch [15/50] batch [20/23] time 0.208 (0.278) data 0.000 (0.072) loss 0.8172 (0.8593) acc 77.3585 (71.3121) lr 1.6845e-03 eta 0:03:44
>>> alpha1: 0.283  alpha2: -0.096 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.23 <<<
epoch [16/50] batch [5/23] time 0.231 (0.441) data 0.000 (0.226) loss 0.5408 (0.6120) acc 83.4821 (79.1241) lr 1.6374e-03 eta 0:05:52
epoch [16/50] batch [10/23] time 0.180 (0.317) data 0.000 (0.113) loss 0.7831 (0.7005) acc 75.0000 (76.7699) lr 1.6374e-03 eta 0:04:12
epoch [16/50] batch [15/23] time 0.193 (0.275) data 0.000 (0.075) loss 0.5608 (0.7116) acc 82.0755 (76.3478) lr 1.6374e-03 eta 0:03:37
epoch [16/50] batch [20/23] time 0.156 (0.243) data 0.000 (0.057) loss 0.9099 (0.7618) acc 69.5455 (74.6449) lr 1.6374e-03 eta 0:03:11
>>> alpha1: 0.255  alpha2: -0.105 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [17/50] batch [5/23] time 0.166 (0.462) data 0.000 (0.307) loss 0.5533 (0.6337) acc 79.5000 (79.9948) lr 1.5878e-03 eta 0:05:58
epoch [17/50] batch [10/23] time 0.183 (0.314) data 0.001 (0.154) loss 0.6501 (0.6749) acc 82.6087 (77.2756) lr 1.5878e-03 eta 0:04:02
epoch [17/50] batch [15/23] time 0.155 (0.264) data 0.001 (0.103) loss 0.6552 (0.7001) acc 71.9388 (75.7063) lr 1.5878e-03 eta 0:03:22
epoch [17/50] batch [20/23] time 0.167 (0.239) data 0.000 (0.077) loss 0.8965 (0.7262) acc 70.7547 (74.7463) lr 1.5878e-03 eta 0:03:01
>>> alpha1: 0.246  alpha2: -0.095 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.21 <<<
epoch [18/50] batch [5/23] time 0.147 (0.444) data 0.000 (0.293) loss 0.9557 (0.7032) acc 66.6667 (76.9896) lr 1.5358e-03 eta 0:05:34
epoch [18/50] batch [10/23] time 0.147 (0.296) data 0.000 (0.147) loss 0.8595 (0.7233) acc 74.4565 (76.6600) lr 1.5358e-03 eta 0:03:41
epoch [18/50] batch [15/23] time 0.142 (0.247) data 0.000 (0.098) loss 0.7715 (0.7050) acc 83.7209 (78.2538) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [20/23] time 0.169 (0.227) data 0.000 (0.074) loss 0.6557 (0.6850) acc 78.8889 (78.4296) lr 1.5358e-03 eta 0:02:47
>>> alpha1: 0.234  alpha2: -0.097 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [19/50] batch [5/23] time 0.296 (0.658) data 0.000 (0.364) loss 0.8381 (0.6843) acc 73.5849 (77.7544) lr 1.4818e-03 eta 0:08:00
epoch [19/50] batch [10/23] time 0.279 (0.481) data 0.000 (0.182) loss 0.8075 (0.6354) acc 69.5000 (78.5195) lr 1.4818e-03 eta 0:05:49
epoch [19/50] batch [15/23] time 0.257 (0.414) data 0.000 (0.122) loss 0.5029 (0.6369) acc 89.2045 (79.3666) lr 1.4818e-03 eta 0:04:58
epoch [19/50] batch [20/23] time 0.273 (0.382) data 0.000 (0.091) loss 0.6590 (0.6446) acc 82.5000 (79.1816) lr 1.4818e-03 eta 0:04:33
>>> alpha1: 0.222  alpha2: -0.097 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [20/50] batch [5/23] time 0.145 (0.634) data 0.000 (0.271) loss 0.7648 (0.5803) acc 67.7083 (78.5814) lr 1.4258e-03 eta 0:07:28
epoch [20/50] batch [10/23] time 0.273 (0.454) data 0.000 (0.136) loss 0.7772 (0.6179) acc 74.0000 (77.3725) lr 1.4258e-03 eta 0:05:19
epoch [20/50] batch [15/23] time 0.282 (0.398) data 0.000 (0.091) loss 0.8079 (0.6640) acc 76.4423 (76.8285) lr 1.4258e-03 eta 0:04:37
epoch [20/50] batch [20/23] time 0.304 (0.374) data 0.000 (0.068) loss 0.6344 (0.6530) acc 75.9091 (77.3775) lr 1.4258e-03 eta 0:04:19
>>> alpha1: 0.213  alpha2: -0.091 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.24 <<<
epoch [21/50] batch [5/23] time 0.180 (0.419) data 0.000 (0.256) loss 0.3956 (0.5528) acc 80.1887 (79.8213) lr 1.3681e-03 eta 0:04:47
epoch [21/50] batch [10/23] time 0.174 (0.287) data 0.001 (0.129) loss 0.6784 (0.6270) acc 77.8846 (77.6565) lr 1.3681e-03 eta 0:03:15
epoch [21/50] batch [15/23] time 0.235 (0.250) data 0.000 (0.086) loss 0.4640 (0.6251) acc 84.2105 (77.9738) lr 1.3681e-03 eta 0:02:48
epoch [21/50] batch [20/23] time 0.195 (0.235) data 0.000 (0.065) loss 0.6778 (0.6273) acc 76.5306 (79.0921) lr 1.3681e-03 eta 0:02:37
>>> alpha1: 0.212  alpha2: -0.088 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [22/50] batch [5/23] time 0.270 (0.508) data 0.000 (0.289) loss 0.3216 (0.5313) acc 89.3519 (82.1780) lr 1.3090e-03 eta 0:05:36
epoch [22/50] batch [10/23] time 0.197 (0.353) data 0.000 (0.146) loss 0.5492 (0.5366) acc 76.4706 (82.4400) lr 1.3090e-03 eta 0:03:52
epoch [22/50] batch [15/23] time 0.192 (0.302) data 0.000 (0.098) loss 0.5899 (0.5426) acc 79.5000 (81.9295) lr 1.3090e-03 eta 0:03:16
epoch [22/50] batch [20/23] time 0.201 (0.276) data 0.000 (0.073) loss 0.7967 (0.5734) acc 65.3846 (80.6971) lr 1.3090e-03 eta 0:02:58
>>> alpha1: 0.201  alpha2: -0.091 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [23/50] batch [5/23] time 0.213 (0.518) data 0.000 (0.307) loss 0.4710 (0.5831) acc 86.3208 (82.4803) lr 1.2487e-03 eta 0:05:30
epoch [23/50] batch [10/23] time 0.189 (0.367) data 0.000 (0.154) loss 0.7421 (0.5620) acc 70.4082 (80.8669) lr 1.2487e-03 eta 0:03:52
epoch [23/50] batch [15/23] time 0.208 (0.311) data 0.000 (0.102) loss 0.7827 (0.5742) acc 68.5185 (80.0222) lr 1.2487e-03 eta 0:03:15
epoch [23/50] batch [20/23] time 0.202 (0.285) data 0.000 (0.077) loss 0.5506 (0.5589) acc 82.5472 (81.0760) lr 1.2487e-03 eta 0:02:57
>>> alpha1: 0.196  alpha2: -0.091 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [24/50] batch [5/23] time 0.153 (0.469) data 0.000 (0.258) loss 0.6723 (0.5555) acc 77.4510 (81.3550) lr 1.1874e-03 eta 0:04:48
epoch [24/50] batch [10/23] time 0.164 (0.314) data 0.000 (0.129) loss 0.4540 (0.5689) acc 79.3269 (79.6365) lr 1.1874e-03 eta 0:03:11
epoch [24/50] batch [15/23] time 0.156 (0.260) data 0.000 (0.086) loss 0.6013 (0.5596) acc 79.7170 (81.0121) lr 1.1874e-03 eta 0:02:37
epoch [24/50] batch [20/23] time 0.293 (0.260) data 0.000 (0.065) loss 0.5123 (0.5697) acc 85.5263 (81.3314) lr 1.1874e-03 eta 0:02:36
>>> alpha1: 0.189  alpha2: -0.090 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [25/50] batch [5/23] time 0.211 (0.466) data 0.000 (0.260) loss 0.4752 (0.5415) acc 88.5000 (83.6399) lr 1.1253e-03 eta 0:04:36
epoch [25/50] batch [10/23] time 0.198 (0.336) data 0.000 (0.130) loss 0.4874 (0.5394) acc 90.0000 (83.8958) lr 1.1253e-03 eta 0:03:17
epoch [25/50] batch [15/23] time 0.211 (0.293) data 0.000 (0.087) loss 0.5594 (0.5107) acc 82.8704 (83.3327) lr 1.1253e-03 eta 0:02:50
epoch [25/50] batch [20/23] time 0.194 (0.273) data 0.000 (0.065) loss 0.5576 (0.5248) acc 84.4340 (83.2180) lr 1.1253e-03 eta 0:02:37
>>> alpha1: 0.186  alpha2: -0.092 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [26/50] batch [5/23] time 0.214 (0.525) data 0.000 (0.299) loss 0.3281 (0.4997) acc 88.8889 (81.1914) lr 1.0628e-03 eta 0:04:59
epoch [26/50] batch [10/23] time 0.216 (0.368) data 0.000 (0.151) loss 0.4956 (0.5183) acc 84.2105 (83.7063) lr 1.0628e-03 eta 0:03:27
epoch [26/50] batch [15/23] time 0.206 (0.312) data 0.000 (0.101) loss 0.5842 (0.5330) acc 81.6038 (82.2429) lr 1.0628e-03 eta 0:02:54
epoch [26/50] batch [20/23] time 0.198 (0.287) data 0.000 (0.076) loss 0.6690 (0.5455) acc 71.1538 (81.1882) lr 1.0628e-03 eta 0:02:39
>>> alpha1: 0.175  alpha2: -0.096 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [27/50] batch [5/23] time 0.219 (0.442) data 0.001 (0.230) loss 0.7652 (0.5028) acc 71.2264 (84.1134) lr 1.0000e-03 eta 0:04:01
epoch [27/50] batch [10/23] time 0.208 (0.325) data 0.000 (0.115) loss 0.4349 (0.4871) acc 76.8519 (84.2789) lr 1.0000e-03 eta 0:02:56
epoch [27/50] batch [15/23] time 0.236 (0.288) data 0.000 (0.077) loss 0.6165 (0.4787) acc 81.0000 (84.5360) lr 1.0000e-03 eta 0:02:34
epoch [27/50] batch [20/23] time 0.196 (0.266) data 0.000 (0.058) loss 0.4337 (0.4869) acc 90.5000 (83.9699) lr 1.0000e-03 eta 0:02:21
>>> alpha1: 0.172  alpha2: -0.094 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [28/50] batch [5/23] time 0.272 (0.588) data 0.001 (0.318) loss 0.4170 (0.4703) acc 88.8393 (84.2188) lr 9.3721e-04 eta 0:05:07
epoch [28/50] batch [10/23] time 0.285 (0.435) data 0.000 (0.159) loss 0.3301 (0.5000) acc 92.1296 (84.5356) lr 9.3721e-04 eta 0:03:45
epoch [28/50] batch [15/23] time 0.307 (0.384) data 0.000 (0.106) loss 0.3856 (0.4742) acc 90.4546 (85.0509) lr 9.3721e-04 eta 0:03:17
epoch [28/50] batch [20/23] time 0.215 (0.349) data 0.000 (0.080) loss 0.5917 (0.4878) acc 84.0000 (84.7585) lr 9.3721e-04 eta 0:02:57
>>> alpha1: 0.166  alpha2: -0.088 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [29/50] batch [5/23] time 0.196 (0.475) data 0.000 (0.263) loss 0.5976 (0.5182) acc 79.4118 (82.0376) lr 8.7467e-04 eta 0:03:57
epoch [29/50] batch [10/23] time 0.193 (0.340) data 0.000 (0.132) loss 0.4599 (0.4553) acc 85.0000 (84.3821) lr 8.7467e-04 eta 0:02:48
epoch [29/50] batch [15/23] time 0.211 (0.300) data 0.000 (0.088) loss 0.6010 (0.4707) acc 81.3636 (84.4806) lr 8.7467e-04 eta 0:02:27
epoch [29/50] batch [20/23] time 0.201 (0.275) data 0.000 (0.066) loss 0.5912 (0.4715) acc 85.1852 (84.3344) lr 8.7467e-04 eta 0:02:13
>>> alpha1: 0.161  alpha2: -0.086 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.24 <<<
epoch [30/50] batch [5/23] time 0.200 (0.475) data 0.000 (0.268) loss 0.4335 (0.5088) acc 77.8302 (84.4307) lr 8.1262e-04 eta 0:03:46
epoch [30/50] batch [10/23] time 0.187 (0.336) data 0.000 (0.134) loss 0.5556 (0.4795) acc 83.3333 (85.1847) lr 8.1262e-04 eta 0:02:38
epoch [30/50] batch [15/23] time 0.216 (0.299) data 0.000 (0.089) loss 0.4507 (0.4829) acc 87.9310 (84.4531) lr 8.1262e-04 eta 0:02:19
epoch [30/50] batch [20/23] time 0.211 (0.275) data 0.000 (0.067) loss 0.3852 (0.4707) acc 83.6364 (83.6714) lr 8.1262e-04 eta 0:02:07
>>> alpha1: 0.158  alpha2: -0.081 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [31/50] batch [5/23] time 0.194 (0.495) data 0.000 (0.283) loss 0.4068 (0.4161) acc 90.0000 (87.3767) lr 7.5131e-04 eta 0:03:45
epoch [31/50] batch [10/23] time 0.202 (0.352) data 0.000 (0.142) loss 0.5851 (0.4280) acc 84.6154 (86.6739) lr 7.5131e-04 eta 0:02:38
epoch [31/50] batch [15/23] time 0.207 (0.302) data 0.000 (0.095) loss 0.3587 (0.4482) acc 90.5660 (86.2064) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [20/23] time 0.192 (0.276) data 0.000 (0.071) loss 0.3413 (0.4464) acc 84.1837 (85.9837) lr 7.5131e-04 eta 0:02:01
>>> alpha1: 0.156  alpha2: -0.079 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [32/50] batch [5/23] time 0.176 (0.539) data 0.000 (0.344) loss 0.5722 (0.4736) acc 82.6923 (85.9019) lr 6.9098e-04 eta 0:03:52
epoch [32/50] batch [10/23] time 0.201 (0.370) data 0.000 (0.172) loss 0.3446 (0.4531) acc 85.9091 (86.0302) lr 6.9098e-04 eta 0:02:38
epoch [32/50] batch [15/23] time 0.198 (0.312) data 0.000 (0.115) loss 0.3016 (0.4283) acc 93.3962 (87.1280) lr 6.9098e-04 eta 0:02:11
epoch [32/50] batch [20/23] time 0.203 (0.282) data 0.000 (0.086) loss 0.4054 (0.4605) acc 83.1818 (85.6791) lr 6.9098e-04 eta 0:01:57
>>> alpha1: 0.152  alpha2: -0.084 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [33/50] batch [5/23] time 0.202 (0.465) data 0.000 (0.252) loss 0.4310 (0.4309) acc 86.0577 (86.9878) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [10/23] time 0.193 (0.340) data 0.001 (0.126) loss 0.3455 (0.3925) acc 91.0000 (88.6262) lr 6.3188e-04 eta 0:02:17
epoch [33/50] batch [15/23] time 0.212 (0.293) data 0.000 (0.084) loss 0.2178 (0.3901) acc 91.9643 (87.2031) lr 6.3188e-04 eta 0:01:56
epoch [33/50] batch [20/23] time 0.197 (0.269) data 0.000 (0.063) loss 0.4190 (0.4085) acc 84.6154 (86.6304) lr 6.3188e-04 eta 0:01:46
>>> alpha1: 0.148  alpha2: -0.081 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [34/50] batch [5/23] time 0.205 (0.458) data 0.000 (0.247) loss 0.2818 (0.4145) acc 95.3704 (86.9357) lr 5.7422e-04 eta 0:02:56
epoch [34/50] batch [10/23] time 0.187 (0.338) data 0.000 (0.125) loss 0.4394 (0.4068) acc 88.0208 (87.1373) lr 5.7422e-04 eta 0:02:08
epoch [34/50] batch [15/23] time 0.200 (0.294) data 0.000 (0.083) loss 0.4632 (0.4183) acc 83.1731 (86.7488) lr 5.7422e-04 eta 0:01:50
epoch [34/50] batch [20/23] time 0.195 (0.271) data 0.000 (0.063) loss 0.4414 (0.4316) acc 82.3529 (86.1976) lr 5.7422e-04 eta 0:01:40
>>> alpha1: 0.146  alpha2: -0.095 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [35/50] batch [5/23] time 0.303 (0.585) data 0.000 (0.300) loss 0.3049 (0.3691) acc 93.1818 (88.9754) lr 5.1825e-04 eta 0:03:32
epoch [35/50] batch [10/23] time 0.301 (0.443) data 0.000 (0.150) loss 0.4826 (0.3715) acc 87.9808 (89.2117) lr 5.1825e-04 eta 0:02:38
epoch [35/50] batch [15/23] time 0.272 (0.485) data 0.000 (0.100) loss 0.5682 (0.3947) acc 80.2632 (87.7940) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [20/23] time 0.297 (0.431) data 0.001 (0.075) loss 0.3560 (0.4017) acc 93.7500 (87.4187) lr 5.1825e-04 eta 0:02:30
>>> alpha1: 0.144  alpha2: -0.091 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.24 <<<
epoch [36/50] batch [5/23] time 0.214 (0.478) data 0.000 (0.269) loss 0.4388 (0.3758) acc 88.8889 (86.7896) lr 4.6417e-04 eta 0:02:42
epoch [36/50] batch [10/23] time 0.197 (0.348) data 0.000 (0.135) loss 0.5086 (0.4226) acc 81.3726 (85.8806) lr 4.6417e-04 eta 0:01:56
epoch [36/50] batch [15/23] time 0.224 (0.303) data 0.000 (0.090) loss 0.3008 (0.4112) acc 89.0909 (86.4448) lr 4.6417e-04 eta 0:01:39
epoch [36/50] batch [20/23] time 0.194 (0.277) data 0.000 (0.068) loss 0.4445 (0.4157) acc 87.5000 (86.2484) lr 4.6417e-04 eta 0:01:30
>>> alpha1: 0.143  alpha2: -0.090 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [37/50] batch [5/23] time 0.203 (0.439) data 0.001 (0.226) loss 0.3245 (0.4058) acc 93.8679 (88.5080) lr 4.1221e-04 eta 0:02:19
epoch [37/50] batch [10/23] time 0.270 (0.325) data 0.000 (0.113) loss 0.2582 (0.3887) acc 94.2308 (87.3421) lr 4.1221e-04 eta 0:01:41
epoch [37/50] batch [15/23] time 0.208 (0.285) data 0.000 (0.076) loss 0.5696 (0.3981) acc 84.2593 (87.6342) lr 4.1221e-04 eta 0:01:27
epoch [37/50] batch [20/23] time 0.199 (0.264) data 0.000 (0.057) loss 0.2617 (0.3912) acc 90.3846 (87.8977) lr 4.1221e-04 eta 0:01:19
>>> alpha1: 0.143  alpha2: -0.091 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [38/50] batch [5/23] time 0.199 (0.492) data 0.000 (0.289) loss 0.4270 (0.5070) acc 88.9423 (86.7642) lr 3.6258e-04 eta 0:02:24
epoch [38/50] batch [10/23] time 0.207 (0.347) data 0.000 (0.145) loss 0.3496 (0.4540) acc 88.6792 (86.8765) lr 3.6258e-04 eta 0:01:40
epoch [38/50] batch [15/23] time 0.191 (0.303) data 0.000 (0.097) loss 0.4275 (0.4211) acc 94.3878 (88.3545) lr 3.6258e-04 eta 0:01:25
epoch [38/50] batch [20/23] time 0.198 (0.277) data 0.000 (0.073) loss 0.3989 (0.4074) acc 87.0192 (87.9852) lr 3.6258e-04 eta 0:01:17
>>> alpha1: 0.143  alpha2: -0.087 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [39/50] batch [5/23] time 0.287 (0.454) data 0.000 (0.259) loss 0.4130 (0.3687) acc 92.7885 (91.4264) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [10/23] time 0.188 (0.323) data 0.000 (0.130) loss 0.3408 (0.3812) acc 90.1042 (90.6583) lr 3.1545e-04 eta 0:01:25
epoch [39/50] batch [15/23] time 0.208 (0.283) data 0.000 (0.086) loss 0.4384 (0.4024) acc 85.9091 (89.6484) lr 3.1545e-04 eta 0:01:13
epoch [39/50] batch [20/23] time 0.211 (0.262) data 0.000 (0.065) loss 0.3183 (0.3886) acc 92.5439 (89.2945) lr 3.1545e-04 eta 0:01:06
>>> alpha1: 0.142  alpha2: -0.086 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [40/50] batch [5/23] time 0.194 (0.505) data 0.000 (0.307) loss 0.5320 (0.3959) acc 85.5000 (84.8166) lr 2.7103e-04 eta 0:02:05
epoch [40/50] batch [10/23] time 0.204 (0.361) data 0.000 (0.154) loss 0.2630 (0.3861) acc 96.2963 (87.0743) lr 2.7103e-04 eta 0:01:27
epoch [40/50] batch [15/23] time 0.192 (0.306) data 0.000 (0.103) loss 0.2663 (0.3837) acc 88.5000 (88.0297) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [20/23] time 0.204 (0.280) data 0.000 (0.077) loss 0.3348 (0.3842) acc 91.6667 (88.3270) lr 2.7103e-04 eta 0:01:05
>>> alpha1: 0.143  alpha2: -0.079 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [41/50] batch [5/23] time 0.204 (0.469) data 0.000 (0.248) loss 0.2260 (0.3111) acc 89.3519 (88.9303) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [10/23] time 0.207 (0.335) data 0.000 (0.124) loss 0.3093 (0.3565) acc 84.7222 (87.6220) lr 2.2949e-04 eta 0:01:13
epoch [41/50] batch [15/23] time 0.211 (0.290) data 0.000 (0.083) loss 0.5711 (0.3770) acc 81.8182 (87.8466) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [20/23] time 0.255 (0.271) data 0.000 (0.062) loss 0.3604 (0.3872) acc 91.8182 (88.0025) lr 2.2949e-04 eta 0:00:56
>>> alpha1: 0.141  alpha2: -0.078 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [42/50] batch [5/23] time 0.299 (0.575) data 0.000 (0.259) loss 0.4423 (0.3905) acc 87.9808 (89.6834) lr 1.9098e-04 eta 0:01:56
epoch [42/50] batch [10/23] time 0.323 (0.439) data 0.000 (0.129) loss 0.4354 (0.3745) acc 89.9123 (89.1691) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [15/23] time 0.288 (0.392) data 0.000 (0.086) loss 0.3068 (0.3704) acc 90.5660 (89.3234) lr 1.9098e-04 eta 0:01:15
epoch [42/50] batch [20/23] time 0.159 (0.405) data 0.000 (0.065) loss 0.3330 (0.3773) acc 88.2353 (88.9306) lr 1.9098e-04 eta 0:01:15
>>> alpha1: 0.141  alpha2: -0.072 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [43/50] batch [5/23] time 0.224 (0.469) data 0.000 (0.268) loss 0.2901 (0.3789) acc 92.3077 (89.3212) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [10/23] time 0.207 (0.341) data 0.000 (0.134) loss 0.3654 (0.3897) acc 87.7358 (87.6186) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [15/23] time 0.198 (0.298) data 0.000 (0.089) loss 0.3673 (0.3782) acc 88.7255 (87.6643) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [20/23] time 0.197 (0.273) data 0.000 (0.067) loss 0.4990 (0.3726) acc 81.8627 (88.0491) lr 1.5567e-04 eta 0:00:44
>>> alpha1: 0.139  alpha2: -0.082 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [44/50] batch [5/23] time 0.230 (0.419) data 0.000 (0.215) loss 0.2544 (0.5629) acc 90.7895 (88.6309) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [10/23] time 0.203 (0.310) data 0.000 (0.108) loss 0.3665 (0.4455) acc 89.4231 (89.3303) lr 1.2369e-04 eta 0:00:46
epoch [44/50] batch [15/23] time 0.293 (0.279) data 0.000 (0.072) loss 0.3342 (0.4139) acc 92.3729 (89.2422) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [20/23] time 0.207 (0.258) data 0.000 (0.054) loss 0.3291 (0.3953) acc 91.6667 (89.0764) lr 1.2369e-04 eta 0:00:36
>>> alpha1: 0.138  alpha2: -0.083 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.21 <<<
epoch [45/50] batch [5/23] time 0.197 (0.452) data 0.000 (0.241) loss 0.3308 (0.3006) acc 87.2549 (91.2906) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [10/23] time 0.202 (0.323) data 0.000 (0.121) loss 0.4495 (0.3465) acc 89.4231 (89.6407) lr 9.5173e-05 eta 0:00:41
epoch [45/50] batch [15/23] time 0.192 (0.278) data 0.000 (0.081) loss 0.3757 (0.3487) acc 90.5000 (89.0096) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [20/23] time 0.213 (0.261) data 0.000 (0.061) loss 0.3066 (0.3615) acc 87.2727 (88.8279) lr 9.5173e-05 eta 0:00:30
>>> alpha1: 0.136  alpha2: -0.083 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [46/50] batch [5/23] time 0.172 (0.476) data 0.000 (0.290) loss 0.3955 (0.4192) acc 87.5000 (86.0698) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [10/23] time 0.200 (0.341) data 0.000 (0.145) loss 0.2135 (0.3881) acc 94.8113 (87.0261) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [15/23] time 0.192 (0.293) data 0.000 (0.097) loss 0.4318 (0.3834) acc 88.7255 (87.8378) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [20/23] time 0.208 (0.269) data 0.000 (0.073) loss 0.2337 (0.3691) acc 93.3962 (88.5433) lr 7.0224e-05 eta 0:00:25
>>> alpha1: 0.134  alpha2: -0.081 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.20 <<<
epoch [47/50] batch [5/23] time 0.198 (0.409) data 0.013 (0.214) loss 0.3506 (0.3481) acc 88.2979 (88.8875) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [10/23] time 0.186 (0.301) data 0.000 (0.107) loss 0.3089 (0.3633) acc 90.6250 (88.5554) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [15/23] time 0.184 (0.265) data 0.000 (0.072) loss 0.3989 (0.3730) acc 86.7021 (88.5106) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [20/23] time 0.194 (0.246) data 0.000 (0.054) loss 0.2318 (0.3481) acc 95.5882 (89.4734) lr 4.8943e-05 eta 0:00:17
>>> alpha1: 0.134  alpha2: -0.083 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [48/50] batch [5/23] time 0.209 (0.440) data 0.000 (0.230) loss 0.4598 (0.3872) acc 85.2941 (88.8253) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.193 (0.323) data 0.000 (0.115) loss 0.3857 (0.3827) acc 88.7755 (88.1250) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [15/23] time 0.198 (0.286) data 0.000 (0.077) loss 0.3361 (0.3702) acc 88.9423 (88.3778) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.207 (0.264) data 0.000 (0.058) loss 0.2949 (0.3654) acc 88.4615 (88.5635) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.133  alpha2: -0.080 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [49/50] batch [5/23] time 0.171 (0.426) data 0.000 (0.256) loss 0.5272 (0.3880) acc 88.2075 (88.8385) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.152 (0.291) data 0.000 (0.128) loss 0.3265 (0.3713) acc 85.4167 (87.8045) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.294 (0.277) data 0.000 (0.085) loss 0.4087 (0.3666) acc 91.5000 (89.0393) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.289 (0.283) data 0.000 (0.064) loss 0.3000 (0.3581) acc 88.5000 (89.0480) lr 1.7713e-05 eta 0:00:07
>>> alpha1: 0.132  alpha2: -0.075 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [50/50] batch [5/23] time 0.203 (0.447) data 0.000 (0.240) loss 0.3798 (0.3568) acc 88.2075 (88.4713) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.213 (0.326) data 0.000 (0.120) loss 0.2582 (0.3315) acc 91.5179 (90.0213) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.212 (0.286) data 0.000 (0.080) loss 0.5104 (0.3544) acc 87.9808 (89.8493) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.207 (0.264) data 0.000 (0.060) loss 0.2245 (0.3548) acc 88.1818 (89.9873) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.12, 0.12, 0.12, 0.13, 0.13, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.14, 0.15, 0.14, 0.15, 0.15, 0.15, 0.15, 0.14, 0.15, 0.14, 0.15, 0.15, 0.15, 0.14, 0.14, 0.15, 0.14, 0.14, 0.14, 0.14, 0.15, 0.14, 0.15, 0.14, 0.15]
* matched noise rate: [0.08, 0.08, 0.07, 0.07, 0.07, 0.08, 0.08, 0.08, 0.09, 0.09, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.1, 0.09, 0.09, 0.08, 0.09, 0.08, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.08, 0.08, 0.08, 0.08, 0.09, 0.09, 0.09]
* unmatched noise rate: [0.18, 0.17, 0.18, 0.19, 0.23, 0.23, 0.24, 0.21, 0.24, 0.25, 0.24, 0.24, 0.24, 0.26, 0.24, 0.25, 0.26, 0.25, 0.25, 0.24, 0.24, 0.24, 0.25, 0.25, 0.25, 0.24, 0.24, 0.23, 0.24, 0.23, 0.24, 0.24, 0.23, 0.24, 0.21, 0.25, 0.2, 0.25, 0.23, 0.25]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:42,  2.66s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.31it/s] 29%|██▉       | 5/17 [00:03<00:05,  2.38it/s] 41%|████      | 7/17 [00:03<00:02,  3.53it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.65it/s] 59%|█████▉    | 10/17 [00:03<00:01,  4.99it/s] 71%|███████   | 12/17 [00:03<00:00,  6.22it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.25it/s] 94%|█████████▍| 16/17 [00:04<00:00,  8.13it/s]100%|██████████| 17/17 [00:04<00:00,  4.46it/s]100%|██████████| 17/17 [00:04<00:00,  3.42it/s]
=> result
* total: 1,692
* correct: 1,045
* accuracy: 61.8%
* error: 38.2%
* macro_f1: 61.1%
=> per-class result
* class: 0 (banded)	total: 36	correct: 19	acc: 52.8%
* class: 1 (blotchy)	total: 36	correct: 7	acc: 19.4%
* class: 2 (braided)	total: 36	correct: 17	acc: 47.2%
* class: 3 (bubbly)	total: 36	correct: 33	acc: 91.7%
* class: 4 (bumpy)	total: 36	correct: 6	acc: 16.7%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 29	acc: 80.6%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 17	acc: 47.2%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 5	acc: 13.9%
* class: 11 (fibrous)	total: 36	correct: 26	acc: 72.2%
* class: 12 (flecked)	total: 36	correct: 9	acc: 25.0%
* class: 13 (freckled)	total: 36	correct: 30	acc: 83.3%
* class: 14 (frilly)	total: 36	correct: 22	acc: 61.1%
* class: 15 (gauzy)	total: 36	correct: 23	acc: 63.9%
* class: 16 (grid)	total: 36	correct: 17	acc: 47.2%
* class: 17 (grooved)	total: 36	correct: 20	acc: 55.6%
* class: 18 (honeycombed)	total: 36	correct: 24	acc: 66.7%
* class: 19 (interlaced)	total: 36	correct: 24	acc: 66.7%
* class: 20 (knitted)	total: 36	correct: 29	acc: 80.6%
* class: 21 (lacelike)	total: 36	correct: 35	acc: 97.2%
* class: 22 (lined)	total: 36	correct: 18	acc: 50.0%
* class: 23 (marbled)	total: 36	correct: 22	acc: 61.1%
* class: 24 (matted)	total: 36	correct: 26	acc: 72.2%
* class: 25 (meshed)	total: 36	correct: 15	acc: 41.7%
* class: 26 (paisley)	total: 36	correct: 33	acc: 91.7%
* class: 27 (perforated)	total: 36	correct: 25	acc: 69.4%
* class: 28 (pitted)	total: 36	correct: 12	acc: 33.3%
* class: 29 (pleated)	total: 36	correct: 22	acc: 61.1%
* class: 30 (polka-dotted)	total: 36	correct: 23	acc: 63.9%
* class: 31 (porous)	total: 36	correct: 23	acc: 63.9%
* class: 32 (potholed)	total: 36	correct: 32	acc: 88.9%
* class: 33 (scaly)	total: 36	correct: 26	acc: 72.2%
* class: 34 (smeared)	total: 36	correct: 18	acc: 50.0%
* class: 35 (spiralled)	total: 36	correct: 18	acc: 50.0%
* class: 36 (sprinkled)	total: 36	correct: 15	acc: 41.7%
* class: 37 (stained)	total: 36	correct: 9	acc: 25.0%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 25	acc: 69.4%
* class: 41 (swirly)	total: 36	correct: 24	acc: 66.7%
* class: 42 (veined)	total: 36	correct: 19	acc: 52.8%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 24	acc: 66.7%
* class: 45 (wrinkled)	total: 36	correct: 22	acc: 61.1%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 61.8%
Elapsed: 0:16:39
Run this job and save the output to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_2-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.259 (1.004) data 0.000 (0.264) loss 3.3691 (3.4587) acc 37.5000 (23.1250) lr 1.0000e-05 eta 0:19:09
epoch [1/50] batch [10/23] time 0.208 (0.608) data 0.000 (0.132) loss 3.2891 (3.4450) acc 25.0000 (22.1875) lr 1.0000e-05 eta 0:11:33
epoch [1/50] batch [15/23] time 0.199 (0.472) data 0.000 (0.088) loss 3.4690 (3.4326) acc 21.8750 (22.5000) lr 1.0000e-05 eta 0:08:56
epoch [1/50] batch [20/23] time 0.202 (0.404) data 0.000 (0.066) loss 3.4842 (3.4347) acc 15.6250 (21.5625) lr 1.0000e-05 eta 0:07:36
epoch [2/50] batch [5/23] time 0.216 (0.489) data 0.000 (0.255) loss 2.6779 (3.0493) acc 34.3750 (26.8750) lr 2.0000e-03 eta 0:09:08
epoch [2/50] batch [10/23] time 0.207 (0.350) data 0.000 (0.128) loss 2.8186 (2.9287) acc 25.0000 (28.4375) lr 2.0000e-03 eta 0:06:30
epoch [2/50] batch [15/23] time 0.205 (0.302) data 0.000 (0.085) loss 3.1403 (2.8497) acc 25.0000 (30.8333) lr 2.0000e-03 eta 0:05:35
epoch [2/50] batch [20/23] time 0.205 (0.281) data 0.000 (0.064) loss 2.2114 (2.7660) acc 40.6250 (31.8750) lr 2.0000e-03 eta 0:05:10
epoch [3/50] batch [5/23] time 0.213 (0.513) data 0.000 (0.282) loss 2.4595 (2.3839) acc 37.5000 (36.2500) lr 1.9980e-03 eta 0:09:23
epoch [3/50] batch [10/23] time 0.206 (0.364) data 0.000 (0.141) loss 1.6950 (2.2370) acc 68.7500 (44.6875) lr 1.9980e-03 eta 0:06:38
epoch [3/50] batch [15/23] time 0.203 (0.316) data 0.000 (0.094) loss 2.4127 (2.1979) acc 40.6250 (45.6250) lr 1.9980e-03 eta 0:05:44
epoch [3/50] batch [20/23] time 0.210 (0.288) data 0.000 (0.071) loss 3.1794 (2.3618) acc 28.1250 (41.5625) lr 1.9980e-03 eta 0:05:12
epoch [4/50] batch [5/23] time 0.200 (0.448) data 0.000 (0.256) loss 2.6610 (2.2112) acc 34.3750 (48.1250) lr 1.9921e-03 eta 0:08:01
epoch [4/50] batch [10/23] time 0.302 (0.362) data 0.000 (0.128) loss 2.1143 (2.2433) acc 56.2500 (46.8750) lr 1.9921e-03 eta 0:06:27
epoch [4/50] batch [15/23] time 0.285 (0.325) data 0.000 (0.085) loss 2.5451 (2.1800) acc 25.0000 (46.0417) lr 1.9921e-03 eta 0:05:46
epoch [4/50] batch [20/23] time 0.229 (0.309) data 0.000 (0.064) loss 1.9941 (2.1820) acc 56.2500 (46.2500) lr 1.9921e-03 eta 0:05:28
epoch [5/50] batch [5/23] time 0.289 (0.573) data 0.000 (0.296) loss 1.5872 (1.7659) acc 56.2500 (58.7500) lr 1.9823e-03 eta 0:10:03
epoch [5/50] batch [10/23] time 0.321 (0.427) data 0.000 (0.148) loss 1.8906 (1.9058) acc 68.7500 (56.5625) lr 1.9823e-03 eta 0:07:27
epoch [5/50] batch [15/23] time 0.209 (0.384) data 0.000 (0.099) loss 2.0451 (1.9699) acc 53.1250 (54.1667) lr 1.9823e-03 eta 0:06:40
epoch [5/50] batch [20/23] time 0.153 (0.326) data 0.000 (0.074) loss 1.8954 (2.0658) acc 53.1250 (51.5625) lr 1.9823e-03 eta 0:05:38
epoch [6/50] batch [5/23] time 0.271 (0.582) data 0.000 (0.278) loss 2.3756 (2.2907) acc 43.7500 (47.5000) lr 1.9686e-03 eta 0:09:59
epoch [6/50] batch [10/23] time 0.273 (0.432) data 0.000 (0.139) loss 1.8312 (2.1055) acc 59.3750 (51.2500) lr 1.9686e-03 eta 0:07:22
epoch [6/50] batch [15/23] time 0.151 (0.341) data 0.000 (0.093) loss 1.4021 (1.9913) acc 68.7500 (53.3333) lr 1.9686e-03 eta 0:05:48
epoch [6/50] batch [20/23] time 0.164 (0.294) data 0.000 (0.070) loss 1.5536 (1.9502) acc 68.7500 (54.6875) lr 1.9686e-03 eta 0:04:58
epoch [7/50] batch [5/23] time 0.200 (0.484) data 0.000 (0.274) loss 1.9973 (1.7100) acc 68.7500 (63.1250) lr 1.9511e-03 eta 0:08:07
epoch [7/50] batch [10/23] time 0.203 (0.353) data 0.000 (0.137) loss 1.5616 (1.8064) acc 50.0000 (58.7500) lr 1.9511e-03 eta 0:05:54
epoch [7/50] batch [15/23] time 0.208 (0.305) data 0.000 (0.091) loss 2.2812 (1.8279) acc 53.1250 (58.3333) lr 1.9511e-03 eta 0:05:03
epoch [7/50] batch [20/23] time 0.209 (0.280) data 0.000 (0.069) loss 1.4892 (1.8480) acc 50.0000 (56.7188) lr 1.9511e-03 eta 0:04:38
epoch [8/50] batch [5/23] time 0.228 (0.539) data 0.000 (0.299) loss 1.6890 (1.7794) acc 59.3750 (58.7500) lr 1.9298e-03 eta 0:08:50
epoch [8/50] batch [10/23] time 0.208 (0.375) data 0.000 (0.150) loss 1.6547 (1.7148) acc 68.7500 (62.1875) lr 1.9298e-03 eta 0:06:06
epoch [8/50] batch [15/23] time 0.210 (0.319) data 0.000 (0.100) loss 2.6561 (1.7664) acc 46.8750 (60.6250) lr 1.9298e-03 eta 0:05:11
epoch [8/50] batch [20/23] time 0.211 (0.295) data 0.000 (0.075) loss 1.6629 (1.7782) acc 59.3750 (60.4688) lr 1.9298e-03 eta 0:04:46
epoch [9/50] batch [5/23] time 0.207 (0.494) data 0.000 (0.279) loss 1.7704 (1.7655) acc 59.3750 (61.2500) lr 1.9048e-03 eta 0:07:54
epoch [9/50] batch [10/23] time 0.208 (0.357) data 0.000 (0.140) loss 1.7678 (1.8600) acc 53.1250 (58.4375) lr 1.9048e-03 eta 0:05:41
epoch [9/50] batch [15/23] time 0.233 (0.312) data 0.000 (0.093) loss 1.4322 (1.8003) acc 62.5000 (57.9167) lr 1.9048e-03 eta 0:04:56
epoch [9/50] batch [20/23] time 0.206 (0.285) data 0.000 (0.070) loss 1.4847 (1.7615) acc 62.5000 (60.1562) lr 1.9048e-03 eta 0:04:29
epoch [10/50] batch [5/23] time 0.222 (0.499) data 0.000 (0.266) loss 1.9169 (1.9741) acc 56.2500 (58.1250) lr 1.8763e-03 eta 0:07:47
epoch [10/50] batch [10/23] time 0.274 (0.363) data 0.000 (0.133) loss 1.1150 (1.7009) acc 75.0000 (62.1875) lr 1.8763e-03 eta 0:05:38
epoch [10/50] batch [15/23] time 0.207 (0.312) data 0.000 (0.089) loss 1.7507 (1.7028) acc 65.6250 (62.9167) lr 1.8763e-03 eta 0:04:49
epoch [10/50] batch [20/23] time 0.214 (0.286) data 0.000 (0.067) loss 1.8437 (1.6704) acc 59.3750 (63.2812) lr 1.8763e-03 eta 0:04:24
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.459  alpha2: -0.047 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.18 <<<
epoch [11/50] batch [5/23] time 0.192 (1.316) data 0.000 (0.269) loss 0.9504 (0.9442) acc 71.4286 (77.0910) lr 1.8443e-03 eta 0:20:04
epoch [11/50] batch [10/23] time 0.198 (1.019) data 0.000 (0.135) loss 0.7891 (0.9742) acc 82.6923 (75.8384) lr 1.8443e-03 eta 0:15:27
epoch [11/50] batch [15/23] time 0.191 (0.935) data 0.000 (0.090) loss 1.0773 (0.9787) acc 63.7755 (74.9575) lr 1.8443e-03 eta 0:14:06
epoch [11/50] batch [20/23] time 0.185 (0.749) data 0.000 (0.067) loss 1.1048 (0.9991) acc 70.7447 (73.6329) lr 1.8443e-03 eta 0:11:13
>>> alpha1: 0.390  alpha2: -0.076 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.21 <<<
epoch [12/50] batch [5/23] time 0.191 (0.467) data 0.001 (0.262) loss 0.7919 (0.8371) acc 82.8431 (77.4802) lr 1.8090e-03 eta 0:06:56
epoch [12/50] batch [10/23] time 0.189 (0.438) data 0.000 (0.131) loss 0.7770 (0.8181) acc 78.5714 (77.9364) lr 1.8090e-03 eta 0:06:28
epoch [12/50] batch [15/23] time 0.193 (0.360) data 0.000 (0.087) loss 0.8172 (0.8600) acc 71.0000 (74.5048) lr 1.8090e-03 eta 0:05:17
epoch [12/50] batch [20/23] time 0.197 (0.315) data 0.000 (0.066) loss 0.7686 (0.8604) acc 79.8077 (74.7923) lr 1.8090e-03 eta 0:04:36
>>> alpha1: 0.345  alpha2: -0.077 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.22 <<<
epoch [13/50] batch [5/23] time 0.191 (0.457) data 0.000 (0.264) loss 0.7219 (0.7182) acc 80.6122 (79.3599) lr 1.7705e-03 eta 0:06:37
epoch [13/50] batch [10/23] time 0.194 (0.325) data 0.000 (0.132) loss 0.7740 (0.7043) acc 80.6122 (78.9842) lr 1.7705e-03 eta 0:04:40
epoch [13/50] batch [15/23] time 0.189 (0.284) data 0.000 (0.088) loss 0.5852 (0.6774) acc 82.9545 (80.1306) lr 1.7705e-03 eta 0:04:04
epoch [13/50] batch [20/23] time 1.002 (0.300) data 0.000 (0.066) loss 0.8046 (0.6991) acc 72.6744 (79.2590) lr 1.7705e-03 eta 0:04:16
>>> alpha1: 0.316  alpha2: -0.090 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.21 <<<
epoch [14/50] batch [5/23] time 0.196 (0.504) data 0.000 (0.310) loss 0.6466 (0.7029) acc 79.2553 (81.1732) lr 1.7290e-03 eta 0:07:06
epoch [14/50] batch [10/23] time 0.186 (0.352) data 0.000 (0.155) loss 0.6756 (0.8656) acc 78.1915 (76.4367) lr 1.7290e-03 eta 0:04:56
epoch [14/50] batch [15/23] time 0.191 (0.295) data 0.000 (0.104) loss 0.5204 (0.8175) acc 82.6531 (76.4932) lr 1.7290e-03 eta 0:04:06
epoch [14/50] batch [20/23] time 0.177 (0.268) data 0.000 (0.078) loss 0.4614 (0.7596) acc 78.2609 (77.1787) lr 1.7290e-03 eta 0:03:42
>>> alpha1: 0.293  alpha2: -0.095 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [15/50] batch [5/23] time 0.138 (0.772) data 0.000 (0.243) loss 0.5755 (0.6053) acc 86.4583 (81.6492) lr 1.6845e-03 eta 0:10:35
epoch [15/50] batch [10/23] time 0.251 (0.574) data 0.000 (0.122) loss 0.5910 (0.6148) acc 80.5556 (79.2927) lr 1.6845e-03 eta 0:07:49
epoch [15/50] batch [15/23] time 0.249 (0.467) data 0.001 (0.081) loss 0.6447 (0.6349) acc 80.3922 (78.1538) lr 1.6845e-03 eta 0:06:19
epoch [15/50] batch [20/23] time 0.227 (0.486) data 0.000 (0.061) loss 0.6728 (0.6439) acc 80.3922 (78.8866) lr 1.6845e-03 eta 0:06:32
>>> alpha1: 0.246  alpha2: -0.122 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.26 <<<
epoch [16/50] batch [5/23] time 0.192 (0.480) data 0.000 (0.275) loss 0.6893 (0.6134) acc 83.1633 (80.3693) lr 1.6374e-03 eta 0:06:24
epoch [16/50] batch [10/23] time 0.192 (0.348) data 0.000 (0.138) loss 0.5828 (0.5919) acc 84.0000 (81.9688) lr 1.6374e-03 eta 0:04:36
epoch [16/50] batch [15/23] time 0.205 (0.298) data 0.000 (0.092) loss 0.8131 (0.6084) acc 71.7593 (80.6503) lr 1.6374e-03 eta 0:03:55
epoch [16/50] batch [20/23] time 0.202 (0.274) data 0.000 (0.069) loss 0.7708 (0.6115) acc 76.4423 (80.4613) lr 1.6374e-03 eta 0:03:34
>>> alpha1: 0.226  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [17/50] batch [5/23] time 0.214 (0.513) data 0.000 (0.310) loss 0.9645 (0.6910) acc 82.1429 (83.2803) lr 1.5878e-03 eta 0:06:38
epoch [17/50] batch [10/23] time 0.191 (0.362) data 0.000 (0.155) loss 0.4058 (0.6318) acc 86.5000 (82.3192) lr 1.5878e-03 eta 0:04:39
epoch [17/50] batch [15/23] time 0.197 (0.306) data 0.000 (0.103) loss 0.6825 (0.6410) acc 76.4423 (81.8264) lr 1.5878e-03 eta 0:03:55
epoch [17/50] batch [20/23] time 0.205 (0.280) data 0.000 (0.078) loss 0.5357 (0.6196) acc 84.9057 (82.0417) lr 1.5878e-03 eta 0:03:33
>>> alpha1: 0.212  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [18/50] batch [5/23] time 0.199 (0.476) data 0.000 (0.278) loss 0.5328 (0.5530) acc 81.7308 (82.2791) lr 1.5358e-03 eta 0:05:58
epoch [18/50] batch [10/23] time 0.198 (0.345) data 0.000 (0.139) loss 0.5289 (0.5795) acc 84.6154 (82.8618) lr 1.5358e-03 eta 0:04:18
epoch [18/50] batch [15/23] time 0.206 (0.296) data 0.000 (0.093) loss 0.5010 (0.5515) acc 78.3019 (82.9855) lr 1.5358e-03 eta 0:03:39
epoch [18/50] batch [20/23] time 0.182 (0.271) data 0.000 (0.070) loss 0.5460 (0.5533) acc 78.8043 (83.0605) lr 1.5358e-03 eta 0:03:19
>>> alpha1: 0.205  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [19/50] batch [5/23] time 0.212 (0.510) data 0.000 (0.297) loss 0.5700 (0.5031) acc 83.4821 (83.8200) lr 1.4818e-03 eta 0:06:12
epoch [19/50] batch [10/23] time 0.203 (0.353) data 0.000 (0.149) loss 0.5780 (0.5367) acc 85.5769 (82.2252) lr 1.4818e-03 eta 0:04:16
epoch [19/50] batch [15/23] time 0.224 (0.308) data 0.000 (0.099) loss 0.6559 (0.5361) acc 81.9444 (82.2396) lr 1.4818e-03 eta 0:03:41
epoch [19/50] batch [20/23] time 0.207 (0.280) data 0.000 (0.075) loss 0.5140 (0.5450) acc 86.1111 (82.5010) lr 1.4818e-03 eta 0:03:20
>>> alpha1: 0.202  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [20/50] batch [5/23] time 0.195 (0.516) data 0.000 (0.301) loss 0.4243 (0.5243) acc 88.3333 (84.7579) lr 1.4258e-03 eta 0:06:05
epoch [20/50] batch [10/23] time 0.187 (0.356) data 0.000 (0.151) loss 0.5868 (0.5313) acc 80.8511 (83.1379) lr 1.4258e-03 eta 0:04:10
epoch [20/50] batch [15/23] time 0.185 (0.302) data 0.000 (0.100) loss 0.4612 (0.5169) acc 82.4468 (83.1386) lr 1.4258e-03 eta 0:03:30
epoch [20/50] batch [20/23] time 0.243 (0.278) data 0.000 (0.075) loss 0.6706 (0.5320) acc 74.5370 (82.7073) lr 1.4258e-03 eta 0:03:12
>>> alpha1: 0.193  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.23 <<<
epoch [21/50] batch [5/23] time 0.174 (0.466) data 0.000 (0.264) loss 0.3866 (0.4337) acc 86.9318 (85.4029) lr 1.3681e-03 eta 0:05:19
epoch [21/50] batch [10/23] time 0.196 (0.330) data 0.000 (0.132) loss 0.5500 (0.5248) acc 87.5000 (83.7188) lr 1.3681e-03 eta 0:03:44
epoch [21/50] batch [15/23] time 0.187 (0.281) data 0.000 (0.088) loss 0.4518 (0.5147) acc 91.4894 (84.2458) lr 1.3681e-03 eta 0:03:09
epoch [21/50] batch [20/23] time 0.186 (0.298) data 0.000 (0.066) loss 0.8053 (0.5258) acc 76.0638 (83.4211) lr 1.3681e-03 eta 0:03:19
>>> alpha1: 0.191  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [22/50] batch [5/23] time 0.198 (0.523) data 0.000 (0.320) loss 0.4004 (0.4075) acc 87.7551 (87.8753) lr 1.3090e-03 eta 0:05:45
epoch [22/50] batch [10/23] time 0.178 (0.355) data 0.000 (0.160) loss 0.5639 (0.4559) acc 79.8913 (86.5887) lr 1.3090e-03 eta 0:03:52
epoch [22/50] batch [15/23] time 0.181 (0.299) data 0.000 (0.107) loss 0.4678 (0.4467) acc 86.9792 (86.9632) lr 1.3090e-03 eta 0:03:14
epoch [22/50] batch [20/23] time 0.181 (0.269) data 0.000 (0.080) loss 0.5718 (0.4535) acc 81.9149 (86.3314) lr 1.3090e-03 eta 0:02:54
>>> alpha1: 0.190  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.26 <<<
epoch [23/50] batch [5/23] time 0.197 (0.469) data 0.000 (0.267) loss 0.3889 (0.3893) acc 91.3462 (90.4846) lr 1.2487e-03 eta 0:04:59
epoch [23/50] batch [10/23] time 0.193 (0.336) data 0.000 (0.134) loss 0.5731 (0.4774) acc 82.6531 (86.8079) lr 1.2487e-03 eta 0:03:33
epoch [23/50] batch [15/23] time 0.220 (0.289) data 0.000 (0.089) loss 0.3480 (0.4841) acc 91.5094 (86.3981) lr 1.2487e-03 eta 0:03:01
epoch [23/50] batch [20/23] time 0.191 (0.264) data 0.000 (0.067) loss 0.3642 (0.4817) acc 88.2653 (85.2688) lr 1.2487e-03 eta 0:02:44
>>> alpha1: 0.185  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [24/50] batch [5/23] time 0.215 (0.543) data 0.000 (0.323) loss 0.5323 (0.5009) acc 87.9464 (86.1707) lr 1.1874e-03 eta 0:05:34
epoch [24/50] batch [10/23] time 0.208 (0.369) data 0.000 (0.162) loss 0.7465 (0.4820) acc 84.4340 (85.5999) lr 1.1874e-03 eta 0:03:45
epoch [24/50] batch [15/23] time 0.205 (0.310) data 0.000 (0.108) loss 0.3954 (0.4754) acc 85.6481 (85.9896) lr 1.1874e-03 eta 0:03:07
epoch [24/50] batch [20/23] time 0.263 (0.285) data 0.000 (0.081) loss 0.5635 (0.4792) acc 85.2941 (86.6658) lr 1.1874e-03 eta 0:02:51
>>> alpha1: 0.180  alpha2: -0.130 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [25/50] batch [5/23] time 0.309 (0.583) data 0.000 (0.284) loss 0.4772 (0.4119) acc 87.0000 (87.8341) lr 1.1253e-03 eta 0:05:45
epoch [25/50] batch [10/23] time 0.311 (0.442) data 0.000 (0.142) loss 0.4391 (0.4019) acc 82.6923 (87.1991) lr 1.1253e-03 eta 0:04:19
epoch [25/50] batch [15/23] time 0.298 (0.397) data 0.000 (0.095) loss 0.6213 (0.4214) acc 84.3137 (87.0591) lr 1.1253e-03 eta 0:03:51
epoch [25/50] batch [20/23] time 0.307 (0.372) data 0.000 (0.071) loss 0.5747 (0.4409) acc 85.5769 (87.2604) lr 1.1253e-03 eta 0:03:34
>>> alpha1: 0.175  alpha2: -0.139 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [26/50] batch [5/23] time 0.186 (0.506) data 0.000 (0.288) loss 0.4531 (0.3888) acc 84.5000 (87.6450) lr 1.0628e-03 eta 0:04:48
epoch [26/50] batch [10/23] time 0.184 (0.351) data 0.000 (0.144) loss 0.5587 (0.4238) acc 82.4468 (86.1715) lr 1.0628e-03 eta 0:03:18
epoch [26/50] batch [15/23] time 0.185 (0.297) data 0.000 (0.096) loss 0.3469 (0.4312) acc 90.9574 (86.7842) lr 1.0628e-03 eta 0:02:46
epoch [26/50] batch [20/23] time 0.207 (0.273) data 0.000 (0.072) loss 0.4218 (0.4497) acc 87.2449 (86.7934) lr 1.0628e-03 eta 0:02:31
>>> alpha1: 0.168  alpha2: -0.136 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [27/50] batch [5/23] time 0.197 (0.481) data 0.000 (0.266) loss 0.3616 (0.4501) acc 92.6471 (87.0577) lr 1.0000e-03 eta 0:04:23
epoch [27/50] batch [10/23] time 0.191 (0.335) data 0.000 (0.133) loss 0.3573 (0.4145) acc 84.6939 (87.5844) lr 1.0000e-03 eta 0:03:01
epoch [27/50] batch [15/23] time 0.251 (0.292) data 0.000 (0.089) loss 0.4041 (0.3922) acc 91.3462 (88.4908) lr 1.0000e-03 eta 0:02:36
epoch [27/50] batch [20/23] time 0.173 (0.266) data 0.000 (0.067) loss 0.5586 (0.4062) acc 85.4651 (88.3910) lr 1.0000e-03 eta 0:02:21
>>> alpha1: 0.165  alpha2: -0.137 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [28/50] batch [5/23] time 0.230 (0.480) data 0.000 (0.264) loss 0.3738 (0.3422) acc 87.9464 (89.1684) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [10/23] time 0.278 (0.350) data 0.000 (0.132) loss 0.5800 (0.3767) acc 84.8039 (89.2129) lr 9.3721e-04 eta 0:03:01
epoch [28/50] batch [15/23] time 0.192 (0.299) data 0.000 (0.088) loss 0.4095 (0.3859) acc 85.5000 (88.8890) lr 9.3721e-04 eta 0:02:33
epoch [28/50] batch [20/23] time 0.199 (0.333) data 0.000 (0.066) loss 0.4393 (0.3975) acc 87.0192 (87.9566) lr 9.3721e-04 eta 0:02:49
>>> alpha1: 0.162  alpha2: -0.134 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [29/50] batch [5/23] time 0.189 (0.490) data 0.000 (0.299) loss 0.3158 (0.3423) acc 93.8775 (88.9335) lr 8.7467e-04 eta 0:04:05
epoch [29/50] batch [10/23] time 0.190 (0.344) data 0.000 (0.151) loss 0.3618 (0.3960) acc 87.2449 (87.5438) lr 8.7467e-04 eta 0:02:50
epoch [29/50] batch [15/23] time 0.184 (0.292) data 0.000 (0.101) loss 0.4169 (0.4029) acc 87.7778 (87.9656) lr 8.7467e-04 eta 0:02:23
epoch [29/50] batch [20/23] time 0.198 (0.268) data 0.000 (0.076) loss 0.4165 (0.4020) acc 92.6471 (88.2889) lr 8.7467e-04 eta 0:02:10
>>> alpha1: 0.157  alpha2: -0.130 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [30/50] batch [5/23] time 0.178 (0.498) data 0.000 (0.297) loss 0.2900 (0.2781) acc 94.6808 (94.2078) lr 8.1262e-04 eta 0:03:58
epoch [30/50] batch [10/23] time 0.187 (0.348) data 0.000 (0.149) loss 0.4605 (0.3523) acc 79.2553 (90.5611) lr 8.1262e-04 eta 0:02:44
epoch [30/50] batch [15/23] time 0.189 (0.304) data 0.000 (0.099) loss 0.3322 (0.3727) acc 91.6667 (90.1254) lr 8.1262e-04 eta 0:02:22
epoch [30/50] batch [20/23] time 0.179 (0.275) data 0.000 (0.074) loss 0.2850 (0.3735) acc 92.7778 (89.7017) lr 8.1262e-04 eta 0:02:07
>>> alpha1: 0.155  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [31/50] batch [5/23] time 0.203 (0.494) data 0.000 (0.284) loss 0.4004 (0.4002) acc 89.3519 (86.9623) lr 7.5131e-04 eta 0:03:44
epoch [31/50] batch [10/23] time 0.207 (0.348) data 0.000 (0.142) loss 0.4340 (0.3917) acc 89.3519 (87.9055) lr 7.5131e-04 eta 0:02:36
epoch [31/50] batch [15/23] time 0.207 (0.303) data 0.000 (0.095) loss 0.3181 (0.3858) acc 92.1296 (88.8956) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [20/23] time 0.198 (0.277) data 0.000 (0.071) loss 0.3582 (0.3799) acc 91.8269 (88.8923) lr 7.5131e-04 eta 0:02:01
>>> alpha1: 0.153  alpha2: -0.128 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [32/50] batch [5/23] time 0.288 (0.449) data 0.000 (0.259) loss 0.2701 (0.2684) acc 97.7273 (93.0268) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [10/23] time 0.260 (0.494) data 0.000 (0.130) loss 0.5055 (0.3432) acc 81.6038 (89.9830) lr 6.9098e-04 eta 0:03:30
epoch [32/50] batch [15/23] time 0.256 (0.414) data 0.000 (0.086) loss 0.2131 (0.3456) acc 95.9091 (89.9891) lr 6.9098e-04 eta 0:02:54
epoch [32/50] batch [20/23] time 0.239 (0.374) data 0.000 (0.065) loss 0.3950 (0.3591) acc 83.6735 (89.2738) lr 6.9098e-04 eta 0:02:36
>>> alpha1: 0.151  alpha2: -0.121 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [33/50] batch [5/23] time 0.192 (0.488) data 0.000 (0.281) loss 0.6167 (0.3990) acc 78.0612 (85.9437) lr 6.3188e-04 eta 0:03:19
epoch [33/50] batch [10/23] time 0.207 (0.355) data 0.000 (0.141) loss 0.2300 (0.3528) acc 97.6852 (88.6678) lr 6.3188e-04 eta 0:02:23
epoch [33/50] batch [15/23] time 0.214 (0.305) data 0.000 (0.094) loss 0.2373 (0.3507) acc 92.5439 (88.8246) lr 6.3188e-04 eta 0:02:01
epoch [33/50] batch [20/23] time 1.371 (0.339) data 0.000 (0.070) loss 0.3737 (0.3493) acc 84.3220 (88.8474) lr 6.3188e-04 eta 0:02:13
>>> alpha1: 0.152  alpha2: -0.127 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [34/50] batch [5/23] time 0.201 (0.506) data 0.000 (0.296) loss 0.2872 (0.3471) acc 93.2692 (87.4306) lr 5.7422e-04 eta 0:03:15
epoch [34/50] batch [10/23] time 0.213 (0.365) data 0.000 (0.150) loss 0.3486 (0.3480) acc 84.0909 (88.6238) lr 5.7422e-04 eta 0:02:18
epoch [34/50] batch [15/23] time 0.207 (0.312) data 0.000 (0.100) loss 0.1567 (0.3384) acc 99.0566 (89.8357) lr 5.7422e-04 eta 0:01:57
epoch [34/50] batch [20/23] time 0.191 (0.283) data 0.000 (0.075) loss 0.3518 (0.3484) acc 85.2041 (89.0760) lr 5.7422e-04 eta 0:01:45
>>> alpha1: 0.153  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [35/50] batch [5/23] time 0.217 (0.480) data 0.001 (0.270) loss 0.2103 (0.2911) acc 97.5962 (92.0466) lr 5.1825e-04 eta 0:02:54
epoch [35/50] batch [10/23] time 0.147 (0.326) data 0.000 (0.135) loss 0.3945 (0.3185) acc 88.9423 (90.0015) lr 5.1825e-04 eta 0:01:56
epoch [35/50] batch [15/23] time 0.147 (0.269) data 0.000 (0.090) loss 0.4007 (0.3302) acc 87.2449 (89.9057) lr 5.1825e-04 eta 0:01:34
epoch [35/50] batch [20/23] time 0.161 (0.238) data 0.001 (0.068) loss 0.2296 (0.3367) acc 93.7500 (89.8803) lr 5.1825e-04 eta 0:01:22
>>> alpha1: 0.153  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [36/50] batch [5/23] time 0.185 (0.483) data 0.000 (0.282) loss 0.3798 (0.3443) acc 89.0625 (91.8459) lr 4.6417e-04 eta 0:02:44
epoch [36/50] batch [10/23] time 0.211 (0.342) data 0.000 (0.141) loss 0.3062 (0.5427) acc 91.3636 (87.7765) lr 4.6417e-04 eta 0:01:54
epoch [36/50] batch [15/23] time 0.235 (0.300) data 0.000 (0.094) loss 0.3015 (0.4702) acc 91.3793 (88.4078) lr 4.6417e-04 eta 0:01:38
epoch [36/50] batch [20/23] time 0.199 (0.276) data 0.000 (0.071) loss 0.2616 (0.4514) acc 96.1538 (88.2086) lr 4.6417e-04 eta 0:01:29
>>> alpha1: 0.147  alpha2: -0.127 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [37/50] batch [5/23] time 0.190 (0.496) data 0.000 (0.275) loss 0.5412 (0.3194) acc 85.5556 (91.5220) lr 4.1221e-04 eta 0:02:37
epoch [37/50] batch [10/23] time 0.194 (0.351) data 0.000 (0.138) loss 0.4677 (0.3516) acc 86.7647 (89.7157) lr 4.1221e-04 eta 0:01:49
epoch [37/50] batch [15/23] time 0.218 (0.303) data 0.000 (0.092) loss 0.3481 (0.3584) acc 93.1034 (89.1558) lr 4.1221e-04 eta 0:01:32
epoch [37/50] batch [20/23] time 0.209 (0.281) data 0.000 (0.069) loss 0.4469 (0.3632) acc 90.4546 (89.5767) lr 4.1221e-04 eta 0:01:24
>>> alpha1: 0.147  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [38/50] batch [5/23] time 0.200 (0.450) data 0.000 (0.243) loss 0.3692 (0.2904) acc 87.0192 (89.5622) lr 3.6258e-04 eta 0:02:12
epoch [38/50] batch [10/23] time 0.232 (0.333) data 0.000 (0.122) loss 0.4079 (0.3405) acc 89.9123 (90.3488) lr 3.6258e-04 eta 0:01:36
epoch [38/50] batch [15/23] time 0.192 (0.293) data 0.000 (0.081) loss 0.2978 (0.3303) acc 88.7755 (90.4512) lr 3.6258e-04 eta 0:01:23
epoch [38/50] batch [20/23] time 0.207 (0.271) data 0.000 (0.061) loss 0.2026 (0.3370) acc 97.6415 (90.2873) lr 3.6258e-04 eta 0:01:15
>>> alpha1: 0.143  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [39/50] batch [5/23] time 0.271 (0.537) data 0.000 (0.273) loss 0.5491 (0.3574) acc 89.5455 (90.1138) lr 3.1545e-04 eta 0:02:25
epoch [39/50] batch [10/23] time 0.153 (0.359) data 0.000 (0.137) loss 0.4922 (0.3748) acc 87.5000 (89.8601) lr 3.1545e-04 eta 0:01:35
epoch [39/50] batch [15/23] time 0.171 (0.293) data 0.000 (0.091) loss 0.3590 (0.3549) acc 87.9464 (90.3087) lr 3.1545e-04 eta 0:01:16
epoch [39/50] batch [20/23] time 0.223 (0.265) data 0.000 (0.068) loss 0.3761 (0.3471) acc 92.7885 (90.0149) lr 3.1545e-04 eta 0:01:07
>>> alpha1: 0.139  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [40/50] batch [5/23] time 0.233 (0.518) data 0.000 (0.303) loss 0.4036 (0.3736) acc 90.1786 (88.3378) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [10/23] time 0.216 (0.364) data 0.000 (0.152) loss 0.2492 (0.3209) acc 93.8596 (90.2661) lr 2.7103e-04 eta 0:01:28
epoch [40/50] batch [15/23] time 0.194 (0.313) data 0.000 (0.101) loss 0.2774 (0.3320) acc 93.0000 (89.9152) lr 2.7103e-04 eta 0:01:14
epoch [40/50] batch [20/23] time 0.209 (0.285) data 0.000 (0.076) loss 0.3016 (0.3327) acc 89.5455 (89.9625) lr 2.7103e-04 eta 0:01:06
>>> alpha1: 0.136  alpha2: -0.118 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [41/50] batch [5/23] time 0.206 (0.485) data 0.000 (0.279) loss 0.4526 (0.3809) acc 80.5556 (86.6730) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [10/23] time 0.203 (0.348) data 0.000 (0.140) loss 0.2179 (0.3487) acc 96.7593 (88.7241) lr 2.2949e-04 eta 0:01:16
epoch [41/50] batch [15/23] time 0.209 (0.305) data 0.000 (0.093) loss 0.4192 (0.3444) acc 93.9815 (89.7079) lr 2.2949e-04 eta 0:01:05
epoch [41/50] batch [20/23] time 0.191 (0.280) data 0.000 (0.070) loss 0.3690 (0.3273) acc 90.3061 (90.4899) lr 2.2949e-04 eta 0:00:58
>>> alpha1: 0.139  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [42/50] batch [5/23] time 0.210 (0.451) data 0.000 (0.235) loss 0.3301 (0.3351) acc 95.0000 (92.2349) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [10/23] time 0.205 (0.330) data 0.000 (0.118) loss 0.3254 (0.3297) acc 91.5094 (92.7121) lr 1.9098e-04 eta 0:01:04
epoch [42/50] batch [15/23] time 0.156 (0.274) data 0.000 (0.079) loss 0.2068 (0.3264) acc 93.6364 (91.3352) lr 1.9098e-04 eta 0:00:52
epoch [42/50] batch [20/23] time 0.164 (0.246) data 0.000 (0.059) loss 0.3867 (0.3235) acc 91.3636 (90.8918) lr 1.9098e-04 eta 0:00:46
>>> alpha1: 0.140  alpha2: -0.116 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [43/50] batch [5/23] time 0.214 (0.464) data 0.000 (0.249) loss 0.3177 (0.2845) acc 93.3036 (92.9361) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [10/23] time 0.203 (0.343) data 0.000 (0.125) loss 0.2763 (0.2738) acc 90.8654 (92.0001) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [15/23] time 0.214 (0.297) data 0.000 (0.083) loss 0.3938 (0.3006) acc 91.5179 (91.0178) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [20/23] time 0.193 (0.272) data 0.000 (0.062) loss 0.3484 (0.3215) acc 91.5000 (90.8358) lr 1.5567e-04 eta 0:00:44
>>> alpha1: 0.140  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [44/50] batch [5/23] time 0.256 (0.462) data 0.000 (0.248) loss 0.2398 (0.3099) acc 91.9811 (91.3986) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [10/23] time 0.218 (0.336) data 0.000 (0.124) loss 0.4192 (0.5561) acc 87.5000 (87.2902) lr 1.2369e-04 eta 0:00:50
epoch [44/50] batch [15/23] time 0.215 (0.296) data 0.000 (0.083) loss 0.2721 (0.4755) acc 90.7895 (87.5194) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [20/23] time 0.241 (0.275) data 0.000 (0.062) loss 0.2206 (0.4266) acc 96.6981 (88.7128) lr 1.2369e-04 eta 0:00:38
>>> alpha1: 0.140  alpha2: -0.115 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.29 <<<
epoch [45/50] batch [5/23] time 0.232 (0.445) data 0.013 (0.227) loss 0.3665 (0.3112) acc 92.1053 (89.4434) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [10/23] time 0.214 (0.334) data 0.000 (0.114) loss 0.3312 (0.3307) acc 93.7500 (90.6965) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [15/23] time 0.192 (0.292) data 0.000 (0.076) loss 0.4176 (0.3235) acc 88.5000 (90.3604) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [20/23] time 0.191 (0.270) data 0.000 (0.057) loss 0.3617 (0.3146) acc 91.3265 (91.3423) lr 9.5173e-05 eta 0:00:31
>>> alpha1: 0.138  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.28 <<<
epoch [46/50] batch [5/23] time 0.269 (0.521) data 0.000 (0.247) loss 0.3964 (0.2959) acc 84.3750 (90.2467) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [10/23] time 0.159 (0.370) data 0.000 (0.124) loss 0.2651 (0.2794) acc 94.6429 (91.9530) lr 7.0224e-05 eta 0:00:38
epoch [46/50] batch [15/23] time 0.156 (0.297) data 0.000 (0.083) loss 0.3793 (0.3114) acc 87.5000 (91.6218) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [20/23] time 0.184 (0.263) data 0.000 (0.062) loss 0.2673 (0.3136) acc 90.7407 (91.5690) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.137  alpha2: -0.117 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [47/50] batch [5/23] time 0.204 (0.481) data 0.000 (0.276) loss 0.3645 (0.3759) acc 92.2727 (89.6727) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [10/23] time 0.272 (0.347) data 0.013 (0.140) loss 0.2458 (0.3137) acc 90.3061 (90.7576) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [15/23] time 0.206 (0.299) data 0.000 (0.093) loss 0.2944 (0.3052) acc 94.3396 (91.1234) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [20/23] time 0.191 (0.275) data 0.000 (0.070) loss 0.3348 (0.3189) acc 92.0000 (90.9450) lr 4.8943e-05 eta 0:00:19
>>> alpha1: 0.136  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [48/50] batch [5/23] time 0.204 (0.512) data 0.000 (0.300) loss 0.4004 (0.2825) acc 85.8491 (91.6586) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [10/23] time 0.198 (0.368) data 0.000 (0.150) loss 0.4116 (0.3385) acc 86.2245 (89.9743) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [15/23] time 0.203 (0.313) data 0.000 (0.100) loss 0.2706 (0.3197) acc 91.5094 (90.3346) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [20/23] time 0.210 (0.286) data 0.000 (0.075) loss 0.3419 (0.3134) acc 85.9091 (90.5526) lr 3.1417e-05 eta 0:00:14
>>> alpha1: 0.137  alpha2: -0.122 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.29 <<<
epoch [49/50] batch [5/23] time 0.163 (0.463) data 0.000 (0.261) loss 0.2646 (0.3264) acc 92.1569 (89.1879) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.142 (0.314) data 0.001 (0.131) loss 0.2141 (0.3277) acc 96.8750 (90.5171) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.166 (0.259) data 0.000 (0.087) loss 0.1527 (0.3116) acc 98.7069 (90.5633) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.329 (0.256) data 0.000 (0.066) loss 0.2435 (0.3013) acc 94.0678 (90.7763) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.135  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.27 <<<
epoch [50/50] batch [5/23] time 0.197 (0.477) data 0.000 (0.266) loss 0.2142 (0.2950) acc 93.6274 (93.3614) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.208 (0.349) data 0.000 (0.133) loss 0.2490 (0.3085) acc 92.1296 (91.9222) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.187 (0.300) data 0.000 (0.089) loss 0.3605 (0.3254) acc 94.2708 (91.2269) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.216 (0.276) data 0.000 (0.067) loss 0.2658 (0.3080) acc 95.0893 (91.5151) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.13, 0.14, 0.15, 0.15, 0.16, 0.17, 0.17, 0.16, 0.17, 0.17, 0.16, 0.16, 0.17, 0.17, 0.18, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.17, 0.18, 0.18, 0.17, 0.18, 0.18, 0.18, 0.18]
* matched noise rate: [0.07, 0.08, 0.08, 0.08, 0.09, 0.11, 0.1, 0.09, 0.1, 0.1, 0.08, 0.08, 0.1, 0.09, 0.1, 0.1, 0.09, 0.1, 0.09, 0.1, 0.11, 0.12, 0.12, 0.11, 0.1, 0.11, 0.11, 0.11, 0.12, 0.12, 0.1, 0.11, 0.11, 0.12, 0.13, 0.12, 0.12, 0.12, 0.12, 0.12]
* unmatched noise rate: [0.18, 0.21, 0.22, 0.21, 0.26, 0.26, 0.29, 0.26, 0.29, 0.27, 0.23, 0.25, 0.26, 0.28, 0.29, 0.27, 0.27, 0.3, 0.25, 0.29, 0.3, 0.31, 0.31, 0.3, 0.29, 0.31, 0.3, 0.31, 0.3, 0.29, 0.31, 0.31, 0.3, 0.3, 0.29, 0.28, 0.29, 0.3, 0.29, 0.27]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:40,  2.53s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.38it/s] 24%|██▎       | 4/17 [00:02<00:06,  1.96it/s] 35%|███▌      | 6/17 [00:03<00:03,  3.30it/s] 47%|████▋     | 8/17 [00:03<00:01,  4.64it/s] 59%|█████▉    | 10/17 [00:03<00:01,  5.88it/s] 71%|███████   | 12/17 [00:03<00:00,  6.98it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.89it/s] 94%|█████████▍| 16/17 [00:03<00:00,  8.63it/s]100%|██████████| 17/17 [00:04<00:00,  3.58it/s]
=> result
* total: 1,692
* correct: 1,060
* accuracy: 62.6%
* error: 37.4%
* macro_f1: 62.1%
=> per-class result
* class: 0 (banded)	total: 36	correct: 22	acc: 61.1%
* class: 1 (blotchy)	total: 36	correct: 3	acc: 8.3%
* class: 2 (braided)	total: 36	correct: 16	acc: 44.4%
* class: 3 (bubbly)	total: 36	correct: 28	acc: 77.8%
* class: 4 (bumpy)	total: 36	correct: 11	acc: 30.6%
* class: 5 (chequered)	total: 36	correct: 30	acc: 83.3%
* class: 6 (cobwebbed)	total: 36	correct: 28	acc: 77.8%
* class: 7 (cracked)	total: 36	correct: 29	acc: 80.6%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 16	acc: 44.4%
* class: 11 (fibrous)	total: 36	correct: 28	acc: 77.8%
* class: 12 (flecked)	total: 36	correct: 17	acc: 47.2%
* class: 13 (freckled)	total: 36	correct: 29	acc: 80.6%
* class: 14 (frilly)	total: 36	correct: 31	acc: 86.1%
* class: 15 (gauzy)	total: 36	correct: 20	acc: 55.6%
* class: 16 (grid)	total: 36	correct: 12	acc: 33.3%
* class: 17 (grooved)	total: 36	correct: 18	acc: 50.0%
* class: 18 (honeycombed)	total: 36	correct: 24	acc: 66.7%
* class: 19 (interlaced)	total: 36	correct: 26	acc: 72.2%
* class: 20 (knitted)	total: 36	correct: 31	acc: 86.1%
* class: 21 (lacelike)	total: 36	correct: 33	acc: 91.7%
* class: 22 (lined)	total: 36	correct: 16	acc: 44.4%
* class: 23 (marbled)	total: 36	correct: 22	acc: 61.1%
* class: 24 (matted)	total: 36	correct: 25	acc: 69.4%
* class: 25 (meshed)	total: 36	correct: 26	acc: 72.2%
* class: 26 (paisley)	total: 36	correct: 32	acc: 88.9%
* class: 27 (perforated)	total: 36	correct: 23	acc: 63.9%
* class: 28 (pitted)	total: 36	correct: 16	acc: 44.4%
* class: 29 (pleated)	total: 36	correct: 19	acc: 52.8%
* class: 30 (polka-dotted)	total: 36	correct: 30	acc: 83.3%
* class: 31 (porous)	total: 36	correct: 10	acc: 27.8%
* class: 32 (potholed)	total: 36	correct: 33	acc: 91.7%
* class: 33 (scaly)	total: 36	correct: 20	acc: 55.6%
* class: 34 (smeared)	total: 36	correct: 20	acc: 55.6%
* class: 35 (spiralled)	total: 36	correct: 21	acc: 58.3%
* class: 36 (sprinkled)	total: 36	correct: 19	acc: 52.8%
* class: 37 (stained)	total: 36	correct: 16	acc: 44.4%
* class: 38 (stratified)	total: 36	correct: 26	acc: 72.2%
* class: 39 (striped)	total: 36	correct: 25	acc: 69.4%
* class: 40 (studded)	total: 36	correct: 28	acc: 77.8%
* class: 41 (swirly)	total: 36	correct: 25	acc: 69.4%
* class: 42 (veined)	total: 36	correct: 17	acc: 47.2%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 16	acc: 44.4%
* class: 45 (wrinkled)	total: 36	correct: 21	acc: 58.3%
* class: 46 (zigzagged)	total: 36	correct: 31	acc: 86.1%
* average: 62.6%
Elapsed: 0:16:28
Run this job and save the output to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_2-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.218 (0.998) data 0.000 (0.287) loss 3.4464 (3.5127) acc 28.1250 (25.0000) lr 1.0000e-05 eta 0:19:03
epoch [1/50] batch [10/23] time 0.200 (0.602) data 0.000 (0.144) loss 3.3890 (3.5106) acc 31.2500 (22.1875) lr 1.0000e-05 eta 0:11:26
epoch [1/50] batch [15/23] time 0.201 (0.468) data 0.000 (0.096) loss 3.3248 (3.4817) acc 28.1250 (21.0417) lr 1.0000e-05 eta 0:08:51
epoch [1/50] batch [20/23] time 0.206 (0.406) data 0.000 (0.072) loss 3.3745 (3.4690) acc 21.8750 (20.1562) lr 1.0000e-05 eta 0:07:38
epoch [2/50] batch [5/23] time 0.221 (0.514) data 0.000 (0.293) loss 2.3888 (3.0594) acc 31.2500 (21.2500) lr 2.0000e-03 eta 0:09:37
epoch [2/50] batch [10/23] time 0.209 (0.365) data 0.000 (0.146) loss 2.6195 (2.8528) acc 37.5000 (24.3750) lr 2.0000e-03 eta 0:06:47
epoch [2/50] batch [15/23] time 0.208 (0.317) data 0.000 (0.098) loss 2.6869 (2.8181) acc 34.3750 (27.9167) lr 2.0000e-03 eta 0:05:52
epoch [2/50] batch [20/23] time 0.213 (0.290) data 0.000 (0.073) loss 2.2324 (2.6988) acc 28.1250 (30.3125) lr 2.0000e-03 eta 0:05:21
epoch [3/50] batch [5/23] time 0.212 (0.468) data 0.000 (0.245) loss 2.4265 (2.4382) acc 28.1250 (37.5000) lr 1.9980e-03 eta 0:08:33
epoch [3/50] batch [10/23] time 0.262 (0.346) data 0.000 (0.123) loss 2.6308 (2.5428) acc 31.2500 (34.6875) lr 1.9980e-03 eta 0:06:18
epoch [3/50] batch [15/23] time 0.207 (0.300) data 0.000 (0.082) loss 2.2085 (2.4716) acc 46.8750 (37.7083) lr 1.9980e-03 eta 0:05:27
epoch [3/50] batch [20/23] time 0.211 (0.277) data 0.000 (0.061) loss 2.2979 (2.4171) acc 34.3750 (38.4375) lr 1.9980e-03 eta 0:05:00
epoch [4/50] batch [5/23] time 0.211 (0.458) data 0.000 (0.234) loss 2.2661 (2.3093) acc 46.8750 (40.6250) lr 1.9921e-03 eta 0:08:12
epoch [4/50] batch [10/23] time 0.162 (0.319) data 0.000 (0.117) loss 1.6061 (2.1992) acc 65.6250 (44.3750) lr 1.9921e-03 eta 0:05:41
epoch [4/50] batch [15/23] time 0.156 (0.264) data 0.000 (0.078) loss 2.6195 (2.2714) acc 50.0000 (44.5833) lr 1.9921e-03 eta 0:04:41
epoch [4/50] batch [20/23] time 0.295 (0.252) data 0.000 (0.059) loss 1.9264 (2.2656) acc 46.8750 (43.1250) lr 1.9921e-03 eta 0:04:27
epoch [5/50] batch [5/23] time 0.317 (0.599) data 0.000 (0.260) loss 2.0727 (2.0653) acc 43.7500 (50.6250) lr 1.9823e-03 eta 0:10:30
epoch [5/50] batch [10/23] time 0.152 (0.409) data 0.000 (0.130) loss 1.8775 (2.1311) acc 50.0000 (49.0625) lr 1.9823e-03 eta 0:07:09
epoch [5/50] batch [15/23] time 0.153 (0.323) data 0.000 (0.087) loss 1.8559 (2.1479) acc 46.8750 (47.9167) lr 1.9823e-03 eta 0:05:37
epoch [5/50] batch [20/23] time 0.263 (0.286) data 0.000 (0.065) loss 2.2941 (2.1215) acc 40.6250 (47.3438) lr 1.9823e-03 eta 0:04:57
epoch [6/50] batch [5/23] time 0.173 (0.525) data 0.001 (0.279) loss 2.2346 (2.1245) acc 34.3750 (48.1250) lr 1.9686e-03 eta 0:09:00
epoch [6/50] batch [10/23] time 0.154 (0.342) data 0.000 (0.140) loss 2.5597 (2.0884) acc 40.6250 (50.0000) lr 1.9686e-03 eta 0:05:50
epoch [6/50] batch [15/23] time 0.230 (0.285) data 0.000 (0.093) loss 1.5893 (2.0374) acc 53.1250 (48.9583) lr 1.9686e-03 eta 0:04:50
epoch [6/50] batch [20/23] time 0.212 (0.263) data 0.000 (0.070) loss 2.1726 (2.0291) acc 46.8750 (49.0625) lr 1.9686e-03 eta 0:04:27
epoch [7/50] batch [5/23] time 0.220 (0.495) data 0.000 (0.267) loss 2.5146 (1.7434) acc 46.8750 (53.1250) lr 1.9511e-03 eta 0:08:18
epoch [7/50] batch [10/23] time 0.205 (0.355) data 0.000 (0.134) loss 2.2399 (1.9288) acc 53.1250 (54.0625) lr 1.9511e-03 eta 0:05:56
epoch [7/50] batch [15/23] time 0.200 (0.310) data 0.000 (0.089) loss 1.7156 (1.9363) acc 43.7500 (52.0833) lr 1.9511e-03 eta 0:05:09
epoch [7/50] batch [20/23] time 0.208 (0.285) data 0.000 (0.067) loss 1.9095 (1.9307) acc 53.1250 (52.1875) lr 1.9511e-03 eta 0:04:42
epoch [8/50] batch [5/23] time 0.203 (0.445) data 0.000 (0.218) loss 1.8781 (2.1431) acc 56.2500 (50.6250) lr 1.9298e-03 eta 0:07:18
epoch [8/50] batch [10/23] time 0.276 (0.338) data 0.000 (0.109) loss 1.6790 (1.9303) acc 50.0000 (52.8125) lr 1.9298e-03 eta 0:05:30
epoch [8/50] batch [15/23] time 0.210 (0.295) data 0.000 (0.073) loss 2.3607 (1.8541) acc 56.2500 (55.0000) lr 1.9298e-03 eta 0:04:47
epoch [8/50] batch [20/23] time 0.211 (0.274) data 0.000 (0.055) loss 1.3300 (1.8505) acc 65.6250 (56.0938) lr 1.9298e-03 eta 0:04:25
epoch [9/50] batch [5/23] time 0.206 (0.440) data 0.000 (0.215) loss 1.8744 (1.6489) acc 43.7500 (62.5000) lr 1.9048e-03 eta 0:07:03
epoch [9/50] batch [10/23] time 0.201 (0.334) data 0.000 (0.107) loss 1.3968 (1.6943) acc 71.8750 (61.2500) lr 1.9048e-03 eta 0:05:19
epoch [9/50] batch [15/23] time 0.207 (0.292) data 0.000 (0.072) loss 2.0639 (1.7150) acc 46.8750 (59.5833) lr 1.9048e-03 eta 0:04:37
epoch [9/50] batch [20/23] time 0.209 (0.271) data 0.000 (0.054) loss 1.7237 (1.7531) acc 68.7500 (59.6875) lr 1.9048e-03 eta 0:04:16
epoch [10/50] batch [5/23] time 0.290 (0.471) data 0.000 (0.227) loss 1.6648 (1.6605) acc 56.2500 (59.3750) lr 1.8763e-03 eta 0:07:22
epoch [10/50] batch [10/23] time 0.210 (0.341) data 0.000 (0.114) loss 2.0927 (1.7401) acc 59.3750 (58.4375) lr 1.8763e-03 eta 0:05:18
epoch [10/50] batch [15/23] time 0.206 (0.297) data 0.000 (0.076) loss 2.3326 (1.8555) acc 46.8750 (55.0000) lr 1.8763e-03 eta 0:04:35
epoch [10/50] batch [20/23] time 0.241 (0.278) data 0.000 (0.057) loss 1.7300 (1.7755) acc 53.1250 (57.3438) lr 1.8763e-03 eta 0:04:16
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.512  alpha2: -0.082 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.22 <<<
epoch [11/50] batch [5/23] time 1.029 (1.009) data 0.000 (0.282) loss 1.1895 (1.0578) acc 56.3953 (67.4611) lr 1.8443e-03 eta 0:15:23
epoch [11/50] batch [10/23] time 1.229 (1.003) data 0.000 (0.141) loss 1.0157 (1.1043) acc 75.0000 (67.8218) lr 1.8443e-03 eta 0:15:12
epoch [11/50] batch [15/23] time 0.192 (0.885) data 0.000 (0.094) loss 1.0549 (1.1239) acc 59.0000 (66.7482) lr 1.8443e-03 eta 0:13:21
epoch [11/50] batch [20/23] time 0.187 (0.710) data 0.000 (0.071) loss 1.1413 (1.1062) acc 68.2292 (67.5383) lr 1.8443e-03 eta 0:10:39
>>> alpha1: 0.493  alpha2: -0.079 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.18 <<<
epoch [12/50] batch [5/23] time 0.188 (0.703) data 0.000 (0.287) loss 0.8878 (0.8971) acc 65.3061 (74.9404) lr 1.8090e-03 eta 0:10:27
epoch [12/50] batch [10/23] time 0.189 (0.445) data 0.000 (0.144) loss 1.0246 (0.9770) acc 61.2245 (69.3826) lr 1.8090e-03 eta 0:06:34
epoch [12/50] batch [15/23] time 0.207 (0.427) data 0.000 (0.096) loss 0.9982 (0.9890) acc 64.6226 (68.9253) lr 1.8090e-03 eta 0:06:16
epoch [12/50] batch [20/23] time 0.230 (0.415) data 0.000 (0.072) loss 0.8951 (1.0119) acc 72.1591 (67.2233) lr 1.8090e-03 eta 0:06:03
>>> alpha1: 0.480  alpha2: -0.001 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.16 <<<
epoch [13/50] batch [5/23] time 0.190 (0.434) data 0.000 (0.249) loss 1.2747 (1.0790) acc 61.7347 (69.8914) lr 1.7705e-03 eta 0:06:16
epoch [13/50] batch [10/23] time 0.217 (0.321) data 0.000 (0.124) loss 0.9699 (1.0388) acc 74.0000 (70.1436) lr 1.7705e-03 eta 0:04:37
epoch [13/50] batch [15/23] time 1.303 (0.349) data 0.000 (0.083) loss 0.8934 (1.0287) acc 75.8929 (69.8740) lr 1.7705e-03 eta 0:05:00
epoch [13/50] batch [20/23] time 0.194 (0.312) data 0.000 (0.062) loss 0.6915 (1.0087) acc 72.0588 (69.7157) lr 1.7705e-03 eta 0:04:26
>>> alpha1: 0.456  alpha2: -0.012 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [14/50] batch [5/23] time 0.201 (0.474) data 0.000 (0.280) loss 0.8809 (0.8240) acc 67.8571 (72.6905) lr 1.7290e-03 eta 0:06:41
epoch [14/50] batch [10/23] time 0.188 (0.333) data 0.000 (0.140) loss 0.9121 (0.8697) acc 72.9167 (71.2768) lr 1.7290e-03 eta 0:04:40
epoch [14/50] batch [15/23] time 0.230 (0.288) data 0.000 (0.094) loss 0.7798 (0.9038) acc 75.0000 (70.4208) lr 1.7290e-03 eta 0:04:00
epoch [14/50] batch [20/23] time 0.193 (0.262) data 0.000 (0.070) loss 0.9976 (0.8965) acc 69.5000 (70.0244) lr 1.7290e-03 eta 0:03:37
>>> alpha1: 0.443  alpha2: 0.028 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [15/50] batch [5/23] time 0.162 (0.447) data 0.000 (0.301) loss 0.6303 (0.7905) acc 72.9167 (73.9258) lr 1.6845e-03 eta 0:06:07
epoch [15/50] batch [10/23] time 0.134 (0.296) data 0.000 (0.151) loss 0.6891 (0.7644) acc 78.2609 (74.0875) lr 1.6845e-03 eta 0:04:02
epoch [15/50] batch [15/23] time 0.251 (0.266) data 0.000 (0.100) loss 0.9176 (0.8249) acc 69.2708 (71.8096) lr 1.6845e-03 eta 0:03:36
epoch [15/50] batch [20/23] time 0.235 (0.262) data 0.000 (0.075) loss 0.7853 (0.8256) acc 81.8182 (72.7885) lr 1.6845e-03 eta 0:03:32
>>> alpha1: 0.388  alpha2: -0.055 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.16 <<<
epoch [16/50] batch [5/23] time 0.193 (0.488) data 0.000 (0.287) loss 0.5087 (0.6909) acc 82.5000 (79.5311) lr 1.6374e-03 eta 0:06:30
epoch [16/50] batch [10/23] time 0.176 (0.351) data 0.000 (0.144) loss 0.8356 (0.7683) acc 76.1364 (76.3602) lr 1.6374e-03 eta 0:04:39
epoch [16/50] batch [15/23] time 0.187 (0.299) data 0.000 (0.096) loss 0.8578 (0.7640) acc 75.5319 (76.4923) lr 1.6374e-03 eta 0:03:56
epoch [16/50] batch [20/23] time 0.198 (0.273) data 0.000 (0.072) loss 0.8550 (0.7802) acc 64.4231 (75.1933) lr 1.6374e-03 eta 0:03:34
>>> alpha1: 0.351  alpha2: -0.078 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [17/50] batch [5/23] time 0.176 (0.456) data 0.000 (0.269) loss 0.9565 (0.7423) acc 65.0000 (75.8122) lr 1.5878e-03 eta 0:05:54
epoch [17/50] batch [10/23] time 0.185 (0.322) data 0.000 (0.135) loss 0.7835 (0.7468) acc 70.7447 (75.6322) lr 1.5878e-03 eta 0:04:08
epoch [17/50] batch [15/23] time 0.174 (0.281) data 0.001 (0.090) loss 0.7779 (0.7614) acc 77.8409 (74.8495) lr 1.5878e-03 eta 0:03:35
epoch [17/50] batch [20/23] time 0.191 (0.257) data 0.000 (0.068) loss 0.7271 (0.7540) acc 72.9592 (74.5009) lr 1.5878e-03 eta 0:03:16
>>> alpha1: 0.325  alpha2: -0.106 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [18/50] batch [5/23] time 0.204 (0.428) data 0.000 (0.236) loss 0.7910 (1.1408) acc 75.0000 (71.2050) lr 1.5358e-03 eta 0:05:22
epoch [18/50] batch [10/23] time 0.200 (0.310) data 0.000 (0.118) loss 0.6169 (0.9002) acc 86.7021 (75.4942) lr 1.5358e-03 eta 0:03:51
epoch [18/50] batch [15/23] time 0.181 (0.272) data 0.000 (0.079) loss 0.7149 (0.8212) acc 79.8913 (77.7455) lr 1.5358e-03 eta 0:03:22
epoch [18/50] batch [20/23] time 0.173 (0.249) data 0.000 (0.059) loss 0.9844 (0.8394) acc 72.1591 (76.0335) lr 1.5358e-03 eta 0:03:04
>>> alpha1: 0.298  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.16 <<<
epoch [19/50] batch [5/23] time 0.188 (0.438) data 0.000 (0.248) loss 0.4994 (0.6018) acc 85.2041 (81.0996) lr 1.4818e-03 eta 0:05:20
epoch [19/50] batch [10/23] time 0.980 (0.392) data 0.000 (0.124) loss 0.8386 (0.6312) acc 73.7500 (80.0996) lr 1.4818e-03 eta 0:04:44
epoch [19/50] batch [15/23] time 0.181 (0.328) data 0.000 (0.083) loss 0.7481 (0.6596) acc 76.6667 (78.9654) lr 1.4818e-03 eta 0:03:56
epoch [19/50] batch [20/23] time 0.185 (0.292) data 0.000 (0.062) loss 0.7527 (0.6603) acc 73.4043 (78.3732) lr 1.4818e-03 eta 0:03:28
>>> alpha1: 0.285  alpha2: -0.142 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.16 <<<
epoch [20/50] batch [5/23] time 0.175 (0.464) data 0.000 (0.282) loss 0.4822 (0.6607) acc 80.9783 (78.2173) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [10/23] time 0.173 (0.332) data 0.000 (0.141) loss 0.6519 (0.6467) acc 80.1136 (78.2203) lr 1.4258e-03 eta 0:03:53
epoch [20/50] batch [15/23] time 0.188 (0.284) data 0.000 (0.094) loss 0.5659 (0.6393) acc 80.1020 (78.3734) lr 1.4258e-03 eta 0:03:18
epoch [20/50] batch [20/23] time 0.227 (0.303) data 0.000 (0.071) loss 0.5265 (0.6864) acc 84.4445 (77.0930) lr 1.4258e-03 eta 0:03:29
>>> alpha1: 0.276  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [21/50] batch [5/23] time 0.205 (0.477) data 0.000 (0.285) loss 0.8727 (0.6211) acc 74.4792 (80.7150) lr 1.3681e-03 eta 0:05:26
epoch [21/50] batch [10/23] time 0.191 (0.335) data 0.000 (0.143) loss 0.8298 (0.6059) acc 70.9184 (79.3332) lr 1.3681e-03 eta 0:03:48
epoch [21/50] batch [15/23] time 0.187 (0.284) data 0.000 (0.095) loss 0.3375 (0.6013) acc 91.1458 (79.7558) lr 1.3681e-03 eta 0:03:11
epoch [21/50] batch [20/23] time 0.183 (0.262) data 0.001 (0.071) loss 0.5658 (0.6312) acc 85.6383 (78.3773) lr 1.3681e-03 eta 0:02:55
>>> alpha1: 0.273  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.14 <<<
epoch [22/50] batch [5/23] time 0.136 (0.387) data 0.000 (0.241) loss 0.6051 (0.6349) acc 77.7174 (79.0035) lr 1.3090e-03 eta 0:04:16
epoch [22/50] batch [10/23] time 0.157 (0.269) data 0.000 (0.121) loss 0.7709 (0.6193) acc 75.0000 (78.9233) lr 1.3090e-03 eta 0:02:56
epoch [22/50] batch [15/23] time 0.234 (0.248) data 0.001 (0.081) loss 0.7564 (0.6374) acc 75.0000 (78.5202) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [20/23] time 0.242 (0.248) data 0.001 (0.061) loss 0.7783 (0.6340) acc 68.3333 (78.5647) lr 1.3090e-03 eta 0:02:40
>>> alpha1: 0.267  alpha2: -0.124 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.16 <<<
epoch [23/50] batch [5/23] time 0.197 (0.475) data 0.000 (0.270) loss 0.4210 (0.5493) acc 82.3529 (79.4294) lr 1.2487e-03 eta 0:05:03
epoch [23/50] batch [10/23] time 0.190 (0.333) data 0.000 (0.135) loss 0.8180 (0.6009) acc 76.5625 (79.3416) lr 1.2487e-03 eta 0:03:31
epoch [23/50] batch [15/23] time 0.255 (0.290) data 0.001 (0.090) loss 0.4836 (0.6046) acc 79.5918 (79.2764) lr 1.2487e-03 eta 0:03:02
epoch [23/50] batch [20/23] time 0.191 (0.265) data 0.000 (0.068) loss 0.6409 (0.6113) acc 83.5000 (79.4506) lr 1.2487e-03 eta 0:02:45
>>> alpha1: 0.261  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [24/50] batch [5/23] time 0.208 (0.484) data 0.000 (0.274) loss 0.6782 (0.6330) acc 83.0189 (80.8885) lr 1.1874e-03 eta 0:04:57
epoch [24/50] batch [10/23] time 0.212 (0.341) data 0.000 (0.137) loss 0.6689 (0.6426) acc 75.9091 (79.7446) lr 1.1874e-03 eta 0:03:28
epoch [24/50] batch [15/23] time 0.197 (0.297) data 0.000 (0.092) loss 0.5927 (0.6402) acc 81.8627 (79.1580) lr 1.1874e-03 eta 0:03:00
epoch [24/50] batch [20/23] time 0.189 (0.272) data 0.001 (0.069) loss 0.4634 (0.6151) acc 83.0000 (79.0804) lr 1.1874e-03 eta 0:02:43
>>> alpha1: 0.252  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [25/50] batch [5/23] time 0.178 (0.507) data 0.000 (0.316) loss 0.8647 (0.5884) acc 66.0714 (79.6043) lr 1.1253e-03 eta 0:05:00
epoch [25/50] batch [10/23] time 0.187 (0.355) data 0.000 (0.158) loss 0.6884 (0.5900) acc 78.8043 (80.1583) lr 1.1253e-03 eta 0:03:28
epoch [25/50] batch [15/23] time 0.168 (0.297) data 0.001 (0.106) loss 0.6840 (0.5852) acc 70.8333 (79.8880) lr 1.1253e-03 eta 0:02:53
epoch [25/50] batch [20/23] time 0.184 (0.269) data 0.000 (0.079) loss 0.5294 (0.5793) acc 83.5106 (80.2900) lr 1.1253e-03 eta 0:02:35
>>> alpha1: 0.241  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.19 <<<
epoch [26/50] batch [5/23] time 0.203 (0.476) data 0.001 (0.275) loss 0.4034 (0.5555) acc 89.5833 (83.2208) lr 1.0628e-03 eta 0:04:31
epoch [26/50] batch [10/23] time 1.281 (0.454) data 0.001 (0.138) loss 0.6104 (0.5219) acc 80.2632 (83.6458) lr 1.0628e-03 eta 0:04:16
epoch [26/50] batch [15/23] time 0.195 (0.370) data 0.001 (0.092) loss 0.7562 (0.5350) acc 71.5000 (82.4858) lr 1.0628e-03 eta 0:03:27
epoch [26/50] batch [20/23] time 0.186 (0.329) data 0.001 (0.069) loss 0.7433 (0.5602) acc 73.4043 (81.8664) lr 1.0628e-03 eta 0:03:02
>>> alpha1: 0.229  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.16 <<<
epoch [27/50] batch [5/23] time 0.197 (0.463) data 0.000 (0.260) loss 0.5215 (0.5881) acc 88.5870 (80.7430) lr 1.0000e-03 eta 0:04:13
epoch [27/50] batch [10/23] time 0.161 (0.324) data 0.000 (0.130) loss 0.6953 (0.5438) acc 73.7805 (82.8391) lr 1.0000e-03 eta 0:02:55
epoch [27/50] batch [15/23] time 0.176 (0.277) data 0.000 (0.087) loss 0.5234 (0.5560) acc 81.6667 (82.3366) lr 1.0000e-03 eta 0:02:28
epoch [27/50] batch [20/23] time 0.197 (0.258) data 0.000 (0.065) loss 0.3564 (0.5411) acc 88.2979 (82.2447) lr 1.0000e-03 eta 0:02:17
>>> alpha1: 0.221  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.18 <<<
epoch [28/50] batch [5/23] time 0.179 (0.469) data 0.000 (0.289) loss 2.7515 (0.9532) acc 45.1087 (76.2052) lr 9.3721e-04 eta 0:04:05
epoch [28/50] batch [10/23] time 0.182 (0.326) data 0.001 (0.145) loss 0.5501 (0.8738) acc 77.6596 (79.3526) lr 9.3721e-04 eta 0:02:49
epoch [28/50] batch [15/23] time 0.189 (0.278) data 0.000 (0.097) loss 0.5829 (0.7734) acc 83.0000 (80.2563) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [20/23] time 0.180 (0.258) data 0.000 (0.073) loss 0.4793 (0.7008) acc 86.4583 (81.2104) lr 9.3721e-04 eta 0:02:11
>>> alpha1: 0.220  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.20 <<<
epoch [29/50] batch [5/23] time 0.256 (0.573) data 0.000 (0.316) loss 0.3303 (0.4290) acc 89.9038 (86.7406) lr 8.7467e-04 eta 0:04:47
epoch [29/50] batch [10/23] time 0.246 (0.413) data 0.000 (0.158) loss 0.4990 (0.4731) acc 81.9149 (84.2765) lr 8.7467e-04 eta 0:03:25
epoch [29/50] batch [15/23] time 0.142 (0.346) data 0.000 (0.106) loss 0.3836 (0.4746) acc 80.6122 (84.3992) lr 8.7467e-04 eta 0:02:50
epoch [29/50] batch [20/23] time 0.144 (0.298) data 0.001 (0.079) loss 0.4748 (0.4852) acc 80.5000 (84.0008) lr 8.7467e-04 eta 0:02:24
>>> alpha1: 0.212  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [30/50] batch [5/23] time 0.172 (0.459) data 0.000 (0.265) loss 0.4187 (0.4663) acc 87.2093 (84.8925) lr 8.1262e-04 eta 0:03:39
epoch [30/50] batch [10/23] time 0.182 (0.320) data 0.000 (0.133) loss 0.7150 (0.4919) acc 73.9130 (84.2220) lr 8.1262e-04 eta 0:02:31
epoch [30/50] batch [15/23] time 0.168 (0.281) data 0.000 (0.089) loss 0.6359 (0.4893) acc 82.1429 (85.0641) lr 8.1262e-04 eta 0:02:11
epoch [30/50] batch [20/23] time 0.198 (0.258) data 0.000 (0.067) loss 0.5400 (0.5013) acc 79.5000 (84.1134) lr 8.1262e-04 eta 0:01:59
>>> alpha1: 0.208  alpha2: -0.123 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [31/50] batch [5/23] time 0.179 (0.497) data 0.000 (0.304) loss 0.3261 (0.3815) acc 90.2174 (87.8647) lr 7.5131e-04 eta 0:03:46
epoch [31/50] batch [10/23] time 0.175 (0.337) data 0.001 (0.152) loss 0.4237 (0.4140) acc 82.2222 (86.2403) lr 7.5131e-04 eta 0:02:31
epoch [31/50] batch [15/23] time 0.200 (0.293) data 0.000 (0.102) loss 0.6705 (0.4521) acc 72.1154 (85.1651) lr 7.5131e-04 eta 0:02:10
epoch [31/50] batch [20/23] time 0.187 (0.266) data 0.000 (0.076) loss 0.5043 (0.4691) acc 84.3750 (85.1392) lr 7.5131e-04 eta 0:01:57
>>> alpha1: 0.210  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [32/50] batch [5/23] time 0.192 (0.457) data 0.001 (0.258) loss 0.4518 (0.4255) acc 81.6667 (85.9144) lr 6.9098e-04 eta 0:03:17
epoch [32/50] batch [10/23] time 0.152 (0.324) data 0.000 (0.129) loss 0.3055 (0.4043) acc 91.4894 (87.2895) lr 6.9098e-04 eta 0:02:18
epoch [32/50] batch [15/23] time 0.128 (0.263) data 0.000 (0.086) loss 0.5418 (0.4227) acc 80.8139 (86.9166) lr 6.9098e-04 eta 0:01:50
epoch [32/50] batch [20/23] time 0.138 (0.230) data 0.000 (0.065) loss 0.5284 (0.4387) acc 84.0425 (86.6572) lr 6.9098e-04 eta 0:01:35
>>> alpha1: 0.209  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [33/50] batch [5/23] time 0.229 (0.500) data 0.000 (0.294) loss 0.5903 (0.5223) acc 79.1667 (83.3644) lr 6.3188e-04 eta 0:03:24
epoch [33/50] batch [10/23] time 0.168 (0.342) data 0.000 (0.147) loss 0.6811 (0.5235) acc 78.6585 (83.5061) lr 6.3188e-04 eta 0:02:18
epoch [33/50] batch [15/23] time 0.196 (0.289) data 0.000 (0.098) loss 0.3528 (0.4887) acc 94.1176 (84.8005) lr 6.3188e-04 eta 0:01:55
epoch [33/50] batch [20/23] time 0.188 (0.263) data 0.001 (0.074) loss 0.4156 (0.4777) acc 91.6667 (85.7319) lr 6.3188e-04 eta 0:01:43
>>> alpha1: 0.204  alpha2: -0.139 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.16 <<<
epoch [34/50] batch [5/23] time 0.203 (0.515) data 0.000 (0.310) loss 0.2685 (0.4578) acc 88.9423 (85.1941) lr 5.7422e-04 eta 0:03:18
epoch [34/50] batch [10/23] time 0.185 (0.350) data 0.000 (0.155) loss 0.5237 (0.4621) acc 86.1702 (84.8347) lr 5.7422e-04 eta 0:02:13
epoch [34/50] batch [15/23] time 0.176 (0.294) data 0.000 (0.104) loss 0.3626 (0.4841) acc 86.6667 (84.0531) lr 5.7422e-04 eta 0:01:50
epoch [34/50] batch [20/23] time 0.190 (0.266) data 0.000 (0.078) loss 0.5002 (0.4724) acc 87.2449 (84.9077) lr 5.7422e-04 eta 0:01:38
>>> alpha1: 0.198  alpha2: -0.144 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.18 <<<
epoch [35/50] batch [5/23] time 0.192 (0.478) data 0.000 (0.277) loss 0.6047 (0.4527) acc 85.7143 (86.4545) lr 5.1825e-04 eta 0:02:53
epoch [35/50] batch [10/23] time 0.195 (0.335) data 0.000 (0.139) loss 0.4379 (0.4379) acc 81.8627 (86.2691) lr 5.1825e-04 eta 0:02:00
epoch [35/50] batch [15/23] time 0.194 (0.289) data 0.000 (0.093) loss 0.3021 (0.4293) acc 93.0000 (86.8978) lr 5.1825e-04 eta 0:01:41
epoch [35/50] batch [20/23] time 0.202 (0.269) data 0.000 (0.070) loss 0.4565 (0.4307) acc 78.3654 (86.5682) lr 5.1825e-04 eta 0:01:33
>>> alpha1: 0.193  alpha2: -0.146 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.17 <<<
epoch [36/50] batch [5/23] time 0.198 (0.541) data 0.001 (0.348) loss 0.3863 (0.4282) acc 87.5000 (85.3726) lr 4.6417e-04 eta 0:03:03
epoch [36/50] batch [10/23] time 0.167 (0.358) data 0.000 (0.174) loss 0.4826 (0.4615) acc 86.1111 (85.2459) lr 4.6417e-04 eta 0:02:00
epoch [36/50] batch [15/23] time 0.185 (0.298) data 0.001 (0.116) loss 0.4229 (0.5945) acc 93.8775 (83.9064) lr 4.6417e-04 eta 0:01:38
epoch [36/50] batch [20/23] time 0.166 (0.268) data 0.000 (0.087) loss 0.3291 (0.5582) acc 93.0233 (84.3355) lr 4.6417e-04 eta 0:01:26
>>> alpha1: 0.188  alpha2: -0.141 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.21 <<<
epoch [37/50] batch [5/23] time 0.210 (0.516) data 0.000 (0.303) loss 0.3613 (0.4119) acc 83.8235 (88.0078) lr 4.1221e-04 eta 0:02:43
epoch [37/50] batch [10/23] time 0.207 (0.358) data 0.000 (0.152) loss 0.3515 (0.3981) acc 87.7358 (88.1833) lr 4.1221e-04 eta 0:01:51
epoch [37/50] batch [15/23] time 0.199 (0.304) data 0.000 (0.101) loss 0.4401 (0.4024) acc 87.5000 (87.5605) lr 4.1221e-04 eta 0:01:33
epoch [37/50] batch [20/23] time 0.191 (0.281) data 0.000 (0.076) loss 0.3944 (0.4110) acc 86.5000 (87.0969) lr 4.1221e-04 eta 0:01:24
>>> alpha1: 0.188  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.22 <<<
epoch [38/50] batch [5/23] time 0.203 (0.481) data 0.000 (0.269) loss 0.2353 (0.3884) acc 93.7500 (89.4692) lr 3.6258e-04 eta 0:02:21
epoch [38/50] batch [10/23] time 0.207 (0.344) data 0.000 (0.135) loss 0.2832 (0.4131) acc 94.8113 (88.1697) lr 3.6258e-04 eta 0:01:39
epoch [38/50] batch [15/23] time 0.198 (0.300) data 0.000 (0.090) loss 0.5907 (0.4253) acc 83.6538 (87.4327) lr 3.6258e-04 eta 0:01:25
epoch [38/50] batch [20/23] time 0.193 (0.274) data 0.000 (0.068) loss 0.4823 (0.4182) acc 85.5000 (87.2506) lr 3.6258e-04 eta 0:01:16
>>> alpha1: 0.184  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.22 <<<
epoch [39/50] batch [5/23] time 0.318 (0.601) data 0.000 (0.279) loss 0.2036 (0.3685) acc 93.8679 (89.9025) lr 3.1545e-04 eta 0:02:42
epoch [39/50] batch [10/23] time 0.218 (0.443) data 0.001 (0.140) loss 0.3640 (0.3918) acc 92.3469 (87.9036) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [15/23] time 0.150 (0.346) data 0.000 (0.093) loss 0.5579 (0.4131) acc 84.6154 (87.0522) lr 3.1545e-04 eta 0:01:30
epoch [39/50] batch [20/23] time 0.143 (0.297) data 0.000 (0.070) loss 0.5071 (0.4160) acc 86.7347 (86.6149) lr 3.1545e-04 eta 0:01:16
>>> alpha1: 0.183  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.21 <<<
epoch [40/50] batch [5/23] time 0.216 (0.540) data 0.000 (0.328) loss 0.5223 (0.4503) acc 83.9623 (87.7447) lr 2.7103e-04 eta 0:02:13
epoch [40/50] batch [10/23] time 0.192 (0.370) data 0.000 (0.164) loss 0.4177 (0.4222) acc 89.2857 (87.8388) lr 2.7103e-04 eta 0:01:29
epoch [40/50] batch [15/23] time 0.194 (0.316) data 0.000 (0.110) loss 0.3106 (0.4103) acc 85.2941 (87.6011) lr 2.7103e-04 eta 0:01:15
epoch [40/50] batch [20/23] time 0.202 (0.287) data 0.000 (0.082) loss 0.5134 (0.4234) acc 88.9423 (86.4592) lr 2.7103e-04 eta 0:01:06
>>> alpha1: 0.181  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [41/50] batch [5/23] time 0.193 (0.499) data 0.000 (0.301) loss 0.4949 (0.4085) acc 81.3830 (88.4222) lr 2.2949e-04 eta 0:01:52
epoch [41/50] batch [10/23] time 0.176 (0.440) data 0.000 (0.151) loss 0.4591 (0.4133) acc 84.4445 (86.6850) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [15/23] time 0.181 (0.356) data 0.000 (0.100) loss 0.3797 (0.5338) acc 86.4130 (85.4975) lr 2.2949e-04 eta 0:01:16
epoch [41/50] batch [20/23] time 0.191 (0.315) data 0.000 (0.075) loss 0.6522 (0.5104) acc 85.2041 (86.1127) lr 2.2949e-04 eta 0:01:06
>>> alpha1: 0.180  alpha2: -0.118 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [42/50] batch [5/23] time 0.222 (0.434) data 0.000 (0.229) loss 0.4591 (0.3845) acc 85.3261 (87.1699) lr 1.9098e-04 eta 0:01:27
epoch [42/50] batch [10/23] time 0.189 (0.309) data 0.000 (0.114) loss 0.3962 (0.3953) acc 89.0625 (88.1987) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [15/23] time 0.191 (0.267) data 0.000 (0.076) loss 0.3985 (0.3964) acc 86.7347 (88.2963) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [20/23] time 0.184 (0.246) data 0.000 (0.057) loss 0.3820 (0.4051) acc 82.9787 (87.3643) lr 1.9098e-04 eta 0:00:46
>>> alpha1: 0.177  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [43/50] batch [5/23] time 0.195 (0.482) data 0.000 (0.286) loss 0.3433 (0.4399) acc 88.0435 (87.1673) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [10/23] time 0.196 (0.337) data 0.000 (0.143) loss 0.4091 (0.3898) acc 90.1961 (89.6067) lr 1.5567e-04 eta 0:00:58
epoch [43/50] batch [15/23] time 0.195 (0.290) data 0.000 (0.096) loss 0.3537 (0.3863) acc 85.1064 (88.8867) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [20/23] time 0.189 (0.264) data 0.000 (0.072) loss 0.3597 (0.3951) acc 85.0000 (88.3416) lr 1.5567e-04 eta 0:00:43
>>> alpha1: 0.179  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.19 <<<
epoch [44/50] batch [5/23] time 0.184 (0.471) data 0.001 (0.281) loss 0.6154 (0.4052) acc 75.5435 (86.6582) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [10/23] time 0.176 (0.332) data 0.000 (0.141) loss 0.3943 (0.4684) acc 85.5556 (87.2194) lr 1.2369e-04 eta 0:00:50
epoch [44/50] batch [15/23] time 0.194 (0.287) data 0.001 (0.094) loss 0.3164 (0.4287) acc 92.1569 (88.0696) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [20/23] time 0.191 (0.261) data 0.000 (0.070) loss 0.4535 (0.4382) acc 88.7755 (87.5097) lr 1.2369e-04 eta 0:00:36
>>> alpha1: 0.175  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.17 <<<
epoch [45/50] batch [5/23] time 0.216 (0.503) data 0.000 (0.302) loss 0.3606 (0.3835) acc 89.0000 (87.0613) lr 9.5173e-05 eta 0:01:06
epoch [45/50] batch [10/23] time 0.193 (0.350) data 0.001 (0.151) loss 0.3485 (0.3917) acc 91.3265 (88.0563) lr 9.5173e-05 eta 0:00:44
epoch [45/50] batch [15/23] time 0.179 (0.297) data 0.000 (0.101) loss 0.4819 (0.3940) acc 86.1111 (87.6419) lr 9.5173e-05 eta 0:00:36
epoch [45/50] batch [20/23] time 0.179 (0.269) data 0.000 (0.076) loss 0.4438 (0.3970) acc 86.9565 (87.8090) lr 9.5173e-05 eta 0:00:31
>>> alpha1: 0.173  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.17 <<<
epoch [46/50] batch [5/23] time 0.166 (0.491) data 0.000 (0.326) loss 0.3334 (0.3963) acc 92.1569 (86.6383) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [10/23] time 0.130 (0.317) data 0.001 (0.163) loss 0.5962 (0.3954) acc 84.3023 (87.8487) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [15/23] time 0.167 (0.269) data 0.000 (0.109) loss 0.4054 (0.3910) acc 91.8478 (88.2764) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [20/23] time 0.174 (0.245) data 0.000 (0.082) loss 0.4723 (0.3869) acc 78.8889 (88.0986) lr 7.0224e-05 eta 0:00:23
>>> alpha1: 0.174  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [47/50] batch [5/23] time 0.177 (0.483) data 0.000 (0.272) loss 0.4155 (0.4279) acc 87.7778 (87.9676) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [10/23] time 0.173 (0.331) data 0.000 (0.136) loss 0.4267 (0.4230) acc 89.2045 (87.5348) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [15/23] time 0.189 (0.283) data 0.000 (0.091) loss 0.3956 (0.4064) acc 85.9375 (88.2563) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [20/23] time 0.229 (0.263) data 0.000 (0.068) loss 0.3529 (0.4066) acc 89.7059 (88.1536) lr 4.8943e-05 eta 0:00:18
>>> alpha1: 0.175  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.17 <<<
epoch [48/50] batch [5/23] time 0.199 (0.477) data 0.000 (0.279) loss 0.5653 (0.4020) acc 84.5745 (88.5331) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [10/23] time 0.190 (0.333) data 0.000 (0.140) loss 0.4827 (0.3867) acc 87.2449 (88.8297) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [15/23] time 0.171 (0.284) data 0.000 (0.093) loss 0.3241 (0.3664) acc 88.3721 (88.9208) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.192 (0.263) data 0.000 (0.070) loss 0.4462 (0.3823) acc 88.2653 (88.2892) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.172  alpha2: -0.115 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.18 <<<
epoch [49/50] batch [5/23] time 0.163 (0.420) data 0.000 (0.269) loss 0.3199 (0.3450) acc 84.4445 (88.7787) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.251 (0.318) data 0.000 (0.135) loss 0.3784 (0.3525) acc 90.3409 (90.0752) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.256 (0.302) data 0.000 (0.090) loss 0.5754 (0.3842) acc 74.4186 (88.5305) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [20/23] time 0.266 (0.296) data 0.000 (0.068) loss 0.3669 (0.3781) acc 90.1163 (89.0082) lr 1.7713e-05 eta 0:00:07
>>> alpha1: 0.170  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [50/50] batch [5/23] time 0.184 (0.469) data 0.000 (0.279) loss 0.4245 (0.4257) acc 91.4894 (89.0857) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.188 (0.329) data 0.000 (0.140) loss 0.4324 (0.4004) acc 84.0425 (88.1271) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.183 (0.282) data 0.000 (0.093) loss 0.3694 (0.4037) acc 89.3617 (87.1914) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.185 (0.259) data 0.000 (0.070) loss 0.3240 (0.3854) acc 90.3061 (87.9387) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.13, 0.11, 0.1, 0.1, 0.09, 0.1, 0.09, 0.1, 0.11, 0.11, 0.11, 0.1, 0.11, 0.11, 0.11, 0.12, 0.11, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.13, 0.12, 0.13, 0.12, 0.12, 0.12, 0.12, 0.13, 0.12, 0.12, 0.12, 0.12, 0.13, 0.12]
* matched noise rate: [0.05, 0.05, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.06, 0.07, 0.06, 0.06, 0.06, 0.05, 0.05, 0.07, 0.06, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.06, 0.07, 0.06, 0.06, 0.06, 0.06, 0.06, 0.07, 0.07, 0.06, 0.07, 0.07, 0.06]
* unmatched noise rate: [0.22, 0.18, 0.16, 0.15, 0.14, 0.16, 0.14, 0.14, 0.16, 0.16, 0.17, 0.14, 0.16, 0.17, 0.15, 0.19, 0.16, 0.18, 0.2, 0.17, 0.17, 0.17, 0.17, 0.16, 0.18, 0.17, 0.21, 0.22, 0.22, 0.21, 0.17, 0.18, 0.18, 0.19, 0.17, 0.17, 0.18, 0.17, 0.18, 0.18]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:44,  2.75s/it] 12%|█▏        | 2/17 [00:02<00:18,  1.21s/it] 24%|██▎       | 4/17 [00:03<00:06,  1.96it/s] 35%|███▌      | 6/17 [00:03<00:03,  3.21it/s] 47%|████▋     | 8/17 [00:03<00:02,  4.47it/s] 59%|█████▉    | 10/17 [00:03<00:01,  5.67it/s] 71%|███████   | 12/17 [00:03<00:00,  6.80it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.79it/s] 94%|█████████▍| 16/17 [00:04<00:00,  8.57it/s]100%|██████████| 17/17 [00:05<00:00,  3.37it/s]
=> result
* total: 1,692
* correct: 1,033
* accuracy: 61.1%
* error: 38.9%
* macro_f1: 60.4%
=> per-class result
* class: 0 (banded)	total: 36	correct: 25	acc: 69.4%
* class: 1 (blotchy)	total: 36	correct: 10	acc: 27.8%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 29	acc: 80.6%
* class: 4 (bumpy)	total: 36	correct: 11	acc: 30.6%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 28	acc: 77.8%
* class: 7 (cracked)	total: 36	correct: 27	acc: 75.0%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 32	acc: 88.9%
* class: 10 (dotted)	total: 36	correct: 17	acc: 47.2%
* class: 11 (fibrous)	total: 36	correct: 25	acc: 69.4%
* class: 12 (flecked)	total: 36	correct: 17	acc: 47.2%
* class: 13 (freckled)	total: 36	correct: 28	acc: 77.8%
* class: 14 (frilly)	total: 36	correct: 29	acc: 80.6%
* class: 15 (gauzy)	total: 36	correct: 19	acc: 52.8%
* class: 16 (grid)	total: 36	correct: 17	acc: 47.2%
* class: 17 (grooved)	total: 36	correct: 18	acc: 50.0%
* class: 18 (honeycombed)	total: 36	correct: 25	acc: 69.4%
* class: 19 (interlaced)	total: 36	correct: 26	acc: 72.2%
* class: 20 (knitted)	total: 36	correct: 23	acc: 63.9%
* class: 21 (lacelike)	total: 36	correct: 34	acc: 94.4%
* class: 22 (lined)	total: 36	correct: 12	acc: 33.3%
* class: 23 (marbled)	total: 36	correct: 18	acc: 50.0%
* class: 24 (matted)	total: 36	correct: 20	acc: 55.6%
* class: 25 (meshed)	total: 36	correct: 17	acc: 47.2%
* class: 26 (paisley)	total: 36	correct: 33	acc: 91.7%
* class: 27 (perforated)	total: 36	correct: 20	acc: 55.6%
* class: 28 (pitted)	total: 36	correct: 13	acc: 36.1%
* class: 29 (pleated)	total: 36	correct: 18	acc: 50.0%
* class: 30 (polka-dotted)	total: 36	correct: 28	acc: 77.8%
* class: 31 (porous)	total: 36	correct: 9	acc: 25.0%
* class: 32 (potholed)	total: 36	correct: 32	acc: 88.9%
* class: 33 (scaly)	total: 36	correct: 23	acc: 63.9%
* class: 34 (smeared)	total: 36	correct: 9	acc: 25.0%
* class: 35 (spiralled)	total: 36	correct: 22	acc: 61.1%
* class: 36 (sprinkled)	total: 36	correct: 15	acc: 41.7%
* class: 37 (stained)	total: 36	correct: 9	acc: 25.0%
* class: 38 (stratified)	total: 36	correct: 30	acc: 83.3%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 28	acc: 77.8%
* class: 42 (veined)	total: 36	correct: 23	acc: 63.9%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 21	acc: 58.3%
* class: 45 (wrinkled)	total: 36	correct: 24	acc: 66.7%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 61.1%
Elapsed: 0:16:24
Run this job and save the output to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_4-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.212 (1.083) data 0.000 (0.408) loss 3.4894 (3.5325) acc 18.7500 (18.1250) lr 1.0000e-05 eta 0:20:40
epoch [1/50] batch [10/23] time 0.201 (0.655) data 0.000 (0.204) loss 3.6324 (3.5870) acc 6.2500 (15.0000) lr 1.0000e-05 eta 0:12:26
epoch [1/50] batch [15/23] time 0.207 (0.505) data 0.000 (0.136) loss 3.3331 (3.5595) acc 28.1250 (15.6250) lr 1.0000e-05 eta 0:09:32
epoch [1/50] batch [20/23] time 0.199 (0.429) data 0.000 (0.102) loss 3.3755 (3.5282) acc 21.8750 (16.7188) lr 1.0000e-05 eta 0:08:04
epoch [2/50] batch [5/23] time 0.273 (0.468) data 0.000 (0.226) loss 3.3382 (3.4787) acc 21.8750 (14.3750) lr 2.0000e-03 eta 0:08:45
epoch [2/50] batch [10/23] time 0.204 (0.337) data 0.000 (0.113) loss 2.9181 (3.2450) acc 25.0000 (22.8125) lr 2.0000e-03 eta 0:06:16
epoch [2/50] batch [15/23] time 0.204 (0.293) data 0.000 (0.076) loss 3.3957 (3.1815) acc 21.8750 (24.3750) lr 2.0000e-03 eta 0:05:25
epoch [2/50] batch [20/23] time 0.253 (0.273) data 0.000 (0.057) loss 2.7121 (3.1313) acc 43.7500 (26.2500) lr 2.0000e-03 eta 0:05:02
epoch [3/50] batch [5/23] time 0.214 (0.475) data 0.000 (0.238) loss 2.7739 (2.8318) acc 34.3750 (32.5000) lr 1.9980e-03 eta 0:08:42
epoch [3/50] batch [10/23] time 0.206 (0.343) data 0.000 (0.119) loss 3.4349 (2.8405) acc 25.0000 (33.7500) lr 1.9980e-03 eta 0:06:15
epoch [3/50] batch [15/23] time 0.209 (0.297) data 0.000 (0.080) loss 2.4068 (2.7820) acc 50.0000 (35.6250) lr 1.9980e-03 eta 0:05:23
epoch [3/50] batch [20/23] time 0.203 (0.277) data 0.000 (0.060) loss 3.1376 (2.8154) acc 25.0000 (34.2188) lr 1.9980e-03 eta 0:05:00
epoch [4/50] batch [5/23] time 0.211 (0.500) data 0.000 (0.271) loss 2.2635 (2.7068) acc 56.2500 (41.8750) lr 1.9921e-03 eta 0:08:58
epoch [4/50] batch [10/23] time 0.206 (0.355) data 0.000 (0.136) loss 2.8009 (2.7374) acc 46.8750 (39.6875) lr 1.9921e-03 eta 0:06:19
epoch [4/50] batch [15/23] time 0.192 (0.304) data 0.000 (0.090) loss 2.4559 (2.6741) acc 50.0000 (40.8333) lr 1.9921e-03 eta 0:05:24
epoch [4/50] batch [20/23] time 0.151 (0.266) data 0.000 (0.068) loss 2.5230 (2.6647) acc 46.8750 (40.9375) lr 1.9921e-03 eta 0:04:42
epoch [5/50] batch [5/23] time 0.318 (0.607) data 0.004 (0.281) loss 2.3246 (2.5703) acc 43.7500 (40.6250) lr 1.9823e-03 eta 0:10:39
epoch [5/50] batch [10/23] time 0.320 (0.462) data 0.000 (0.140) loss 2.2469 (2.5339) acc 46.8750 (43.4375) lr 1.9823e-03 eta 0:08:03
epoch [5/50] batch [15/23] time 0.294 (0.410) data 0.000 (0.094) loss 2.4839 (2.5325) acc 34.3750 (43.7500) lr 1.9823e-03 eta 0:07:07
epoch [5/50] batch [20/23] time 0.156 (0.346) data 0.000 (0.070) loss 2.4887 (2.5447) acc 46.8750 (42.5000) lr 1.9823e-03 eta 0:05:59
epoch [6/50] batch [5/23] time 0.289 (0.622) data 0.000 (0.313) loss 2.4002 (2.6855) acc 46.8750 (38.7500) lr 1.9686e-03 eta 0:10:40
epoch [6/50] batch [10/23] time 0.204 (0.451) data 0.000 (0.156) loss 2.4489 (2.5085) acc 37.5000 (41.8750) lr 1.9686e-03 eta 0:07:42
epoch [6/50] batch [15/23] time 0.150 (0.351) data 0.000 (0.104) loss 2.1704 (2.4718) acc 53.1250 (44.3750) lr 1.9686e-03 eta 0:05:58
epoch [6/50] batch [20/23] time 0.153 (0.302) data 0.000 (0.078) loss 2.7592 (2.4944) acc 46.8750 (44.5312) lr 1.9686e-03 eta 0:05:06
epoch [7/50] batch [5/23] time 0.201 (0.523) data 0.000 (0.287) loss 2.0983 (2.4245) acc 50.0000 (43.1250) lr 1.9511e-03 eta 0:08:47
epoch [7/50] batch [10/23] time 0.210 (0.377) data 0.000 (0.144) loss 2.2750 (2.4039) acc 56.2500 (45.6250) lr 1.9511e-03 eta 0:06:17
epoch [7/50] batch [15/23] time 0.207 (0.320) data 0.000 (0.096) loss 2.6795 (2.4576) acc 46.8750 (45.0000) lr 1.9511e-03 eta 0:05:19
epoch [7/50] batch [20/23] time 0.209 (0.292) data 0.000 (0.072) loss 3.0890 (2.4558) acc 25.0000 (45.7812) lr 1.9511e-03 eta 0:04:49
epoch [8/50] batch [5/23] time 0.225 (0.498) data 0.000 (0.270) loss 2.7256 (2.5355) acc 40.6250 (41.8750) lr 1.9298e-03 eta 0:08:10
epoch [8/50] batch [10/23] time 0.200 (0.360) data 0.000 (0.135) loss 2.9274 (2.6016) acc 34.3750 (40.6250) lr 1.9298e-03 eta 0:05:52
epoch [8/50] batch [15/23] time 0.204 (0.307) data 0.000 (0.090) loss 2.0936 (2.4854) acc 56.2500 (43.9583) lr 1.9298e-03 eta 0:04:59
epoch [8/50] batch [20/23] time 0.207 (0.282) data 0.000 (0.068) loss 2.2306 (2.4099) acc 50.0000 (45.4688) lr 1.9298e-03 eta 0:04:33
epoch [9/50] batch [5/23] time 0.222 (0.501) data 0.000 (0.258) loss 2.3035 (2.2776) acc 53.1250 (49.3750) lr 1.9048e-03 eta 0:08:01
epoch [9/50] batch [10/23] time 0.205 (0.355) data 0.000 (0.129) loss 2.2644 (2.2982) acc 40.6250 (46.5625) lr 1.9048e-03 eta 0:05:39
epoch [9/50] batch [15/23] time 0.205 (0.304) data 0.000 (0.086) loss 1.8335 (2.2705) acc 59.3750 (48.3333) lr 1.9048e-03 eta 0:04:49
epoch [9/50] batch [20/23] time 0.220 (0.283) data 0.000 (0.065) loss 2.6182 (2.3236) acc 34.3750 (46.5625) lr 1.9048e-03 eta 0:04:27
epoch [10/50] batch [5/23] time 0.205 (0.502) data 0.000 (0.261) loss 2.2724 (2.3099) acc 43.7500 (49.3750) lr 1.8763e-03 eta 0:07:50
epoch [10/50] batch [10/23] time 0.206 (0.359) data 0.000 (0.131) loss 2.2733 (2.3676) acc 59.3750 (48.4375) lr 1.8763e-03 eta 0:05:34
epoch [10/50] batch [15/23] time 0.207 (0.308) data 0.000 (0.087) loss 1.8676 (2.2739) acc 53.1250 (49.5833) lr 1.8763e-03 eta 0:04:45
epoch [10/50] batch [20/23] time 0.207 (0.286) data 0.000 (0.065) loss 2.9079 (2.3586) acc 31.2500 (47.9688) lr 1.8763e-03 eta 0:04:23
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.744  alpha2: 0.120 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.33 <<<
epoch [11/50] batch [5/23] time 0.196 (1.267) data 0.001 (0.309) loss 1.3194 (1.3534) acc 70.6731 (70.1945) lr 1.8443e-03 eta 0:19:19
epoch [11/50] batch [10/23] time 0.171 (0.917) data 0.000 (0.154) loss 1.0778 (1.4210) acc 72.7273 (66.0180) lr 1.8443e-03 eta 0:13:54
epoch [11/50] batch [15/23] time 0.186 (0.825) data 0.000 (0.103) loss 1.3636 (1.4032) acc 55.6122 (63.9667) lr 1.8443e-03 eta 0:12:26
epoch [11/50] batch [20/23] time 0.181 (0.710) data 0.000 (0.077) loss 1.2569 (1.4017) acc 64.5833 (63.2632) lr 1.8443e-03 eta 0:10:38
>>> alpha1: 0.577  alpha2: 0.121 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/23] time 0.260 (0.747) data 0.000 (0.295) loss 0.9950 (1.0552) acc 75.0000 (68.7785) lr 1.8090e-03 eta 0:11:06
epoch [12/50] batch [10/23] time 0.251 (0.493) data 0.000 (0.148) loss 1.2523 (1.1446) acc 58.5106 (67.6332) lr 1.8090e-03 eta 0:07:17
epoch [12/50] batch [15/23] time 0.243 (0.412) data 0.000 (0.098) loss 1.1660 (1.1177) acc 73.9362 (68.3244) lr 1.8090e-03 eta 0:06:03
epoch [12/50] batch [20/23] time 0.136 (0.409) data 0.000 (0.074) loss 1.0721 (1.1097) acc 72.2222 (69.1033) lr 1.8090e-03 eta 0:05:58
>>> alpha1: 0.514  alpha2: 0.121 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.28 <<<
epoch [13/50] batch [5/23] time 0.223 (0.451) data 0.000 (0.255) loss 1.0880 (1.0476) acc 70.6522 (67.2270) lr 1.7705e-03 eta 0:06:31
epoch [13/50] batch [10/23] time 0.192 (0.409) data 0.000 (0.128) loss 0.7799 (1.0437) acc 78.8462 (69.8568) lr 1.7705e-03 eta 0:05:53
epoch [13/50] batch [15/23] time 0.183 (0.332) data 0.000 (0.085) loss 1.2182 (1.0221) acc 64.1304 (69.1692) lr 1.7705e-03 eta 0:04:45
epoch [13/50] batch [20/23] time 0.234 (0.298) data 0.000 (0.064) loss 1.1130 (1.0452) acc 62.2549 (67.8870) lr 1.7705e-03 eta 0:04:14
>>> alpha1: 0.499  alpha2: 0.124 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.25 <<<
epoch [14/50] batch [5/23] time 0.184 (0.811) data 0.000 (0.300) loss 0.8875 (0.8745) acc 74.4792 (75.7402) lr 1.7290e-03 eta 0:11:26
epoch [14/50] batch [10/23] time 0.171 (0.495) data 0.000 (0.150) loss 1.1537 (0.9590) acc 58.1395 (73.1430) lr 1.7290e-03 eta 0:06:56
epoch [14/50] batch [15/23] time 0.174 (0.391) data 0.000 (0.100) loss 0.8038 (0.9763) acc 76.1111 (71.6946) lr 1.7290e-03 eta 0:05:26
epoch [14/50] batch [20/23] time 0.204 (0.340) data 0.000 (0.075) loss 1.0388 (0.9978) acc 69.2308 (70.5354) lr 1.7290e-03 eta 0:04:42
>>> alpha1: 0.475  alpha2: 0.110 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.28 <<<
epoch [15/50] batch [5/23] time 0.178 (0.651) data 0.001 (0.298) loss 0.9420 (0.9255) acc 73.3696 (73.5737) lr 1.6845e-03 eta 0:08:55
epoch [15/50] batch [10/23] time 0.188 (0.415) data 0.000 (0.149) loss 0.8392 (0.9006) acc 81.0000 (74.3373) lr 1.6845e-03 eta 0:05:39
epoch [15/50] batch [15/23] time 0.191 (0.397) data 0.000 (0.100) loss 0.8345 (0.9325) acc 77.0408 (72.0806) lr 1.6845e-03 eta 0:05:22
epoch [15/50] batch [20/23] time 0.187 (0.344) data 0.000 (0.075) loss 1.0298 (0.9607) acc 71.8750 (71.5722) lr 1.6845e-03 eta 0:04:38
>>> alpha1: 0.393  alpha2: 0.023 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.29 <<<
epoch [16/50] batch [5/23] time 0.214 (0.526) data 0.000 (0.321) loss 0.7721 (0.7807) acc 80.6604 (76.7031) lr 1.6374e-03 eta 0:07:00
epoch [16/50] batch [10/23] time 0.168 (0.353) data 0.000 (0.161) loss 1.0074 (0.8337) acc 66.0714 (73.3684) lr 1.6374e-03 eta 0:04:40
epoch [16/50] batch [15/23] time 0.190 (0.296) data 0.000 (0.107) loss 0.7079 (0.8406) acc 80.1020 (74.1082) lr 1.6374e-03 eta 0:03:54
epoch [16/50] batch [20/23] time 0.241 (0.269) data 0.000 (0.080) loss 0.7427 (0.8498) acc 77.9412 (73.3154) lr 1.6374e-03 eta 0:03:31
>>> alpha1: 0.361  alpha2: -0.002 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [17/50] batch [5/23] time 0.183 (0.442) data 0.013 (0.261) loss 0.6715 (0.7835) acc 75.0000 (76.1621) lr 1.5878e-03 eta 0:05:43
epoch [17/50] batch [10/23] time 0.168 (0.311) data 0.000 (0.131) loss 0.8777 (0.7922) acc 80.8139 (76.8226) lr 1.5878e-03 eta 0:04:00
epoch [17/50] batch [15/23] time 0.162 (0.266) data 0.000 (0.087) loss 0.8237 (0.8281) acc 69.5122 (74.4734) lr 1.5878e-03 eta 0:03:23
epoch [17/50] batch [20/23] time 0.190 (0.249) data 0.001 (0.066) loss 0.9490 (0.8469) acc 73.4375 (73.9620) lr 1.5878e-03 eta 0:03:09
>>> alpha1: 0.341  alpha2: -0.020 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [18/50] batch [5/23] time 0.181 (0.458) data 0.001 (0.274) loss 1.1230 (0.8303) acc 65.9091 (75.3013) lr 1.5358e-03 eta 0:05:45
epoch [18/50] batch [10/23] time 0.176 (0.317) data 0.000 (0.137) loss 0.6750 (0.8032) acc 82.4468 (75.8232) lr 1.5358e-03 eta 0:03:57
epoch [18/50] batch [15/23] time 0.215 (0.276) data 0.000 (0.092) loss 1.0261 (0.7678) acc 68.1818 (76.1673) lr 1.5358e-03 eta 0:03:25
epoch [18/50] batch [20/23] time 0.176 (0.253) data 0.000 (0.069) loss 0.8856 (0.7588) acc 72.7778 (76.7822) lr 1.5358e-03 eta 0:03:06
>>> alpha1: 0.312  alpha2: -0.030 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.29 <<<
epoch [19/50] batch [5/23] time 0.279 (0.616) data 0.001 (0.307) loss 0.8051 (0.6427) acc 80.2083 (82.7343) lr 1.4818e-03 eta 0:07:30
epoch [19/50] batch [10/23] time 0.291 (0.457) data 0.000 (0.153) loss 0.9008 (0.7032) acc 68.1373 (79.0916) lr 1.4818e-03 eta 0:05:31
epoch [19/50] batch [15/23] time 0.268 (0.397) data 0.000 (0.102) loss 0.5759 (0.6899) acc 84.2391 (79.0074) lr 1.4818e-03 eta 0:04:46
epoch [19/50] batch [20/23] time 0.293 (0.370) data 0.000 (0.077) loss 0.8620 (0.7230) acc 71.5686 (78.0273) lr 1.4818e-03 eta 0:04:24
>>> alpha1: 0.291  alpha2: -0.033 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [20/50] batch [5/23] time 0.198 (0.777) data 0.000 (0.325) loss 0.8323 (0.6374) acc 68.1373 (78.3877) lr 1.4258e-03 eta 0:09:10
epoch [20/50] batch [10/23] time 0.198 (0.490) data 0.001 (0.163) loss 0.5939 (0.6415) acc 85.0962 (79.4394) lr 1.4258e-03 eta 0:05:44
epoch [20/50] batch [15/23] time 0.195 (0.392) data 0.000 (0.109) loss 0.7882 (0.6427) acc 77.4510 (79.3133) lr 1.4258e-03 eta 0:04:33
epoch [20/50] batch [20/23] time 0.211 (0.345) data 0.001 (0.081) loss 0.8342 (0.6732) acc 72.0588 (78.3034) lr 1.4258e-03 eta 0:03:59
>>> alpha1: 0.269  alpha2: -0.067 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.33 <<<
epoch [21/50] batch [5/23] time 0.197 (0.501) data 0.000 (0.281) loss 0.4650 (0.6194) acc 80.3922 (79.0645) lr 1.3681e-03 eta 0:05:43
epoch [21/50] batch [10/23] time 0.198 (0.348) data 0.000 (0.141) loss 0.7708 (0.7178) acc 78.9216 (77.5942) lr 1.3681e-03 eta 0:03:56
epoch [21/50] batch [15/23] time 0.204 (0.298) data 0.000 (0.094) loss 0.4819 (0.6722) acc 89.8148 (79.2713) lr 1.3681e-03 eta 0:03:21
epoch [21/50] batch [20/23] time 1.258 (0.330) data 0.000 (0.071) loss 0.6844 (0.6682) acc 81.2500 (79.3068) lr 1.3681e-03 eta 0:03:41
>>> alpha1: 0.253  alpha2: -0.088 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.32 <<<
epoch [22/50] batch [5/23] time 0.204 (0.497) data 0.000 (0.293) loss 0.4740 (0.5408) acc 81.4815 (82.1370) lr 1.3090e-03 eta 0:05:29
epoch [22/50] batch [10/23] time 0.200 (0.355) data 0.000 (0.147) loss 0.7866 (0.5915) acc 68.3962 (80.3194) lr 1.3090e-03 eta 0:03:52
epoch [22/50] batch [15/23] time 0.192 (0.300) data 0.001 (0.098) loss 0.4993 (0.5886) acc 87.7451 (81.0618) lr 1.3090e-03 eta 0:03:15
epoch [22/50] batch [20/23] time 0.189 (0.275) data 0.000 (0.073) loss 1.0489 (0.6145) acc 68.5000 (80.4968) lr 1.3090e-03 eta 0:02:57
>>> alpha1: 0.234  alpha2: -0.098 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [23/50] batch [5/23] time 0.198 (0.493) data 0.001 (0.293) loss 0.6522 (0.5917) acc 77.4038 (82.7140) lr 1.2487e-03 eta 0:05:14
epoch [23/50] batch [10/23] time 0.195 (0.346) data 0.000 (0.147) loss 0.4893 (0.5702) acc 80.3922 (82.0652) lr 1.2487e-03 eta 0:03:39
epoch [23/50] batch [15/23] time 0.210 (0.302) data 0.000 (0.098) loss 0.6841 (0.5961) acc 80.9091 (80.8547) lr 1.2487e-03 eta 0:03:10
epoch [23/50] batch [20/23] time 0.215 (0.277) data 0.000 (0.073) loss 0.7368 (0.6226) acc 75.8772 (80.1613) lr 1.2487e-03 eta 0:02:52
>>> alpha1: 0.217  alpha2: -0.115 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [24/50] batch [5/23] time 0.188 (0.501) data 0.000 (0.296) loss 0.4677 (0.4829) acc 84.1837 (84.9352) lr 1.1874e-03 eta 0:05:08
epoch [24/50] batch [10/23] time 0.189 (0.355) data 0.001 (0.148) loss 0.5596 (0.5364) acc 77.9412 (82.9508) lr 1.1874e-03 eta 0:03:37
epoch [24/50] batch [15/23] time 0.208 (0.304) data 0.000 (0.099) loss 0.5565 (0.5503) acc 78.7037 (82.6495) lr 1.1874e-03 eta 0:03:03
epoch [24/50] batch [20/23] time 1.349 (0.335) data 0.000 (0.074) loss 0.5397 (0.5879) acc 80.1724 (81.1626) lr 1.1874e-03 eta 0:03:21
>>> alpha1: 0.211  alpha2: -0.110 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.33 <<<
epoch [25/50] batch [5/23] time 0.195 (0.507) data 0.000 (0.299) loss 0.5270 (0.5501) acc 85.5769 (83.3664) lr 1.1253e-03 eta 0:05:00
epoch [25/50] batch [10/23] time 0.191 (0.355) data 0.000 (0.150) loss 0.5018 (0.5711) acc 86.2245 (82.8630) lr 1.1253e-03 eta 0:03:28
epoch [25/50] batch [15/23] time 0.202 (0.304) data 0.000 (0.100) loss 0.4218 (0.5360) acc 87.2642 (82.9064) lr 1.1253e-03 eta 0:02:57
epoch [25/50] batch [20/23] time 0.190 (0.278) data 0.000 (0.075) loss 0.7794 (0.5437) acc 78.5714 (83.4860) lr 1.1253e-03 eta 0:02:40
>>> alpha1: 0.206  alpha2: -0.094 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [26/50] batch [5/23] time 0.168 (0.453) data 0.000 (0.278) loss 0.5399 (0.5536) acc 82.6531 (82.1735) lr 1.0628e-03 eta 0:04:17
epoch [26/50] batch [10/23] time 0.163 (0.303) data 0.000 (0.139) loss 0.6122 (0.5810) acc 86.6071 (83.1622) lr 1.0628e-03 eta 0:02:51
epoch [26/50] batch [15/23] time 0.297 (0.264) data 0.000 (0.093) loss 0.6315 (0.5775) acc 82.6923 (83.1811) lr 1.0628e-03 eta 0:02:27
epoch [26/50] batch [20/23] time 0.277 (0.272) data 0.001 (0.070) loss 0.5251 (0.5742) acc 80.2083 (82.8240) lr 1.0628e-03 eta 0:02:30
>>> alpha1: 0.201  alpha2: -0.093 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.31 <<<
epoch [27/50] batch [5/23] time 0.187 (0.434) data 0.000 (0.275) loss 0.5821 (0.5640) acc 82.8431 (83.8229) lr 1.0000e-03 eta 0:03:57
epoch [27/50] batch [10/23] time 0.169 (0.306) data 0.000 (0.138) loss 0.4075 (0.5063) acc 81.3830 (85.1159) lr 1.0000e-03 eta 0:02:45
epoch [27/50] batch [15/23] time 0.161 (0.259) data 0.000 (0.092) loss 0.5733 (0.4969) acc 81.3726 (85.2167) lr 1.0000e-03 eta 0:02:18
epoch [27/50] batch [20/23] time 0.158 (0.232) data 0.000 (0.069) loss 0.4754 (0.5045) acc 90.3061 (84.6157) lr 1.0000e-03 eta 0:02:03
>>> alpha1: 0.199  alpha2: -0.088 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [28/50] batch [5/23] time 0.290 (0.612) data 0.000 (0.325) loss 0.3866 (0.5016) acc 89.1509 (85.9708) lr 9.3721e-04 eta 0:05:20
epoch [28/50] batch [10/23] time 0.284 (0.446) data 0.000 (0.163) loss 0.3237 (0.4844) acc 88.9423 (85.6912) lr 9.3721e-04 eta 0:03:51
epoch [28/50] batch [15/23] time 0.149 (0.371) data 0.000 (0.108) loss 0.3252 (0.4637) acc 92.7885 (85.7090) lr 9.3721e-04 eta 0:03:10
epoch [28/50] batch [20/23] time 0.146 (0.315) data 0.000 (0.081) loss 0.5207 (0.4885) acc 88.2653 (85.3207) lr 9.3721e-04 eta 0:02:40
>>> alpha1: 0.195  alpha2: -0.093 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.33 <<<
epoch [29/50] batch [5/23] time 0.298 (0.641) data 0.000 (0.327) loss 0.5956 (0.4994) acc 79.4118 (81.8682) lr 8.7467e-04 eta 0:05:21
epoch [29/50] batch [10/23] time 0.275 (0.464) data 0.000 (0.164) loss 0.5352 (0.4666) acc 86.2245 (85.5265) lr 8.7467e-04 eta 0:03:50
epoch [29/50] batch [15/23] time 0.284 (0.407) data 0.000 (0.109) loss 0.5232 (0.5089) acc 84.0000 (84.3309) lr 8.7467e-04 eta 0:03:19
epoch [29/50] batch [20/23] time 0.147 (0.358) data 0.000 (0.082) loss 0.4079 (0.4974) acc 91.5000 (84.7888) lr 8.7467e-04 eta 0:02:53
>>> alpha1: 0.193  alpha2: -0.085 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [30/50] batch [5/23] time 0.302 (0.638) data 0.000 (0.352) loss 0.3281 (0.4674) acc 84.7222 (84.4364) lr 8.1262e-04 eta 0:05:05
epoch [30/50] batch [10/23] time 0.258 (0.456) data 0.000 (0.176) loss 0.4218 (0.4617) acc 90.2174 (85.5467) lr 8.1262e-04 eta 0:03:35
epoch [30/50] batch [15/23] time 0.321 (0.405) data 0.000 (0.118) loss 0.3873 (0.4738) acc 92.6724 (85.4557) lr 8.1262e-04 eta 0:03:09
epoch [30/50] batch [20/23] time 0.276 (0.374) data 0.000 (0.088) loss 0.4711 (0.4835) acc 86.0000 (84.8586) lr 8.1262e-04 eta 0:02:52
>>> alpha1: 0.188  alpha2: -0.090 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [31/50] batch [5/23] time 0.190 (0.496) data 0.000 (0.289) loss 0.5139 (0.4291) acc 87.5000 (87.0979) lr 7.5131e-04 eta 0:03:45
epoch [31/50] batch [10/23] time 0.200 (0.357) data 0.000 (0.144) loss 0.6255 (0.4629) acc 84.1346 (86.2332) lr 7.5131e-04 eta 0:02:40
epoch [31/50] batch [15/23] time 0.201 (0.304) data 0.001 (0.096) loss 0.4964 (0.4501) acc 81.7308 (86.9893) lr 7.5131e-04 eta 0:02:15
epoch [31/50] batch [20/23] time 0.201 (0.278) data 0.000 (0.072) loss 0.3334 (0.4558) acc 85.5769 (86.8662) lr 7.5131e-04 eta 0:02:02
>>> alpha1: 0.182  alpha2: -0.087 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.33 <<<
epoch [32/50] batch [5/23] time 0.194 (0.504) data 0.000 (0.307) loss 0.5037 (0.5497) acc 81.8627 (84.2681) lr 6.9098e-04 eta 0:03:37
epoch [32/50] batch [10/23] time 0.193 (0.353) data 0.000 (0.154) loss 0.4374 (0.5385) acc 82.3529 (83.8161) lr 6.9098e-04 eta 0:02:30
epoch [32/50] batch [15/23] time 0.205 (0.306) data 0.001 (0.103) loss 0.4515 (0.4939) acc 86.3208 (84.9826) lr 6.9098e-04 eta 0:02:09
epoch [32/50] batch [20/23] time 0.198 (0.279) data 0.000 (0.077) loss 0.5924 (0.5024) acc 75.4808 (84.7973) lr 6.9098e-04 eta 0:01:56
>>> alpha1: 0.177  alpha2: -0.102 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.29 <<<
epoch [33/50] batch [5/23] time 0.187 (0.499) data 0.000 (0.299) loss 0.6233 (0.4648) acc 76.5625 (86.7816) lr 6.3188e-04 eta 0:03:23
epoch [33/50] batch [10/23] time 0.202 (0.348) data 0.000 (0.150) loss 0.5560 (0.4418) acc 80.7292 (88.0054) lr 6.3188e-04 eta 0:02:20
epoch [33/50] batch [15/23] time 0.191 (0.343) data 0.000 (0.100) loss 0.3656 (0.4335) acc 91.5000 (87.5402) lr 6.3188e-04 eta 0:02:16
epoch [33/50] batch [20/23] time 0.184 (0.303) data 0.000 (0.075) loss 0.5197 (0.4393) acc 83.5106 (87.4006) lr 6.3188e-04 eta 0:01:59
>>> alpha1: 0.173  alpha2: -0.096 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.29 <<<
epoch [34/50] batch [5/23] time 0.280 (0.577) data 0.000 (0.319) loss 0.3948 (0.4246) acc 88.5870 (88.8416) lr 5.7422e-04 eta 0:03:42
epoch [34/50] batch [10/23] time 0.142 (0.403) data 0.000 (0.160) loss 0.3949 (0.4188) acc 84.0425 (88.8186) lr 5.7422e-04 eta 0:02:33
epoch [34/50] batch [15/23] time 0.142 (0.316) data 0.000 (0.107) loss 0.3690 (0.4275) acc 86.9792 (88.1097) lr 5.7422e-04 eta 0:01:58
epoch [34/50] batch [20/23] time 0.145 (0.272) data 0.000 (0.080) loss 0.3350 (0.4382) acc 88.5870 (87.5798) lr 5.7422e-04 eta 0:01:40
>>> alpha1: 0.169  alpha2: -0.094 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [35/50] batch [5/23] time 0.204 (0.472) data 0.000 (0.262) loss 0.3097 (0.4297) acc 96.6981 (87.6430) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [10/23] time 0.195 (0.340) data 0.000 (0.131) loss 0.4297 (0.4050) acc 89.0000 (89.4687) lr 5.1825e-04 eta 0:02:01
epoch [35/50] batch [15/23] time 0.232 (0.296) data 0.000 (0.088) loss 0.4076 (0.4086) acc 86.7924 (88.4770) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [20/23] time 0.211 (0.272) data 0.001 (0.066) loss 0.2376 (0.4167) acc 95.3704 (87.9815) lr 5.1825e-04 eta 0:01:34
>>> alpha1: 0.166  alpha2: -0.093 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [36/50] batch [5/23] time 0.239 (0.563) data 0.000 (0.345) loss 0.5672 (0.4191) acc 85.9091 (86.8840) lr 4.6417e-04 eta 0:03:11
epoch [36/50] batch [10/23] time 0.187 (0.382) data 0.000 (0.172) loss 0.5545 (0.4274) acc 80.2083 (87.5754) lr 4.6417e-04 eta 0:02:07
epoch [36/50] batch [15/23] time 0.205 (0.323) data 0.000 (0.115) loss 0.3020 (0.4119) acc 91.5094 (88.2228) lr 4.6417e-04 eta 0:01:46
epoch [36/50] batch [20/23] time 0.250 (0.294) data 0.000 (0.086) loss 0.6470 (0.4236) acc 80.8511 (88.1293) lr 4.6417e-04 eta 0:01:35
>>> alpha1: 0.164  alpha2: -0.090 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.35 <<<
epoch [37/50] batch [5/23] time 0.230 (0.544) data 0.000 (0.306) loss 0.3835 (0.4615) acc 91.8269 (87.2366) lr 4.1221e-04 eta 0:02:52
epoch [37/50] batch [10/23] time 0.151 (0.367) data 0.000 (0.153) loss 0.4469 (0.4676) acc 85.5769 (85.4120) lr 4.1221e-04 eta 0:01:54
epoch [37/50] batch [15/23] time 0.153 (0.296) data 0.000 (0.102) loss 0.5140 (0.4374) acc 84.4340 (86.1505) lr 4.1221e-04 eta 0:01:30
epoch [37/50] batch [20/23] time 0.209 (0.341) data 0.001 (0.077) loss 0.3165 (0.4263) acc 91.5094 (87.1552) lr 4.1221e-04 eta 0:01:43
>>> alpha1: 0.161  alpha2: -0.085 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.35 <<<
epoch [38/50] batch [5/23] time 0.222 (0.523) data 0.000 (0.299) loss 0.4275 (0.5100) acc 88.5965 (87.1640) lr 3.6258e-04 eta 0:02:33
epoch [38/50] batch [10/23] time 0.204 (0.364) data 0.000 (0.150) loss 0.3905 (0.4686) acc 89.1509 (87.1717) lr 3.6258e-04 eta 0:01:45
epoch [38/50] batch [15/23] time 0.218 (0.313) data 0.000 (0.100) loss 0.5822 (0.4435) acc 77.4038 (88.0432) lr 3.6258e-04 eta 0:01:28
epoch [38/50] batch [20/23] time 0.209 (0.288) data 0.000 (0.075) loss 0.4973 (0.4296) acc 82.8704 (87.6892) lr 3.6258e-04 eta 0:01:20
>>> alpha1: 0.159  alpha2: -0.079 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.34 <<<
epoch [39/50] batch [5/23] time 0.215 (0.527) data 0.000 (0.313) loss 0.3645 (0.4286) acc 92.9825 (87.1342) lr 3.1545e-04 eta 0:02:22
epoch [39/50] batch [10/23] time 0.185 (0.363) data 0.000 (0.156) loss 0.3484 (0.3936) acc 92.1875 (88.2193) lr 3.1545e-04 eta 0:01:36
epoch [39/50] batch [15/23] time 0.247 (0.316) data 0.000 (0.104) loss 0.3837 (0.4296) acc 86.0170 (87.5191) lr 3.1545e-04 eta 0:01:22
epoch [39/50] batch [20/23] time 0.218 (0.288) data 0.000 (0.078) loss 0.3367 (0.4280) acc 93.8596 (86.8640) lr 3.1545e-04 eta 0:01:13
>>> alpha1: 0.159  alpha2: -0.081 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [40/50] batch [5/23] time 0.199 (0.523) data 0.000 (0.299) loss 0.5450 (0.4731) acc 86.5385 (83.8981) lr 2.7103e-04 eta 0:02:09
epoch [40/50] batch [10/23] time 0.207 (0.365) data 0.000 (0.150) loss 0.3322 (0.4361) acc 93.1818 (86.2851) lr 2.7103e-04 eta 0:01:28
epoch [40/50] batch [15/23] time 0.202 (0.310) data 0.000 (0.100) loss 0.2778 (0.4372) acc 89.1509 (86.5236) lr 2.7103e-04 eta 0:01:13
epoch [40/50] batch [20/23] time 0.203 (0.286) data 0.000 (0.075) loss 0.3411 (0.4159) acc 90.9091 (87.4993) lr 2.7103e-04 eta 0:01:06
>>> alpha1: 0.158  alpha2: -0.076 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.33 <<<
epoch [41/50] batch [5/23] time 0.224 (0.512) data 0.002 (0.320) loss 0.2122 (0.3114) acc 86.8182 (88.4659) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [10/23] time 0.196 (0.350) data 0.000 (0.160) loss 0.3168 (0.3456) acc 91.0377 (90.2769) lr 2.2949e-04 eta 0:01:17
epoch [41/50] batch [15/23] time 0.192 (0.298) data 0.000 (0.107) loss 0.4709 (0.3752) acc 88.4615 (89.7479) lr 2.2949e-04 eta 0:01:04
epoch [41/50] batch [20/23] time 0.206 (0.272) data 0.000 (0.080) loss 0.4689 (0.3892) acc 87.9464 (89.5299) lr 2.2949e-04 eta 0:00:57
>>> alpha1: 0.158  alpha2: -0.082 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [42/50] batch [5/23] time 0.238 (0.533) data 0.000 (0.309) loss 0.5332 (0.3785) acc 85.5769 (88.9254) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [10/23] time 0.220 (0.370) data 0.000 (0.154) loss 0.3179 (0.3678) acc 95.6731 (89.5161) lr 1.9098e-04 eta 0:01:12
epoch [42/50] batch [15/23] time 0.197 (0.313) data 0.000 (0.103) loss 0.3394 (0.3737) acc 92.3077 (89.8882) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [20/23] time 0.208 (0.284) data 0.000 (0.077) loss 0.3702 (0.3797) acc 85.5000 (89.5827) lr 1.9098e-04 eta 0:00:53
>>> alpha1: 0.157  alpha2: -0.081 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [43/50] batch [5/23] time 0.210 (0.502) data 0.000 (0.300) loss 0.4064 (0.3888) acc 85.9091 (89.2668) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [10/23] time 0.193 (0.355) data 0.000 (0.150) loss 0.5464 (0.4011) acc 85.2041 (89.4357) lr 1.5567e-04 eta 0:01:01
epoch [43/50] batch [15/23] time 0.194 (0.306) data 0.000 (0.100) loss 0.3040 (0.3858) acc 91.6667 (89.2903) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [20/23] time 0.188 (0.278) data 0.000 (0.075) loss 0.4100 (0.3919) acc 87.5000 (89.5666) lr 1.5567e-04 eta 0:00:45
>>> alpha1: 0.154  alpha2: -0.086 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [44/50] batch [5/23] time 0.240 (0.566) data 0.000 (0.311) loss 0.3019 (0.5416) acc 92.8571 (89.6070) lr 1.2369e-04 eta 0:01:28
epoch [44/50] batch [10/23] time 0.269 (0.408) data 0.000 (0.156) loss 0.4123 (0.4489) acc 90.0000 (90.6680) lr 1.2369e-04 eta 0:01:01
epoch [44/50] batch [15/23] time 0.314 (0.368) data 0.000 (0.104) loss 0.3605 (0.4214) acc 89.5455 (90.4968) lr 1.2369e-04 eta 0:00:53
epoch [44/50] batch [20/23] time 0.146 (0.314) data 0.000 (0.078) loss 0.3518 (0.4042) acc 91.5000 (90.1534) lr 1.2369e-04 eta 0:00:44
>>> alpha1: 0.153  alpha2: -0.088 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [45/50] batch [5/23] time 0.196 (0.530) data 0.000 (0.329) loss 0.4095 (0.3486) acc 84.9057 (90.2000) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [10/23] time 0.193 (0.362) data 0.000 (0.165) loss 0.3381 (0.4046) acc 92.7885 (88.6905) lr 9.5173e-05 eta 0:00:46
epoch [45/50] batch [15/23] time 0.198 (0.311) data 0.001 (0.110) loss 0.3650 (0.3906) acc 89.6226 (88.4521) lr 9.5173e-05 eta 0:00:38
epoch [45/50] batch [20/23] time 0.218 (0.282) data 0.000 (0.082) loss 0.2614 (0.3859) acc 91.9492 (89.2908) lr 9.5173e-05 eta 0:00:33
>>> alpha1: 0.154  alpha2: -0.087 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [46/50] batch [5/23] time 0.201 (0.461) data 0.000 (0.262) loss 0.3393 (0.3925) acc 91.3043 (88.5927) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [10/23] time 0.182 (0.334) data 0.000 (0.131) loss 0.3959 (0.3807) acc 91.8478 (88.9507) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [15/23] time 0.193 (0.287) data 0.000 (0.087) loss 0.3448 (0.3860) acc 88.7755 (88.4680) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.189 (0.261) data 0.000 (0.066) loss 0.1787 (0.3780) acc 98.4375 (89.5081) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.152  alpha2: -0.085 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [47/50] batch [5/23] time 0.173 (0.479) data 0.000 (0.290) loss 0.2601 (0.3395) acc 92.2222 (88.8931) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [10/23] time 0.190 (0.342) data 0.000 (0.145) loss 0.2769 (0.3452) acc 93.8775 (89.4518) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [15/23] time 0.175 (0.287) data 0.000 (0.097) loss 0.3379 (0.3567) acc 94.0217 (89.8985) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [20/23] time 0.192 (0.261) data 0.000 (0.073) loss 0.2495 (0.3511) acc 93.0000 (90.0163) lr 4.8943e-05 eta 0:00:18
>>> alpha1: 0.151  alpha2: -0.087 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [48/50] batch [5/23] time 0.178 (0.466) data 0.000 (0.276) loss 0.4646 (0.4121) acc 85.5000 (89.1697) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [10/23] time 0.190 (0.330) data 0.000 (0.138) loss 0.4105 (0.3832) acc 87.7551 (89.6236) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [15/23] time 0.187 (0.282) data 0.001 (0.092) loss 0.3087 (0.3876) acc 86.1702 (88.7855) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.190 (0.259) data 0.000 (0.069) loss 0.2677 (0.3869) acc 92.5000 (89.2857) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.152  alpha2: -0.092 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [49/50] batch [5/23] time 0.178 (0.512) data 0.000 (0.325) loss 0.5059 (0.3807) acc 86.4130 (90.1862) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [10/23] time 0.163 (0.343) data 0.000 (0.163) loss 0.4125 (0.4109) acc 84.1463 (88.2175) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [15/23] time 0.184 (0.290) data 0.000 (0.109) loss 0.3055 (0.3848) acc 93.2292 (89.8114) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.182 (0.267) data 0.000 (0.082) loss 0.4430 (0.3788) acc 83.6956 (89.7305) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.151  alpha2: -0.096 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [50/50] batch [5/23] time 0.194 (0.444) data 0.000 (0.252) loss 0.3116 (0.3627) acc 92.0000 (89.9846) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.192 (0.318) data 0.000 (0.126) loss 0.2501 (0.3685) acc 91.5000 (90.0079) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.193 (0.279) data 0.000 (0.084) loss 0.3064 (0.3742) acc 91.4894 (90.2651) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.181 (0.255) data 0.000 (0.063) loss 0.3297 (0.3901) acc 89.8936 (89.9101) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.19, 0.17, 0.17, 0.16, 0.17, 0.18, 0.17, 0.19, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.18, 0.18, 0.19, 0.19, 0.19, 0.18, 0.19, 0.18, 0.19, 0.18, 0.19, 0.19, 0.18, 0.19, 0.19, 0.18]
* matched noise rate: [0.07, 0.06, 0.04, 0.04, 0.04, 0.05, 0.07, 0.07, 0.08, 0.09, 0.09, 0.08, 0.09, 0.1, 0.09, 0.1, 0.08, 0.09, 0.09, 0.09, 0.1, 0.09, 0.08, 0.08, 0.09, 0.11, 0.1, 0.11, 0.11, 0.11, 0.11, 0.1, 0.1, 0.09, 0.1, 0.1, 0.09, 0.1, 0.1, 0.09]
* unmatched noise rate: [0.33, 0.28, 0.28, 0.25, 0.28, 0.29, 0.25, 0.28, 0.29, 0.31, 0.33, 0.32, 0.32, 0.33, 0.33, 0.33, 0.31, 0.32, 0.33, 0.32, 0.32, 0.33, 0.29, 0.29, 0.31, 0.3, 0.35, 0.35, 0.34, 0.32, 0.33, 0.31, 0.32, 0.27, 0.33, 0.28, 0.27, 0.29, 0.27, 0.27]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:44,  2.75s/it] 18%|█▊        | 3/17 [00:02<00:11,  1.25it/s] 29%|██▉       | 5/17 [00:03<00:05,  2.29it/s] 35%|███▌      | 6/17 [00:03<00:03,  2.86it/s] 41%|████      | 7/17 [00:03<00:02,  3.45it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.94it/s] 65%|██████▍   | 11/17 [00:03<00:00,  6.25it/s] 76%|███████▋  | 13/17 [00:03<00:00,  7.37it/s] 88%|████████▊ | 15/17 [00:04<00:00,  8.24it/s]100%|██████████| 17/17 [00:04<00:00,  5.09it/s]100%|██████████| 17/17 [00:05<00:00,  3.38it/s]
=> result
* total: 1,692
* correct: 1,018
* accuracy: 60.2%
* error: 39.8%
* macro_f1: 59.4%
=> per-class result
* class: 0 (banded)	total: 36	correct: 13	acc: 36.1%
* class: 1 (blotchy)	total: 36	correct: 9	acc: 25.0%
* class: 2 (braided)	total: 36	correct: 16	acc: 44.4%
* class: 3 (bubbly)	total: 36	correct: 32	acc: 88.9%
* class: 4 (bumpy)	total: 36	correct: 9	acc: 25.0%
* class: 5 (chequered)	total: 36	correct: 34	acc: 94.4%
* class: 6 (cobwebbed)	total: 36	correct: 29	acc: 80.6%
* class: 7 (cracked)	total: 36	correct: 30	acc: 83.3%
* class: 8 (crosshatched)	total: 36	correct: 7	acc: 19.4%
* class: 9 (crystalline)	total: 36	correct: 31	acc: 86.1%
* class: 10 (dotted)	total: 36	correct: 6	acc: 16.7%
* class: 11 (fibrous)	total: 36	correct: 29	acc: 80.6%
* class: 12 (flecked)	total: 36	correct: 21	acc: 58.3%
* class: 13 (freckled)	total: 36	correct: 27	acc: 75.0%
* class: 14 (frilly)	total: 36	correct: 19	acc: 52.8%
* class: 15 (gauzy)	total: 36	correct: 20	acc: 55.6%
* class: 16 (grid)	total: 36	correct: 16	acc: 44.4%
* class: 17 (grooved)	total: 36	correct: 20	acc: 55.6%
* class: 18 (honeycombed)	total: 36	correct: 27	acc: 75.0%
* class: 19 (interlaced)	total: 36	correct: 23	acc: 63.9%
* class: 20 (knitted)	total: 36	correct: 28	acc: 77.8%
* class: 21 (lacelike)	total: 36	correct: 34	acc: 94.4%
* class: 22 (lined)	total: 36	correct: 17	acc: 47.2%
* class: 23 (marbled)	total: 36	correct: 15	acc: 41.7%
* class: 24 (matted)	total: 36	correct: 23	acc: 63.9%
* class: 25 (meshed)	total: 36	correct: 15	acc: 41.7%
* class: 26 (paisley)	total: 36	correct: 35	acc: 97.2%
* class: 27 (perforated)	total: 36	correct: 21	acc: 58.3%
* class: 28 (pitted)	total: 36	correct: 8	acc: 22.2%
* class: 29 (pleated)	total: 36	correct: 23	acc: 63.9%
* class: 30 (polka-dotted)	total: 36	correct: 23	acc: 63.9%
* class: 31 (porous)	total: 36	correct: 16	acc: 44.4%
* class: 32 (potholed)	total: 36	correct: 32	acc: 88.9%
* class: 33 (scaly)	total: 36	correct: 24	acc: 66.7%
* class: 34 (smeared)	total: 36	correct: 18	acc: 50.0%
* class: 35 (spiralled)	total: 36	correct: 16	acc: 44.4%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 10	acc: 27.8%
* class: 38 (stratified)	total: 36	correct: 27	acc: 75.0%
* class: 39 (striped)	total: 36	correct: 30	acc: 83.3%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 23	acc: 63.9%
* class: 42 (veined)	total: 36	correct: 21	acc: 58.3%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 19	acc: 52.8%
* class: 45 (wrinkled)	total: 36	correct: 23	acc: 63.9%
* class: 46 (zigzagged)	total: 36	correct: 29	acc: 80.6%
* average: 60.2%
Elapsed: 0:16:42
Run this job and save the output to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_4-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.213 (1.036) data 0.000 (0.301) loss 3.3852 (3.4952) acc 28.1250 (21.8750) lr 1.0000e-05 eta 0:19:46
epoch [1/50] batch [10/23] time 0.199 (0.628) data 0.000 (0.151) loss 3.3601 (3.5010) acc 25.0000 (20.3125) lr 1.0000e-05 eta 0:11:55
epoch [1/50] batch [15/23] time 0.210 (0.488) data 0.000 (0.101) loss 3.5745 (3.5064) acc 25.0000 (20.4167) lr 1.0000e-05 eta 0:09:13
epoch [1/50] batch [20/23] time 0.206 (0.418) data 0.000 (0.075) loss 3.5692 (3.5155) acc 15.6250 (19.3750) lr 1.0000e-05 eta 0:07:51
epoch [2/50] batch [5/23] time 0.211 (0.490) data 0.000 (0.247) loss 2.5582 (3.1923) acc 34.3750 (22.5000) lr 2.0000e-03 eta 0:09:09
epoch [2/50] batch [10/23] time 0.206 (0.355) data 0.000 (0.124) loss 3.7068 (3.2659) acc 12.5000 (20.6250) lr 2.0000e-03 eta 0:06:36
epoch [2/50] batch [15/23] time 0.208 (0.306) data 0.000 (0.083) loss 3.7433 (3.2493) acc 18.7500 (22.0833) lr 2.0000e-03 eta 0:05:40
epoch [2/50] batch [20/23] time 0.199 (0.284) data 0.000 (0.062) loss 2.4466 (3.1534) acc 40.6250 (24.6875) lr 2.0000e-03 eta 0:05:14
epoch [3/50] batch [5/23] time 0.216 (0.516) data 0.000 (0.294) loss 3.2684 (3.1061) acc 28.1250 (26.8750) lr 1.9980e-03 eta 0:09:27
epoch [3/50] batch [10/23] time 0.207 (0.366) data 0.000 (0.147) loss 2.2488 (2.8681) acc 53.1250 (35.9375) lr 1.9980e-03 eta 0:06:40
epoch [3/50] batch [15/23] time 0.206 (0.317) data 0.000 (0.098) loss 2.8651 (2.7372) acc 34.3750 (37.9167) lr 1.9980e-03 eta 0:05:45
epoch [3/50] batch [20/23] time 0.205 (0.289) data 0.000 (0.074) loss 3.6708 (2.8415) acc 18.7500 (34.5312) lr 1.9980e-03 eta 0:05:13
epoch [4/50] batch [5/23] time 0.213 (0.482) data 0.000 (0.247) loss 3.1086 (2.8319) acc 28.1250 (38.7500) lr 1.9921e-03 eta 0:08:38
epoch [4/50] batch [10/23] time 0.208 (0.354) data 0.000 (0.124) loss 2.5243 (2.7618) acc 50.0000 (40.3125) lr 1.9921e-03 eta 0:06:19
epoch [4/50] batch [15/23] time 0.208 (0.305) data 0.000 (0.083) loss 2.5874 (2.6779) acc 40.6250 (40.6250) lr 1.9921e-03 eta 0:05:25
epoch [4/50] batch [20/23] time 0.208 (0.281) data 0.000 (0.062) loss 2.4811 (2.6770) acc 43.7500 (39.2188) lr 1.9921e-03 eta 0:04:57
epoch [5/50] batch [5/23] time 0.229 (0.499) data 0.000 (0.261) loss 2.2568 (2.2336) acc 53.1250 (46.8750) lr 1.9823e-03 eta 0:08:45
epoch [5/50] batch [10/23] time 0.223 (0.356) data 0.000 (0.130) loss 2.6282 (2.3158) acc 46.8750 (47.5000) lr 1.9823e-03 eta 0:06:13
epoch [5/50] batch [15/23] time 0.203 (0.305) data 0.000 (0.087) loss 2.0168 (2.4623) acc 62.5000 (46.8750) lr 1.9823e-03 eta 0:05:18
epoch [5/50] batch [20/23] time 0.203 (0.284) data 0.000 (0.065) loss 3.0544 (2.5690) acc 21.8750 (43.2812) lr 1.9823e-03 eta 0:04:54
epoch [6/50] batch [5/23] time 0.227 (0.510) data 0.000 (0.281) loss 2.7624 (2.7723) acc 34.3750 (35.0000) lr 1.9686e-03 eta 0:08:44
epoch [6/50] batch [10/23] time 0.207 (0.362) data 0.000 (0.141) loss 2.3000 (2.6000) acc 46.8750 (40.6250) lr 1.9686e-03 eta 0:06:11
epoch [6/50] batch [15/23] time 0.210 (0.315) data 0.000 (0.094) loss 1.9797 (2.5416) acc 56.2500 (41.8750) lr 1.9686e-03 eta 0:05:21
epoch [6/50] batch [20/23] time 0.211 (0.288) data 0.000 (0.070) loss 2.6779 (2.5707) acc 46.8750 (42.6562) lr 1.9686e-03 eta 0:04:52
epoch [7/50] batch [5/23] time 0.235 (0.526) data 0.000 (0.298) loss 2.4762 (2.1971) acc 50.0000 (53.1250) lr 1.9511e-03 eta 0:08:49
epoch [7/50] batch [10/23] time 0.210 (0.376) data 0.000 (0.149) loss 2.3170 (2.3637) acc 46.8750 (47.5000) lr 1.9511e-03 eta 0:06:16
epoch [7/50] batch [15/23] time 0.202 (0.319) data 0.000 (0.100) loss 2.5219 (2.4366) acc 46.8750 (45.2083) lr 1.9511e-03 eta 0:05:18
epoch [7/50] batch [20/23] time 0.207 (0.292) data 0.000 (0.075) loss 2.5708 (2.4538) acc 53.1250 (45.6250) lr 1.9511e-03 eta 0:04:49
epoch [8/50] batch [5/23] time 0.224 (0.522) data 0.000 (0.284) loss 2.0587 (2.5445) acc 56.2500 (45.6250) lr 1.9298e-03 eta 0:08:33
epoch [8/50] batch [10/23] time 0.218 (0.368) data 0.000 (0.142) loss 2.3373 (2.4610) acc 43.7500 (44.0625) lr 1.9298e-03 eta 0:06:00
epoch [8/50] batch [15/23] time 0.204 (0.314) data 0.000 (0.095) loss 3.4150 (2.4703) acc 31.2500 (44.5833) lr 1.9298e-03 eta 0:05:05
epoch [8/50] batch [20/23] time 0.204 (0.290) data 0.000 (0.071) loss 2.1173 (2.5019) acc 50.0000 (44.8438) lr 1.9298e-03 eta 0:04:41
epoch [9/50] batch [5/23] time 0.248 (0.536) data 0.000 (0.308) loss 2.4000 (2.3413) acc 50.0000 (46.2500) lr 1.9048e-03 eta 0:08:34
epoch [9/50] batch [10/23] time 0.272 (0.382) data 0.000 (0.154) loss 2.3163 (2.4622) acc 50.0000 (45.3125) lr 1.9048e-03 eta 0:06:05
epoch [9/50] batch [15/23] time 0.208 (0.324) data 0.000 (0.103) loss 1.9367 (2.3775) acc 50.0000 (47.7083) lr 1.9048e-03 eta 0:05:07
epoch [9/50] batch [20/23] time 0.207 (0.294) data 0.000 (0.077) loss 2.3907 (2.3668) acc 34.3750 (47.5000) lr 1.9048e-03 eta 0:04:38
epoch [10/50] batch [5/23] time 0.336 (0.607) data 0.000 (0.297) loss 2.8556 (2.6490) acc 40.6250 (43.1250) lr 1.8763e-03 eta 0:09:29
epoch [10/50] batch [10/23] time 0.313 (0.463) data 0.000 (0.148) loss 2.0341 (2.3686) acc 53.1250 (48.1250) lr 1.8763e-03 eta 0:07:11
epoch [10/50] batch [15/23] time 0.324 (0.415) data 0.000 (0.099) loss 2.2979 (2.4046) acc 50.0000 (47.2917) lr 1.8763e-03 eta 0:06:24
epoch [10/50] batch [20/23] time 0.323 (0.391) data 0.000 (0.074) loss 2.3663 (2.3511) acc 50.0000 (47.9688) lr 1.8763e-03 eta 0:06:01
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.698  alpha2: 0.129 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.27 <<<
epoch [11/50] batch [5/23] time 1.130 (1.168) data 0.001 (0.246) loss 1.2872 (1.4224) acc 67.1875 (67.1645) lr 1.8443e-03 eta 0:17:48
epoch [11/50] batch [10/23] time 0.187 (0.937) data 0.001 (0.123) loss 1.1728 (1.3936) acc 70.3125 (65.4903) lr 1.8443e-03 eta 0:14:12
epoch [11/50] batch [15/23] time 0.194 (0.804) data 0.000 (0.082) loss 1.4571 (1.3631) acc 49.5000 (64.7197) lr 1.8443e-03 eta 0:12:07
epoch [11/50] batch [20/23] time 0.174 (0.648) data 0.001 (0.062) loss 1.4233 (1.3550) acc 58.1522 (64.1240) lr 1.8443e-03 eta 0:09:42
>>> alpha1: 0.569  alpha2: 0.122 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/23] time 0.193 (0.452) data 0.000 (0.254) loss 1.1915 (1.0325) acc 73.4375 (72.4539) lr 1.8090e-03 eta 0:06:42
epoch [12/50] batch [10/23] time 0.183 (0.319) data 0.000 (0.127) loss 0.9238 (0.9855) acc 72.3958 (73.6651) lr 1.8090e-03 eta 0:04:42
epoch [12/50] batch [15/23] time 0.251 (0.280) data 0.000 (0.085) loss 1.2405 (1.0718) acc 67.1875 (70.2888) lr 1.8090e-03 eta 0:04:06
epoch [12/50] batch [20/23] time 0.193 (0.291) data 0.000 (0.064) loss 0.8214 (1.0650) acc 77.5000 (70.2426) lr 1.8090e-03 eta 0:04:15
>>> alpha1: 0.500  alpha2: 0.110 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.24 <<<
epoch [13/50] batch [5/23] time 0.209 (0.439) data 0.000 (0.229) loss 0.7290 (0.9061) acc 75.0000 (74.3673) lr 1.7705e-03 eta 0:06:21
epoch [13/50] batch [10/23] time 0.173 (0.309) data 0.000 (0.115) loss 1.2270 (0.9337) acc 72.2826 (72.7367) lr 1.7705e-03 eta 0:04:27
epoch [13/50] batch [15/23] time 0.169 (0.332) data 0.000 (0.076) loss 1.1564 (0.9088) acc 65.9091 (73.5508) lr 1.7705e-03 eta 0:04:45
epoch [13/50] batch [20/23] time 0.121 (0.322) data 0.000 (0.057) loss 1.3951 (0.9347) acc 57.3171 (72.7451) lr 1.7705e-03 eta 0:04:34
>>> alpha1: 0.459  alpha2: 0.110 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.21 <<<
epoch [14/50] batch [5/23] time 0.178 (0.440) data 0.000 (0.246) loss 0.8205 (0.8696) acc 76.1111 (75.5995) lr 1.7290e-03 eta 0:06:12
epoch [14/50] batch [10/23] time 0.188 (0.314) data 0.000 (0.123) loss 0.8687 (1.0221) acc 76.5625 (73.9803) lr 1.7290e-03 eta 0:04:23
epoch [14/50] batch [15/23] time 0.195 (0.267) data 0.000 (0.082) loss 0.8112 (0.9834) acc 72.5000 (73.4069) lr 1.7290e-03 eta 0:03:43
epoch [14/50] batch [20/23] time 0.241 (0.249) data 0.000 (0.062) loss 0.8636 (0.9606) acc 72.7273 (72.9352) lr 1.7290e-03 eta 0:03:26
>>> alpha1: 0.423  alpha2: 0.092 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [15/50] batch [5/23] time 0.191 (0.877) data 0.000 (0.219) loss 0.7589 (0.6641) acc 78.0612 (81.2858) lr 1.6845e-03 eta 0:12:01
epoch [15/50] batch [10/23] time 0.209 (0.641) data 0.000 (0.110) loss 0.7236 (0.7479) acc 74.0566 (77.2874) lr 1.6845e-03 eta 0:08:44
epoch [15/50] batch [15/23] time 0.191 (0.495) data 0.000 (0.073) loss 0.5920 (0.7527) acc 85.7143 (77.1228) lr 1.6845e-03 eta 0:06:42
epoch [15/50] batch [20/23] time 0.214 (0.477) data 0.000 (0.055) loss 0.6804 (0.7697) acc 79.3269 (76.8938) lr 1.6845e-03 eta 0:06:25
>>> alpha1: 0.329  alpha2: 0.002 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [16/50] batch [5/23] time 0.199 (0.463) data 0.000 (0.255) loss 1.0061 (0.7130) acc 71.4286 (78.4943) lr 1.6374e-03 eta 0:06:10
epoch [16/50] batch [10/23] time 0.194 (0.328) data 0.000 (0.128) loss 0.6802 (0.7137) acc 77.0000 (78.8183) lr 1.6374e-03 eta 0:04:20
epoch [16/50] batch [15/23] time 0.234 (0.285) data 0.000 (0.085) loss 0.8519 (0.7337) acc 76.4706 (77.8117) lr 1.6374e-03 eta 0:03:45
epoch [16/50] batch [20/23] time 0.183 (0.265) data 0.000 (0.064) loss 0.6657 (0.7162) acc 81.7708 (78.1816) lr 1.6374e-03 eta 0:03:28
>>> alpha1: 0.295  alpha2: -0.010 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [17/50] batch [5/23] time 0.273 (0.482) data 0.000 (0.262) loss 1.1414 (0.7598) acc 75.0000 (79.7806) lr 1.5878e-03 eta 0:06:14
epoch [17/50] batch [10/23] time 0.250 (0.367) data 0.000 (0.131) loss 0.5421 (0.6835) acc 79.5000 (80.3038) lr 1.5878e-03 eta 0:04:43
epoch [17/50] batch [15/23] time 0.234 (0.327) data 0.000 (0.088) loss 0.9149 (0.7036) acc 73.4375 (80.2805) lr 1.5878e-03 eta 0:04:11
epoch [17/50] batch [20/23] time 0.257 (0.309) data 0.000 (0.066) loss 0.5757 (0.6832) acc 82.4074 (80.3540) lr 1.5878e-03 eta 0:03:55
>>> alpha1: 0.272  alpha2: -0.019 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.22 <<<
epoch [18/50] batch [5/23] time 0.192 (0.480) data 0.000 (0.280) loss 0.6915 (0.5916) acc 79.0000 (81.0727) lr 1.5358e-03 eta 0:06:02
epoch [18/50] batch [10/23] time 0.199 (0.330) data 0.000 (0.140) loss 0.4579 (0.6432) acc 85.5769 (79.4750) lr 1.5358e-03 eta 0:04:07
epoch [18/50] batch [15/23] time 0.182 (0.283) data 0.000 (0.093) loss 0.6812 (0.6331) acc 80.9783 (80.5965) lr 1.5358e-03 eta 0:03:30
epoch [18/50] batch [20/23] time 0.173 (0.258) data 0.000 (0.070) loss 0.5644 (0.6282) acc 73.2955 (80.8280) lr 1.5358e-03 eta 0:03:10
>>> alpha1: 0.251  alpha2: -0.028 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [19/50] batch [5/23] time 0.181 (0.454) data 0.001 (0.260) loss 0.6465 (0.5554) acc 83.3333 (83.6357) lr 1.4818e-03 eta 0:05:31
epoch [19/50] batch [10/23] time 0.185 (0.321) data 0.000 (0.130) loss 0.6303 (0.6026) acc 78.7234 (81.6070) lr 1.4818e-03 eta 0:03:53
epoch [19/50] batch [15/23] time 0.183 (0.277) data 0.000 (0.087) loss 0.6196 (0.5808) acc 81.3830 (82.5861) lr 1.4818e-03 eta 0:03:19
epoch [19/50] batch [20/23] time 0.178 (0.257) data 0.000 (0.065) loss 0.5097 (0.5782) acc 80.4348 (82.9973) lr 1.4818e-03 eta 0:03:03
>>> alpha1: 0.225  alpha2: -0.054 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.23 <<<
epoch [20/50] batch [5/23] time 0.173 (0.455) data 0.000 (0.266) loss 0.3749 (0.4829) acc 92.6136 (87.6089) lr 1.4258e-03 eta 0:05:22
epoch [20/50] batch [10/23] time 0.173 (0.324) data 0.000 (0.133) loss 0.4658 (0.5219) acc 91.4773 (86.3696) lr 1.4258e-03 eta 0:03:47
epoch [20/50] batch [15/23] time 0.171 (0.276) data 0.000 (0.089) loss 0.4770 (0.5304) acc 87.2093 (86.1675) lr 1.4258e-03 eta 0:03:12
epoch [20/50] batch [20/23] time 0.194 (0.254) data 0.000 (0.067) loss 0.6016 (0.5345) acc 75.5000 (85.2979) lr 1.4258e-03 eta 0:02:55
>>> alpha1: 0.212  alpha2: -0.056 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.23 <<<
epoch [21/50] batch [5/23] time 0.187 (0.504) data 0.000 (0.303) loss 0.5288 (0.4575) acc 86.0465 (86.0440) lr 1.3681e-03 eta 0:05:45
epoch [21/50] batch [10/23] time 0.194 (0.345) data 0.000 (0.152) loss 0.4488 (0.5358) acc 90.0000 (84.6516) lr 1.3681e-03 eta 0:03:54
epoch [21/50] batch [15/23] time 0.194 (0.290) data 0.000 (0.101) loss 0.4846 (0.5525) acc 87.7451 (84.2499) lr 1.3681e-03 eta 0:03:15
epoch [21/50] batch [20/23] time 0.184 (0.263) data 0.000 (0.076) loss 0.5958 (0.5586) acc 85.5556 (83.6605) lr 1.3681e-03 eta 0:02:56
>>> alpha1: 0.196  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [22/50] batch [5/23] time 0.186 (0.457) data 0.000 (0.251) loss 0.4630 (0.4651) acc 87.7778 (87.3893) lr 1.3090e-03 eta 0:05:02
epoch [22/50] batch [10/23] time 0.183 (0.320) data 0.000 (0.126) loss 0.5166 (0.4984) acc 85.9375 (85.8850) lr 1.3090e-03 eta 0:03:30
epoch [22/50] batch [15/23] time 0.203 (0.276) data 0.000 (0.084) loss 0.4362 (0.4869) acc 87.2340 (85.7522) lr 1.3090e-03 eta 0:02:59
epoch [22/50] batch [20/23] time 0.175 (0.257) data 0.000 (0.063) loss 0.4732 (0.4963) acc 81.9767 (85.1680) lr 1.3090e-03 eta 0:02:46
>>> alpha1: 0.187  alpha2: -0.055 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [23/50] batch [5/23] time 0.214 (0.457) data 0.000 (0.251) loss 0.4480 (0.4780) acc 89.0625 (88.1144) lr 1.2487e-03 eta 0:04:51
epoch [23/50] batch [10/23] time 0.194 (0.323) data 0.000 (0.126) loss 0.5688 (0.5011) acc 80.0000 (87.6903) lr 1.2487e-03 eta 0:03:24
epoch [23/50] batch [15/23] time 0.276 (0.281) data 0.000 (0.084) loss 0.4720 (0.5107) acc 82.4074 (85.9350) lr 1.2487e-03 eta 0:02:56
epoch [23/50] batch [20/23] time 0.191 (0.257) data 0.000 (0.063) loss 0.4184 (0.4974) acc 88.7755 (85.7267) lr 1.2487e-03 eta 0:02:40
>>> alpha1: 0.181  alpha2: -0.059 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.24 <<<
epoch [24/50] batch [5/23] time 0.186 (0.505) data 0.001 (0.314) loss 0.5759 (0.4666) acc 84.6939 (86.8841) lr 1.1874e-03 eta 0:05:10
epoch [24/50] batch [10/23] time 0.141 (0.325) data 0.000 (0.157) loss 0.5362 (0.4730) acc 88.5417 (86.8291) lr 1.1874e-03 eta 0:03:18
epoch [24/50] batch [15/23] time 0.161 (0.266) data 0.001 (0.105) loss 0.3868 (0.4490) acc 86.7924 (87.6289) lr 1.1874e-03 eta 0:02:40
epoch [24/50] batch [20/23] time 0.292 (0.253) data 0.000 (0.079) loss 0.5718 (0.4726) acc 83.0000 (87.0707) lr 1.1874e-03 eta 0:02:32
>>> alpha1: 0.175  alpha2: -0.050 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [25/50] batch [5/23] time 0.177 (0.457) data 0.001 (0.258) loss 0.4725 (0.4003) acc 87.7660 (88.9199) lr 1.1253e-03 eta 0:04:30
epoch [25/50] batch [10/23] time 0.178 (0.317) data 0.000 (0.129) loss 0.5709 (0.4513) acc 80.7292 (86.7203) lr 1.1253e-03 eta 0:03:06
epoch [25/50] batch [15/23] time 0.173 (0.279) data 0.000 (0.086) loss 0.4173 (0.4648) acc 90.9574 (86.5660) lr 1.1253e-03 eta 0:02:42
epoch [25/50] batch [20/23] time 0.187 (0.256) data 0.000 (0.065) loss 0.4324 (0.4646) acc 92.1875 (87.0078) lr 1.1253e-03 eta 0:02:28
>>> alpha1: 0.170  alpha2: -0.045 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [26/50] batch [5/23] time 0.189 (0.467) data 0.000 (0.271) loss 0.4767 (0.4097) acc 87.2449 (87.6483) lr 1.0628e-03 eta 0:04:26
epoch [26/50] batch [10/23] time 0.172 (0.332) data 0.000 (0.136) loss 0.7035 (0.4673) acc 80.6818 (86.0992) lr 1.0628e-03 eta 0:03:07
epoch [26/50] batch [15/23] time 0.190 (0.283) data 0.000 (0.091) loss 0.3690 (0.4670) acc 91.1765 (86.7469) lr 1.0628e-03 eta 0:02:38
epoch [26/50] batch [20/23] time 0.189 (0.256) data 0.000 (0.068) loss 0.3904 (0.4658) acc 87.0000 (86.6282) lr 1.0628e-03 eta 0:02:22
>>> alpha1: 0.162  alpha2: -0.055 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [27/50] batch [5/23] time 0.191 (0.465) data 0.000 (0.258) loss 0.3946 (0.4068) acc 90.6863 (89.3860) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [10/23] time 0.201 (0.327) data 0.000 (0.129) loss 0.4893 (0.4694) acc 82.5472 (86.3121) lr 1.0000e-03 eta 0:02:57
epoch [27/50] batch [15/23] time 0.200 (0.281) data 0.001 (0.086) loss 0.4552 (0.4548) acc 89.6226 (86.8916) lr 1.0000e-03 eta 0:02:30
epoch [27/50] batch [20/23] time 0.178 (0.257) data 0.000 (0.065) loss 0.5981 (0.4469) acc 85.5556 (87.3741) lr 1.0000e-03 eta 0:02:16
>>> alpha1: 0.158  alpha2: -0.058 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [28/50] batch [5/23] time 0.334 (0.591) data 0.001 (0.283) loss 0.3594 (0.3491) acc 89.0909 (88.3630) lr 9.3721e-04 eta 0:05:09
epoch [28/50] batch [10/23] time 0.310 (0.438) data 0.000 (0.142) loss 0.6674 (0.4093) acc 81.0185 (87.5985) lr 9.3721e-04 eta 0:03:47
epoch [28/50] batch [15/23] time 0.240 (0.376) data 0.000 (0.095) loss 0.4516 (0.4139) acc 85.3774 (87.2824) lr 9.3721e-04 eta 0:03:13
epoch [28/50] batch [20/23] time 0.146 (0.338) data 0.001 (0.071) loss 0.5083 (0.4180) acc 88.0000 (87.4432) lr 9.3721e-04 eta 0:02:51
>>> alpha1: 0.155  alpha2: -0.054 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [29/50] batch [5/23] time 0.254 (0.473) data 0.000 (0.269) loss 0.3810 (0.3574) acc 91.6667 (89.9702) lr 8.7467e-04 eta 0:03:56
epoch [29/50] batch [10/23] time 0.208 (0.335) data 0.000 (0.135) loss 0.2877 (0.4119) acc 90.2778 (87.9878) lr 8.7467e-04 eta 0:02:46
epoch [29/50] batch [15/23] time 0.184 (0.286) data 0.000 (0.090) loss 0.3816 (0.4099) acc 89.8936 (87.8053) lr 8.7467e-04 eta 0:02:20
epoch [29/50] batch [20/23] time 0.206 (0.262) data 0.000 (0.067) loss 0.5111 (0.4311) acc 84.4340 (87.0456) lr 8.7467e-04 eta 0:02:07
>>> alpha1: 0.151  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [30/50] batch [5/23] time 0.241 (0.467) data 0.001 (0.254) loss 0.3480 (0.3608) acc 91.4894 (91.9407) lr 8.1262e-04 eta 0:03:43
epoch [30/50] batch [10/23] time 0.172 (0.325) data 0.000 (0.127) loss 0.3940 (0.3508) acc 86.0465 (90.5905) lr 8.1262e-04 eta 0:02:33
epoch [30/50] batch [15/23] time 0.219 (0.280) data 0.000 (0.085) loss 0.4320 (0.3772) acc 89.2857 (90.0789) lr 8.1262e-04 eta 0:02:11
epoch [30/50] batch [20/23] time 0.186 (0.254) data 0.000 (0.064) loss 0.4020 (0.3963) acc 89.7959 (88.9379) lr 8.1262e-04 eta 0:01:57
>>> alpha1: 0.146  alpha2: -0.055 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [31/50] batch [5/23] time 0.201 (0.465) data 0.000 (0.257) loss 0.2706 (0.3508) acc 93.6274 (88.7465) lr 7.5131e-04 eta 0:03:31
epoch [31/50] batch [10/23] time 0.209 (0.337) data 0.000 (0.129) loss 0.4507 (0.3778) acc 84.9057 (88.3842) lr 7.5131e-04 eta 0:02:31
epoch [31/50] batch [15/23] time 0.206 (0.292) data 0.000 (0.086) loss 0.3292 (0.3855) acc 88.8889 (88.8196) lr 7.5131e-04 eta 0:02:09
epoch [31/50] batch [20/23] time 0.200 (0.271) data 0.000 (0.065) loss 0.3978 (0.3883) acc 87.9808 (88.6635) lr 7.5131e-04 eta 0:01:59
>>> alpha1: 0.147  alpha2: -0.053 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [32/50] batch [5/23] time 0.252 (0.579) data 0.000 (0.309) loss 0.6074 (0.3577) acc 86.7647 (90.4429) lr 6.9098e-04 eta 0:04:09
epoch [32/50] batch [10/23] time 0.262 (0.420) data 0.000 (0.155) loss 0.4086 (0.3777) acc 90.9091 (89.8657) lr 6.9098e-04 eta 0:02:59
epoch [32/50] batch [15/23] time 0.255 (0.364) data 0.000 (0.103) loss 0.3291 (0.3738) acc 94.2308 (90.2887) lr 6.9098e-04 eta 0:02:33
epoch [32/50] batch [20/23] time 0.176 (0.368) data 0.000 (0.077) loss 0.5772 (0.3972) acc 77.1277 (89.0241) lr 6.9098e-04 eta 0:02:33
>>> alpha1: 0.145  alpha2: -0.046 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.28 <<<
epoch [33/50] batch [5/23] time 0.197 (0.501) data 0.000 (0.278) loss 0.5659 (0.3572) acc 79.5000 (88.1066) lr 6.3188e-04 eta 0:03:24
epoch [33/50] batch [10/23] time 0.215 (0.350) data 0.000 (0.139) loss 0.2507 (0.3290) acc 96.0526 (89.8281) lr 6.3188e-04 eta 0:02:21
epoch [33/50] batch [15/23] time 0.210 (0.299) data 0.000 (0.093) loss 0.4666 (0.3602) acc 86.8182 (88.7132) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [20/23] time 0.202 (0.333) data 0.000 (0.070) loss 0.3056 (0.3627) acc 88.4259 (89.0325) lr 6.3188e-04 eta 0:02:11
>>> alpha1: 0.143  alpha2: -0.039 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [34/50] batch [5/23] time 0.190 (0.416) data 0.000 (0.211) loss 0.3725 (0.3429) acc 93.0000 (88.1209) lr 5.7422e-04 eta 0:02:40
epoch [34/50] batch [10/23] time 0.201 (0.312) data 0.000 (0.107) loss 0.3329 (0.3286) acc 86.5385 (89.1792) lr 5.7422e-04 eta 0:01:58
epoch [34/50] batch [15/23] time 0.222 (0.278) data 0.000 (0.071) loss 0.3451 (0.3451) acc 90.8654 (89.7048) lr 5.7422e-04 eta 0:01:44
epoch [34/50] batch [20/23] time 0.183 (0.256) data 0.000 (0.054) loss 0.2455 (0.3509) acc 92.5532 (89.7629) lr 5.7422e-04 eta 0:01:35
>>> alpha1: 0.142  alpha2: -0.045 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [35/50] batch [5/23] time 0.189 (0.475) data 0.000 (0.264) loss 0.4139 (0.3013) acc 89.7959 (91.7716) lr 5.1825e-04 eta 0:02:52
epoch [35/50] batch [10/23] time 0.216 (0.342) data 0.000 (0.132) loss 0.2680 (0.3235) acc 88.8889 (89.7233) lr 5.1825e-04 eta 0:02:02
epoch [35/50] batch [15/23] time 0.183 (0.298) data 0.000 (0.088) loss 0.4608 (0.3345) acc 90.4255 (90.0981) lr 5.1825e-04 eta 0:01:45
epoch [35/50] batch [20/23] time 0.206 (0.273) data 0.000 (0.066) loss 0.4060 (0.3338) acc 89.0909 (90.0916) lr 5.1825e-04 eta 0:01:34
>>> alpha1: 0.143  alpha2: -0.047 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [36/50] batch [5/23] time 0.179 (0.484) data 0.000 (0.282) loss 0.3308 (0.3756) acc 90.6250 (90.4856) lr 4.6417e-04 eta 0:02:44
epoch [36/50] batch [10/23] time 0.184 (0.335) data 0.000 (0.143) loss 0.3166 (0.6262) acc 90.8163 (86.7595) lr 4.6417e-04 eta 0:01:52
epoch [36/50] batch [15/23] time 0.216 (0.292) data 0.000 (0.095) loss 0.3186 (0.5258) acc 92.4107 (88.2236) lr 4.6417e-04 eta 0:01:36
epoch [36/50] batch [20/23] time 0.196 (0.267) data 0.000 (0.071) loss 0.2598 (0.4821) acc 96.0000 (88.6884) lr 4.6417e-04 eta 0:01:26
>>> alpha1: 0.142  alpha2: -0.046 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [37/50] batch [5/23] time 0.163 (0.474) data 0.000 (0.266) loss 0.3958 (0.3172) acc 89.0244 (90.7551) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [10/23] time 0.184 (0.335) data 0.000 (0.133) loss 0.3767 (0.3341) acc 90.6250 (89.6605) lr 4.1221e-04 eta 0:01:44
epoch [37/50] batch [15/23] time 0.204 (0.288) data 0.000 (0.089) loss 0.2590 (0.3388) acc 94.9074 (89.8222) lr 4.1221e-04 eta 0:01:28
epoch [37/50] batch [20/23] time 0.188 (0.268) data 0.000 (0.067) loss 0.4840 (0.3525) acc 87.7551 (89.6397) lr 4.1221e-04 eta 0:01:20
>>> alpha1: 0.141  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [38/50] batch [5/23] time 0.194 (0.429) data 0.001 (0.240) loss 0.3344 (0.3036) acc 88.0000 (91.0556) lr 3.6258e-04 eta 0:02:06
epoch [38/50] batch [10/23] time 0.196 (0.311) data 0.000 (0.120) loss 0.3945 (0.3323) acc 88.0000 (91.2014) lr 3.6258e-04 eta 0:01:30
epoch [38/50] batch [15/23] time 0.190 (0.274) data 0.000 (0.080) loss 0.3665 (0.3321) acc 87.5000 (90.8525) lr 3.6258e-04 eta 0:01:17
epoch [38/50] batch [20/23] time 0.181 (0.251) data 0.000 (0.060) loss 0.2936 (0.3465) acc 93.6170 (90.7936) lr 3.6258e-04 eta 0:01:09
>>> alpha1: 0.138  alpha2: -0.055 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [39/50] batch [5/23] time 0.276 (0.477) data 0.000 (0.256) loss 0.3849 (0.3710) acc 88.8889 (88.9874) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [10/23] time 0.186 (0.334) data 0.000 (0.128) loss 0.3555 (0.3540) acc 89.7959 (89.9257) lr 3.1545e-04 eta 0:01:28
epoch [39/50] batch [15/23] time 0.201 (0.289) data 0.000 (0.086) loss 0.2678 (0.3416) acc 95.3704 (90.8667) lr 3.1545e-04 eta 0:01:15
epoch [39/50] batch [20/23] time 0.177 (0.264) data 0.000 (0.064) loss 0.5083 (0.3528) acc 90.0000 (89.7165) lr 3.1545e-04 eta 0:01:07
>>> alpha1: 0.137  alpha2: -0.056 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [40/50] batch [5/23] time 0.202 (0.468) data 0.000 (0.272) loss 0.4134 (0.3569) acc 90.9091 (91.0568) lr 2.7103e-04 eta 0:01:55
epoch [40/50] batch [10/23] time 0.215 (0.336) data 0.001 (0.136) loss 0.3074 (0.3263) acc 94.5455 (91.5245) lr 2.7103e-04 eta 0:01:21
epoch [40/50] batch [15/23] time 0.188 (0.290) data 0.001 (0.091) loss 0.3605 (0.3527) acc 91.8367 (91.1228) lr 2.7103e-04 eta 0:01:08
epoch [40/50] batch [20/23] time 0.210 (0.270) data 0.000 (0.068) loss 0.3802 (0.3552) acc 87.0536 (90.8747) lr 2.7103e-04 eta 0:01:02
>>> alpha1: 0.137  alpha2: -0.057 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [41/50] batch [5/23] time 0.211 (0.467) data 0.000 (0.253) loss 0.5125 (0.3685) acc 80.6604 (87.9588) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [10/23] time 0.199 (0.333) data 0.001 (0.127) loss 0.2472 (0.3381) acc 94.8113 (88.8746) lr 2.2949e-04 eta 0:01:13
epoch [41/50] batch [15/23] time 0.207 (0.288) data 0.000 (0.085) loss 0.3878 (0.3273) acc 92.4528 (90.1675) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [20/23] time 0.185 (0.271) data 0.000 (0.064) loss 0.3385 (0.3319) acc 93.6170 (90.5126) lr 2.2949e-04 eta 0:00:56
>>> alpha1: 0.135  alpha2: -0.059 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [42/50] batch [5/23] time 0.185 (0.487) data 0.000 (0.282) loss 0.4102 (0.3297) acc 92.1875 (92.8466) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [10/23] time 0.276 (0.355) data 0.000 (0.141) loss 0.3299 (0.3229) acc 91.2037 (92.5099) lr 1.9098e-04 eta 0:01:10
epoch [42/50] batch [15/23] time 0.204 (0.304) data 0.000 (0.094) loss 0.1870 (0.3095) acc 94.8113 (92.1841) lr 1.9098e-04 eta 0:00:58
epoch [42/50] batch [20/23] time 0.207 (0.279) data 0.000 (0.071) loss 0.3661 (0.3105) acc 89.1509 (91.5595) lr 1.9098e-04 eta 0:00:52
>>> alpha1: 0.137  alpha2: -0.055 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [43/50] batch [5/23] time 0.313 (0.578) data 0.000 (0.278) loss 0.2274 (0.2797) acc 95.3704 (93.0284) lr 1.5567e-04 eta 0:01:43
epoch [43/50] batch [10/23] time 0.294 (0.447) data 0.000 (0.139) loss 0.3432 (0.2703) acc 87.7451 (92.5734) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [15/23] time 0.307 (0.397) data 0.000 (0.093) loss 0.3384 (0.2904) acc 91.9811 (92.3820) lr 1.5567e-04 eta 0:01:07
epoch [43/50] batch [20/23] time 0.151 (0.347) data 0.001 (0.070) loss 0.3361 (0.3104) acc 94.5000 (92.3305) lr 1.5567e-04 eta 0:00:56
>>> alpha1: 0.137  alpha2: -0.058 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [44/50] batch [5/23] time 0.173 (0.505) data 0.000 (0.306) loss 0.1484 (0.3386) acc 96.2766 (92.0744) lr 1.2369e-04 eta 0:01:18
epoch [44/50] batch [10/23] time 0.173 (0.343) data 0.000 (0.153) loss 0.2602 (0.5648) acc 95.6522 (88.1497) lr 1.2369e-04 eta 0:00:51
epoch [44/50] batch [15/23] time 0.184 (0.289) data 0.001 (0.102) loss 0.2452 (0.4749) acc 92.8571 (89.3850) lr 1.2369e-04 eta 0:00:42
epoch [44/50] batch [20/23] time 0.186 (0.262) data 0.000 (0.077) loss 0.1541 (0.4290) acc 97.3958 (89.9742) lr 1.2369e-04 eta 0:00:36
>>> alpha1: 0.137  alpha2: -0.055 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [45/50] batch [5/23] time 0.179 (0.462) data 0.000 (0.267) loss 0.3470 (0.3656) acc 93.2292 (88.3316) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [10/23] time 0.187 (0.326) data 0.000 (0.134) loss 0.3576 (0.3457) acc 94.3878 (90.8425) lr 9.5173e-05 eta 0:00:41
epoch [45/50] batch [15/23] time 0.186 (0.284) data 0.000 (0.089) loss 0.3535 (0.3197) acc 90.6250 (91.4250) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [20/23] time 0.191 (0.266) data 0.000 (0.067) loss 0.3380 (0.3148) acc 88.7255 (91.7726) lr 9.5173e-05 eta 0:00:31
>>> alpha1: 0.135  alpha2: -0.048 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [46/50] batch [5/23] time 0.183 (0.456) data 0.000 (0.260) loss 0.3325 (0.2810) acc 88.2979 (93.6357) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [10/23] time 0.189 (0.325) data 0.000 (0.130) loss 0.3859 (0.2916) acc 89.0625 (92.7959) lr 7.0224e-05 eta 0:00:34
epoch [46/50] batch [15/23] time 0.187 (0.281) data 0.000 (0.087) loss 0.4256 (0.3104) acc 85.9375 (92.3680) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.184 (0.256) data 0.000 (0.065) loss 0.2656 (0.3043) acc 90.4255 (92.4346) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.136  alpha2: -0.040 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.25 <<<
epoch [47/50] batch [5/23] time 0.249 (0.538) data 0.000 (0.300) loss 0.2522 (0.3579) acc 94.7917 (91.1406) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [10/23] time 0.246 (0.392) data 0.000 (0.150) loss 0.3034 (0.3109) acc 90.4255 (91.4866) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [15/23] time 0.234 (0.342) data 0.000 (0.100) loss 0.3147 (0.3236) acc 94.6808 (91.2239) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [20/23] time 0.141 (0.292) data 0.000 (0.075) loss 0.2843 (0.3161) acc 90.1042 (91.2871) lr 4.8943e-05 eta 0:00:21
>>> alpha1: 0.133  alpha2: -0.042 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.26 <<<
epoch [48/50] batch [5/23] time 0.204 (0.444) data 0.000 (0.235) loss 0.4162 (0.3446) acc 85.0000 (92.2552) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.189 (0.321) data 0.000 (0.118) loss 0.3925 (0.3350) acc 90.3061 (91.6876) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [15/23] time 0.193 (0.281) data 0.000 (0.078) loss 0.2297 (0.3145) acc 89.0000 (91.4506) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.204 (0.261) data 0.000 (0.059) loss 0.2195 (0.3027) acc 92.9245 (91.6150) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.132  alpha2: -0.043 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.27 <<<
epoch [49/50] batch [5/23] time 0.202 (0.457) data 0.000 (0.241) loss 0.1972 (0.3671) acc 91.0377 (88.7390) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.210 (0.329) data 0.014 (0.122) loss 0.2829 (0.3423) acc 93.3673 (90.5220) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.203 (0.284) data 0.000 (0.082) loss 0.2587 (0.3132) acc 96.2264 (91.4585) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.279 (0.265) data 0.000 (0.061) loss 0.4017 (0.3161) acc 89.2857 (91.0314) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.131  alpha2: -0.042 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.28 <<<
epoch [50/50] batch [5/23] time 0.206 (0.438) data 0.000 (0.230) loss 0.2231 (0.2375) acc 95.0000 (94.6370) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.205 (0.318) data 0.000 (0.115) loss 0.2653 (0.3172) acc 87.2642 (90.8423) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.182 (0.280) data 0.000 (0.077) loss 0.3293 (0.3151) acc 92.1875 (90.9075) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.209 (0.259) data 0.000 (0.058) loss 0.2667 (0.3077) acc 96.3636 (91.3583) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.17, 0.18, 0.16, 0.15, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.17, 0.18, 0.17, 0.18, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.18, 0.18, 0.17, 0.18, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18]
* matched noise rate: [0.05, 0.06, 0.06, 0.05, 0.07, 0.09, 0.09, 0.07, 0.09, 0.09, 0.08, 0.08, 0.1, 0.09, 0.1, 0.08, 0.09, 0.09, 0.09, 0.09, 0.1, 0.11, 0.11, 0.1, 0.09, 0.09, 0.09, 0.09, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.11, 0.11, 0.11]
* unmatched noise rate: [0.27, 0.28, 0.24, 0.21, 0.25, 0.27, 0.26, 0.22, 0.23, 0.23, 0.23, 0.24, 0.25, 0.24, 0.25, 0.25, 0.27, 0.28, 0.26, 0.25, 0.29, 0.29, 0.28, 0.29, 0.3, 0.28, 0.27, 0.25, 0.29, 0.3, 0.3, 0.32, 0.28, 0.25, 0.28, 0.25, 0.25, 0.26, 0.27, 0.28]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:43,  2.69s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.30it/s] 29%|██▉       | 5/17 [00:03<00:05,  2.39it/s] 41%|████      | 7/17 [00:03<00:02,  3.56it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.76it/s] 65%|██████▍   | 11/17 [00:03<00:01,  5.91it/s] 76%|███████▋  | 13/17 [00:03<00:00,  6.99it/s] 88%|████████▊ | 15/17 [00:03<00:00,  8.18it/s]100%|██████████| 17/17 [00:04<00:00,  5.55it/s]100%|██████████| 17/17 [00:04<00:00,  3.56it/s]
=> result
* total: 1,692
* correct: 1,067
* accuracy: 63.1%
* error: 36.9%
* macro_f1: 62.3%
=> per-class result
* class: 0 (banded)	total: 36	correct: 20	acc: 55.6%
* class: 1 (blotchy)	total: 36	correct: 8	acc: 22.2%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 30	acc: 83.3%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 30	acc: 83.3%
* class: 7 (cracked)	total: 36	correct: 29	acc: 80.6%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 34	acc: 94.4%
* class: 10 (dotted)	total: 36	correct: 21	acc: 58.3%
* class: 11 (fibrous)	total: 36	correct: 31	acc: 86.1%
* class: 12 (flecked)	total: 36	correct: 15	acc: 41.7%
* class: 13 (freckled)	total: 36	correct: 29	acc: 80.6%
* class: 14 (frilly)	total: 36	correct: 28	acc: 77.8%
* class: 15 (gauzy)	total: 36	correct: 22	acc: 61.1%
* class: 16 (grid)	total: 36	correct: 15	acc: 41.7%
* class: 17 (grooved)	total: 36	correct: 15	acc: 41.7%
* class: 18 (honeycombed)	total: 36	correct: 24	acc: 66.7%
* class: 19 (interlaced)	total: 36	correct: 26	acc: 72.2%
* class: 20 (knitted)	total: 36	correct: 30	acc: 83.3%
* class: 21 (lacelike)	total: 36	correct: 36	acc: 100.0%
* class: 22 (lined)	total: 36	correct: 26	acc: 72.2%
* class: 23 (marbled)	total: 36	correct: 20	acc: 55.6%
* class: 24 (matted)	total: 36	correct: 20	acc: 55.6%
* class: 25 (meshed)	total: 36	correct: 22	acc: 61.1%
* class: 26 (paisley)	total: 36	correct: 34	acc: 94.4%
* class: 27 (perforated)	total: 36	correct: 24	acc: 66.7%
* class: 28 (pitted)	total: 36	correct: 17	acc: 47.2%
* class: 29 (pleated)	total: 36	correct: 17	acc: 47.2%
* class: 30 (polka-dotted)	total: 36	correct: 29	acc: 80.6%
* class: 31 (porous)	total: 36	correct: 5	acc: 13.9%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 20	acc: 55.6%
* class: 34 (smeared)	total: 36	correct: 16	acc: 44.4%
* class: 35 (spiralled)	total: 36	correct: 23	acc: 63.9%
* class: 36 (sprinkled)	total: 36	correct: 15	acc: 41.7%
* class: 37 (stained)	total: 36	correct: 23	acc: 63.9%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 29	acc: 80.6%
* class: 40 (studded)	total: 36	correct: 27	acc: 75.0%
* class: 41 (swirly)	total: 36	correct: 22	acc: 61.1%
* class: 42 (veined)	total: 36	correct: 21	acc: 58.3%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 15	acc: 41.7%
* class: 45 (wrinkled)	total: 36	correct: 18	acc: 50.0%
* class: 46 (zigzagged)	total: 36	correct: 29	acc: 80.6%
* average: 63.1%
Elapsed: 0:16:07
Run this job and save the output to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_4-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.215 (1.031) data 0.000 (0.317) loss 3.5088 (3.5808) acc 31.2500 (23.7500) lr 1.0000e-05 eta 0:19:40
epoch [1/50] batch [10/23] time 0.201 (0.625) data 0.000 (0.159) loss 3.4835 (3.5879) acc 25.0000 (19.3750) lr 1.0000e-05 eta 0:11:52
epoch [1/50] batch [15/23] time 0.205 (0.484) data 0.000 (0.106) loss 3.3767 (3.5505) acc 31.2500 (18.7500) lr 1.0000e-05 eta 0:09:09
epoch [1/50] batch [20/23] time 0.205 (0.414) data 0.000 (0.079) loss 3.4107 (3.5353) acc 21.8750 (18.2812) lr 1.0000e-05 eta 0:07:47
epoch [2/50] batch [5/23] time 0.274 (0.525) data 0.000 (0.292) loss 2.8583 (3.2570) acc 34.3750 (20.0000) lr 2.0000e-03 eta 0:09:48
epoch [2/50] batch [10/23] time 0.204 (0.369) data 0.000 (0.146) loss 2.9705 (3.2023) acc 25.0000 (21.8750) lr 2.0000e-03 eta 0:06:51
epoch [2/50] batch [15/23] time 0.204 (0.314) data 0.000 (0.097) loss 2.8883 (3.2124) acc 28.1250 (23.9583) lr 2.0000e-03 eta 0:05:48
epoch [2/50] batch [20/23] time 0.274 (0.290) data 0.000 (0.073) loss 2.5435 (3.0838) acc 25.0000 (25.3125) lr 2.0000e-03 eta 0:05:21
epoch [3/50] batch [5/23] time 0.206 (0.507) data 0.000 (0.257) loss 3.0886 (2.9257) acc 21.8750 (25.6250) lr 1.9980e-03 eta 0:09:17
epoch [3/50] batch [10/23] time 0.198 (0.358) data 0.000 (0.129) loss 3.2350 (3.0844) acc 28.1250 (26.2500) lr 1.9980e-03 eta 0:06:31
epoch [3/50] batch [15/23] time 0.194 (0.303) data 0.000 (0.086) loss 2.9038 (2.9999) acc 34.3750 (29.3750) lr 1.9980e-03 eta 0:05:29
epoch [3/50] batch [20/23] time 0.201 (0.277) data 0.000 (0.064) loss 2.7995 (2.9370) acc 34.3750 (30.9375) lr 1.9980e-03 eta 0:04:59
epoch [4/50] batch [5/23] time 0.286 (0.518) data 0.000 (0.276) loss 2.5708 (2.7716) acc 34.3750 (34.3750) lr 1.9921e-03 eta 0:09:17
epoch [4/50] batch [10/23] time 0.205 (0.362) data 0.000 (0.138) loss 2.5786 (2.7258) acc 28.1250 (35.3125) lr 1.9921e-03 eta 0:06:27
epoch [4/50] batch [15/23] time 0.195 (0.306) data 0.000 (0.092) loss 2.7583 (2.7336) acc 31.2500 (35.6250) lr 1.9921e-03 eta 0:05:26
epoch [4/50] batch [20/23] time 0.205 (0.280) data 0.000 (0.069) loss 2.6059 (2.7292) acc 50.0000 (36.4062) lr 1.9921e-03 eta 0:04:56
epoch [5/50] batch [5/23] time 0.289 (0.568) data 0.000 (0.326) loss 2.6151 (2.6233) acc 31.2500 (37.5000) lr 1.9823e-03 eta 0:09:57
epoch [5/50] batch [10/23] time 0.203 (0.390) data 0.000 (0.163) loss 2.4664 (2.5829) acc 46.8750 (39.6875) lr 1.9823e-03 eta 0:06:48
epoch [5/50] batch [15/23] time 0.202 (0.328) data 0.000 (0.109) loss 2.4932 (2.6250) acc 50.0000 (39.3750) lr 1.9823e-03 eta 0:05:42
epoch [5/50] batch [20/23] time 0.252 (0.300) data 0.000 (0.082) loss 2.7347 (2.6081) acc 40.6250 (39.2188) lr 1.9823e-03 eta 0:05:11
epoch [6/50] batch [5/23] time 0.211 (0.496) data 0.000 (0.247) loss 3.0222 (2.6442) acc 34.3750 (43.1250) lr 1.9686e-03 eta 0:08:31
epoch [6/50] batch [10/23] time 0.199 (0.354) data 0.000 (0.124) loss 2.9130 (2.5592) acc 40.6250 (42.8125) lr 1.9686e-03 eta 0:06:03
epoch [6/50] batch [15/23] time 0.197 (0.302) data 0.000 (0.083) loss 2.3762 (2.5524) acc 53.1250 (41.4583) lr 1.9686e-03 eta 0:05:07
epoch [6/50] batch [20/23] time 0.226 (0.278) data 0.000 (0.062) loss 2.4621 (2.5016) acc 46.8750 (42.8125) lr 1.9686e-03 eta 0:04:42
epoch [7/50] batch [5/23] time 0.216 (0.515) data 0.000 (0.271) loss 2.4896 (2.2620) acc 40.6250 (50.0000) lr 1.9511e-03 eta 0:08:38
epoch [7/50] batch [10/23] time 0.206 (0.361) data 0.000 (0.136) loss 2.4634 (2.3906) acc 50.0000 (45.3125) lr 1.9511e-03 eta 0:06:01
epoch [7/50] batch [15/23] time 0.205 (0.309) data 0.000 (0.091) loss 2.4788 (2.3993) acc 43.7500 (45.0000) lr 1.9511e-03 eta 0:05:07
epoch [7/50] batch [20/23] time 0.201 (0.286) data 0.000 (0.068) loss 2.4874 (2.4561) acc 40.6250 (44.0625) lr 1.9511e-03 eta 0:04:43
epoch [8/50] batch [5/23] time 0.231 (0.532) data 0.000 (0.313) loss 2.4362 (2.4914) acc 50.0000 (44.3750) lr 1.9298e-03 eta 0:08:43
epoch [8/50] batch [10/23] time 0.210 (0.370) data 0.000 (0.156) loss 2.2106 (2.3772) acc 46.8750 (46.8750) lr 1.9298e-03 eta 0:06:02
epoch [8/50] batch [15/23] time 0.202 (0.320) data 0.000 (0.104) loss 2.5691 (2.3213) acc 40.6250 (47.7083) lr 1.9298e-03 eta 0:05:11
epoch [8/50] batch [20/23] time 0.204 (0.291) data 0.000 (0.078) loss 2.1651 (2.3521) acc 43.7500 (47.5000) lr 1.9298e-03 eta 0:04:41
epoch [9/50] batch [5/23] time 0.222 (0.498) data 0.000 (0.264) loss 2.4783 (2.3639) acc 43.7500 (52.5000) lr 1.9048e-03 eta 0:07:58
epoch [9/50] batch [10/23] time 0.212 (0.353) data 0.000 (0.132) loss 1.6308 (2.2400) acc 75.0000 (54.6875) lr 1.9048e-03 eta 0:05:37
epoch [9/50] batch [15/23] time 0.201 (0.306) data 0.000 (0.088) loss 2.8434 (2.2905) acc 34.3750 (51.6667) lr 1.9048e-03 eta 0:04:51
epoch [9/50] batch [20/23] time 0.206 (0.281) data 0.000 (0.066) loss 2.7610 (2.3321) acc 37.5000 (49.5312) lr 1.9048e-03 eta 0:04:25
epoch [10/50] batch [5/23] time 0.328 (0.554) data 0.000 (0.311) loss 2.1776 (1.9974) acc 50.0000 (53.1250) lr 1.8763e-03 eta 0:08:39
epoch [10/50] batch [10/23] time 0.310 (0.437) data 0.000 (0.156) loss 2.6917 (2.2030) acc 40.6250 (51.2500) lr 1.8763e-03 eta 0:06:47
epoch [10/50] batch [15/23] time 0.321 (0.397) data 0.000 (0.104) loss 2.3397 (2.3202) acc 43.7500 (48.1250) lr 1.8763e-03 eta 0:06:08
epoch [10/50] batch [20/23] time 0.312 (0.376) data 0.000 (0.078) loss 1.6868 (2.2629) acc 50.0000 (49.8438) lr 1.8763e-03 eta 0:05:47
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.687  alpha2: 0.122 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [11/50] batch [5/23] time 0.918 (0.988) data 0.001 (0.312) loss 1.6540 (1.4182) acc 58.9286 (63.7584) lr 1.8443e-03 eta 0:15:04
epoch [11/50] batch [10/23] time 1.257 (1.083) data 0.000 (0.156) loss 1.1945 (1.4042) acc 62.7551 (63.0225) lr 1.8443e-03 eta 0:16:25
epoch [11/50] batch [15/23] time 0.177 (0.915) data 0.000 (0.104) loss 1.2482 (1.3718) acc 59.4444 (62.8886) lr 1.8443e-03 eta 0:13:47
epoch [11/50] batch [20/23] time 1.168 (0.836) data 0.000 (0.078) loss 1.0942 (1.3441) acc 71.0000 (63.7974) lr 1.8443e-03 eta 0:12:32
>>> alpha1: 0.580  alpha2: 0.160 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/23] time 0.195 (0.494) data 0.000 (0.293) loss 0.7780 (0.9727) acc 74.0000 (71.8998) lr 1.8090e-03 eta 0:07:20
epoch [12/50] batch [10/23] time 0.198 (0.345) data 0.000 (0.147) loss 1.0496 (1.0041) acc 66.8269 (71.2380) lr 1.8090e-03 eta 0:05:06
epoch [12/50] batch [15/23] time 0.179 (0.291) data 0.000 (0.098) loss 1.2863 (1.0446) acc 68.7500 (70.3938) lr 1.8090e-03 eta 0:04:16
epoch [12/50] batch [20/23] time 0.173 (0.357) data 0.000 (0.073) loss 0.9941 (1.0896) acc 68.4783 (69.2376) lr 1.8090e-03 eta 0:05:12
>>> alpha1: 0.512  alpha2: 0.129 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.27 <<<
epoch [13/50] batch [5/23] time 0.154 (0.436) data 0.000 (0.295) loss 1.2794 (1.0904) acc 65.4255 (67.2776) lr 1.7705e-03 eta 0:06:19
epoch [13/50] batch [10/23] time 0.276 (0.312) data 0.000 (0.148) loss 0.8151 (1.0327) acc 82.2917 (70.1814) lr 1.7705e-03 eta 0:04:29
epoch [13/50] batch [15/23] time 0.304 (0.298) data 0.000 (0.099) loss 0.7012 (1.0104) acc 84.3137 (71.2841) lr 1.7705e-03 eta 0:04:15
epoch [13/50] batch [20/23] time 0.270 (0.295) data 0.000 (0.074) loss 0.8743 (1.0005) acc 73.3696 (70.9492) lr 1.7705e-03 eta 0:04:11
>>> alpha1: 0.473  alpha2: 0.104 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.25 <<<
epoch [14/50] batch [5/23] time 0.183 (0.480) data 0.000 (0.280) loss 1.1349 (0.8779) acc 59.3750 (71.3540) lr 1.7290e-03 eta 0:06:46
epoch [14/50] batch [10/23] time 0.170 (0.327) data 0.000 (0.140) loss 0.6886 (0.9056) acc 80.4348 (71.4539) lr 1.7290e-03 eta 0:04:35
epoch [14/50] batch [15/23] time 0.187 (0.276) data 0.000 (0.094) loss 0.6402 (0.9215) acc 84.8039 (71.4421) lr 1.7290e-03 eta 0:03:50
epoch [14/50] batch [20/23] time 0.183 (0.251) data 0.000 (0.070) loss 1.1270 (0.9132) acc 62.2449 (71.2896) lr 1.7290e-03 eta 0:03:28
>>> alpha1: 0.439  alpha2: 0.086 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.31 <<<
epoch [15/50] batch [5/23] time 0.198 (0.767) data 0.000 (0.349) loss 0.6350 (0.8247) acc 80.8824 (75.3705) lr 1.6845e-03 eta 0:10:30
epoch [15/50] batch [10/23] time 0.217 (0.487) data 0.000 (0.175) loss 0.5859 (0.7783) acc 85.0000 (75.6946) lr 1.6845e-03 eta 0:06:38
epoch [15/50] batch [15/23] time 0.207 (0.389) data 0.000 (0.117) loss 0.7737 (0.8319) acc 74.5283 (72.9099) lr 1.6845e-03 eta 0:05:16
epoch [15/50] batch [20/23] time 0.177 (0.342) data 0.000 (0.087) loss 0.8137 (0.8115) acc 79.3478 (74.3694) lr 1.6845e-03 eta 0:04:36
>>> alpha1: 0.351  alpha2: -0.022 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.30 <<<
epoch [16/50] batch [5/23] time 0.189 (0.484) data 0.000 (0.294) loss 0.6080 (0.7173) acc 81.6327 (77.8471) lr 1.6374e-03 eta 0:06:27
epoch [16/50] batch [10/23] time 0.185 (0.347) data 0.001 (0.147) loss 0.7778 (0.7775) acc 76.6667 (76.3891) lr 1.6374e-03 eta 0:04:35
epoch [16/50] batch [15/23] time 0.190 (0.297) data 0.000 (0.098) loss 0.7670 (0.7614) acc 80.1020 (76.8786) lr 1.6374e-03 eta 0:03:54
epoch [16/50] batch [20/23] time 0.191 (0.270) data 0.000 (0.074) loss 0.7839 (0.7578) acc 70.5000 (76.1074) lr 1.6374e-03 eta 0:03:31
>>> alpha1: 0.308  alpha2: -0.057 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [17/50] batch [5/23] time 0.235 (0.585) data 0.001 (0.333) loss 0.7913 (0.6547) acc 75.0000 (79.0523) lr 1.5878e-03 eta 0:07:34
epoch [17/50] batch [10/23] time 0.144 (0.406) data 0.000 (0.167) loss 0.5275 (0.6989) acc 70.9184 (77.6005) lr 1.5878e-03 eta 0:05:13
epoch [17/50] batch [15/23] time 0.246 (0.392) data 0.000 (0.111) loss 0.7877 (0.7447) acc 78.1250 (75.0467) lr 1.5878e-03 eta 0:05:00
epoch [17/50] batch [20/23] time 0.190 (0.340) data 0.000 (0.083) loss 0.4876 (0.7222) acc 86.2745 (76.1189) lr 1.5878e-03 eta 0:04:19
>>> alpha1: 0.283  alpha2: -0.064 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.28 <<<
epoch [18/50] batch [5/23] time 0.200 (0.499) data 0.000 (0.311) loss 0.7580 (1.0231) acc 73.5294 (73.8128) lr 1.5358e-03 eta 0:06:16
epoch [18/50] batch [10/23] time 0.176 (0.349) data 0.000 (0.156) loss 0.8013 (0.9002) acc 77.8409 (74.3293) lr 1.5358e-03 eta 0:04:21
epoch [18/50] batch [15/23] time 0.172 (0.292) data 0.000 (0.104) loss 0.6846 (0.8216) acc 75.5682 (76.8459) lr 1.5358e-03 eta 0:03:37
epoch [18/50] batch [20/23] time 0.170 (0.263) data 0.000 (0.078) loss 0.8834 (0.8155) acc 72.6744 (76.0802) lr 1.5358e-03 eta 0:03:14
>>> alpha1: 0.258  alpha2: -0.080 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [19/50] batch [5/23] time 0.191 (0.503) data 0.000 (0.312) loss 0.4630 (0.5388) acc 87.7551 (84.6939) lr 1.4818e-03 eta 0:06:07
epoch [19/50] batch [10/23] time 0.174 (0.342) data 0.000 (0.156) loss 0.7151 (0.6003) acc 79.1667 (81.7436) lr 1.4818e-03 eta 0:04:08
epoch [19/50] batch [15/23] time 0.170 (0.288) data 0.000 (0.104) loss 0.8450 (0.6251) acc 69.7674 (80.9680) lr 1.4818e-03 eta 0:03:27
epoch [19/50] batch [20/23] time 0.191 (0.265) data 0.000 (0.078) loss 0.5541 (0.6403) acc 79.5918 (80.6127) lr 1.4818e-03 eta 0:03:09
>>> alpha1: 0.240  alpha2: -0.092 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [20/50] batch [5/23] time 0.174 (0.474) data 0.000 (0.291) loss 0.6210 (0.6354) acc 76.1364 (79.6785) lr 1.4258e-03 eta 0:05:35
epoch [20/50] batch [10/23] time 0.171 (0.328) data 0.000 (0.146) loss 0.6881 (0.6108) acc 81.9767 (80.9682) lr 1.4258e-03 eta 0:03:50
epoch [20/50] batch [15/23] time 0.182 (0.285) data 0.000 (0.097) loss 0.4866 (0.6065) acc 82.4468 (80.8459) lr 1.4258e-03 eta 0:03:19
epoch [20/50] batch [20/23] time 0.192 (0.261) data 0.000 (0.073) loss 0.4840 (0.6120) acc 87.7551 (81.2105) lr 1.4258e-03 eta 0:03:00
>>> alpha1: 0.226  alpha2: -0.094 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [21/50] batch [5/23] time 0.172 (0.468) data 0.000 (0.276) loss 0.5453 (0.6413) acc 85.9375 (80.8904) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [10/23] time 0.187 (0.323) data 0.000 (0.138) loss 0.6184 (0.6287) acc 78.5000 (81.1308) lr 1.3681e-03 eta 0:03:39
epoch [21/50] batch [15/23] time 0.189 (0.274) data 0.000 (0.092) loss 0.4126 (0.5873) acc 88.0000 (82.4358) lr 1.3681e-03 eta 0:03:05
epoch [21/50] batch [20/23] time 0.178 (0.251) data 0.000 (0.069) loss 0.5511 (0.6061) acc 83.5106 (81.6609) lr 1.3681e-03 eta 0:02:48
>>> alpha1: 0.223  alpha2: -0.084 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [22/50] batch [5/23] time 0.188 (0.511) data 0.000 (0.299) loss 0.8427 (0.6111) acc 73.3333 (80.8156) lr 1.3090e-03 eta 0:05:38
epoch [22/50] batch [10/23] time 0.166 (0.349) data 0.000 (0.150) loss 0.6293 (0.5835) acc 80.1136 (82.2361) lr 1.3090e-03 eta 0:03:49
epoch [22/50] batch [15/23] time 0.162 (0.291) data 0.000 (0.100) loss 0.6005 (0.5885) acc 89.0244 (82.7701) lr 1.3090e-03 eta 0:03:09
epoch [22/50] batch [20/23] time 0.165 (0.262) data 0.001 (0.075) loss 0.7590 (0.5911) acc 75.0000 (83.0286) lr 1.3090e-03 eta 0:02:49
>>> alpha1: 0.217  alpha2: -0.087 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.31 <<<
epoch [23/50] batch [5/23] time 0.179 (0.467) data 0.000 (0.275) loss 0.6351 (0.5305) acc 80.3191 (84.7033) lr 1.2487e-03 eta 0:04:58
epoch [23/50] batch [10/23] time 0.164 (0.327) data 0.000 (0.138) loss 0.7843 (0.5648) acc 75.0000 (82.0704) lr 1.2487e-03 eta 0:03:27
epoch [23/50] batch [15/23] time 0.163 (0.277) data 0.000 (0.092) loss 0.5925 (0.5799) acc 80.3571 (81.2157) lr 1.2487e-03 eta 0:02:53
epoch [23/50] batch [20/23] time 0.196 (0.257) data 0.000 (0.069) loss 0.6173 (0.5848) acc 79.9020 (81.8841) lr 1.2487e-03 eta 0:02:40
>>> alpha1: 0.203  alpha2: -0.088 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.29 <<<
epoch [24/50] batch [5/23] time 0.285 (0.608) data 0.000 (0.353) loss 0.4348 (0.5724) acc 88.2979 (84.9612) lr 1.1874e-03 eta 0:06:14
epoch [24/50] batch [10/23] time 0.291 (0.441) data 0.000 (0.177) loss 0.5158 (0.5490) acc 83.1633 (84.1016) lr 1.1874e-03 eta 0:04:29
epoch [24/50] batch [15/23] time 0.275 (0.382) data 0.000 (0.118) loss 0.4703 (0.5340) acc 83.6956 (83.8198) lr 1.1874e-03 eta 0:03:51
epoch [24/50] batch [20/23] time 0.271 (0.356) data 0.000 (0.088) loss 0.3512 (0.5300) acc 91.1111 (83.4054) lr 1.1874e-03 eta 0:03:33
>>> alpha1: 0.194  alpha2: -0.087 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [25/50] batch [5/23] time 0.187 (0.492) data 0.000 (0.311) loss 0.5743 (0.4921) acc 79.7619 (83.5958) lr 1.1253e-03 eta 0:04:51
epoch [25/50] batch [10/23] time 0.173 (0.336) data 0.000 (0.155) loss 0.4661 (0.5129) acc 87.7778 (83.5741) lr 1.1253e-03 eta 0:03:17
epoch [25/50] batch [15/23] time 0.169 (0.287) data 0.000 (0.104) loss 0.4189 (0.5252) acc 86.1111 (83.4663) lr 1.1253e-03 eta 0:02:47
epoch [25/50] batch [20/23] time 0.175 (0.298) data 0.000 (0.078) loss 0.5649 (0.5515) acc 86.9318 (82.8221) lr 1.1253e-03 eta 0:02:52
>>> alpha1: 0.191  alpha2: -0.083 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [26/50] batch [5/23] time 0.191 (0.497) data 0.000 (0.310) loss 0.5725 (0.5607) acc 85.5556 (84.6886) lr 1.0628e-03 eta 0:04:43
epoch [26/50] batch [10/23] time 0.177 (0.338) data 0.000 (0.155) loss 0.4449 (0.5045) acc 84.0425 (85.4752) lr 1.0628e-03 eta 0:03:11
epoch [26/50] batch [15/23] time 0.225 (0.290) data 0.000 (0.103) loss 0.5878 (0.4837) acc 80.6818 (85.5780) lr 1.0628e-03 eta 0:02:42
epoch [26/50] batch [20/23] time 0.175 (0.263) data 0.000 (0.078) loss 0.7749 (0.5149) acc 75.5556 (84.7450) lr 1.0628e-03 eta 0:02:25
>>> alpha1: 0.180  alpha2: -0.074 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [27/50] batch [5/23] time 0.252 (0.509) data 0.000 (0.314) loss 0.4416 (0.4847) acc 89.6739 (85.1488) lr 1.0000e-03 eta 0:04:38
epoch [27/50] batch [10/23] time 0.158 (0.343) data 0.000 (0.157) loss 0.5202 (0.4839) acc 85.6250 (84.8058) lr 1.0000e-03 eta 0:03:06
epoch [27/50] batch [15/23] time 0.175 (0.289) data 0.000 (0.105) loss 0.4840 (0.4983) acc 82.0652 (84.6966) lr 1.0000e-03 eta 0:02:35
epoch [27/50] batch [20/23] time 0.173 (0.261) data 0.000 (0.079) loss 0.6754 (0.5236) acc 77.2222 (83.9375) lr 1.0000e-03 eta 0:02:18
>>> alpha1: 0.177  alpha2: -0.078 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [28/50] batch [5/23] time 0.287 (0.550) data 0.000 (0.318) loss 2.5972 (0.9077) acc 54.6875 (78.5981) lr 9.3721e-04 eta 0:04:48
epoch [28/50] batch [10/23] time 0.246 (0.393) data 0.000 (0.159) loss 0.5204 (0.8293) acc 86.1702 (81.8748) lr 9.3721e-04 eta 0:03:23
epoch [28/50] batch [15/23] time 0.157 (0.329) data 0.000 (0.106) loss 0.7034 (0.7246) acc 84.5000 (82.9979) lr 9.3721e-04 eta 0:02:49
epoch [28/50] batch [20/23] time 0.137 (0.283) data 0.000 (0.080) loss 0.5024 (0.6686) acc 88.8298 (83.4649) lr 9.3721e-04 eta 0:02:24
>>> alpha1: 0.174  alpha2: -0.079 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [29/50] batch [5/23] time 0.223 (0.709) data 0.000 (0.248) loss 0.3818 (0.4642) acc 90.0943 (87.7042) lr 8.7467e-04 eta 0:05:55
epoch [29/50] batch [10/23] time 0.185 (0.456) data 0.000 (0.124) loss 0.4618 (0.4651) acc 87.5000 (86.3493) lr 8.7467e-04 eta 0:03:46
epoch [29/50] batch [15/23] time 0.199 (0.377) data 0.001 (0.083) loss 0.4496 (0.4790) acc 82.2115 (85.9763) lr 8.7467e-04 eta 0:03:04
epoch [29/50] batch [20/23] time 0.202 (0.332) data 0.000 (0.062) loss 0.4452 (0.4943) acc 81.1321 (85.2782) lr 8.7467e-04 eta 0:02:41
>>> alpha1: 0.172  alpha2: -0.081 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [30/50] batch [5/23] time 0.169 (0.534) data 0.001 (0.343) loss 0.3628 (0.4425) acc 93.0233 (87.5682) lr 8.1262e-04 eta 0:04:15
epoch [30/50] batch [10/23] time 0.181 (0.357) data 0.000 (0.172) loss 0.4011 (0.4579) acc 87.5000 (86.3288) lr 8.1262e-04 eta 0:02:48
epoch [30/50] batch [15/23] time 0.194 (0.302) data 0.000 (0.115) loss 0.4479 (0.4511) acc 87.5000 (87.2698) lr 8.1262e-04 eta 0:02:21
epoch [30/50] batch [20/23] time 0.176 (0.271) data 0.000 (0.086) loss 0.4671 (0.4631) acc 86.1702 (87.0976) lr 8.1262e-04 eta 0:02:05
>>> alpha1: 0.171  alpha2: -0.090 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [31/50] batch [5/23] time 0.183 (0.510) data 0.000 (0.323) loss 0.4079 (0.3797) acc 91.3043 (89.3084) lr 7.5131e-04 eta 0:03:52
epoch [31/50] batch [10/23] time 0.174 (0.347) data 0.000 (0.161) loss 0.3910 (0.3994) acc 81.9767 (87.9294) lr 7.5131e-04 eta 0:02:36
epoch [31/50] batch [15/23] time 0.185 (0.290) data 0.000 (0.108) loss 0.4745 (0.4382) acc 87.0000 (86.9430) lr 7.5131e-04 eta 0:02:09
epoch [31/50] batch [20/23] time 0.170 (0.262) data 0.000 (0.081) loss 0.5289 (0.4443) acc 86.6667 (86.9807) lr 7.5131e-04 eta 0:01:55
>>> alpha1: 0.171  alpha2: -0.092 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [32/50] batch [5/23] time 0.231 (0.555) data 0.000 (0.354) loss 0.5039 (0.4568) acc 80.5556 (86.0661) lr 6.9098e-04 eta 0:03:59
epoch [32/50] batch [10/23] time 0.181 (0.370) data 0.000 (0.177) loss 0.4799 (0.4309) acc 85.9375 (87.5261) lr 6.9098e-04 eta 0:02:37
epoch [32/50] batch [15/23] time 0.167 (0.306) data 0.000 (0.118) loss 0.5846 (0.4534) acc 76.2500 (86.7906) lr 6.9098e-04 eta 0:02:09
epoch [32/50] batch [20/23] time 0.195 (0.315) data 0.001 (0.089) loss 0.3326 (0.4437) acc 92.3913 (87.1515) lr 6.9098e-04 eta 0:02:11
>>> alpha1: 0.163  alpha2: -0.092 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [33/50] batch [5/23] time 0.178 (0.494) data 0.000 (0.302) loss 0.6178 (0.4772) acc 82.6087 (87.2314) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [10/23] time 0.169 (0.337) data 0.000 (0.151) loss 0.4208 (0.4629) acc 89.8810 (87.0710) lr 6.3188e-04 eta 0:02:16
epoch [33/50] batch [15/23] time 0.192 (0.284) data 0.000 (0.101) loss 0.3317 (0.4557) acc 91.8367 (87.7799) lr 6.3188e-04 eta 0:01:53
epoch [33/50] batch [20/23] time 0.181 (0.261) data 0.000 (0.076) loss 0.3235 (0.4516) acc 90.4255 (87.1440) lr 6.3188e-04 eta 0:01:43
>>> alpha1: 0.162  alpha2: -0.091 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.28 <<<
epoch [34/50] batch [5/23] time 0.190 (0.535) data 0.000 (0.335) loss 0.2747 (0.3836) acc 89.0000 (91.0194) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [10/23] time 0.186 (0.361) data 0.000 (0.168) loss 0.4490 (0.4190) acc 88.0208 (89.2855) lr 5.7422e-04 eta 0:02:17
epoch [34/50] batch [15/23] time 0.178 (0.302) data 0.000 (0.112) loss 0.4741 (0.4285) acc 84.4445 (88.2850) lr 5.7422e-04 eta 0:01:53
epoch [34/50] batch [20/23] time 0.184 (0.271) data 0.000 (0.084) loss 0.4286 (0.4151) acc 90.5556 (88.6180) lr 5.7422e-04 eta 0:01:40
>>> alpha1: 0.158  alpha2: -0.089 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [35/50] batch [5/23] time 0.290 (0.601) data 0.000 (0.326) loss 0.4379 (0.4135) acc 90.4255 (88.6904) lr 5.1825e-04 eta 0:03:38
epoch [35/50] batch [10/23] time 0.310 (0.448) data 0.000 (0.163) loss 0.4984 (0.4459) acc 83.3333 (87.6241) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [15/23] time 0.308 (0.397) data 0.000 (0.109) loss 0.2133 (0.4210) acc 95.6731 (88.0499) lr 5.1825e-04 eta 0:02:20
epoch [35/50] batch [20/23] time 0.320 (0.372) data 0.000 (0.082) loss 0.5504 (0.4363) acc 79.0909 (87.2320) lr 5.1825e-04 eta 0:02:09
>>> alpha1: 0.154  alpha2: -0.086 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [36/50] batch [5/23] time 0.206 (0.528) data 0.000 (0.327) loss 0.2976 (0.4083) acc 94.6429 (85.7754) lr 4.6417e-04 eta 0:02:59
epoch [36/50] batch [10/23] time 0.275 (0.368) data 0.000 (0.164) loss 0.4676 (0.4156) acc 85.7843 (86.4930) lr 4.6417e-04 eta 0:02:03
epoch [36/50] batch [15/23] time 0.205 (0.309) data 0.000 (0.109) loss 0.4535 (0.5526) acc 91.2037 (84.9799) lr 4.6417e-04 eta 0:01:41
epoch [36/50] batch [20/23] time 0.179 (0.280) data 0.000 (0.082) loss 0.3018 (0.5281) acc 93.4783 (85.2113) lr 4.6417e-04 eta 0:01:30
>>> alpha1: 0.152  alpha2: -0.082 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [37/50] batch [5/23] time 0.188 (0.557) data 0.000 (0.341) loss 0.2596 (0.3437) acc 90.5000 (89.6456) lr 4.1221e-04 eta 0:02:56
epoch [37/50] batch [10/23] time 0.194 (0.377) data 0.000 (0.171) loss 0.5369 (0.3961) acc 82.2115 (88.5809) lr 4.1221e-04 eta 0:01:57
epoch [37/50] batch [15/23] time 0.201 (0.317) data 0.001 (0.114) loss 0.3726 (0.3871) acc 93.2692 (89.1924) lr 4.1221e-04 eta 0:01:37
epoch [37/50] batch [20/23] time 0.188 (0.346) data 0.000 (0.086) loss 0.4583 (0.4169) acc 86.5385 (87.9097) lr 4.1221e-04 eta 0:01:44
>>> alpha1: 0.150  alpha2: -0.079 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.34 <<<
epoch [38/50] batch [5/23] time 0.266 (0.578) data 0.000 (0.362) loss 0.2423 (0.3857) acc 94.2982 (88.8787) lr 3.6258e-04 eta 0:02:49
epoch [38/50] batch [10/23] time 0.204 (0.385) data 0.000 (0.181) loss 0.2851 (0.3859) acc 95.0893 (90.1315) lr 3.6258e-04 eta 0:01:51
epoch [38/50] batch [15/23] time 0.190 (0.322) data 0.000 (0.121) loss 0.6154 (0.3976) acc 80.2885 (89.6972) lr 3.6258e-04 eta 0:01:31
epoch [38/50] batch [20/23] time 0.192 (0.290) data 0.000 (0.091) loss 0.3667 (0.3983) acc 90.5000 (89.3056) lr 3.6258e-04 eta 0:01:20
>>> alpha1: 0.147  alpha2: -0.076 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.33 <<<
epoch [39/50] batch [5/23] time 0.272 (0.562) data 0.000 (0.296) loss 0.2181 (0.3112) acc 94.3396 (92.7820) lr 3.1545e-04 eta 0:02:32
epoch [39/50] batch [10/23] time 0.146 (0.364) data 0.000 (0.148) loss 0.5333 (0.3890) acc 88.5417 (89.8228) lr 3.1545e-04 eta 0:01:36
epoch [39/50] batch [15/23] time 0.159 (0.292) data 0.000 (0.099) loss 0.4912 (0.4117) acc 87.5000 (89.4354) lr 3.1545e-04 eta 0:01:16
epoch [39/50] batch [20/23] time 0.179 (0.266) data 0.000 (0.074) loss 0.2761 (0.4065) acc 90.0000 (88.7250) lr 3.1545e-04 eta 0:01:08
>>> alpha1: 0.147  alpha2: -0.072 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.34 <<<
epoch [40/50] batch [5/23] time 0.204 (0.527) data 0.000 (0.318) loss 0.3223 (0.4161) acc 89.3519 (89.2551) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [10/23] time 0.193 (0.366) data 0.001 (0.159) loss 0.2800 (0.4158) acc 94.2308 (89.4862) lr 2.7103e-04 eta 0:01:28
epoch [40/50] batch [15/23] time 0.205 (0.312) data 0.000 (0.106) loss 0.2840 (0.4075) acc 86.0577 (88.3955) lr 2.7103e-04 eta 0:01:14
epoch [40/50] batch [20/23] time 0.192 (0.283) data 0.000 (0.080) loss 0.5401 (0.4109) acc 90.8654 (88.0065) lr 2.7103e-04 eta 0:01:06
>>> alpha1: 0.145  alpha2: -0.066 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.34 <<<
epoch [41/50] batch [5/23] time 0.209 (0.469) data 0.000 (0.254) loss 0.5040 (0.3903) acc 87.9464 (89.8440) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [10/23] time 0.206 (0.335) data 0.000 (0.127) loss 0.3632 (0.3447) acc 83.3333 (89.0519) lr 2.2949e-04 eta 0:01:13
epoch [41/50] batch [15/23] time 0.202 (0.294) data 0.000 (0.085) loss 0.3001 (0.4440) acc 90.2778 (87.7750) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [20/23] time 0.198 (0.270) data 0.000 (0.064) loss 0.5295 (0.4402) acc 86.7924 (88.0579) lr 2.2949e-04 eta 0:00:56
>>> alpha1: 0.147  alpha2: -0.067 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.28 <<<
epoch [42/50] batch [5/23] time 0.179 (0.469) data 0.000 (0.284) loss 0.3717 (0.3782) acc 86.9318 (89.3121) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [10/23] time 0.187 (0.333) data 0.001 (0.142) loss 0.3242 (0.3985) acc 94.5000 (89.6425) lr 1.9098e-04 eta 0:01:05
epoch [42/50] batch [15/23] time 0.177 (0.283) data 0.000 (0.095) loss 0.3521 (0.3941) acc 88.3333 (89.3126) lr 1.9098e-04 eta 0:00:54
epoch [42/50] batch [20/23] time 0.186 (0.258) data 0.000 (0.071) loss 0.3655 (0.3878) acc 82.2917 (89.3129) lr 1.9098e-04 eta 0:00:48
>>> alpha1: 0.147  alpha2: -0.070 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [43/50] batch [5/23] time 0.165 (0.456) data 0.000 (0.280) loss 0.3850 (0.4053) acc 83.3333 (87.1995) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [10/23] time 0.197 (0.321) data 0.000 (0.140) loss 0.3506 (0.3813) acc 94.1176 (89.3991) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [15/23] time 0.184 (0.276) data 0.000 (0.094) loss 0.3796 (0.3767) acc 85.9375 (89.5998) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [20/23] time 0.177 (0.256) data 0.000 (0.070) loss 0.4279 (0.3970) acc 85.6383 (88.9625) lr 1.5567e-04 eta 0:00:41
>>> alpha1: 0.151  alpha2: -0.074 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [44/50] batch [5/23] time 0.182 (0.454) data 0.000 (0.270) loss 0.4906 (0.3529) acc 77.0833 (88.6044) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [10/23] time 0.169 (0.318) data 0.000 (0.135) loss 0.4022 (0.4350) acc 87.5000 (89.5410) lr 1.2369e-04 eta 0:00:47
epoch [44/50] batch [15/23] time 0.267 (0.276) data 0.000 (0.090) loss 0.3777 (0.4062) acc 88.9423 (89.9368) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [20/23] time 0.191 (0.253) data 0.001 (0.068) loss 0.4328 (0.4037) acc 89.7959 (89.8834) lr 1.2369e-04 eta 0:00:35
>>> alpha1: 0.147  alpha2: -0.079 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [45/50] batch [5/23] time 0.205 (0.419) data 0.000 (0.220) loss 0.4215 (0.3767) acc 86.2245 (87.1246) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [10/23] time 0.192 (0.308) data 0.000 (0.110) loss 0.4978 (0.3887) acc 84.8958 (87.9815) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [15/23] time 0.166 (0.265) data 0.000 (0.074) loss 0.5345 (0.3963) acc 84.3023 (88.1292) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [20/23] time 0.170 (0.244) data 0.000 (0.055) loss 0.3608 (0.3939) acc 95.5556 (88.6897) lr 9.5173e-05 eta 0:00:28
>>> alpha1: 0.144  alpha2: -0.088 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.32 <<<
epoch [46/50] batch [5/23] time 0.196 (0.522) data 0.001 (0.305) loss 0.3255 (0.3776) acc 90.6863 (87.8483) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [10/23] time 0.191 (0.357) data 0.001 (0.153) loss 0.6387 (0.4096) acc 85.2041 (88.5115) lr 7.0224e-05 eta 0:00:37
epoch [46/50] batch [15/23] time 0.178 (0.300) data 0.000 (0.102) loss 0.3308 (0.3957) acc 93.4783 (89.0925) lr 7.0224e-05 eta 0:00:30
epoch [46/50] batch [20/23] time 0.182 (0.273) data 0.000 (0.077) loss 0.3973 (0.3718) acc 82.9787 (89.3807) lr 7.0224e-05 eta 0:00:25
>>> alpha1: 0.140  alpha2: -0.091 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [47/50] batch [5/23] time 0.196 (0.456) data 0.000 (0.262) loss 0.2885 (0.3400) acc 94.8980 (91.4760) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [10/23] time 0.184 (0.326) data 0.000 (0.131) loss 0.3754 (0.3871) acc 91.1458 (90.2889) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [15/23] time 0.188 (0.278) data 0.000 (0.088) loss 0.2414 (0.3810) acc 93.3673 (90.3129) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [20/23] time 0.182 (0.256) data 0.000 (0.066) loss 0.2666 (0.3638) acc 92.8571 (90.4576) lr 4.8943e-05 eta 0:00:18
>>> alpha1: 0.139  alpha2: -0.094 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [48/50] batch [5/23] time 0.192 (0.479) data 0.001 (0.279) loss 0.5537 (0.4180) acc 85.4167 (87.7741) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [10/23] time 0.198 (0.342) data 0.001 (0.140) loss 0.4670 (0.4054) acc 87.5000 (88.6759) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [15/23] time 0.172 (0.292) data 0.000 (0.093) loss 0.4017 (0.3892) acc 84.8837 (88.7414) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.200 (0.268) data 0.000 (0.070) loss 0.2891 (0.3803) acc 94.1176 (89.3243) lr 3.1417e-05 eta 0:00:13
>>> alpha1: 0.136  alpha2: -0.092 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.29 <<<
epoch [49/50] batch [5/23] time 0.175 (0.468) data 0.001 (0.280) loss 0.2714 (0.3353) acc 87.5000 (90.5008) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [10/23] time 0.170 (0.322) data 0.001 (0.140) loss 0.3443 (0.3522) acc 91.0714 (90.2863) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.174 (0.277) data 0.000 (0.094) loss 0.4676 (0.3651) acc 78.4884 (89.7643) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.162 (0.255) data 0.000 (0.070) loss 0.6299 (0.3810) acc 84.6591 (89.1936) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.139  alpha2: -0.087 <<<
>>> noisy rate: 0.24 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.28 <<<
epoch [50/50] batch [5/23] time 0.201 (0.434) data 0.001 (0.279) loss 0.4751 (0.3806) acc 88.2979 (91.6144) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.256 (0.350) data 0.001 (0.139) loss 0.4940 (0.3646) acc 83.3333 (91.0006) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.292 (0.328) data 0.000 (0.093) loss 0.3362 (0.3593) acc 89.2857 (89.7196) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.296 (0.318) data 0.000 (0.070) loss 0.4843 (0.3727) acc 84.8958 (89.3581) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.18, 0.16, 0.17, 0.16, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.2, 0.19, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.18, 0.19, 0.19, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19]
* matched noise rate: [0.06, 0.05, 0.05, 0.06, 0.07, 0.07, 0.09, 0.05, 0.06, 0.07, 0.07, 0.07, 0.06, 0.05, 0.07, 0.07, 0.07, 0.08, 0.1, 0.08, 0.08, 0.09, 0.08, 0.08, 0.1, 0.09, 0.1, 0.1, 0.1, 0.1, 0.1, 0.08, 0.09, 0.09, 0.08, 0.08, 0.09, 0.09, 0.07, 0.08]
* unmatched noise rate: [0.28, 0.28, 0.27, 0.25, 0.31, 0.3, 0.32, 0.28, 0.28, 0.27, 0.28, 0.28, 0.31, 0.29, 0.28, 0.27, 0.28, 0.27, 0.33, 0.27, 0.27, 0.27, 0.27, 0.28, 0.32, 0.31, 0.33, 0.34, 0.33, 0.34, 0.34, 0.28, 0.28, 0.28, 0.27, 0.32, 0.29, 0.31, 0.29, 0.28]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:03<00:48,  3.01s/it] 18%|█▊        | 3/17 [00:03<00:11,  1.19it/s] 24%|██▎       | 4/17 [00:03<00:07,  1.67it/s] 35%|███▌      | 6/17 [00:03<00:03,  2.80it/s] 41%|████      | 7/17 [00:03<00:02,  3.38it/s] 47%|████▋     | 8/17 [00:03<00:02,  3.94it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.70it/s] 59%|█████▉    | 10/17 [00:04<00:01,  5.20it/s] 65%|██████▍   | 11/17 [00:04<00:00,  6.01it/s] 71%|███████   | 12/17 [00:04<00:00,  6.40it/s] 76%|███████▋  | 13/17 [00:04<00:00,  6.97it/s] 82%|████████▏ | 14/17 [00:04<00:00,  6.95it/s] 88%|████████▊ | 15/17 [00:04<00:00,  7.62it/s] 94%|█████████▍| 16/17 [00:04<00:00,  7.44it/s]100%|██████████| 17/17 [00:05<00:00,  2.62it/s]100%|██████████| 17/17 [00:05<00:00,  2.87it/s]
=> result
* total: 1,692
* correct: 1,013
* accuracy: 59.9%
* error: 40.1%
* macro_f1: 58.9%
=> per-class result
* class: 0 (banded)	total: 36	correct: 26	acc: 72.2%
* class: 1 (blotchy)	total: 36	correct: 9	acc: 25.0%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 31	acc: 86.1%
* class: 4 (bumpy)	total: 36	correct: 6	acc: 16.7%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 31	acc: 86.1%
* class: 7 (cracked)	total: 36	correct: 23	acc: 63.9%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 31	acc: 86.1%
* class: 10 (dotted)	total: 36	correct: 10	acc: 27.8%
* class: 11 (fibrous)	total: 36	correct: 25	acc: 69.4%
* class: 12 (flecked)	total: 36	correct: 20	acc: 55.6%
* class: 13 (freckled)	total: 36	correct: 29	acc: 80.6%
* class: 14 (frilly)	total: 36	correct: 31	acc: 86.1%
* class: 15 (gauzy)	total: 36	correct: 19	acc: 52.8%
* class: 16 (grid)	total: 36	correct: 16	acc: 44.4%
* class: 17 (grooved)	total: 36	correct: 13	acc: 36.1%
* class: 18 (honeycombed)	total: 36	correct: 25	acc: 69.4%
* class: 19 (interlaced)	total: 36	correct: 26	acc: 72.2%
* class: 20 (knitted)	total: 36	correct: 28	acc: 77.8%
* class: 21 (lacelike)	total: 36	correct: 30	acc: 83.3%
* class: 22 (lined)	total: 36	correct: 17	acc: 47.2%
* class: 23 (marbled)	total: 36	correct: 24	acc: 66.7%
* class: 24 (matted)	total: 36	correct: 25	acc: 69.4%
* class: 25 (meshed)	total: 36	correct: 19	acc: 52.8%
* class: 26 (paisley)	total: 36	correct: 32	acc: 88.9%
* class: 27 (perforated)	total: 36	correct: 21	acc: 58.3%
* class: 28 (pitted)	total: 36	correct: 9	acc: 25.0%
* class: 29 (pleated)	total: 36	correct: 18	acc: 50.0%
* class: 30 (polka-dotted)	total: 36	correct: 23	acc: 63.9%
* class: 31 (porous)	total: 36	correct: 8	acc: 22.2%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 25	acc: 69.4%
* class: 34 (smeared)	total: 36	correct: 10	acc: 27.8%
* class: 35 (spiralled)	total: 36	correct: 22	acc: 61.1%
* class: 36 (sprinkled)	total: 36	correct: 14	acc: 38.9%
* class: 37 (stained)	total: 36	correct: 11	acc: 30.6%
* class: 38 (stratified)	total: 36	correct: 28	acc: 77.8%
* class: 39 (striped)	total: 36	correct: 26	acc: 72.2%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 26	acc: 72.2%
* class: 42 (veined)	total: 36	correct: 18	acc: 50.0%
* class: 43 (waffled)	total: 36	correct: 24	acc: 66.7%
* class: 44 (woven)	total: 36	correct: 15	acc: 41.7%
* class: 45 (wrinkled)	total: 36	correct: 24	acc: 66.7%
* class: 46 (zigzagged)	total: 36	correct: 28	acc: 77.8%
* average: 59.9%
Elapsed: 0:16:34
Run this job and save the output to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_6-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.215 (1.068) data 0.000 (0.429) loss 3.6152 (3.6328) acc 12.5000 (13.7500) lr 1.0000e-05 eta 0:20:22
epoch [1/50] batch [10/23] time 0.205 (0.638) data 0.000 (0.214) loss 3.7278 (3.6686) acc 6.2500 (10.9375) lr 1.0000e-05 eta 0:12:06
epoch [1/50] batch [15/23] time 0.207 (0.494) data 0.000 (0.143) loss 3.4374 (3.6355) acc 21.8750 (11.4583) lr 1.0000e-05 eta 0:09:21
epoch [1/50] batch [20/23] time 0.206 (0.426) data 0.000 (0.107) loss 3.4758 (3.6152) acc 12.5000 (12.1875) lr 1.0000e-05 eta 0:08:01
epoch [2/50] batch [5/23] time 0.231 (0.562) data 0.000 (0.334) loss 3.4077 (3.6570) acc 15.6250 (13.1250) lr 2.0000e-03 eta 0:10:31
epoch [2/50] batch [10/23] time 0.216 (0.388) data 0.000 (0.167) loss 3.2928 (3.4729) acc 25.0000 (18.4375) lr 2.0000e-03 eta 0:07:13
epoch [2/50] batch [15/23] time 0.236 (0.329) data 0.000 (0.112) loss 3.5043 (3.4721) acc 6.2500 (17.2917) lr 2.0000e-03 eta 0:06:05
epoch [2/50] batch [20/23] time 0.197 (0.296) data 0.000 (0.084) loss 3.0060 (3.4323) acc 34.3750 (19.2188) lr 2.0000e-03 eta 0:05:27
epoch [3/50] batch [5/23] time 0.216 (0.500) data 0.000 (0.272) loss 3.1900 (3.2572) acc 21.8750 (22.5000) lr 1.9980e-03 eta 0:09:09
epoch [3/50] batch [10/23] time 0.207 (0.357) data 0.000 (0.136) loss 3.4364 (3.2343) acc 21.8750 (25.0000) lr 1.9980e-03 eta 0:06:30
epoch [3/50] batch [15/23] time 0.251 (0.311) data 0.000 (0.091) loss 2.6717 (3.1233) acc 43.7500 (28.5417) lr 1.9980e-03 eta 0:05:38
epoch [3/50] batch [20/23] time 0.206 (0.285) data 0.000 (0.068) loss 3.2901 (3.1981) acc 31.2500 (26.5625) lr 1.9980e-03 eta 0:05:09
epoch [4/50] batch [5/23] time 0.211 (0.529) data 0.000 (0.288) loss 2.6335 (3.1218) acc 50.0000 (31.2500) lr 1.9921e-03 eta 0:09:29
epoch [4/50] batch [10/23] time 0.204 (0.373) data 0.000 (0.144) loss 3.5233 (3.1316) acc 25.0000 (30.6250) lr 1.9921e-03 eta 0:06:38
epoch [4/50] batch [15/23] time 0.207 (0.321) data 0.000 (0.096) loss 2.6368 (3.0176) acc 40.6250 (31.6667) lr 1.9921e-03 eta 0:05:41
epoch [4/50] batch [20/23] time 0.204 (0.291) data 0.000 (0.072) loss 3.7634 (3.0488) acc 28.1250 (31.7188) lr 1.9921e-03 eta 0:05:09
epoch [5/50] batch [5/23] time 0.240 (0.560) data 0.000 (0.319) loss 3.0857 (3.0805) acc 43.7500 (32.5000) lr 1.9823e-03 eta 0:09:49
epoch [5/50] batch [10/23] time 0.202 (0.386) data 0.000 (0.160) loss 2.9118 (3.0786) acc 43.7500 (34.0625) lr 1.9823e-03 eta 0:06:44
epoch [5/50] batch [15/23] time 0.201 (0.329) data 0.000 (0.106) loss 2.8101 (2.9888) acc 28.1250 (34.7917) lr 1.9823e-03 eta 0:05:43
epoch [5/50] batch [20/23] time 0.205 (0.297) data 0.000 (0.080) loss 3.0852 (3.0017) acc 25.0000 (33.5938) lr 1.9823e-03 eta 0:05:08
epoch [6/50] batch [5/23] time 0.207 (0.480) data 0.000 (0.257) loss 3.0286 (3.0370) acc 43.7500 (34.3750) lr 1.9686e-03 eta 0:08:14
epoch [6/50] batch [10/23] time 0.229 (0.347) data 0.000 (0.129) loss 3.0391 (2.9881) acc 31.2500 (31.5625) lr 1.9686e-03 eta 0:05:55
epoch [6/50] batch [15/23] time 0.207 (0.302) data 0.000 (0.086) loss 2.5106 (2.9306) acc 46.8750 (34.1667) lr 1.9686e-03 eta 0:05:07
epoch [6/50] batch [20/23] time 0.197 (0.276) data 0.000 (0.064) loss 3.2633 (2.9261) acc 31.2500 (35.1562) lr 1.9686e-03 eta 0:04:39
epoch [7/50] batch [5/23] time 0.218 (0.510) data 0.000 (0.292) loss 3.0172 (3.0213) acc 34.3750 (31.8750) lr 1.9511e-03 eta 0:08:33
epoch [7/50] batch [10/23] time 0.208 (0.361) data 0.000 (0.146) loss 2.8272 (2.9574) acc 43.7500 (35.0000) lr 1.9511e-03 eta 0:06:01
epoch [7/50] batch [15/23] time 0.201 (0.313) data 0.000 (0.097) loss 2.8389 (2.9350) acc 34.3750 (34.1667) lr 1.9511e-03 eta 0:05:11
epoch [7/50] batch [20/23] time 0.200 (0.285) data 0.000 (0.073) loss 3.0930 (2.8911) acc 28.1250 (35.6250) lr 1.9511e-03 eta 0:04:43
epoch [8/50] batch [5/23] time 0.205 (0.480) data 0.000 (0.253) loss 2.9938 (2.9492) acc 34.3750 (33.7500) lr 1.9298e-03 eta 0:07:52
epoch [8/50] batch [10/23] time 0.167 (0.340) data 0.000 (0.127) loss 3.5090 (2.9999) acc 15.6250 (32.1875) lr 1.9298e-03 eta 0:05:32
epoch [8/50] batch [15/23] time 0.152 (0.277) data 0.000 (0.084) loss 2.4393 (2.9296) acc 50.0000 (34.1667) lr 1.9298e-03 eta 0:04:29
epoch [8/50] batch [20/23] time 0.306 (0.255) data 0.000 (0.063) loss 2.4530 (2.8617) acc 50.0000 (35.9375) lr 1.9298e-03 eta 0:04:07
epoch [9/50] batch [5/23] time 0.338 (0.673) data 0.000 (0.331) loss 2.1666 (2.7015) acc 53.1250 (38.7500) lr 1.9048e-03 eta 0:10:46
epoch [9/50] batch [10/23] time 0.152 (0.428) data 0.000 (0.166) loss 2.8531 (2.7538) acc 31.2500 (36.5625) lr 1.9048e-03 eta 0:06:49
epoch [9/50] batch [15/23] time 0.152 (0.337) data 0.000 (0.111) loss 2.4100 (2.7890) acc 43.7500 (36.8750) lr 1.9048e-03 eta 0:05:20
epoch [9/50] batch [20/23] time 0.282 (0.310) data 0.000 (0.083) loss 2.9759 (2.8264) acc 34.3750 (36.0938) lr 1.9048e-03 eta 0:04:53
epoch [10/50] batch [5/23] time 0.154 (0.500) data 0.000 (0.311) loss 3.1933 (2.8031) acc 46.8750 (40.6250) lr 1.8763e-03 eta 0:07:49
epoch [10/50] batch [10/23] time 0.260 (0.349) data 0.000 (0.157) loss 3.0390 (2.7701) acc 34.3750 (37.8125) lr 1.8763e-03 eta 0:05:25
epoch [10/50] batch [15/23] time 0.201 (0.297) data 0.000 (0.105) loss 2.8077 (2.7624) acc 37.5000 (38.1250) lr 1.8763e-03 eta 0:04:35
epoch [10/50] batch [20/23] time 0.201 (0.272) data 0.000 (0.079) loss 3.0910 (2.8354) acc 37.5000 (37.5000) lr 1.8763e-03 eta 0:04:11
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.969  alpha2: 0.270 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.48 <<<
epoch [11/50] batch [5/23] time 1.256 (1.265) data 0.000 (0.299) loss 1.7413 (1.9995) acc 56.4815 (51.9676) lr 1.8443e-03 eta 0:19:17
epoch [11/50] batch [10/23] time 0.182 (1.001) data 0.000 (0.150) loss 1.9243 (1.9411) acc 50.5319 (53.0559) lr 1.8443e-03 eta 0:15:11
epoch [11/50] batch [15/23] time 1.327 (0.808) data 0.000 (0.100) loss 2.0339 (1.9533) acc 49.5098 (53.0195) lr 1.8443e-03 eta 0:12:11
epoch [11/50] batch [20/23] time 0.184 (0.744) data 0.000 (0.075) loss 1.9930 (1.9505) acc 57.6531 (53.4305) lr 1.8443e-03 eta 0:11:09
>>> alpha1: 0.707  alpha2: 0.233 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.34 <<<
epoch [12/50] batch [5/23] time 0.215 (0.702) data 0.001 (0.324) loss 1.1134 (1.2727) acc 77.4510 (64.3600) lr 1.8090e-03 eta 0:10:25
epoch [12/50] batch [10/23] time 0.194 (0.692) data 0.000 (0.162) loss 1.6565 (1.3772) acc 57.8125 (63.1144) lr 1.8090e-03 eta 0:10:13
epoch [12/50] batch [15/23] time 1.076 (0.580) data 0.000 (0.108) loss 1.2080 (1.2970) acc 66.4773 (65.3559) lr 1.8090e-03 eta 0:08:31
epoch [12/50] batch [20/23] time 0.177 (0.481) data 0.000 (0.081) loss 0.8725 (1.2803) acc 81.5217 (65.7423) lr 1.8090e-03 eta 0:07:01
>>> alpha1: 0.654  alpha2: 0.259 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [13/50] batch [5/23] time 0.240 (0.554) data 0.000 (0.304) loss 1.1877 (1.1420) acc 69.0217 (68.4849) lr 1.7705e-03 eta 0:08:01
epoch [13/50] batch [10/23] time 0.149 (0.594) data 0.000 (0.152) loss 1.0548 (1.2561) acc 73.5294 (65.9195) lr 1.7705e-03 eta 0:08:32
epoch [13/50] batch [15/23] time 0.131 (0.440) data 0.000 (0.102) loss 1.1855 (1.1940) acc 70.0000 (66.8032) lr 1.7705e-03 eta 0:06:17
epoch [13/50] batch [20/23] time 0.186 (0.373) data 0.000 (0.076) loss 1.2742 (1.2563) acc 68.0851 (65.3186) lr 1.7705e-03 eta 0:05:18
>>> alpha1: 0.605  alpha2: 0.226 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [14/50] batch [5/23] time 0.197 (0.705) data 0.000 (0.276) loss 1.1748 (1.0934) acc 64.4231 (68.9888) lr 1.7290e-03 eta 0:09:56
epoch [14/50] batch [10/23] time 0.203 (0.571) data 0.000 (0.138) loss 1.2885 (1.1440) acc 51.8868 (65.9007) lr 1.7290e-03 eta 0:08:00
epoch [14/50] batch [15/23] time 0.193 (0.449) data 0.000 (0.092) loss 0.8164 (1.0999) acc 78.0000 (68.2938) lr 1.7290e-03 eta 0:06:15
epoch [14/50] batch [20/23] time 0.187 (0.448) data 0.000 (0.069) loss 1.1161 (1.1342) acc 69.6078 (67.8782) lr 1.7290e-03 eta 0:06:11
>>> alpha1: 0.583  alpha2: 0.235 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [15/50] batch [5/23] time 0.207 (0.755) data 0.000 (0.306) loss 1.1324 (1.0812) acc 70.8333 (67.1422) lr 1.6845e-03 eta 0:10:21
epoch [15/50] batch [10/23] time 0.206 (0.482) data 0.000 (0.153) loss 0.9693 (1.0719) acc 75.9091 (69.5209) lr 1.6845e-03 eta 0:06:34
epoch [15/50] batch [15/23] time 0.198 (0.386) data 0.000 (0.102) loss 1.4086 (1.1511) acc 61.5385 (67.0591) lr 1.6845e-03 eta 0:05:13
epoch [15/50] batch [20/23] time 0.229 (0.344) data 0.000 (0.077) loss 1.3864 (1.1527) acc 59.4340 (67.1062) lr 1.6845e-03 eta 0:04:37
>>> alpha1: 0.513  alpha2: 0.189 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.34 <<<
epoch [16/50] batch [5/23] time 0.200 (0.533) data 0.000 (0.328) loss 1.1057 (1.0886) acc 67.1296 (68.8882) lr 1.6374e-03 eta 0:07:06
epoch [16/50] batch [10/23] time 0.181 (0.363) data 0.000 (0.164) loss 1.1343 (1.0839) acc 71.3542 (69.4029) lr 1.6374e-03 eta 0:04:48
epoch [16/50] batch [15/23] time 0.190 (0.304) data 0.000 (0.110) loss 1.0418 (1.0564) acc 70.5000 (69.8787) lr 1.6374e-03 eta 0:04:00
epoch [16/50] batch [20/23] time 0.150 (0.267) data 0.000 (0.082) loss 1.2645 (1.0620) acc 58.1731 (69.4115) lr 1.6374e-03 eta 0:03:29
>>> alpha1: 0.469  alpha2: 0.186 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [17/50] batch [5/23] time 0.182 (0.483) data 0.000 (0.295) loss 1.1331 (1.0172) acc 65.3061 (68.5212) lr 1.5878e-03 eta 0:06:15
epoch [17/50] batch [10/23] time 0.177 (0.339) data 0.000 (0.148) loss 1.2360 (1.0199) acc 75.5319 (70.6604) lr 1.5878e-03 eta 0:04:21
epoch [17/50] batch [15/23] time 0.184 (0.291) data 0.000 (0.099) loss 1.1235 (1.0325) acc 63.2653 (69.9411) lr 1.5878e-03 eta 0:03:43
epoch [17/50] batch [20/23] time 0.189 (0.271) data 0.000 (0.074) loss 0.8662 (1.0065) acc 80.3922 (71.1362) lr 1.5878e-03 eta 0:03:26
>>> alpha1: 0.447  alpha2: 0.181 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.33 <<<
epoch [18/50] batch [5/23] time 0.190 (0.490) data 0.000 (0.298) loss 1.1542 (0.9208) acc 68.6170 (75.7677) lr 1.5358e-03 eta 0:06:09
epoch [18/50] batch [10/23] time 0.184 (0.342) data 0.000 (0.149) loss 1.1107 (0.9578) acc 69.3878 (72.4100) lr 1.5358e-03 eta 0:04:15
epoch [18/50] batch [15/23] time 0.172 (0.295) data 0.001 (0.100) loss 1.1221 (0.9487) acc 74.4318 (73.0810) lr 1.5358e-03 eta 0:03:39
epoch [18/50] batch [20/23] time 0.185 (0.268) data 0.000 (0.075) loss 1.3098 (0.9577) acc 59.6939 (71.9318) lr 1.5358e-03 eta 0:03:17
>>> alpha1: 0.413  alpha2: 0.154 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.33 <<<
epoch [19/50] batch [5/23] time 0.184 (0.535) data 0.000 (0.325) loss 0.9482 (0.8312) acc 70.8333 (75.7110) lr 1.4818e-03 eta 0:06:30
epoch [19/50] batch [10/23] time 0.181 (0.365) data 0.000 (0.163) loss 1.0675 (0.8233) acc 67.1875 (74.9748) lr 1.4818e-03 eta 0:04:24
epoch [19/50] batch [15/23] time 0.175 (0.303) data 0.000 (0.109) loss 0.8821 (0.8683) acc 72.7778 (73.9422) lr 1.4818e-03 eta 0:03:38
epoch [19/50] batch [20/23] time 0.233 (0.277) data 0.000 (0.081) loss 1.0173 (0.8989) acc 71.8085 (72.5669) lr 1.4818e-03 eta 0:03:18
>>> alpha1: 0.379  alpha2: 0.112 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.34 <<<
epoch [20/50] batch [5/23] time 0.317 (0.614) data 0.000 (0.314) loss 0.9016 (0.7621) acc 61.2745 (74.3193) lr 1.4258e-03 eta 0:07:14
epoch [20/50] batch [10/23] time 0.256 (0.424) data 0.000 (0.157) loss 0.9993 (0.8163) acc 64.2857 (72.9530) lr 1.4258e-03 eta 0:04:58
epoch [20/50] batch [15/23] time 0.290 (0.375) data 0.000 (0.105) loss 0.8317 (0.8392) acc 73.5000 (72.5094) lr 1.4258e-03 eta 0:04:21
epoch [20/50] batch [20/23] time 0.154 (0.346) data 0.000 (0.079) loss 0.8311 (0.8427) acc 78.7736 (73.0141) lr 1.4258e-03 eta 0:03:59
>>> alpha1: 0.348  alpha2: 0.092 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [21/50] batch [5/23] time 0.176 (0.484) data 0.000 (0.275) loss 0.7779 (0.8313) acc 71.7391 (74.2139) lr 1.3681e-03 eta 0:05:31
epoch [21/50] batch [10/23] time 0.188 (0.332) data 0.000 (0.138) loss 0.9938 (0.8677) acc 66.3265 (72.8758) lr 1.3681e-03 eta 0:03:46
epoch [21/50] batch [15/23] time 0.191 (0.285) data 0.000 (0.092) loss 0.4625 (0.8163) acc 90.0000 (75.0473) lr 1.3681e-03 eta 0:03:12
epoch [21/50] batch [20/23] time 0.185 (0.260) data 0.000 (0.069) loss 0.8335 (0.8343) acc 79.0816 (75.3029) lr 1.3681e-03 eta 0:02:53
>>> alpha1: 0.335  alpha2: 0.090 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [22/50] batch [5/23] time 0.175 (0.500) data 0.000 (0.315) loss 0.4476 (0.6581) acc 82.9787 (80.6984) lr 1.3090e-03 eta 0:05:31
epoch [22/50] batch [10/23] time 0.165 (0.335) data 0.000 (0.158) loss 0.5706 (0.6615) acc 80.3191 (80.2901) lr 1.3090e-03 eta 0:03:40
epoch [22/50] batch [15/23] time 0.165 (0.280) data 0.000 (0.105) loss 0.9545 (0.7195) acc 67.4419 (78.0262) lr 1.3090e-03 eta 0:03:02
epoch [22/50] batch [20/23] time 0.166 (0.251) data 0.000 (0.079) loss 0.8427 (0.7315) acc 75.5682 (77.7164) lr 1.3090e-03 eta 0:02:42
>>> alpha1: 0.321  alpha2: 0.088 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [23/50] batch [5/23] time 0.189 (0.524) data 0.000 (0.327) loss 0.8712 (0.7860) acc 66.5000 (76.5804) lr 1.2487e-03 eta 0:05:34
epoch [23/50] batch [10/23] time 0.176 (0.367) data 0.000 (0.164) loss 0.8331 (0.7674) acc 69.5652 (75.6271) lr 1.2487e-03 eta 0:03:52
epoch [23/50] batch [15/23] time 0.207 (0.309) data 0.001 (0.109) loss 0.7888 (0.7486) acc 71.3636 (76.1381) lr 1.2487e-03 eta 0:03:14
epoch [23/50] batch [20/23] time 0.202 (0.280) data 0.000 (0.082) loss 0.8073 (0.7497) acc 80.1887 (76.5449) lr 1.2487e-03 eta 0:02:55
>>> alpha1: 0.306  alpha2: 0.087 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.34 <<<
epoch [24/50] batch [5/23] time 0.232 (0.544) data 0.000 (0.295) loss 0.6841 (0.6474) acc 73.5000 (78.6424) lr 1.1874e-03 eta 0:05:35
epoch [24/50] batch [10/23] time 0.265 (0.395) data 0.000 (0.148) loss 0.6831 (0.6631) acc 82.0755 (78.4036) lr 1.1874e-03 eta 0:04:01
epoch [24/50] batch [15/23] time 0.144 (0.325) data 0.000 (0.099) loss 0.6188 (0.6756) acc 82.3529 (79.5629) lr 1.1874e-03 eta 0:03:17
epoch [24/50] batch [20/23] time 0.156 (0.281) data 0.000 (0.074) loss 0.6736 (0.7117) acc 81.8182 (78.7149) lr 1.1874e-03 eta 0:02:48
>>> alpha1: 0.293  alpha2: 0.088 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [25/50] batch [5/23] time 0.200 (0.489) data 0.000 (0.297) loss 0.5936 (0.6838) acc 79.5918 (77.7869) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [10/23] time 0.183 (0.340) data 0.000 (0.149) loss 0.6010 (0.7157) acc 81.3830 (78.0308) lr 1.1253e-03 eta 0:03:19
epoch [25/50] batch [15/23] time 0.180 (0.285) data 0.000 (0.099) loss 0.7311 (0.6733) acc 79.0816 (78.8320) lr 1.1253e-03 eta 0:02:46
epoch [25/50] batch [20/23] time 0.182 (0.257) data 0.000 (0.075) loss 0.8227 (0.6890) acc 75.0000 (78.3778) lr 1.1253e-03 eta 0:02:28
>>> alpha1: 0.282  alpha2: 0.077 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [26/50] batch [5/23] time 0.200 (0.517) data 0.000 (0.299) loss 0.5420 (0.6082) acc 82.0000 (78.8823) lr 1.0628e-03 eta 0:04:54
epoch [26/50] batch [10/23] time 0.205 (0.361) data 0.000 (0.151) loss 0.7814 (0.6279) acc 76.8868 (80.3604) lr 1.0628e-03 eta 0:03:23
epoch [26/50] batch [15/23] time 0.194 (0.305) data 0.000 (0.101) loss 0.6933 (0.6352) acc 79.5000 (79.9627) lr 1.0628e-03 eta 0:02:50
epoch [26/50] batch [20/23] time 0.188 (0.276) data 0.000 (0.076) loss 0.7939 (0.6618) acc 79.5918 (78.8797) lr 1.0628e-03 eta 0:02:33
>>> alpha1: 0.267  alpha2: 0.054 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [27/50] batch [5/23] time 0.187 (0.502) data 0.000 (0.300) loss 0.5745 (0.6021) acc 76.5000 (80.5504) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [10/23] time 0.178 (0.345) data 0.000 (0.150) loss 0.4482 (0.5781) acc 84.3750 (81.8660) lr 1.0000e-03 eta 0:03:06
epoch [27/50] batch [15/23] time 0.181 (0.293) data 0.000 (0.100) loss 0.7833 (0.6181) acc 73.9583 (81.1076) lr 1.0000e-03 eta 0:02:37
epoch [27/50] batch [20/23] time 0.237 (0.268) data 0.000 (0.075) loss 0.5069 (0.6204) acc 83.9623 (81.0697) lr 1.0000e-03 eta 0:02:22
>>> alpha1: 0.259  alpha2: 0.053 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.32 <<<
epoch [28/50] batch [5/23] time 0.190 (0.510) data 0.000 (0.312) loss 0.4588 (0.5793) acc 84.8039 (81.1583) lr 9.3721e-04 eta 0:04:27
epoch [28/50] batch [10/23] time 0.181 (0.350) data 0.000 (0.156) loss 0.4659 (0.5863) acc 88.7755 (81.1535) lr 9.3721e-04 eta 0:03:01
epoch [28/50] batch [15/23] time 0.190 (0.295) data 0.000 (0.104) loss 0.5370 (0.5887) acc 85.7843 (80.9773) lr 9.3721e-04 eta 0:02:31
epoch [28/50] batch [20/23] time 0.191 (0.272) data 0.000 (0.078) loss 0.6209 (0.5975) acc 80.5000 (80.7926) lr 9.3721e-04 eta 0:02:18
>>> alpha1: 0.251  alpha2: 0.044 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.33 <<<
epoch [29/50] batch [5/23] time 0.186 (0.502) data 0.000 (0.304) loss 0.5820 (0.5093) acc 77.5510 (82.4553) lr 8.7467e-04 eta 0:04:11
epoch [29/50] batch [10/23] time 0.171 (0.349) data 0.000 (0.152) loss 0.5481 (0.5205) acc 87.2093 (83.8805) lr 8.7467e-04 eta 0:02:53
epoch [29/50] batch [15/23] time 0.184 (0.297) data 0.000 (0.102) loss 0.6024 (0.5691) acc 80.1020 (82.1936) lr 8.7467e-04 eta 0:02:25
epoch [29/50] batch [20/23] time 0.201 (0.275) data 0.000 (0.076) loss 0.5165 (0.5545) acc 83.0189 (82.5820) lr 8.7467e-04 eta 0:02:13
>>> alpha1: 0.241  alpha2: 0.029 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [30/50] batch [5/23] time 0.194 (0.473) data 0.000 (0.266) loss 0.4732 (0.5371) acc 80.9783 (84.1509) lr 8.1262e-04 eta 0:03:46
epoch [30/50] batch [10/23] time 0.182 (0.334) data 0.000 (0.133) loss 0.5589 (0.5302) acc 83.3333 (83.5030) lr 8.1262e-04 eta 0:02:37
epoch [30/50] batch [15/23] time 0.200 (0.290) data 0.000 (0.089) loss 0.8327 (0.5666) acc 76.4423 (82.9930) lr 8.1262e-04 eta 0:02:15
epoch [30/50] batch [20/23] time 0.188 (0.265) data 0.000 (0.067) loss 0.4742 (0.5582) acc 84.1837 (82.4286) lr 8.1262e-04 eta 0:02:02
>>> alpha1: 0.232  alpha2: 0.025 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.30 <<<
epoch [31/50] batch [5/23] time 0.172 (0.466) data 0.000 (0.265) loss 0.5822 (0.4956) acc 86.6667 (86.0573) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [10/23] time 0.183 (0.325) data 0.000 (0.133) loss 0.5343 (0.5240) acc 86.1702 (84.1168) lr 7.5131e-04 eta 0:02:26
epoch [31/50] batch [15/23] time 0.149 (0.275) data 0.000 (0.089) loss 0.6659 (0.5473) acc 77.5000 (83.1772) lr 7.5131e-04 eta 0:02:02
epoch [31/50] batch [20/23] time 0.143 (0.243) data 0.000 (0.067) loss 0.5346 (0.5518) acc 81.0000 (83.2831) lr 7.5131e-04 eta 0:01:46
>>> alpha1: 0.227  alpha2: 0.031 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [32/50] batch [5/23] time 0.182 (0.504) data 0.000 (0.302) loss 0.5371 (0.4926) acc 83.5106 (84.4133) lr 6.9098e-04 eta 0:03:37
epoch [32/50] batch [10/23] time 0.184 (0.347) data 0.000 (0.151) loss 0.3991 (0.5123) acc 85.0000 (83.9762) lr 6.9098e-04 eta 0:02:28
epoch [32/50] batch [15/23] time 0.185 (0.294) data 0.000 (0.101) loss 0.6299 (0.5161) acc 76.5000 (83.5378) lr 6.9098e-04 eta 0:02:03
epoch [32/50] batch [20/23] time 0.248 (0.270) data 0.000 (0.076) loss 0.5733 (0.5229) acc 79.1667 (83.2594) lr 6.9098e-04 eta 0:01:52
>>> alpha1: 0.220  alpha2: 0.032 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.31 <<<
epoch [33/50] batch [5/23] time 0.190 (0.491) data 0.000 (0.298) loss 0.6466 (0.5154) acc 77.9412 (88.5828) lr 6.3188e-04 eta 0:03:20
epoch [33/50] batch [10/23] time 0.192 (0.344) data 0.000 (0.149) loss 0.5318 (0.5003) acc 85.2941 (87.6422) lr 6.3188e-04 eta 0:02:19
epoch [33/50] batch [15/23] time 0.221 (0.299) data 0.000 (0.100) loss 0.3939 (0.4881) acc 87.7193 (86.2053) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [20/23] time 0.196 (0.272) data 0.000 (0.075) loss 0.5186 (0.5063) acc 79.3269 (85.0932) lr 6.3188e-04 eta 0:01:47
>>> alpha1: 0.217  alpha2: 0.040 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [34/50] batch [5/23] time 0.182 (0.467) data 0.000 (0.263) loss 0.6023 (0.4624) acc 88.2653 (87.1679) lr 5.7422e-04 eta 0:03:00
epoch [34/50] batch [10/23] time 0.185 (0.333) data 0.000 (0.132) loss 0.5480 (0.4565) acc 83.8542 (86.8826) lr 5.7422e-04 eta 0:02:06
epoch [34/50] batch [15/23] time 0.238 (0.292) data 0.000 (0.088) loss 0.6200 (0.4979) acc 80.2885 (85.4624) lr 5.7422e-04 eta 0:01:49
epoch [34/50] batch [20/23] time 0.185 (0.266) data 0.000 (0.066) loss 0.5355 (0.5218) acc 83.8542 (84.4063) lr 5.7422e-04 eta 0:01:38
>>> alpha1: 0.215  alpha2: 0.050 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.30 <<<
epoch [35/50] batch [5/23] time 0.333 (0.595) data 0.000 (0.292) loss 0.3464 (0.4639) acc 91.2281 (86.5188) lr 5.1825e-04 eta 0:03:36
epoch [35/50] batch [10/23] time 0.276 (0.447) data 0.000 (0.146) loss 0.5431 (0.4786) acc 85.4167 (87.0789) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [15/23] time 0.320 (0.394) data 0.000 (0.097) loss 0.5032 (0.4873) acc 85.7759 (85.8303) lr 5.1825e-04 eta 0:02:19
epoch [35/50] batch [20/23] time 0.309 (0.371) data 0.000 (0.073) loss 0.3467 (0.4954) acc 91.8182 (84.9129) lr 5.1825e-04 eta 0:02:09
>>> alpha1: 0.212  alpha2: 0.051 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.32 <<<
epoch [36/50] batch [5/23] time 0.198 (0.481) data 0.000 (0.278) loss 0.5249 (0.4222) acc 85.0962 (87.3924) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [10/23] time 0.185 (0.340) data 0.000 (0.139) loss 0.6174 (0.4836) acc 81.6327 (86.7721) lr 4.6417e-04 eta 0:01:53
epoch [36/50] batch [15/23] time 0.182 (0.297) data 0.000 (0.093) loss 0.5880 (0.4804) acc 80.7292 (86.1435) lr 4.6417e-04 eta 0:01:38
epoch [36/50] batch [20/23] time 0.176 (0.271) data 0.000 (0.070) loss 0.4484 (0.4796) acc 88.5870 (85.8218) lr 4.6417e-04 eta 0:01:28
>>> alpha1: 0.210  alpha2: 0.056 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.32 <<<
epoch [37/50] batch [5/23] time 0.199 (0.465) data 0.000 (0.237) loss 0.6129 (0.4349) acc 86.5385 (87.3078) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [10/23] time 0.195 (0.335) data 0.000 (0.118) loss 0.5368 (0.4388) acc 86.7647 (86.3212) lr 4.1221e-04 eta 0:01:44
epoch [37/50] batch [15/23] time 0.192 (0.288) data 0.000 (0.079) loss 0.4575 (0.4388) acc 89.9038 (86.9620) lr 4.1221e-04 eta 0:01:28
epoch [37/50] batch [20/23] time 0.240 (0.267) data 0.000 (0.059) loss 0.4889 (0.4503) acc 86.0577 (86.9688) lr 4.1221e-04 eta 0:01:20
>>> alpha1: 0.204  alpha2: 0.053 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [38/50] batch [5/23] time 0.202 (0.464) data 0.000 (0.262) loss 0.4043 (0.5807) acc 93.0556 (86.8787) lr 3.6258e-04 eta 0:02:16
epoch [38/50] batch [10/23] time 0.199 (0.328) data 0.000 (0.131) loss 0.2776 (0.5313) acc 90.5660 (86.1154) lr 3.6258e-04 eta 0:01:34
epoch [38/50] batch [15/23] time 0.237 (0.286) data 0.000 (0.088) loss 0.6825 (0.5233) acc 82.2917 (86.0490) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [20/23] time 0.206 (0.264) data 0.000 (0.066) loss 0.2591 (0.5165) acc 92.7273 (85.8746) lr 3.6258e-04 eta 0:01:13
>>> alpha1: 0.200  alpha2: 0.048 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [39/50] batch [5/23] time 0.258 (0.570) data 0.000 (0.313) loss 0.3688 (0.3759) acc 92.0213 (90.1031) lr 3.1545e-04 eta 0:02:34
epoch [39/50] batch [10/23] time 0.243 (0.406) data 0.000 (0.156) loss 0.3653 (0.4437) acc 89.8936 (86.9040) lr 3.1545e-04 eta 0:01:48
epoch [39/50] batch [15/23] time 0.255 (0.356) data 0.000 (0.104) loss 0.4085 (0.4829) acc 88.1818 (86.1273) lr 3.1545e-04 eta 0:01:32
epoch [39/50] batch [20/23] time 0.268 (0.330) data 0.000 (0.078) loss 0.3884 (0.4761) acc 91.9643 (86.2321) lr 3.1545e-04 eta 0:01:24
>>> alpha1: 0.196  alpha2: 0.050 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.34 <<<
epoch [40/50] batch [5/23] time 0.198 (0.500) data 0.000 (0.303) loss 0.6473 (0.4903) acc 77.5510 (83.7046) lr 2.7103e-04 eta 0:02:03
epoch [40/50] batch [10/23] time 0.207 (0.352) data 0.000 (0.152) loss 0.3788 (0.4467) acc 89.3519 (85.3560) lr 2.7103e-04 eta 0:01:25
epoch [40/50] batch [15/23] time 0.262 (0.305) data 0.000 (0.101) loss 0.3357 (0.4523) acc 86.0000 (85.1367) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [20/23] time 0.205 (0.278) data 0.000 (0.076) loss 0.4041 (0.4374) acc 88.2353 (86.1057) lr 2.7103e-04 eta 0:01:04
>>> alpha1: 0.193  alpha2: 0.052 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [41/50] batch [5/23] time 0.209 (0.506) data 0.000 (0.307) loss 0.4043 (0.4107) acc 86.2069 (86.1917) lr 2.2949e-04 eta 0:01:53
epoch [41/50] batch [10/23] time 0.187 (0.350) data 0.000 (0.154) loss 0.5009 (0.4045) acc 78.0000 (86.6714) lr 2.2949e-04 eta 0:01:16
epoch [41/50] batch [15/23] time 0.196 (0.301) data 0.000 (0.103) loss 0.4721 (0.4230) acc 83.9623 (86.9546) lr 2.2949e-04 eta 0:01:04
epoch [41/50] batch [20/23] time 0.190 (0.273) data 0.000 (0.077) loss 0.6461 (0.4354) acc 80.8824 (86.5106) lr 2.2949e-04 eta 0:00:57
>>> alpha1: 0.188  alpha2: 0.048 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.34 <<<
epoch [42/50] batch [5/23] time 0.196 (0.505) data 0.001 (0.290) loss 0.4502 (0.4178) acc 88.2353 (88.3688) lr 1.9098e-04 eta 0:01:42
epoch [42/50] batch [10/23] time 0.202 (0.358) data 0.000 (0.145) loss 0.3130 (0.4214) acc 94.6429 (87.8785) lr 1.9098e-04 eta 0:01:10
epoch [42/50] batch [15/23] time 0.202 (0.304) data 0.000 (0.097) loss 0.3847 (0.4278) acc 89.5455 (88.0398) lr 1.9098e-04 eta 0:00:58
epoch [42/50] batch [20/23] time 0.197 (0.275) data 0.000 (0.073) loss 0.4536 (0.4273) acc 83.9623 (87.4026) lr 1.9098e-04 eta 0:00:51
>>> alpha1: 0.187  alpha2: 0.055 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.32 <<<
epoch [43/50] batch [5/23] time 0.158 (0.432) data 0.000 (0.275) loss 0.5255 (0.4587) acc 86.4583 (88.5884) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [10/23] time 0.161 (0.298) data 0.000 (0.138) loss 0.3113 (0.4312) acc 91.9811 (89.2680) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [15/23] time 0.167 (0.249) data 0.000 (0.092) loss 0.4772 (0.4253) acc 88.6364 (88.6628) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [20/23] time 0.138 (0.224) data 0.000 (0.069) loss 0.4190 (0.4233) acc 88.2653 (88.0855) lr 1.5567e-04 eta 0:00:36
>>> alpha1: 0.185  alpha2: 0.052 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [44/50] batch [5/23] time 0.182 (0.424) data 0.000 (0.272) loss 0.4277 (0.6503) acc 87.0192 (85.0918) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [10/23] time 0.163 (0.291) data 0.000 (0.136) loss 0.3757 (0.5004) acc 87.2549 (87.6828) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [15/23] time 0.158 (0.243) data 0.000 (0.091) loss 0.3512 (0.4753) acc 92.1053 (87.2121) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [20/23] time 0.146 (0.219) data 0.000 (0.068) loss 0.2983 (0.4506) acc 93.1373 (86.9855) lr 1.2369e-04 eta 0:00:30
>>> alpha1: 0.183  alpha2: 0.049 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [45/50] batch [5/23] time 0.318 (0.568) data 0.000 (0.278) loss 0.4203 (0.3970) acc 87.0690 (88.5341) lr 9.5173e-05 eta 0:01:15
epoch [45/50] batch [10/23] time 0.283 (0.430) data 0.001 (0.139) loss 0.4085 (0.4211) acc 93.7500 (88.1154) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [15/23] time 0.291 (0.380) data 0.000 (0.093) loss 0.4365 (0.4278) acc 91.5000 (87.6697) lr 9.5173e-05 eta 0:00:46
epoch [45/50] batch [20/23] time 0.297 (0.357) data 0.000 (0.070) loss 0.3518 (0.4268) acc 86.5741 (88.0094) lr 9.5173e-05 eta 0:00:42
>>> alpha1: 0.180  alpha2: 0.047 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.31 <<<
epoch [46/50] batch [5/23] time 0.153 (0.435) data 0.000 (0.277) loss 0.6181 (0.4441) acc 82.3864 (88.0135) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [10/23] time 0.199 (0.301) data 0.000 (0.139) loss 0.3799 (0.4142) acc 91.0000 (87.9309) lr 7.0224e-05 eta 0:00:31
epoch [46/50] batch [15/23] time 0.293 (0.287) data 0.000 (0.093) loss 0.4578 (0.4238) acc 82.8431 (87.2953) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.286 (0.285) data 0.000 (0.070) loss 0.3121 (0.4255) acc 92.7885 (87.7926) lr 7.0224e-05 eta 0:00:27
>>> alpha1: 0.180  alpha2: 0.049 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.27 <<<
epoch [47/50] batch [5/23] time 0.283 (0.537) data 0.000 (0.248) loss 0.3669 (0.3689) acc 91.1458 (88.8427) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [10/23] time 0.273 (0.404) data 0.000 (0.124) loss 0.2762 (0.4051) acc 90.6250 (86.8394) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [15/23] time 0.148 (0.343) data 0.000 (0.083) loss 0.3388 (0.4193) acc 90.1042 (87.7387) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [20/23] time 0.134 (0.292) data 0.000 (0.062) loss 0.4040 (0.4140) acc 90.6977 (87.9995) lr 4.8943e-05 eta 0:00:21
>>> alpha1: 0.182  alpha2: 0.049 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.30 <<<
epoch [48/50] batch [5/23] time 0.204 (0.467) data 0.000 (0.252) loss 0.6137 (0.5048) acc 83.7963 (85.1267) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [10/23] time 0.204 (0.333) data 0.000 (0.127) loss 0.3530 (0.4411) acc 90.5660 (87.3651) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [15/23] time 0.185 (0.286) data 0.000 (0.085) loss 0.4767 (0.4460) acc 83.5106 (87.0664) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.226 (0.264) data 0.000 (0.064) loss 0.3084 (0.4341) acc 88.6792 (86.8638) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.181  alpha2: 0.046 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.27 <<<
epoch [49/50] batch [5/23] time 0.170 (0.453) data 0.000 (0.272) loss 0.3650 (0.4039) acc 90.5556 (88.1780) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.186 (0.319) data 0.000 (0.136) loss 0.3021 (0.4285) acc 89.5349 (87.4486) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.188 (0.280) data 0.000 (0.091) loss 0.3036 (0.3988) acc 93.2292 (89.4578) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.185 (0.256) data 0.000 (0.068) loss 0.4320 (0.3949) acc 86.7021 (89.2239) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.181  alpha2: 0.047 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.27 <<<
epoch [50/50] batch [5/23] time 0.184 (0.481) data 0.000 (0.279) loss 0.5367 (0.4446) acc 88.2653 (88.8923) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.184 (0.337) data 0.000 (0.139) loss 0.4082 (0.4261) acc 86.7021 (88.9553) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.192 (0.284) data 0.000 (0.093) loss 0.3878 (0.4337) acc 89.7959 (88.6129) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.174 (0.263) data 0.000 (0.070) loss 0.3947 (0.4319) acc 85.5556 (88.7604) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.26, 0.23, 0.23, 0.22, 0.22, 0.22, 0.22, 0.21, 0.21, 0.21, 0.2, 0.2, 0.2, 0.2, 0.2, 0.19, 0.2, 0.19, 0.2, 0.19, 0.19, 0.2, 0.2, 0.2, 0.19, 0.19, 0.19, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]
* matched noise rate: [0.08, 0.09, 0.09, 0.12, 0.12, 0.12, 0.13, 0.11, 0.11, 0.11, 0.11, 0.09, 0.11, 0.11, 0.09, 0.11, 0.11, 0.1, 0.11, 0.11, 0.11, 0.12, 0.11, 0.12, 0.12, 0.12, 0.12, 0.11, 0.11, 0.1, 0.12, 0.11, 0.13, 0.11, 0.12, 0.12, 0.11, 0.13, 0.12, 0.12]
* unmatched noise rate: [0.48, 0.34, 0.32, 0.43, 0.43, 0.34, 0.35, 0.33, 0.33, 0.34, 0.31, 0.29, 0.32, 0.34, 0.29, 0.3, 0.31, 0.32, 0.33, 0.31, 0.3, 0.31, 0.31, 0.31, 0.3, 0.32, 0.32, 0.32, 0.32, 0.34, 0.33, 0.34, 0.32, 0.32, 0.31, 0.31, 0.27, 0.3, 0.27, 0.27]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:40,  2.51s/it] 12%|█▏        | 2/17 [00:02<00:16,  1.10s/it] 24%|██▎       | 4/17 [00:02<00:06,  2.14it/s] 35%|███▌      | 6/17 [00:02<00:03,  3.43it/s] 47%|████▋     | 8/17 [00:03<00:01,  4.71it/s] 59%|█████▉    | 10/17 [00:03<00:01,  5.91it/s] 71%|███████   | 12/17 [00:03<00:00,  6.96it/s] 82%|████████▏ | 14/17 [00:03<00:00,  7.84it/s] 94%|█████████▍| 16/17 [00:03<00:00,  8.56it/s]100%|██████████| 17/17 [00:04<00:00,  3.57it/s]
=> result
* total: 1,692
* correct: 1,023
* accuracy: 60.5%
* error: 39.5%
* macro_f1: 59.7%
=> per-class result
* class: 0 (banded)	total: 36	correct: 25	acc: 69.4%
* class: 1 (blotchy)	total: 36	correct: 11	acc: 30.6%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 30	acc: 83.3%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 27	acc: 75.0%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 32	acc: 88.9%
* class: 10 (dotted)	total: 36	correct: 10	acc: 27.8%
* class: 11 (fibrous)	total: 36	correct: 25	acc: 69.4%
* class: 12 (flecked)	total: 36	correct: 25	acc: 69.4%
* class: 13 (freckled)	total: 36	correct: 30	acc: 83.3%
* class: 14 (frilly)	total: 36	correct: 18	acc: 50.0%
* class: 15 (gauzy)	total: 36	correct: 20	acc: 55.6%
* class: 16 (grid)	total: 36	correct: 15	acc: 41.7%
* class: 17 (grooved)	total: 36	correct: 22	acc: 61.1%
* class: 18 (honeycombed)	total: 36	correct: 28	acc: 77.8%
* class: 19 (interlaced)	total: 36	correct: 27	acc: 75.0%
* class: 20 (knitted)	total: 36	correct: 27	acc: 75.0%
* class: 21 (lacelike)	total: 36	correct: 32	acc: 88.9%
* class: 22 (lined)	total: 36	correct: 9	acc: 25.0%
* class: 23 (marbled)	total: 36	correct: 16	acc: 44.4%
* class: 24 (matted)	total: 36	correct: 23	acc: 63.9%
* class: 25 (meshed)	total: 36	correct: 16	acc: 44.4%
* class: 26 (paisley)	total: 36	correct: 34	acc: 94.4%
* class: 27 (perforated)	total: 36	correct: 23	acc: 63.9%
* class: 28 (pitted)	total: 36	correct: 6	acc: 16.7%
* class: 29 (pleated)	total: 36	correct: 11	acc: 30.6%
* class: 30 (polka-dotted)	total: 36	correct: 30	acc: 83.3%
* class: 31 (porous)	total: 36	correct: 19	acc: 52.8%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 24	acc: 66.7%
* class: 34 (smeared)	total: 36	correct: 17	acc: 47.2%
* class: 35 (spiralled)	total: 36	correct: 19	acc: 52.8%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 7	acc: 19.4%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 21	acc: 58.3%
* class: 42 (veined)	total: 36	correct: 19	acc: 52.8%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 22	acc: 61.1%
* class: 45 (wrinkled)	total: 36	correct: 24	acc: 66.7%
* class: 46 (zigzagged)	total: 36	correct: 32	acc: 88.9%
* average: 60.5%
Elapsed: 0:16:28
Run this job and save the output to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_6-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.206 (1.039) data 0.000 (0.327) loss 3.6537 (3.5789) acc 21.8750 (20.6250) lr 1.0000e-05 eta 0:19:50
epoch [1/50] batch [10/23] time 0.195 (0.631) data 0.000 (0.164) loss 3.4423 (3.6042) acc 18.7500 (17.5000) lr 1.0000e-05 eta 0:11:59
epoch [1/50] batch [15/23] time 0.198 (0.486) data 0.000 (0.109) loss 3.6122 (3.5983) acc 12.5000 (16.6667) lr 1.0000e-05 eta 0:09:11
epoch [1/50] batch [20/23] time 0.207 (0.415) data 0.000 (0.082) loss 3.6721 (3.6025) acc 12.5000 (15.6250) lr 1.0000e-05 eta 0:07:48
epoch [2/50] batch [5/23] time 0.226 (0.489) data 0.000 (0.271) loss 3.1596 (3.4193) acc 31.2500 (21.2500) lr 2.0000e-03 eta 0:09:08
epoch [2/50] batch [10/23] time 0.205 (0.356) data 0.000 (0.136) loss 3.3492 (3.3641) acc 18.7500 (20.6250) lr 2.0000e-03 eta 0:06:37
epoch [2/50] batch [15/23] time 0.207 (0.306) data 0.000 (0.091) loss 3.7277 (3.3741) acc 18.7500 (21.4583) lr 2.0000e-03 eta 0:05:40
epoch [2/50] batch [20/23] time 0.206 (0.281) data 0.000 (0.068) loss 3.3154 (3.3747) acc 15.6250 (19.5312) lr 2.0000e-03 eta 0:05:11
epoch [3/50] batch [5/23] time 0.204 (0.534) data 0.000 (0.299) loss 3.1452 (3.2845) acc 28.1250 (21.8750) lr 1.9980e-03 eta 0:09:46
epoch [3/50] batch [10/23] time 0.205 (0.372) data 0.000 (0.149) loss 2.6304 (3.1043) acc 43.7500 (27.5000) lr 1.9980e-03 eta 0:06:46
epoch [3/50] batch [15/23] time 0.205 (0.316) data 0.000 (0.100) loss 3.0322 (3.0803) acc 34.3750 (28.5417) lr 1.9980e-03 eta 0:05:44
epoch [3/50] batch [20/23] time 0.206 (0.292) data 0.000 (0.075) loss 3.5552 (3.2003) acc 12.5000 (25.1562) lr 1.9980e-03 eta 0:05:16
epoch [4/50] batch [5/23] time 0.203 (0.470) data 0.000 (0.229) loss 3.3139 (3.0775) acc 18.7500 (28.1250) lr 1.9921e-03 eta 0:08:26
epoch [4/50] batch [10/23] time 0.209 (0.345) data 0.000 (0.115) loss 2.9132 (3.0858) acc 28.1250 (29.3750) lr 1.9921e-03 eta 0:06:09
epoch [4/50] batch [15/23] time 0.275 (0.304) data 0.000 (0.076) loss 2.8854 (3.1057) acc 37.5000 (29.3750) lr 1.9921e-03 eta 0:05:23
epoch [4/50] batch [20/23] time 0.207 (0.280) data 0.000 (0.057) loss 2.8995 (3.0820) acc 34.3750 (28.7500) lr 1.9921e-03 eta 0:04:56
epoch [5/50] batch [5/23] time 0.216 (0.515) data 0.001 (0.288) loss 2.7647 (2.8476) acc 40.6250 (38.1250) lr 1.9823e-03 eta 0:09:02
epoch [5/50] batch [10/23] time 0.207 (0.364) data 0.000 (0.144) loss 3.2860 (2.8890) acc 34.3750 (36.2500) lr 1.9823e-03 eta 0:06:21
epoch [5/50] batch [15/23] time 0.206 (0.315) data 0.000 (0.096) loss 2.6747 (2.8987) acc 43.7500 (35.6250) lr 1.9823e-03 eta 0:05:29
epoch [5/50] batch [20/23] time 0.206 (0.288) data 0.000 (0.072) loss 3.1583 (2.9729) acc 25.0000 (33.5938) lr 1.9823e-03 eta 0:04:58
epoch [6/50] batch [5/23] time 0.218 (0.501) data 0.000 (0.278) loss 2.9863 (2.9198) acc 25.0000 (28.7500) lr 1.9686e-03 eta 0:08:36
epoch [6/50] batch [10/23] time 0.206 (0.364) data 0.000 (0.140) loss 2.6128 (2.8392) acc 43.7500 (32.5000) lr 1.9686e-03 eta 0:06:13
epoch [6/50] batch [15/23] time 0.206 (0.312) data 0.000 (0.094) loss 2.4439 (2.8632) acc 46.8750 (33.7500) lr 1.9686e-03 eta 0:05:17
epoch [6/50] batch [20/23] time 0.206 (0.285) data 0.000 (0.070) loss 3.0819 (2.8737) acc 31.2500 (33.9062) lr 1.9686e-03 eta 0:04:49
epoch [7/50] batch [5/23] time 0.212 (0.486) data 0.000 (0.274) loss 2.6411 (2.8014) acc 43.7500 (38.7500) lr 1.9511e-03 eta 0:08:09
epoch [7/50] batch [10/23] time 0.197 (0.353) data 0.000 (0.137) loss 2.7162 (2.8490) acc 43.7500 (35.9375) lr 1.9511e-03 eta 0:05:53
epoch [7/50] batch [15/23] time 0.199 (0.301) data 0.000 (0.091) loss 2.5860 (2.8332) acc 37.5000 (35.0000) lr 1.9511e-03 eta 0:04:59
epoch [7/50] batch [20/23] time 0.197 (0.275) data 0.000 (0.069) loss 2.5959 (2.8419) acc 40.6250 (35.3125) lr 1.9511e-03 eta 0:04:32
epoch [8/50] batch [5/23] time 0.202 (0.501) data 0.000 (0.278) loss 2.6135 (2.6272) acc 43.7500 (43.7500) lr 1.9298e-03 eta 0:08:12
epoch [8/50] batch [10/23] time 0.206 (0.362) data 0.000 (0.139) loss 2.7359 (2.6896) acc 31.2500 (40.9375) lr 1.9298e-03 eta 0:05:54
epoch [8/50] batch [15/23] time 0.207 (0.310) data 0.000 (0.093) loss 3.5105 (2.7542) acc 28.1250 (38.7500) lr 1.9298e-03 eta 0:05:02
epoch [8/50] batch [20/23] time 0.208 (0.285) data 0.000 (0.070) loss 2.8036 (2.7816) acc 37.5000 (37.5000) lr 1.9298e-03 eta 0:04:36
epoch [9/50] batch [5/23] time 0.214 (0.533) data 0.000 (0.298) loss 2.8009 (2.7900) acc 37.5000 (38.1250) lr 1.9048e-03 eta 0:08:31
epoch [9/50] batch [10/23] time 0.218 (0.373) data 0.000 (0.149) loss 2.4475 (2.7903) acc 50.0000 (38.4375) lr 1.9048e-03 eta 0:05:56
epoch [9/50] batch [15/23] time 0.208 (0.318) data 0.000 (0.100) loss 2.5993 (2.6822) acc 40.6250 (41.0417) lr 1.9048e-03 eta 0:05:02
epoch [9/50] batch [20/23] time 0.204 (0.294) data 0.000 (0.075) loss 2.6420 (2.7155) acc 31.2500 (40.4688) lr 1.9048e-03 eta 0:04:37
epoch [10/50] batch [5/23] time 0.226 (0.491) data 0.000 (0.259) loss 2.6561 (2.7971) acc 37.5000 (33.7500) lr 1.8763e-03 eta 0:07:40
epoch [10/50] batch [10/23] time 0.207 (0.350) data 0.000 (0.129) loss 2.4515 (2.6317) acc 46.8750 (40.3125) lr 1.8763e-03 eta 0:05:26
epoch [10/50] batch [15/23] time 0.151 (0.295) data 0.000 (0.086) loss 2.9069 (2.6925) acc 28.1250 (40.2083) lr 1.8763e-03 eta 0:04:34
epoch [10/50] batch [20/23] time 0.149 (0.259) data 0.000 (0.065) loss 2.7480 (2.6834) acc 37.5000 (40.3125) lr 1.8763e-03 eta 0:03:59
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.785  alpha2: 0.207 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [11/50] batch [5/23] time 1.013 (1.142) data 0.000 (0.271) loss 1.8743 (1.7307) acc 47.3958 (62.5269) lr 1.8443e-03 eta 0:17:24
epoch [11/50] batch [10/23] time 0.194 (0.838) data 0.000 (0.136) loss 1.4429 (1.6850) acc 65.3061 (63.1536) lr 1.8443e-03 eta 0:12:42
epoch [11/50] batch [15/23] time 0.186 (0.722) data 0.000 (0.091) loss 1.6380 (1.6535) acc 54.7872 (63.2365) lr 1.8443e-03 eta 0:10:53
epoch [11/50] batch [20/23] time 0.183 (0.634) data 0.000 (0.068) loss 1.8095 (1.6469) acc 54.8913 (61.6464) lr 1.8443e-03 eta 0:09:30
>>> alpha1: 0.658  alpha2: 0.197 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [12/50] batch [5/23] time 0.185 (0.500) data 0.000 (0.307) loss 1.1836 (1.2894) acc 68.7500 (66.1659) lr 1.8090e-03 eta 0:07:26
epoch [12/50] batch [10/23] time 0.186 (0.342) data 0.000 (0.154) loss 1.1680 (1.2084) acc 81.3830 (70.0125) lr 1.8090e-03 eta 0:05:03
epoch [12/50] batch [15/23] time 0.184 (0.293) data 0.000 (0.103) loss 1.7196 (1.2793) acc 53.1915 (66.0469) lr 1.8090e-03 eta 0:04:18
epoch [12/50] batch [20/23] time 0.191 (0.264) data 0.000 (0.077) loss 1.1631 (1.2886) acc 64.0000 (65.6645) lr 1.8090e-03 eta 0:03:51
>>> alpha1: 0.600  alpha2: 0.167 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.32 <<<
epoch [13/50] batch [5/23] time 0.181 (0.469) data 0.000 (0.283) loss 1.0810 (1.1068) acc 68.0851 (67.3583) lr 1.7705e-03 eta 0:06:47
epoch [13/50] batch [10/23] time 0.174 (0.325) data 0.000 (0.142) loss 1.3763 (1.2014) acc 60.7955 (64.7759) lr 1.7705e-03 eta 0:04:41
epoch [13/50] batch [15/23] time 0.934 (0.329) data 0.000 (0.095) loss 1.1238 (1.1688) acc 68.2927 (66.7227) lr 1.7705e-03 eta 0:04:42
epoch [13/50] batch [20/23] time 0.170 (0.290) data 0.000 (0.071) loss 1.2471 (1.1974) acc 60.7143 (66.3658) lr 1.7705e-03 eta 0:04:08
>>> alpha1: 0.544  alpha2: 0.141 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [14/50] batch [5/23] time 0.194 (0.714) data 0.000 (0.323) loss 0.8559 (1.0009) acc 76.4423 (74.4247) lr 1.7290e-03 eta 0:10:03
epoch [14/50] batch [10/23] time 0.195 (0.452) data 0.000 (0.162) loss 1.0147 (1.1556) acc 69.1176 (70.6948) lr 1.7290e-03 eta 0:06:20
epoch [14/50] batch [15/23] time 0.189 (0.366) data 0.000 (0.108) loss 1.0488 (1.1192) acc 71.9388 (70.3121) lr 1.7290e-03 eta 0:05:05
epoch [14/50] batch [20/23] time 0.179 (0.321) data 0.000 (0.081) loss 1.2019 (1.0885) acc 64.3617 (70.2841) lr 1.7290e-03 eta 0:04:26
>>> alpha1: 0.523  alpha2: 0.159 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.36 <<<
epoch [15/50] batch [5/23] time 0.199 (0.507) data 0.000 (0.293) loss 0.8857 (0.8602) acc 71.6346 (80.0127) lr 1.6845e-03 eta 0:06:57
epoch [15/50] batch [10/23] time 0.202 (0.452) data 0.000 (0.147) loss 0.8806 (0.9223) acc 73.1132 (75.8076) lr 1.6845e-03 eta 0:06:09
epoch [15/50] batch [15/23] time 0.190 (0.371) data 0.000 (0.098) loss 0.8959 (0.9256) acc 74.4792 (73.9845) lr 1.6845e-03 eta 0:05:01
epoch [15/50] batch [20/23] time 0.200 (0.327) data 0.000 (0.073) loss 0.9549 (0.9657) acc 73.5294 (72.9574) lr 1.6845e-03 eta 0:04:23
>>> alpha1: 0.414  alpha2: 0.041 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.37 <<<
epoch [16/50] batch [5/23] time 0.217 (0.697) data 0.000 (0.291) loss 1.1234 (0.9476) acc 67.4528 (71.2753) lr 1.6374e-03 eta 0:09:17
epoch [16/50] batch [10/23] time 0.187 (0.450) data 0.000 (0.146) loss 0.6883 (0.9035) acc 84.8958 (74.1483) lr 1.6374e-03 eta 0:05:57
epoch [16/50] batch [15/23] time 0.184 (0.363) data 0.000 (0.097) loss 1.1182 (0.9064) acc 65.9574 (72.8014) lr 1.6374e-03 eta 0:04:47
epoch [16/50] batch [20/23] time 0.194 (0.321) data 0.000 (0.073) loss 0.7118 (0.8666) acc 81.3726 (73.8710) lr 1.6374e-03 eta 0:04:11
>>> alpha1: 0.358  alpha2: -0.008 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.37 <<<
epoch [17/50] batch [5/23] time 0.296 (0.580) data 0.001 (0.281) loss 1.2324 (0.8011) acc 74.0196 (81.3576) lr 1.5878e-03 eta 0:07:30
epoch [17/50] batch [10/23] time 0.247 (0.414) data 0.000 (0.141) loss 0.6166 (0.7963) acc 80.8824 (76.9452) lr 1.5878e-03 eta 0:05:19
epoch [17/50] batch [15/23] time 0.292 (0.368) data 0.000 (0.094) loss 0.8677 (0.7957) acc 79.5000 (77.1556) lr 1.5878e-03 eta 0:04:42
epoch [17/50] batch [20/23] time 0.168 (0.363) data 0.000 (0.070) loss 0.7509 (0.8021) acc 77.6786 (76.9247) lr 1.5878e-03 eta 0:04:36
>>> alpha1: 0.319  alpha2: -0.038 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.39 <<<
epoch [18/50] batch [5/23] time 0.186 (0.493) data 0.000 (0.301) loss 0.6292 (0.7570) acc 82.6531 (76.0032) lr 1.5358e-03 eta 0:06:11
epoch [18/50] batch [10/23] time 0.192 (0.345) data 0.000 (0.151) loss 0.5368 (0.7175) acc 85.5000 (78.5160) lr 1.5358e-03 eta 0:04:18
epoch [18/50] batch [15/23] time 0.192 (0.298) data 0.001 (0.101) loss 0.8213 (0.7204) acc 72.5000 (78.6680) lr 1.5358e-03 eta 0:03:41
epoch [18/50] batch [20/23] time 0.183 (0.273) data 0.000 (0.076) loss 0.6733 (0.7253) acc 77.1739 (78.3199) lr 1.5358e-03 eta 0:03:21
>>> alpha1: 0.288  alpha2: -0.043 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.36 <<<
epoch [19/50] batch [5/23] time 0.203 (0.476) data 0.000 (0.271) loss 0.7690 (0.6657) acc 82.8704 (80.6723) lr 1.4818e-03 eta 0:05:47
epoch [19/50] batch [10/23] time 0.207 (0.340) data 0.000 (0.135) loss 0.4989 (0.6815) acc 89.0909 (80.3301) lr 1.4818e-03 eta 0:04:06
epoch [19/50] batch [15/23] time 0.194 (0.292) data 0.000 (0.090) loss 0.8470 (0.6925) acc 78.4314 (79.8668) lr 1.4818e-03 eta 0:03:30
epoch [19/50] batch [20/23] time 0.190 (0.269) data 0.000 (0.068) loss 0.5402 (0.6819) acc 80.6122 (80.2105) lr 1.4818e-03 eta 0:03:12
>>> alpha1: 0.271  alpha2: -0.059 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [20/50] batch [5/23] time 0.168 (0.489) data 0.000 (0.292) loss 0.6849 (0.6021) acc 79.7619 (83.0315) lr 1.4258e-03 eta 0:05:46
epoch [20/50] batch [10/23] time 0.190 (0.338) data 0.000 (0.146) loss 0.5631 (0.6163) acc 83.7209 (82.3222) lr 1.4258e-03 eta 0:03:57
epoch [20/50] batch [15/23] time 0.169 (0.285) data 0.000 (0.098) loss 0.6028 (0.6348) acc 75.0000 (81.3345) lr 1.4258e-03 eta 0:03:19
epoch [20/50] batch [20/23] time 0.189 (0.264) data 0.000 (0.073) loss 0.6916 (0.6337) acc 73.4694 (80.8794) lr 1.4258e-03 eta 0:03:02
>>> alpha1: 0.243  alpha2: -0.073 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [21/50] batch [5/23] time 0.174 (0.499) data 0.000 (0.326) loss 0.7265 (0.5030) acc 76.0638 (85.5358) lr 1.3681e-03 eta 0:05:41
epoch [21/50] batch [10/23] time 0.184 (0.342) data 0.000 (0.163) loss 0.9279 (0.6002) acc 75.5319 (82.7767) lr 1.3681e-03 eta 0:03:52
epoch [21/50] batch [15/23] time 0.182 (0.287) data 0.000 (0.109) loss 0.6014 (0.6008) acc 83.3333 (82.9066) lr 1.3681e-03 eta 0:03:13
epoch [21/50] batch [20/23] time 0.180 (0.263) data 0.001 (0.082) loss 0.5886 (0.6162) acc 86.4130 (82.1600) lr 1.3681e-03 eta 0:02:56
>>> alpha1: 0.224  alpha2: -0.075 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.32 <<<
epoch [22/50] batch [5/23] time 0.202 (0.472) data 0.000 (0.275) loss 0.4395 (0.5463) acc 85.6383 (83.0506) lr 1.3090e-03 eta 0:05:12
epoch [22/50] batch [10/23] time 0.181 (0.334) data 0.000 (0.138) loss 0.8597 (0.6139) acc 73.3696 (81.2338) lr 1.3090e-03 eta 0:03:39
epoch [22/50] batch [15/23] time 0.192 (0.286) data 0.000 (0.092) loss 0.4699 (0.5782) acc 87.5000 (82.6734) lr 1.3090e-03 eta 0:03:06
epoch [22/50] batch [20/23] time 0.860 (0.293) data 0.000 (0.069) loss 0.7083 (0.5976) acc 81.8750 (82.2065) lr 1.3090e-03 eta 0:03:09
>>> alpha1: 0.214  alpha2: -0.074 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [23/50] batch [5/23] time 0.185 (0.471) data 0.000 (0.276) loss 0.4862 (0.4948) acc 83.8542 (86.5115) lr 1.2487e-03 eta 0:05:01
epoch [23/50] batch [10/23] time 0.191 (0.334) data 0.000 (0.138) loss 0.5773 (0.5792) acc 83.0000 (84.3710) lr 1.2487e-03 eta 0:03:31
epoch [23/50] batch [15/23] time 0.190 (0.285) data 0.000 (0.092) loss 0.6269 (0.5904) acc 84.1837 (84.1966) lr 1.2487e-03 eta 0:02:59
epoch [23/50] batch [20/23] time 0.194 (0.260) data 0.000 (0.069) loss 0.5538 (0.5932) acc 85.2941 (83.2357) lr 1.2487e-03 eta 0:02:42
>>> alpha1: 0.210  alpha2: -0.079 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [24/50] batch [5/23] time 0.150 (0.463) data 0.000 (0.317) loss 0.7555 (0.5741) acc 78.4314 (82.4350) lr 1.1874e-03 eta 0:04:45
epoch [24/50] batch [10/23] time 0.298 (0.346) data 0.000 (0.159) loss 0.5254 (0.5246) acc 86.7347 (83.8603) lr 1.1874e-03 eta 0:03:31
epoch [24/50] batch [15/23] time 0.303 (0.326) data 0.000 (0.106) loss 0.6280 (0.5285) acc 79.7170 (84.3336) lr 1.1874e-03 eta 0:03:17
epoch [24/50] batch [20/23] time 0.242 (0.310) data 0.000 (0.079) loss 0.5968 (0.5550) acc 86.2245 (83.9293) lr 1.1874e-03 eta 0:03:06
>>> alpha1: 0.201  alpha2: -0.079 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.31 <<<
epoch [25/50] batch [5/23] time 0.249 (0.533) data 0.000 (0.329) loss 0.4742 (0.5899) acc 87.9808 (84.5229) lr 1.1253e-03 eta 0:05:16
epoch [25/50] batch [10/23] time 0.190 (0.359) data 0.000 (0.165) loss 0.5422 (0.5567) acc 76.0204 (84.1450) lr 1.1253e-03 eta 0:03:31
epoch [25/50] batch [15/23] time 0.190 (0.303) data 0.000 (0.110) loss 0.3964 (0.5381) acc 94.8980 (84.3663) lr 1.1253e-03 eta 0:02:56
epoch [25/50] batch [20/23] time 0.185 (0.276) data 0.000 (0.083) loss 0.5993 (0.5527) acc 82.9787 (84.6530) lr 1.1253e-03 eta 0:02:39
>>> alpha1: 0.194  alpha2: -0.083 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.32 <<<
epoch [26/50] batch [5/23] time 0.202 (0.473) data 0.000 (0.269) loss 0.3451 (0.4062) acc 91.3265 (89.7937) lr 1.0628e-03 eta 0:04:29
epoch [26/50] batch [10/23] time 0.183 (0.334) data 0.000 (0.135) loss 0.5876 (0.4860) acc 84.7826 (86.5916) lr 1.0628e-03 eta 0:03:08
epoch [26/50] batch [15/23] time 0.195 (0.290) data 0.000 (0.090) loss 0.4657 (0.5015) acc 88.7255 (85.5780) lr 1.0628e-03 eta 0:02:42
epoch [26/50] batch [20/23] time 0.188 (0.263) data 0.000 (0.068) loss 0.4756 (0.5032) acc 84.8958 (85.6503) lr 1.0628e-03 eta 0:02:25
>>> alpha1: 0.189  alpha2: -0.082 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.33 <<<
epoch [27/50] batch [5/23] time 0.207 (0.519) data 0.000 (0.323) loss 0.4856 (0.5527) acc 88.1818 (85.1473) lr 1.0000e-03 eta 0:04:43
epoch [27/50] batch [10/23] time 0.194 (0.352) data 0.000 (0.162) loss 0.4988 (0.5084) acc 78.8462 (84.4159) lr 1.0000e-03 eta 0:03:10
epoch [27/50] batch [15/23] time 0.191 (0.298) data 0.000 (0.108) loss 0.4497 (0.5143) acc 84.5000 (84.9380) lr 1.0000e-03 eta 0:02:39
epoch [27/50] batch [20/23] time 0.178 (0.275) data 0.000 (0.081) loss 0.6646 (0.5080) acc 86.4130 (85.7292) lr 1.0000e-03 eta 0:02:26
>>> alpha1: 0.185  alpha2: -0.083 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.36 <<<
epoch [28/50] batch [5/23] time 0.268 (0.560) data 0.000 (0.297) loss 0.5585 (0.4515) acc 87.5000 (86.3434) lr 9.3721e-04 eta 0:04:53
epoch [28/50] batch [10/23] time 0.266 (0.405) data 0.000 (0.149) loss 0.4470 (0.4986) acc 88.8889 (84.6770) lr 9.3721e-04 eta 0:03:30
epoch [28/50] batch [15/23] time 0.263 (0.354) data 0.000 (0.099) loss 0.4082 (0.4711) acc 87.5000 (85.9731) lr 9.3721e-04 eta 0:03:02
epoch [28/50] batch [20/23] time 0.241 (0.329) data 0.000 (0.074) loss 0.4188 (0.4780) acc 91.0000 (85.9548) lr 9.3721e-04 eta 0:02:47
>>> alpha1: 0.177  alpha2: -0.079 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.32 <<<
epoch [29/50] batch [5/23] time 0.179 (0.506) data 0.000 (0.321) loss 0.4135 (0.3980) acc 90.9574 (88.5482) lr 8.7467e-04 eta 0:04:13
epoch [29/50] batch [10/23] time 0.203 (0.357) data 0.000 (0.162) loss 0.3921 (0.4817) acc 88.5870 (86.1900) lr 8.7467e-04 eta 0:02:56
epoch [29/50] batch [15/23] time 0.174 (0.295) data 0.000 (0.108) loss 0.5548 (0.4983) acc 82.3864 (85.3196) lr 8.7467e-04 eta 0:02:25
epoch [29/50] batch [20/23] time 0.197 (0.267) data 0.000 (0.081) loss 0.5485 (0.5031) acc 83.1731 (84.7942) lr 8.7467e-04 eta 0:02:09
>>> alpha1: 0.173  alpha2: -0.075 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.31 <<<
epoch [30/50] batch [5/23] time 0.181 (0.472) data 0.000 (0.273) loss 0.4747 (0.3767) acc 92.5532 (90.5181) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [10/23] time 0.230 (0.332) data 0.000 (0.136) loss 0.3676 (0.4191) acc 88.5870 (88.4564) lr 8.1262e-04 eta 0:02:36
epoch [30/50] batch [15/23] time 0.187 (0.285) data 0.000 (0.091) loss 0.3020 (0.4148) acc 93.8775 (88.4666) lr 8.1262e-04 eta 0:02:13
epoch [30/50] batch [20/23] time 0.181 (0.258) data 0.000 (0.068) loss 0.4059 (0.4463) acc 89.1304 (87.1779) lr 8.1262e-04 eta 0:01:59
>>> alpha1: 0.168  alpha2: -0.070 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.33 <<<
epoch [31/50] batch [5/23] time 0.205 (0.500) data 0.000 (0.296) loss 0.3804 (0.4298) acc 91.2037 (85.7695) lr 7.5131e-04 eta 0:03:47
epoch [31/50] batch [10/23] time 0.269 (0.356) data 0.000 (0.148) loss 0.4929 (0.4243) acc 85.5769 (86.7645) lr 7.5131e-04 eta 0:02:40
epoch [31/50] batch [15/23] time 0.191 (0.302) data 0.000 (0.099) loss 0.3945 (0.4284) acc 90.0000 (87.3206) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [20/23] time 0.192 (0.274) data 0.000 (0.074) loss 0.6792 (0.4610) acc 82.0000 (86.5478) lr 7.5131e-04 eta 0:02:00
>>> alpha1: 0.164  alpha2: -0.073 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [32/50] batch [5/23] time 0.208 (0.511) data 0.001 (0.306) loss 0.4519 (0.4342) acc 92.9245 (87.3336) lr 6.9098e-04 eta 0:03:40
epoch [32/50] batch [10/23] time 0.187 (0.354) data 0.000 (0.153) loss 0.4295 (0.4469) acc 90.1042 (85.9961) lr 6.9098e-04 eta 0:02:30
epoch [32/50] batch [15/23] time 0.214 (0.305) data 0.000 (0.102) loss 0.4612 (0.4522) acc 88.8889 (86.5760) lr 6.9098e-04 eta 0:02:08
epoch [32/50] batch [20/23] time 0.184 (0.278) data 0.000 (0.077) loss 0.5227 (0.4575) acc 78.2609 (86.5822) lr 6.9098e-04 eta 0:01:55
>>> alpha1: 0.161  alpha2: -0.074 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.36 <<<
epoch [33/50] batch [5/23] time 0.188 (0.472) data 0.000 (0.277) loss 0.7165 (0.4335) acc 76.0870 (87.0835) lr 6.3188e-04 eta 0:03:13
epoch [33/50] batch [10/23] time 0.200 (0.332) data 0.000 (0.139) loss 0.4002 (0.4044) acc 91.3462 (88.1113) lr 6.3188e-04 eta 0:02:14
epoch [33/50] batch [15/23] time 0.193 (0.287) data 0.000 (0.093) loss 0.4794 (0.4134) acc 83.3333 (87.9477) lr 6.3188e-04 eta 0:01:54
epoch [33/50] batch [20/23] time 0.211 (0.269) data 0.000 (0.070) loss 0.3630 (0.4071) acc 89.2857 (88.4585) lr 6.3188e-04 eta 0:01:46
>>> alpha1: 0.160  alpha2: -0.075 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [34/50] batch [5/23] time 0.193 (0.511) data 0.000 (0.295) loss 0.5205 (0.5310) acc 85.2941 (84.9860) lr 5.7422e-04 eta 0:03:17
epoch [34/50] batch [10/23] time 0.197 (0.355) data 0.000 (0.148) loss 0.3630 (0.4595) acc 85.0962 (87.4015) lr 5.7422e-04 eta 0:02:15
epoch [34/50] batch [15/23] time 0.207 (0.302) data 0.000 (0.099) loss 0.2257 (0.4408) acc 96.3636 (87.9713) lr 5.7422e-04 eta 0:01:53
epoch [34/50] batch [20/23] time 0.181 (0.278) data 0.000 (0.074) loss 0.4537 (0.4391) acc 87.7778 (87.8953) lr 5.7422e-04 eta 0:01:43
>>> alpha1: 0.159  alpha2: -0.076 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.32 <<<
epoch [35/50] batch [5/23] time 0.308 (0.608) data 0.000 (0.299) loss 0.3746 (0.4499) acc 94.2308 (88.9025) lr 5.1825e-04 eta 0:03:40
epoch [35/50] batch [10/23] time 0.160 (0.380) data 0.000 (0.150) loss 0.4382 (0.4344) acc 80.7692 (87.3818) lr 5.1825e-04 eta 0:02:16
epoch [35/50] batch [15/23] time 0.147 (0.304) data 0.000 (0.100) loss 0.4817 (0.4306) acc 92.6471 (88.1705) lr 5.1825e-04 eta 0:01:47
epoch [35/50] batch [20/23] time 0.283 (0.282) data 0.000 (0.075) loss 0.3125 (0.4308) acc 88.6364 (87.9875) lr 5.1825e-04 eta 0:01:38
>>> alpha1: 0.159  alpha2: -0.075 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.32 <<<
epoch [36/50] batch [5/23] time 0.180 (0.714) data 0.000 (0.299) loss 0.3951 (0.4425) acc 85.6383 (87.8643) lr 4.6417e-04 eta 0:04:02
epoch [36/50] batch [10/23] time 0.191 (0.453) data 0.000 (0.150) loss 0.4440 (0.6253) acc 88.7755 (85.1003) lr 4.6417e-04 eta 0:02:31
epoch [36/50] batch [15/23] time 0.213 (0.368) data 0.000 (0.100) loss 0.5112 (0.5586) acc 87.5000 (86.0257) lr 4.6417e-04 eta 0:02:01
epoch [36/50] batch [20/23] time 0.202 (0.329) data 0.000 (0.075) loss 0.3268 (0.5346) acc 94.7115 (86.5687) lr 4.6417e-04 eta 0:01:46
>>> alpha1: 0.159  alpha2: -0.071 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.28 <<<
epoch [37/50] batch [5/23] time 0.171 (0.455) data 0.000 (0.249) loss 0.3884 (0.3843) acc 89.2857 (90.6114) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [10/23] time 0.172 (0.317) data 0.000 (0.125) loss 0.3018 (0.4017) acc 89.2045 (88.9714) lr 4.1221e-04 eta 0:01:38
epoch [37/50] batch [15/23] time 0.180 (0.272) data 0.000 (0.083) loss 0.4533 (0.4243) acc 86.1702 (87.8354) lr 4.1221e-04 eta 0:01:23
epoch [37/50] batch [20/23] time 0.263 (0.254) data 0.000 (0.063) loss 0.5694 (0.4376) acc 87.0000 (87.7614) lr 4.1221e-04 eta 0:01:16
>>> alpha1: 0.157  alpha2: -0.071 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.33 <<<
epoch [38/50] batch [5/23] time 0.201 (0.515) data 0.000 (0.311) loss 0.3531 (0.3801) acc 89.1509 (88.1358) lr 3.6258e-04 eta 0:02:31
epoch [38/50] batch [10/23] time 0.199 (0.364) data 0.000 (0.156) loss 0.4368 (0.4129) acc 88.4615 (87.6004) lr 3.6258e-04 eta 0:01:45
epoch [38/50] batch [15/23] time 0.191 (0.307) data 0.000 (0.104) loss 0.5596 (0.4284) acc 84.0000 (86.9691) lr 3.6258e-04 eta 0:01:27
epoch [38/50] batch [20/23] time 0.203 (0.279) data 0.000 (0.078) loss 0.3698 (0.4281) acc 94.2308 (87.5358) lr 3.6258e-04 eta 0:01:17
>>> alpha1: 0.155  alpha2: -0.062 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.34 <<<
epoch [39/50] batch [5/23] time 0.217 (0.518) data 0.000 (0.316) loss 0.3687 (0.3945) acc 89.3519 (87.4844) lr 3.1545e-04 eta 0:02:20
epoch [39/50] batch [10/23] time 0.194 (0.361) data 0.000 (0.158) loss 0.3815 (0.4075) acc 89.9038 (87.8320) lr 3.1545e-04 eta 0:01:35
epoch [39/50] batch [15/23] time 0.201 (0.307) data 0.000 (0.106) loss 0.2525 (0.3865) acc 92.5926 (88.4083) lr 3.1545e-04 eta 0:01:20
epoch [39/50] batch [20/23] time 0.198 (0.282) data 0.000 (0.079) loss 0.5446 (0.4120) acc 89.4231 (87.7993) lr 3.1545e-04 eta 0:01:12
>>> alpha1: 0.152  alpha2: -0.057 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.31 <<<
epoch [40/50] batch [5/23] time 0.206 (0.490) data 0.000 (0.293) loss 0.4033 (0.4763) acc 89.0909 (86.4395) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [10/23] time 0.216 (0.348) data 0.000 (0.147) loss 0.3275 (0.3919) acc 91.6667 (88.0311) lr 2.7103e-04 eta 0:01:24
epoch [40/50] batch [15/23] time 0.199 (0.368) data 0.000 (0.098) loss 0.3474 (0.4093) acc 93.0000 (88.2379) lr 2.7103e-04 eta 0:01:27
epoch [40/50] batch [20/23] time 0.212 (0.326) data 0.000 (0.073) loss 0.4745 (0.4192) acc 83.9286 (87.8662) lr 2.7103e-04 eta 0:01:15
>>> alpha1: 0.154  alpha2: -0.054 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [41/50] batch [5/23] time 0.186 (0.670) data 0.000 (0.255) loss 0.4872 (0.4070) acc 77.0408 (86.5028) lr 2.2949e-04 eta 0:02:30
epoch [41/50] batch [10/23] time 0.245 (0.437) data 0.000 (0.128) loss 0.4223 (0.4077) acc 84.8039 (86.1914) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [15/23] time 0.212 (0.361) data 0.000 (0.085) loss 0.3869 (0.4067) acc 91.8182 (87.1443) lr 2.2949e-04 eta 0:01:17
epoch [41/50] batch [20/23] time 0.192 (0.323) data 0.000 (0.064) loss 0.4278 (0.3996) acc 89.0000 (87.8448) lr 2.2949e-04 eta 0:01:07
>>> alpha1: 0.152  alpha2: -0.056 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [42/50] batch [5/23] time 0.290 (0.680) data 0.000 (0.363) loss 0.3687 (0.3927) acc 90.9574 (90.5436) lr 1.9098e-04 eta 0:02:17
epoch [42/50] batch [10/23] time 0.316 (0.491) data 0.000 (0.182) loss 0.4804 (0.3801) acc 82.5893 (90.9270) lr 1.9098e-04 eta 0:01:36
epoch [42/50] batch [15/23] time 0.155 (0.393) data 0.001 (0.121) loss 0.4422 (0.3978) acc 86.7924 (89.6400) lr 1.9098e-04 eta 0:01:15
epoch [42/50] batch [20/23] time 0.152 (0.334) data 0.001 (0.091) loss 0.4178 (0.3839) acc 90.3846 (89.1326) lr 1.9098e-04 eta 0:01:02
>>> alpha1: 0.151  alpha2: -0.060 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.33 <<<
epoch [43/50] batch [5/23] time 0.210 (0.516) data 0.000 (0.300) loss 0.3757 (0.3527) acc 94.0909 (89.2469) lr 1.5567e-04 eta 0:01:32
epoch [43/50] batch [10/23] time 0.195 (0.360) data 0.000 (0.150) loss 0.2586 (0.3261) acc 91.1765 (90.4180) lr 1.5567e-04 eta 0:01:02
epoch [43/50] batch [15/23] time 0.279 (0.312) data 0.000 (0.100) loss 0.4534 (0.3634) acc 88.1818 (89.1389) lr 1.5567e-04 eta 0:00:52
epoch [43/50] batch [20/23] time 0.211 (0.281) data 0.000 (0.075) loss 0.3349 (0.3851) acc 92.7273 (88.8602) lr 1.5567e-04 eta 0:00:46
>>> alpha1: 0.148  alpha2: -0.063 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [44/50] batch [5/23] time 0.198 (0.464) data 0.000 (0.253) loss 0.2773 (0.4192) acc 91.8269 (89.5083) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [10/23] time 0.205 (0.334) data 0.000 (0.127) loss 0.3493 (0.6190) acc 91.0377 (85.8500) lr 1.2369e-04 eta 0:00:50
epoch [44/50] batch [15/23] time 0.210 (0.291) data 0.000 (0.085) loss 0.3035 (0.5407) acc 93.1818 (86.8351) lr 1.2369e-04 eta 0:00:42
epoch [44/50] batch [20/23] time 0.200 (0.272) data 0.000 (0.063) loss 0.2709 (0.5072) acc 93.7500 (87.3765) lr 1.2369e-04 eta 0:00:38
>>> alpha1: 0.148  alpha2: -0.062 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.32 <<<
epoch [45/50] batch [5/23] time 0.203 (0.452) data 0.000 (0.239) loss 0.3281 (0.4080) acc 91.8269 (87.2758) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [10/23] time 0.189 (0.326) data 0.000 (0.120) loss 0.4972 (0.4184) acc 90.3061 (88.0789) lr 9.5173e-05 eta 0:00:41
epoch [45/50] batch [15/23] time 0.191 (0.284) data 0.000 (0.080) loss 0.4967 (0.4055) acc 85.7143 (88.2332) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [20/23] time 0.187 (0.266) data 0.000 (0.060) loss 0.3784 (0.3919) acc 92.1875 (89.2142) lr 9.5173e-05 eta 0:00:31
>>> alpha1: 0.146  alpha2: -0.059 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [46/50] batch [5/23] time 0.199 (0.493) data 0.000 (0.293) loss 0.5143 (0.4607) acc 79.7170 (87.5715) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [10/23] time 0.203 (0.347) data 0.001 (0.147) loss 0.3220 (0.4176) acc 93.3036 (88.4871) lr 7.0224e-05 eta 0:00:36
epoch [46/50] batch [15/23] time 0.212 (0.295) data 0.001 (0.098) loss 0.3617 (0.4001) acc 87.0536 (89.4259) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [20/23] time 0.206 (0.274) data 0.000 (0.073) loss 0.2521 (0.3911) acc 87.9630 (89.3075) lr 7.0224e-05 eta 0:00:26
>>> alpha1: 0.145  alpha2: -0.058 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.32 <<<
epoch [47/50] batch [5/23] time 0.179 (0.441) data 0.000 (0.257) loss 0.3106 (0.3905) acc 94.2708 (89.5362) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [10/23] time 0.181 (0.319) data 0.000 (0.129) loss 0.5088 (0.3945) acc 87.7551 (89.0882) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [15/23] time 0.193 (0.275) data 0.000 (0.086) loss 0.3804 (0.3890) acc 91.6667 (88.8470) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [20/23] time 0.189 (0.255) data 0.000 (0.064) loss 0.3382 (0.3766) acc 92.3469 (89.5851) lr 4.8943e-05 eta 0:00:18
>>> alpha1: 0.144  alpha2: -0.059 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.29 <<<
epoch [48/50] batch [5/23] time 0.199 (0.485) data 0.000 (0.288) loss 0.3098 (0.3322) acc 92.0000 (90.4768) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [10/23] time 0.216 (0.339) data 0.000 (0.144) loss 0.5281 (0.3659) acc 86.6667 (90.4908) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [15/23] time 0.196 (0.290) data 0.000 (0.096) loss 0.3569 (0.3788) acc 89.7059 (89.8626) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [20/23] time 0.189 (0.265) data 0.000 (0.072) loss 0.4012 (0.3644) acc 87.0000 (90.1497) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.146  alpha2: -0.059 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.29 <<<
epoch [49/50] batch [5/23] time 0.144 (0.464) data 0.000 (0.310) loss 0.2744 (0.3950) acc 91.3043 (89.2626) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [10/23] time 0.234 (0.355) data 0.001 (0.155) loss 0.3706 (0.4014) acc 90.6977 (88.8982) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [15/23] time 0.293 (0.400) data 0.000 (0.104) loss 0.4097 (0.3904) acc 89.2157 (88.9676) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [20/23] time 0.301 (0.370) data 0.000 (0.078) loss 0.4064 (0.3856) acc 88.2075 (89.0580) lr 1.7713e-05 eta 0:00:09
>>> alpha1: 0.147  alpha2: -0.061 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.29 <<<
epoch [50/50] batch [5/23] time 0.205 (0.447) data 0.000 (0.250) loss 0.4240 (0.3179) acc 88.7755 (91.5759) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.190 (0.318) data 0.001 (0.125) loss 0.3263 (0.3550) acc 95.4082 (89.8691) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.210 (0.278) data 0.000 (0.084) loss 0.4826 (0.3715) acc 82.9545 (88.9451) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.194 (0.253) data 0.000 (0.063) loss 0.3529 (0.3716) acc 94.1176 (89.1170) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.26, 0.25, 0.24, 0.23, 0.23, 0.23, 0.23, 0.24, 0.23, 0.23, 0.23, 0.23, 0.23, 0.24, 0.23, 0.22, 0.22, 0.23, 0.23, 0.23, 0.22, 0.23, 0.23, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.21, 0.21, 0.23, 0.22, 0.22, 0.21, 0.22, 0.23, 0.21, 0.23, 0.22]
* matched noise rate: [0.13, 0.13, 0.12, 0.12, 0.13, 0.12, 0.12, 0.12, 0.14, 0.12, 0.12, 0.11, 0.13, 0.14, 0.14, 0.13, 0.14, 0.14, 0.12, 0.13, 0.14, 0.14, 0.14, 0.14, 0.15, 0.16, 0.14, 0.14, 0.15, 0.15, 0.14, 0.14, 0.15, 0.14, 0.14, 0.14, 0.15, 0.15, 0.16, 0.15]
* unmatched noise rate: [0.38, 0.34, 0.32, 0.35, 0.36, 0.37, 0.37, 0.39, 0.36, 0.33, 0.33, 0.32, 0.33, 0.35, 0.31, 0.32, 0.33, 0.36, 0.32, 0.31, 0.33, 0.35, 0.36, 0.35, 0.32, 0.32, 0.28, 0.33, 0.34, 0.31, 0.34, 0.37, 0.33, 0.35, 0.32, 0.34, 0.32, 0.29, 0.29, 0.29]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:40,  2.53s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.37it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.47it/s] 41%|████      | 7/17 [00:03<00:02,  3.67it/s] 53%|█████▎    | 9/17 [00:03<00:01,  4.88it/s] 65%|██████▍   | 11/17 [00:03<00:00,  6.03it/s] 76%|███████▋  | 13/17 [00:03<00:00,  6.88it/s] 82%|████████▏ | 14/17 [00:03<00:00,  6.97it/s] 94%|█████████▍| 16/17 [00:04<00:00,  7.99it/s]100%|██████████| 17/17 [00:04<00:00,  4.50it/s]100%|██████████| 17/17 [00:04<00:00,  3.56it/s]
=> result
* total: 1,692
* correct: 1,044
* accuracy: 61.7%
* error: 38.3%
* macro_f1: 61.2%
=> per-class result
* class: 0 (banded)	total: 36	correct: 20	acc: 55.6%
* class: 1 (blotchy)	total: 36	correct: 8	acc: 22.2%
* class: 2 (braided)	total: 36	correct: 18	acc: 50.0%
* class: 3 (bubbly)	total: 36	correct: 28	acc: 77.8%
* class: 4 (bumpy)	total: 36	correct: 13	acc: 36.1%
* class: 5 (chequered)	total: 36	correct: 34	acc: 94.4%
* class: 6 (cobwebbed)	total: 36	correct: 22	acc: 61.1%
* class: 7 (cracked)	total: 36	correct: 28	acc: 77.8%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 18	acc: 50.0%
* class: 11 (fibrous)	total: 36	correct: 29	acc: 80.6%
* class: 12 (flecked)	total: 36	correct: 16	acc: 44.4%
* class: 13 (freckled)	total: 36	correct: 28	acc: 77.8%
* class: 14 (frilly)	total: 36	correct: 29	acc: 80.6%
* class: 15 (gauzy)	total: 36	correct: 17	acc: 47.2%
* class: 16 (grid)	total: 36	correct: 19	acc: 52.8%
* class: 17 (grooved)	total: 36	correct: 11	acc: 30.6%
* class: 18 (honeycombed)	total: 36	correct: 26	acc: 72.2%
* class: 19 (interlaced)	total: 36	correct: 26	acc: 72.2%
* class: 20 (knitted)	total: 36	correct: 27	acc: 75.0%
* class: 21 (lacelike)	total: 36	correct: 34	acc: 94.4%
* class: 22 (lined)	total: 36	correct: 15	acc: 41.7%
* class: 23 (marbled)	total: 36	correct: 19	acc: 52.8%
* class: 24 (matted)	total: 36	correct: 23	acc: 63.9%
* class: 25 (meshed)	total: 36	correct: 17	acc: 47.2%
* class: 26 (paisley)	total: 36	correct: 31	acc: 86.1%
* class: 27 (perforated)	total: 36	correct: 25	acc: 69.4%
* class: 28 (pitted)	total: 36	correct: 13	acc: 36.1%
* class: 29 (pleated)	total: 36	correct: 20	acc: 55.6%
* class: 30 (polka-dotted)	total: 36	correct: 32	acc: 88.9%
* class: 31 (porous)	total: 36	correct: 8	acc: 22.2%
* class: 32 (potholed)	total: 36	correct: 32	acc: 88.9%
* class: 33 (scaly)	total: 36	correct: 19	acc: 52.8%
* class: 34 (smeared)	total: 36	correct: 22	acc: 61.1%
* class: 35 (spiralled)	total: 36	correct: 25	acc: 69.4%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 18	acc: 50.0%
* class: 38 (stratified)	total: 36	correct: 32	acc: 88.9%
* class: 39 (striped)	total: 36	correct: 30	acc: 83.3%
* class: 40 (studded)	total: 36	correct: 28	acc: 77.8%
* class: 41 (swirly)	total: 36	correct: 24	acc: 66.7%
* class: 42 (veined)	total: 36	correct: 16	acc: 44.4%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 14	acc: 38.9%
* class: 45 (wrinkled)	total: 36	correct: 15	acc: 41.7%
* class: 46 (zigzagged)	total: 36	correct: 28	acc: 77.8%
* average: 61.7%
Elapsed: 0:16:19
Run this job and save the output to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_6-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.204 (1.064) data 0.000 (0.370) loss 3.4722 (3.6564) acc 21.8750 (18.1250) lr 1.0000e-05 eta 0:20:18
epoch [1/50] batch [10/23] time 0.209 (0.636) data 0.000 (0.185) loss 3.5494 (3.6489) acc 18.7500 (15.9375) lr 1.0000e-05 eta 0:12:05
epoch [1/50] batch [15/23] time 0.207 (0.493) data 0.000 (0.123) loss 3.4624 (3.6316) acc 25.0000 (15.0000) lr 1.0000e-05 eta 0:09:19
epoch [1/50] batch [20/23] time 0.206 (0.424) data 0.000 (0.093) loss 3.5143 (3.6140) acc 18.7500 (14.3750) lr 1.0000e-05 eta 0:07:59
epoch [2/50] batch [5/23] time 0.220 (0.488) data 0.000 (0.265) loss 3.3581 (3.4966) acc 25.0000 (14.3750) lr 2.0000e-03 eta 0:09:07
epoch [2/50] batch [10/23] time 0.207 (0.352) data 0.000 (0.133) loss 3.4197 (3.5043) acc 21.8750 (16.5625) lr 2.0000e-03 eta 0:06:33
epoch [2/50] batch [15/23] time 0.206 (0.308) data 0.000 (0.089) loss 3.0868 (3.4476) acc 28.1250 (18.5417) lr 2.0000e-03 eta 0:05:42
epoch [2/50] batch [20/23] time 0.207 (0.283) data 0.000 (0.066) loss 3.2156 (3.3680) acc 15.6250 (19.5312) lr 2.0000e-03 eta 0:05:12
epoch [3/50] batch [5/23] time 0.213 (0.519) data 0.000 (0.300) loss 3.4665 (3.2483) acc 18.7500 (21.2500) lr 1.9980e-03 eta 0:09:30
epoch [3/50] batch [10/23] time 0.206 (0.372) data 0.000 (0.150) loss 3.3887 (3.3382) acc 25.0000 (21.5625) lr 1.9980e-03 eta 0:06:47
epoch [3/50] batch [15/23] time 0.206 (0.317) data 0.000 (0.100) loss 3.2502 (3.2414) acc 31.2500 (24.1667) lr 1.9980e-03 eta 0:05:44
epoch [3/50] batch [20/23] time 0.205 (0.289) data 0.000 (0.075) loss 2.9360 (3.1798) acc 25.0000 (25.3125) lr 1.9980e-03 eta 0:05:13
epoch [4/50] batch [5/23] time 0.178 (0.480) data 0.000 (0.262) loss 2.9205 (2.9687) acc 28.1250 (30.0000) lr 1.9921e-03 eta 0:08:36
epoch [4/50] batch [10/23] time 0.149 (0.319) data 0.000 (0.131) loss 3.4974 (3.0516) acc 25.0000 (29.3750) lr 1.9921e-03 eta 0:05:41
epoch [4/50] batch [15/23] time 0.154 (0.264) data 0.000 (0.087) loss 3.4807 (3.0987) acc 25.0000 (28.9583) lr 1.9921e-03 eta 0:04:41
epoch [4/50] batch [20/23] time 0.314 (0.269) data 0.000 (0.066) loss 3.0036 (3.0775) acc 28.1250 (29.2188) lr 1.9921e-03 eta 0:04:45
epoch [5/50] batch [5/23] time 0.305 (0.559) data 0.000 (0.221) loss 2.9567 (2.8906) acc 31.2500 (31.8750) lr 1.9823e-03 eta 0:09:48
epoch [5/50] batch [10/23] time 0.167 (0.376) data 0.000 (0.111) loss 3.1309 (2.9959) acc 34.3750 (30.9375) lr 1.9823e-03 eta 0:06:34
epoch [5/50] batch [15/23] time 0.153 (0.302) data 0.000 (0.074) loss 2.9710 (3.0676) acc 40.6250 (30.2083) lr 1.9823e-03 eta 0:05:14
epoch [5/50] batch [20/23] time 0.266 (0.281) data 0.000 (0.056) loss 3.3893 (3.0746) acc 25.0000 (29.8438) lr 1.9823e-03 eta 0:04:51
epoch [6/50] batch [5/23] time 0.155 (0.476) data 0.000 (0.244) loss 3.1783 (3.0625) acc 28.1250 (33.7500) lr 1.9686e-03 eta 0:08:10
epoch [6/50] batch [10/23] time 0.164 (0.323) data 0.000 (0.122) loss 3.0915 (2.9784) acc 31.2500 (34.6875) lr 1.9686e-03 eta 0:05:31
epoch [6/50] batch [15/23] time 0.185 (0.268) data 0.000 (0.082) loss 2.6944 (2.9459) acc 46.8750 (34.7917) lr 1.9686e-03 eta 0:04:33
epoch [6/50] batch [20/23] time 0.198 (0.253) data 0.000 (0.061) loss 3.4827 (2.9443) acc 28.1250 (34.3750) lr 1.9686e-03 eta 0:04:16
epoch [7/50] batch [5/23] time 0.223 (0.498) data 0.000 (0.258) loss 3.1394 (2.8429) acc 28.1250 (35.6250) lr 1.9511e-03 eta 0:08:21
epoch [7/50] batch [10/23] time 0.207 (0.358) data 0.000 (0.129) loss 3.4982 (2.9724) acc 31.2500 (32.1875) lr 1.9511e-03 eta 0:05:58
epoch [7/50] batch [15/23] time 0.209 (0.308) data 0.000 (0.086) loss 2.6276 (2.9042) acc 25.0000 (32.2917) lr 1.9511e-03 eta 0:05:06
epoch [7/50] batch [20/23] time 0.208 (0.286) data 0.000 (0.065) loss 2.8486 (2.9210) acc 40.6250 (32.1875) lr 1.9511e-03 eta 0:04:43
epoch [8/50] batch [5/23] time 0.255 (0.520) data 0.000 (0.289) loss 3.2274 (2.9677) acc 28.1250 (35.0000) lr 1.9298e-03 eta 0:08:32
epoch [8/50] batch [10/23] time 0.213 (0.367) data 0.000 (0.146) loss 3.0986 (2.9187) acc 25.0000 (35.3125) lr 1.9298e-03 eta 0:05:59
epoch [8/50] batch [15/23] time 0.208 (0.319) data 0.000 (0.098) loss 3.4521 (2.8640) acc 28.1250 (35.8333) lr 1.9298e-03 eta 0:05:10
epoch [8/50] batch [20/23] time 0.211 (0.292) data 0.000 (0.073) loss 2.4923 (2.8886) acc 43.7500 (35.9375) lr 1.9298e-03 eta 0:04:42
epoch [9/50] batch [5/23] time 0.210 (0.466) data 0.000 (0.238) loss 2.9774 (2.7517) acc 31.2500 (37.5000) lr 1.9048e-03 eta 0:07:28
epoch [9/50] batch [10/23] time 0.208 (0.348) data 0.000 (0.119) loss 2.2033 (2.7627) acc 50.0000 (38.7500) lr 1.9048e-03 eta 0:05:32
epoch [9/50] batch [15/23] time 0.208 (0.302) data 0.000 (0.079) loss 3.1835 (2.8235) acc 31.2500 (36.4583) lr 1.9048e-03 eta 0:04:46
epoch [9/50] batch [20/23] time 0.208 (0.279) data 0.000 (0.060) loss 3.1464 (2.8776) acc 25.0000 (35.3125) lr 1.9048e-03 eta 0:04:23
epoch [10/50] batch [5/23] time 0.222 (0.466) data 0.000 (0.227) loss 2.5474 (2.5767) acc 37.5000 (44.3750) lr 1.8763e-03 eta 0:07:17
epoch [10/50] batch [10/23] time 0.208 (0.346) data 0.000 (0.114) loss 2.5701 (2.7335) acc 37.5000 (39.0625) lr 1.8763e-03 eta 0:05:22
epoch [10/50] batch [15/23] time 0.209 (0.300) data 0.000 (0.076) loss 2.9792 (2.8613) acc 25.0000 (36.4583) lr 1.8763e-03 eta 0:04:38
epoch [10/50] batch [20/23] time 0.207 (0.277) data 0.000 (0.057) loss 2.3025 (2.8002) acc 53.1250 (37.8125) lr 1.8763e-03 eta 0:04:15
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.890  alpha2: 0.273 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.43 <<<
epoch [11/50] batch [5/23] time 0.936 (1.148) data 0.001 (0.309) loss 2.0176 (1.7905) acc 38.7500 (55.3830) lr 1.8443e-03 eta 0:17:30
epoch [11/50] batch [10/23] time 1.200 (1.046) data 0.000 (0.154) loss 1.6942 (1.8754) acc 62.0000 (54.0042) lr 1.8443e-03 eta 0:15:51
epoch [11/50] batch [15/23] time 0.194 (0.834) data 0.000 (0.103) loss 1.4849 (1.8526) acc 68.0000 (54.1331) lr 1.8443e-03 eta 0:12:34
epoch [11/50] batch [20/23] time 0.187 (0.672) data 0.000 (0.077) loss 1.6452 (1.8142) acc 53.1915 (55.9346) lr 1.8443e-03 eta 0:10:04
>>> alpha1: 0.671  alpha2: 0.277 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.40 <<<
epoch [12/50] batch [5/23] time 0.143 (0.541) data 0.000 (0.256) loss 1.3105 (1.3135) acc 66.4894 (68.0201) lr 1.8090e-03 eta 0:08:02
epoch [12/50] batch [10/23] time 0.176 (0.427) data 0.000 (0.128) loss 1.1399 (1.3002) acc 67.1875 (66.0658) lr 1.8090e-03 eta 0:06:18
epoch [12/50] batch [15/23] time 0.174 (0.343) data 0.000 (0.085) loss 1.5321 (1.3409) acc 59.2391 (64.8159) lr 1.8090e-03 eta 0:05:02
epoch [12/50] batch [20/23] time 0.164 (0.338) data 0.000 (0.064) loss 1.4248 (1.3423) acc 57.7381 (62.9011) lr 1.8090e-03 eta 0:04:56
>>> alpha1: 0.604  alpha2: 0.240 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.38 <<<
epoch [13/50] batch [5/23] time 0.181 (0.459) data 0.000 (0.277) loss 1.2767 (1.2533) acc 56.3830 (61.3787) lr 1.7705e-03 eta 0:06:38
epoch [13/50] batch [10/23] time 0.175 (0.321) data 0.000 (0.139) loss 1.2269 (1.2087) acc 66.8605 (65.1646) lr 1.7705e-03 eta 0:04:37
epoch [13/50] batch [15/23] time 0.194 (0.392) data 0.000 (0.093) loss 1.0986 (1.2064) acc 69.1176 (65.1674) lr 1.7705e-03 eta 0:05:36
epoch [13/50] batch [20/23] time 0.175 (0.341) data 0.000 (0.069) loss 0.9350 (1.2184) acc 76.7045 (65.2971) lr 1.7705e-03 eta 0:04:51
>>> alpha1: 0.571  alpha2: 0.218 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.36 <<<
epoch [14/50] batch [5/23] time 0.195 (0.482) data 0.000 (0.282) loss 1.1469 (1.1114) acc 64.3617 (66.4281) lr 1.7290e-03 eta 0:06:47
epoch [14/50] batch [10/23] time 0.174 (0.329) data 0.000 (0.141) loss 0.7922 (1.1058) acc 78.9773 (68.0813) lr 1.7290e-03 eta 0:04:36
epoch [14/50] batch [15/23] time 0.182 (0.279) data 0.000 (0.094) loss 0.9580 (1.1992) acc 70.2128 (66.5069) lr 1.7290e-03 eta 0:03:53
epoch [14/50] batch [20/23] time 0.232 (0.255) data 0.000 (0.071) loss 1.1114 (1.1932) acc 67.9348 (66.2891) lr 1.7290e-03 eta 0:03:31
>>> alpha1: 0.536  alpha2: 0.195 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.46 <<<
epoch [15/50] batch [5/23] time 0.285 (1.235) data 0.000 (0.332) loss 1.2833 (1.1962) acc 69.1489 (66.7971) lr 1.6845e-03 eta 0:16:56
epoch [15/50] batch [10/23] time 0.150 (0.858) data 0.000 (0.166) loss 0.8982 (1.0816) acc 65.8654 (68.0368) lr 1.6845e-03 eta 0:11:42
epoch [15/50] batch [15/23] time 1.403 (0.707) data 0.000 (0.111) loss 1.0555 (1.1038) acc 68.6441 (67.3576) lr 1.6845e-03 eta 0:09:34
epoch [15/50] batch [20/23] time 0.248 (0.667) data 0.000 (0.083) loss 1.1890 (1.1123) acc 66.8367 (67.0292) lr 1.6845e-03 eta 0:08:59
>>> alpha1: 0.421  alpha2: 0.077 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.38 <<<
epoch [16/50] batch [5/23] time 0.188 (0.688) data 0.001 (0.296) loss 0.9295 (0.9070) acc 76.0000 (73.1771) lr 1.6374e-03 eta 0:09:10
epoch [16/50] batch [10/23] time 0.185 (0.447) data 0.000 (0.148) loss 0.8471 (0.9521) acc 73.9583 (71.6695) lr 1.6374e-03 eta 0:05:55
epoch [16/50] batch [15/23] time 0.183 (0.360) data 0.000 (0.099) loss 0.7777 (0.9263) acc 76.5957 (72.3810) lr 1.6374e-03 eta 0:04:44
epoch [16/50] batch [20/23] time 0.193 (0.317) data 0.000 (0.074) loss 0.7248 (0.9318) acc 73.0392 (71.4392) lr 1.6374e-03 eta 0:04:08
>>> alpha1: 0.370  alpha2: 0.037 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.35 <<<
epoch [17/50] batch [5/23] time 0.179 (0.448) data 0.001 (0.262) loss 1.0191 (0.8353) acc 71.5116 (76.9986) lr 1.5878e-03 eta 0:05:47
epoch [17/50] batch [10/23] time 0.226 (0.322) data 0.000 (0.131) loss 0.6736 (0.8321) acc 75.5208 (75.2483) lr 1.5878e-03 eta 0:04:08
epoch [17/50] batch [15/23] time 0.170 (0.276) data 0.000 (0.087) loss 1.0311 (0.8882) acc 69.6429 (73.5151) lr 1.5878e-03 eta 0:03:31
epoch [17/50] batch [20/23] time 0.179 (0.251) data 0.000 (0.066) loss 1.0203 (0.9039) acc 72.7778 (72.4353) lr 1.5878e-03 eta 0:03:11
>>> alpha1: 0.340  alpha2: 0.010 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.35 <<<
epoch [18/50] batch [5/23] time 0.176 (0.473) data 0.001 (0.287) loss 1.0485 (1.2402) acc 66.1111 (67.9737) lr 1.5358e-03 eta 0:05:56
epoch [18/50] batch [10/23] time 0.211 (0.334) data 0.000 (0.144) loss 0.6398 (1.0073) acc 85.3261 (72.4239) lr 1.5358e-03 eta 0:04:10
epoch [18/50] batch [15/23] time 0.176 (0.283) data 0.000 (0.096) loss 0.8502 (0.9240) acc 67.7778 (74.3824) lr 1.5358e-03 eta 0:03:30
epoch [18/50] batch [20/23] time 0.173 (0.256) data 0.000 (0.072) loss 1.0938 (0.9287) acc 70.9302 (73.7508) lr 1.5358e-03 eta 0:03:09
>>> alpha1: 0.309  alpha2: -0.007 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.36 <<<
epoch [19/50] batch [5/23] time 0.198 (0.480) data 0.000 (0.285) loss 0.6731 (0.7002) acc 82.6087 (81.2001) lr 1.4818e-03 eta 0:05:50
epoch [19/50] batch [10/23] time 0.198 (0.333) data 0.000 (0.143) loss 0.7314 (0.6862) acc 80.1282 (80.5983) lr 1.4818e-03 eta 0:04:02
epoch [19/50] batch [15/23] time 0.183 (0.282) data 0.000 (0.095) loss 0.7950 (0.7231) acc 76.0638 (79.6647) lr 1.4818e-03 eta 0:03:23
epoch [19/50] batch [20/23] time 0.191 (0.257) data 0.000 (0.071) loss 0.8173 (0.7530) acc 75.5102 (78.5705) lr 1.4818e-03 eta 0:03:04
>>> alpha1: 0.296  alpha2: -0.006 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.41 <<<
epoch [20/50] batch [5/23] time 0.181 (0.488) data 0.000 (0.293) loss 0.5691 (0.6825) acc 80.8511 (79.3515) lr 1.4258e-03 eta 0:05:45
epoch [20/50] batch [10/23] time 0.185 (0.343) data 0.000 (0.147) loss 0.7731 (0.6637) acc 74.4681 (79.3290) lr 1.4258e-03 eta 0:04:01
epoch [20/50] batch [15/23] time 0.197 (0.300) data 0.000 (0.098) loss 0.7682 (0.6882) acc 73.0769 (78.0730) lr 1.4258e-03 eta 0:03:29
epoch [20/50] batch [20/23] time 0.187 (0.272) data 0.000 (0.073) loss 0.5677 (0.6839) acc 85.2041 (78.9148) lr 1.4258e-03 eta 0:03:08
>>> alpha1: 0.269  alpha2: -0.017 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.34 <<<
epoch [21/50] batch [5/23] time 0.202 (0.509) data 0.000 (0.312) loss 0.7637 (0.6107) acc 78.1915 (82.1237) lr 1.3681e-03 eta 0:05:48
epoch [21/50] batch [10/23] time 0.191 (0.345) data 0.000 (0.156) loss 0.7202 (0.6349) acc 79.0816 (81.0666) lr 1.3681e-03 eta 0:03:54
epoch [21/50] batch [15/23] time 0.191 (0.290) data 0.000 (0.104) loss 0.7081 (0.6449) acc 76.5306 (81.1423) lr 1.3681e-03 eta 0:03:15
epoch [21/50] batch [20/23] time 0.179 (0.266) data 0.000 (0.078) loss 0.7176 (0.6827) acc 81.6667 (79.4744) lr 1.3681e-03 eta 0:02:58
>>> alpha1: 0.253  alpha2: -0.043 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.36 <<<
epoch [22/50] batch [5/23] time 0.151 (0.468) data 0.000 (0.269) loss 0.6643 (0.6260) acc 78.3333 (80.7890) lr 1.3090e-03 eta 0:05:09
epoch [22/50] batch [10/23] time 0.126 (0.303) data 0.000 (0.135) loss 0.8266 (0.6786) acc 72.0930 (78.6098) lr 1.3090e-03 eta 0:03:18
epoch [22/50] batch [15/23] time 0.235 (0.253) data 0.000 (0.090) loss 0.7857 (0.6972) acc 76.2195 (77.8484) lr 1.3090e-03 eta 0:02:45
epoch [22/50] batch [20/23] time 0.228 (0.245) data 0.000 (0.067) loss 0.5876 (0.6769) acc 85.5556 (78.8620) lr 1.3090e-03 eta 0:02:38
>>> alpha1: 0.233  alpha2: -0.060 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.38 <<<
epoch [23/50] batch [5/23] time 0.210 (0.477) data 0.001 (0.274) loss 0.5838 (0.6387) acc 80.4348 (79.2161) lr 1.2487e-03 eta 0:05:04
epoch [23/50] batch [10/23] time 0.191 (0.333) data 0.000 (0.137) loss 0.6332 (0.6326) acc 78.0612 (77.9716) lr 1.2487e-03 eta 0:03:30
epoch [23/50] batch [15/23] time 0.164 (0.279) data 0.000 (0.091) loss 0.7357 (0.6869) acc 79.8781 (76.3027) lr 1.2487e-03 eta 0:02:55
epoch [23/50] batch [20/23] time 0.187 (0.255) data 0.000 (0.069) loss 0.7943 (0.6606) acc 82.8125 (78.3107) lr 1.2487e-03 eta 0:02:38
>>> alpha1: 0.220  alpha2: -0.067 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.42 <<<
epoch [24/50] batch [5/23] time 0.195 (0.475) data 0.000 (0.281) loss 0.6701 (0.6358) acc 77.9412 (81.3642) lr 1.1874e-03 eta 0:04:52
epoch [24/50] batch [10/23] time 0.239 (0.339) data 0.001 (0.141) loss 0.5105 (0.6385) acc 84.3137 (80.1629) lr 1.1874e-03 eta 0:03:27
epoch [24/50] batch [15/23] time 0.178 (0.290) data 0.000 (0.094) loss 0.6314 (0.6369) acc 80.0000 (80.1892) lr 1.1874e-03 eta 0:02:55
epoch [24/50] batch [20/23] time 0.196 (0.266) data 0.000 (0.071) loss 0.4837 (0.6295) acc 83.6538 (80.2977) lr 1.1874e-03 eta 0:02:39
>>> alpha1: 0.211  alpha2: -0.074 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.36 <<<
epoch [25/50] batch [5/23] time 0.168 (0.471) data 0.000 (0.287) loss 0.8273 (0.6820) acc 78.8462 (80.5563) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [10/23] time 0.175 (0.326) data 0.000 (0.144) loss 0.5077 (0.6573) acc 88.5870 (81.1997) lr 1.1253e-03 eta 0:03:11
epoch [25/50] batch [15/23] time 0.189 (0.278) data 0.000 (0.096) loss 0.5793 (0.6366) acc 78.5714 (81.3378) lr 1.1253e-03 eta 0:02:41
epoch [25/50] batch [20/23] time 0.171 (0.255) data 0.000 (0.072) loss 0.6146 (0.6305) acc 80.5556 (81.2412) lr 1.1253e-03 eta 0:02:27
>>> alpha1: 0.211  alpha2: -0.068 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.36 <<<
epoch [26/50] batch [5/23] time 0.219 (0.460) data 0.000 (0.287) loss 0.6418 (0.6228) acc 80.1282 (82.0818) lr 1.0628e-03 eta 0:04:22
epoch [26/50] batch [10/23] time 0.169 (0.317) data 0.000 (0.144) loss 0.5640 (0.6017) acc 90.0000 (81.8679) lr 1.0628e-03 eta 0:02:59
epoch [26/50] batch [15/23] time 0.176 (0.270) data 0.000 (0.096) loss 0.6191 (0.5555) acc 78.1915 (83.4343) lr 1.0628e-03 eta 0:02:30
epoch [26/50] batch [20/23] time 0.167 (0.246) data 0.000 (0.072) loss 0.4714 (0.5567) acc 86.3636 (83.5816) lr 1.0628e-03 eta 0:02:16
>>> alpha1: 0.203  alpha2: -0.060 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.36 <<<
epoch [27/50] batch [5/23] time 0.227 (0.470) data 0.000 (0.281) loss 0.5649 (0.6002) acc 84.2391 (84.3827) lr 1.0000e-03 eta 0:04:17
epoch [27/50] batch [10/23] time 0.163 (0.327) data 0.001 (0.141) loss 0.5901 (0.5457) acc 80.3571 (84.6388) lr 1.0000e-03 eta 0:02:57
epoch [27/50] batch [15/23] time 0.178 (0.277) data 0.000 (0.094) loss 0.5982 (0.5427) acc 78.1915 (84.0890) lr 1.0000e-03 eta 0:02:28
epoch [27/50] batch [20/23] time 0.177 (0.253) data 0.000 (0.070) loss 0.6158 (0.5521) acc 84.2391 (83.7151) lr 1.0000e-03 eta 0:02:14
>>> alpha1: 0.194  alpha2: -0.054 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [28/50] batch [5/23] time 0.197 (0.475) data 0.000 (0.276) loss 2.6986 (0.9784) acc 53.3654 (73.7806) lr 9.3721e-04 eta 0:04:08
epoch [28/50] batch [10/23] time 0.193 (0.335) data 0.000 (0.138) loss 0.3233 (0.8602) acc 87.7451 (79.1931) lr 9.3721e-04 eta 0:02:53
epoch [28/50] batch [15/23] time 0.247 (0.292) data 0.000 (0.092) loss 0.5662 (0.7659) acc 90.3846 (80.8972) lr 9.3721e-04 eta 0:02:29
epoch [28/50] batch [20/23] time 0.195 (0.267) data 0.000 (0.069) loss 0.5760 (0.7194) acc 82.8431 (81.1511) lr 9.3721e-04 eta 0:02:15
>>> alpha1: 0.188  alpha2: -0.043 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [29/50] batch [5/23] time 0.233 (0.489) data 0.000 (0.271) loss 0.5887 (0.5328) acc 84.5000 (87.8800) lr 8.7467e-04 eta 0:04:05
epoch [29/50] batch [10/23] time 0.181 (0.339) data 0.000 (0.136) loss 0.7295 (0.5612) acc 73.4375 (84.0776) lr 8.7467e-04 eta 0:02:48
epoch [29/50] batch [15/23] time 0.195 (0.292) data 0.000 (0.091) loss 0.5269 (0.5331) acc 77.9412 (84.3995) lr 8.7467e-04 eta 0:02:23
epoch [29/50] batch [20/23] time 0.192 (0.267) data 0.000 (0.068) loss 0.5459 (0.5289) acc 75.0000 (83.8162) lr 8.7467e-04 eta 0:02:09
>>> alpha1: 0.183  alpha2: -0.039 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.44 <<<
epoch [30/50] batch [5/23] time 0.186 (0.479) data 0.000 (0.280) loss 0.5259 (0.4482) acc 82.1429 (84.8728) lr 8.1262e-04 eta 0:03:49
epoch [30/50] batch [10/23] time 0.177 (0.343) data 0.000 (0.140) loss 0.4613 (0.4903) acc 81.9149 (83.4012) lr 8.1262e-04 eta 0:02:42
epoch [30/50] batch [15/23] time 0.198 (0.293) data 0.000 (0.094) loss 0.6437 (0.4835) acc 74.0741 (83.9136) lr 8.1262e-04 eta 0:02:17
epoch [30/50] batch [20/23] time 0.210 (0.269) data 0.000 (0.070) loss 0.4585 (0.4961) acc 88.1818 (83.9990) lr 8.1262e-04 eta 0:02:04
>>> alpha1: 0.175  alpha2: -0.042 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [31/50] batch [5/23] time 0.197 (0.491) data 0.001 (0.283) loss 0.5812 (0.5128) acc 83.3333 (83.6162) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [10/23] time 0.190 (0.344) data 0.000 (0.142) loss 0.3108 (0.4979) acc 87.7551 (83.5431) lr 7.5131e-04 eta 0:02:34
epoch [31/50] batch [15/23] time 0.193 (0.295) data 0.000 (0.094) loss 0.5013 (0.5050) acc 88.7255 (83.6382) lr 7.5131e-04 eta 0:02:11
epoch [31/50] batch [20/23] time 0.181 (0.272) data 0.001 (0.071) loss 0.4067 (0.5079) acc 90.2174 (83.8636) lr 7.5131e-04 eta 0:01:59
>>> alpha1: 0.169  alpha2: -0.044 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.36 <<<
epoch [32/50] batch [5/23] time 0.169 (0.512) data 0.000 (0.306) loss 0.4428 (0.4641) acc 83.7209 (86.7808) lr 6.9098e-04 eta 0:03:41
epoch [32/50] batch [10/23] time 0.198 (0.353) data 0.000 (0.153) loss 0.6886 (0.4882) acc 81.7308 (85.4500) lr 6.9098e-04 eta 0:02:30
epoch [32/50] batch [15/23] time 0.174 (0.297) data 0.000 (0.102) loss 0.4450 (0.4836) acc 82.9545 (85.3223) lr 6.9098e-04 eta 0:02:05
epoch [32/50] batch [20/23] time 0.192 (0.270) data 0.000 (0.077) loss 0.3913 (0.4825) acc 89.7959 (85.5614) lr 6.9098e-04 eta 0:01:52
>>> alpha1: 0.165  alpha2: -0.046 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.36 <<<
epoch [33/50] batch [5/23] time 0.286 (0.604) data 0.000 (0.321) loss 0.3776 (0.4086) acc 85.7143 (86.5205) lr 6.3188e-04 eta 0:04:06
epoch [33/50] batch [10/23] time 0.271 (0.439) data 0.000 (0.160) loss 0.5922 (0.4836) acc 83.7209 (85.1593) lr 6.3188e-04 eta 0:02:57
epoch [33/50] batch [15/23] time 0.144 (0.369) data 0.000 (0.107) loss 0.4723 (0.4845) acc 93.6170 (85.9940) lr 6.3188e-04 eta 0:02:27
epoch [33/50] batch [20/23] time 0.138 (0.311) data 0.000 (0.080) loss 0.6378 (0.4837) acc 83.5106 (86.2643) lr 6.3188e-04 eta 0:02:02
>>> alpha1: 0.164  alpha2: -0.051 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [34/50] batch [5/23] time 0.193 (0.461) data 0.000 (0.250) loss 0.3145 (0.4374) acc 89.0000 (88.7446) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [10/23] time 0.207 (0.329) data 0.001 (0.125) loss 0.6558 (0.4742) acc 83.0189 (86.9939) lr 5.7422e-04 eta 0:02:05
epoch [34/50] batch [15/23] time 0.205 (0.283) data 0.000 (0.084) loss 0.3737 (0.4619) acc 87.0192 (87.0281) lr 5.7422e-04 eta 0:01:46
epoch [34/50] batch [20/23] time 0.186 (0.263) data 0.000 (0.063) loss 0.4274 (0.4622) acc 86.7347 (86.5559) lr 5.7422e-04 eta 0:01:37
>>> alpha1: 0.161  alpha2: -0.059 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [35/50] batch [5/23] time 0.189 (0.499) data 0.000 (0.287) loss 0.4924 (0.4791) acc 89.5833 (88.2029) lr 5.1825e-04 eta 0:03:01
epoch [35/50] batch [10/23] time 0.192 (0.347) data 0.000 (0.144) loss 0.5514 (0.4584) acc 83.0000 (88.0949) lr 5.1825e-04 eta 0:02:04
epoch [35/50] batch [15/23] time 0.194 (0.296) data 0.000 (0.096) loss 0.3920 (0.4579) acc 86.7347 (87.4827) lr 5.1825e-04 eta 0:01:44
epoch [35/50] batch [20/23] time 0.204 (0.271) data 0.000 (0.072) loss 0.5087 (0.4688) acc 83.1731 (86.8969) lr 5.1825e-04 eta 0:01:34
>>> alpha1: 0.156  alpha2: -0.056 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [36/50] batch [5/23] time 0.192 (0.479) data 0.000 (0.279) loss 0.4281 (0.4267) acc 89.5000 (88.3813) lr 4.6417e-04 eta 0:02:42
epoch [36/50] batch [10/23] time 0.194 (0.349) data 0.000 (0.140) loss 0.4502 (0.4190) acc 92.6471 (88.8790) lr 4.6417e-04 eta 0:01:56
epoch [36/50] batch [15/23] time 0.194 (0.296) data 0.000 (0.093) loss 0.5375 (0.5788) acc 83.3333 (86.4031) lr 4.6417e-04 eta 0:01:37
epoch [36/50] batch [20/23] time 0.172 (0.268) data 0.000 (0.070) loss 0.5143 (0.5649) acc 81.2500 (85.9371) lr 4.6417e-04 eta 0:01:27
>>> alpha1: 0.152  alpha2: -0.058 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [37/50] batch [5/23] time 0.284 (0.519) data 0.001 (0.331) loss 0.4066 (0.4437) acc 85.2941 (86.0152) lr 4.1221e-04 eta 0:02:44
epoch [37/50] batch [10/23] time 0.174 (0.355) data 0.000 (0.166) loss 0.3784 (0.4183) acc 90.3061 (87.9361) lr 4.1221e-04 eta 0:01:50
epoch [37/50] batch [15/23] time 0.191 (0.298) data 0.000 (0.110) loss 0.3100 (0.4076) acc 94.1176 (88.3509) lr 4.1221e-04 eta 0:01:31
epoch [37/50] batch [20/23] time 0.188 (0.273) data 0.000 (0.083) loss 0.5337 (0.4338) acc 84.1837 (87.3146) lr 4.1221e-04 eta 0:01:22
>>> alpha1: 0.149  alpha2: -0.058 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [38/50] batch [5/23] time 0.221 (0.486) data 0.012 (0.285) loss 0.3256 (0.4661) acc 89.0909 (87.0184) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [10/23] time 0.200 (0.343) data 0.000 (0.143) loss 0.2823 (0.4461) acc 92.5926 (87.0567) lr 3.6258e-04 eta 0:01:39
epoch [38/50] batch [15/23] time 0.182 (0.293) data 0.000 (0.095) loss 0.4889 (0.4362) acc 90.6250 (88.3600) lr 3.6258e-04 eta 0:01:23
epoch [38/50] batch [20/23] time 0.181 (0.271) data 0.000 (0.072) loss 0.5919 (0.4342) acc 80.7292 (87.5574) lr 3.6258e-04 eta 0:01:15
>>> alpha1: 0.144  alpha2: -0.056 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.36 <<<
epoch [39/50] batch [5/23] time 0.192 (0.455) data 0.000 (0.263) loss 0.3807 (0.3820) acc 90.9574 (92.4786) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [10/23] time 0.176 (0.317) data 0.001 (0.132) loss 0.4176 (0.3933) acc 91.6667 (91.0875) lr 3.1545e-04 eta 0:01:24
epoch [39/50] batch [15/23] time 0.177 (0.272) data 0.000 (0.088) loss 0.5492 (0.4027) acc 88.3333 (89.9768) lr 3.1545e-04 eta 0:01:10
epoch [39/50] batch [20/23] time 0.193 (0.253) data 0.000 (0.066) loss 0.4730 (0.4181) acc 86.5000 (88.6040) lr 3.1545e-04 eta 0:01:04
>>> alpha1: 0.141  alpha2: -0.056 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.36 <<<
epoch [40/50] batch [5/23] time 0.186 (0.445) data 0.000 (0.241) loss 0.4590 (0.4625) acc 87.5000 (87.1884) lr 2.7103e-04 eta 0:01:50
epoch [40/50] batch [10/23] time 0.180 (0.317) data 0.000 (0.121) loss 0.3183 (0.4111) acc 93.0851 (88.8297) lr 2.7103e-04 eta 0:01:17
epoch [40/50] batch [15/23] time 0.190 (0.269) data 0.000 (0.081) loss 0.4050 (0.4186) acc 86.7347 (88.4140) lr 2.7103e-04 eta 0:01:03
epoch [40/50] batch [20/23] time 0.178 (0.247) data 0.000 (0.060) loss 0.4064 (0.4316) acc 86.6667 (87.6639) lr 2.7103e-04 eta 0:00:57
>>> alpha1: 0.142  alpha2: -0.053 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.38 <<<
epoch [41/50] batch [5/23] time 0.169 (0.459) data 0.000 (0.269) loss 0.3277 (0.4031) acc 92.6136 (88.8354) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [10/23] time 0.179 (0.400) data 0.000 (0.135) loss 0.5427 (0.4142) acc 83.5106 (87.7093) lr 2.2949e-04 eta 0:01:28
epoch [41/50] batch [15/23] time 0.179 (0.328) data 0.000 (0.090) loss 0.3891 (0.4925) acc 86.4130 (86.7850) lr 2.2949e-04 eta 0:01:10
epoch [41/50] batch [20/23] time 0.183 (0.290) data 0.000 (0.068) loss 0.5512 (0.4877) acc 86.1111 (86.6178) lr 2.2949e-04 eta 0:01:00
>>> alpha1: 0.141  alpha2: -0.048 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.38 <<<
epoch [42/50] batch [5/23] time 0.189 (0.447) data 0.000 (0.243) loss 0.4333 (0.4004) acc 89.8810 (90.4467) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [10/23] time 0.191 (0.318) data 0.001 (0.122) loss 0.4224 (0.3770) acc 92.7083 (91.0691) lr 1.9098e-04 eta 0:01:02
epoch [42/50] batch [15/23] time 0.178 (0.271) data 0.000 (0.081) loss 0.4779 (0.4021) acc 84.3750 (89.7144) lr 1.9098e-04 eta 0:00:52
epoch [42/50] batch [20/23] time 0.173 (0.247) data 0.000 (0.061) loss 0.4983 (0.4186) acc 80.5556 (88.8628) lr 1.9098e-04 eta 0:00:46
>>> alpha1: 0.141  alpha2: -0.048 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.37 <<<
epoch [43/50] batch [5/23] time 0.188 (0.454) data 0.000 (0.263) loss 0.4050 (0.4111) acc 82.6087 (90.4580) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [10/23] time 0.198 (0.321) data 0.000 (0.132) loss 0.4072 (0.3723) acc 86.0577 (90.4980) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [15/23] time 0.177 (0.277) data 0.000 (0.088) loss 0.4446 (0.3900) acc 84.8958 (89.5239) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [20/23] time 0.177 (0.253) data 0.000 (0.066) loss 0.3628 (0.3903) acc 87.7660 (89.4935) lr 1.5567e-04 eta 0:00:41
>>> alpha1: 0.140  alpha2: -0.044 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.37 <<<
epoch [44/50] batch [5/23] time 0.299 (0.531) data 0.000 (0.320) loss 0.3880 (0.3993) acc 83.8235 (87.6623) lr 1.2369e-04 eta 0:01:22
epoch [44/50] batch [10/23] time 0.267 (0.400) data 0.000 (0.160) loss 0.3470 (0.4667) acc 89.1304 (88.1916) lr 1.2369e-04 eta 0:01:00
epoch [44/50] batch [15/23] time 0.299 (0.358) data 0.000 (0.107) loss 0.2543 (0.4238) acc 94.4444 (88.7787) lr 1.2369e-04 eta 0:00:52
epoch [44/50] batch [20/23] time 0.272 (0.337) data 0.000 (0.080) loss 0.4242 (0.4297) acc 90.1042 (89.0340) lr 1.2369e-04 eta 0:00:47
>>> alpha1: 0.140  alpha2: -0.040 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.36 <<<
epoch [45/50] batch [5/23] time 0.190 (0.473) data 0.000 (0.278) loss 0.5292 (0.4376) acc 88.2653 (86.8836) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [10/23] time 0.174 (0.329) data 0.000 (0.139) loss 0.4773 (0.4258) acc 86.9565 (87.5030) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [15/23] time 0.181 (0.283) data 0.000 (0.093) loss 0.3670 (0.4057) acc 88.2653 (88.5227) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [20/23] time 0.181 (0.257) data 0.000 (0.070) loss 0.4029 (0.4074) acc 92.7083 (88.9601) lr 9.5173e-05 eta 0:00:30
>>> alpha1: 0.138  alpha2: -0.044 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.35 <<<
epoch [46/50] batch [5/23] time 0.186 (0.490) data 0.000 (0.292) loss 0.2805 (0.3674) acc 97.4490 (89.0869) lr 7.0224e-05 eta 0:00:53
epoch [46/50] batch [10/23] time 0.175 (0.334) data 0.000 (0.146) loss 0.4787 (0.3755) acc 90.3409 (89.7350) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [15/23] time 0.173 (0.287) data 0.000 (0.097) loss 0.5657 (0.4067) acc 86.3636 (89.1556) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [20/23] time 0.180 (0.261) data 0.000 (0.073) loss 0.3953 (0.4035) acc 86.7021 (88.6678) lr 7.0224e-05 eta 0:00:24
>>> alpha1: 0.138  alpha2: -0.046 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.36 <<<
epoch [47/50] batch [5/23] time 0.255 (0.483) data 0.000 (0.279) loss 0.5116 (0.3914) acc 85.6383 (90.5494) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [10/23] time 0.171 (0.331) data 0.000 (0.140) loss 0.2754 (0.4082) acc 93.1818 (89.2448) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [15/23] time 0.195 (0.283) data 0.000 (0.093) loss 0.4848 (0.4121) acc 86.2745 (89.0058) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [20/23] time 0.191 (0.259) data 0.000 (0.070) loss 0.2743 (0.4020) acc 90.8163 (89.0216) lr 4.8943e-05 eta 0:00:18
>>> alpha1: 0.139  alpha2: -0.036 <<<
>>> noisy rate: 0.37 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [48/50] batch [5/23] time 0.273 (0.550) data 0.001 (0.289) loss 0.3480 (0.2998) acc 92.5532 (92.6463) lr 3.1417e-05 eta 0:00:35
epoch [48/50] batch [10/23] time 0.261 (0.399) data 0.000 (0.145) loss 0.4938 (0.3802) acc 83.6364 (89.9368) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [15/23] time 0.256 (0.350) data 0.000 (0.097) loss 0.5256 (0.4050) acc 81.5000 (88.9478) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [20/23] time 0.253 (0.327) data 0.000 (0.072) loss 0.3891 (0.3887) acc 89.7059 (89.4037) lr 3.1417e-05 eta 0:00:16
>>> alpha1: 0.139  alpha2: -0.037 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.37 <<<
epoch [49/50] batch [5/23] time 0.182 (0.470) data 0.000 (0.281) loss 0.3279 (0.3854) acc 86.9792 (88.9786) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [10/23] time 0.161 (0.324) data 0.000 (0.141) loss 0.4498 (0.3861) acc 85.3659 (88.6655) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.202 (0.281) data 0.000 (0.094) loss 0.5296 (0.4109) acc 83.1395 (88.1762) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.163 (0.258) data 0.001 (0.071) loss 0.6493 (0.4158) acc 79.7619 (88.3441) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.138  alpha2: -0.042 <<<
>>> noisy rate: 0.36 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.42 <<<
epoch [50/50] batch [5/23] time 0.187 (0.513) data 0.000 (0.308) loss 0.5249 (0.4017) acc 85.2041 (89.0061) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [10/23] time 0.174 (0.350) data 0.000 (0.154) loss 0.5428 (0.3943) acc 80.9783 (89.4336) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.188 (0.298) data 0.000 (0.103) loss 0.4675 (0.4035) acc 86.7647 (88.1965) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.256 (0.276) data 0.000 (0.077) loss 0.3696 (0.4024) acc 91.0714 (88.0174) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.26, 0.25, 0.25, 0.24, 0.23, 0.23, 0.24, 0.24, 0.24, 0.24, 0.23, 0.24, 0.25, 0.24, 0.24, 0.25, 0.24, 0.24, 0.25, 0.24, 0.24, 0.24, 0.24, 0.25, 0.25, 0.24, 0.24, 0.23, 0.23, 0.24, 0.25, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24]
* matched noise rate: [0.07, 0.05, 0.06, 0.06, 0.13, 0.1, 0.08, 0.07, 0.08, 0.1, 0.1, 0.09, 0.08, 0.1, 0.08, 0.1, 0.09, 0.11, 0.12, 0.12, 0.11, 0.1, 0.09, 0.11, 0.12, 0.12, 0.12, 0.11, 0.09, 0.1, 0.1, 0.09, 0.09, 0.11, 0.1, 0.11, 0.1, 0.12, 0.11, 0.13]
* unmatched noise rate: [0.43, 0.4, 0.38, 0.36, 0.46, 0.38, 0.35, 0.35, 0.36, 0.41, 0.34, 0.36, 0.38, 0.42, 0.36, 0.36, 0.36, 0.42, 0.43, 0.44, 0.42, 0.36, 0.36, 0.41, 0.42, 0.4, 0.42, 0.42, 0.36, 0.36, 0.38, 0.38, 0.37, 0.37, 0.36, 0.35, 0.36, 0.4, 0.37, 0.42]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:43,  2.72s/it] 12%|█▏        | 2/17 [00:02<00:17,  1.20s/it] 24%|██▎       | 4/17 [00:03<00:06,  1.97it/s] 35%|███▌      | 6/17 [00:03<00:03,  3.19it/s] 47%|████▋     | 8/17 [00:03<00:02,  4.44it/s] 59%|█████▉    | 10/17 [00:03<00:01,  5.62it/s] 71%|███████   | 12/17 [00:03<00:00,  6.68it/s] 82%|████████▏ | 14/17 [00:04<00:00,  7.59it/s] 94%|█████████▍| 16/17 [00:04<00:00,  8.34it/s]100%|██████████| 17/17 [00:05<00:00,  3.35it/s]
=> result
* total: 1,692
* correct: 992
* accuracy: 58.6%
* error: 41.4%
* macro_f1: 57.5%
=> per-class result
* class: 0 (banded)	total: 36	correct: 31	acc: 86.1%
* class: 1 (blotchy)	total: 36	correct: 4	acc: 11.1%
* class: 2 (braided)	total: 36	correct: 12	acc: 33.3%
* class: 3 (bubbly)	total: 36	correct: 30	acc: 83.3%
* class: 4 (bumpy)	total: 36	correct: 3	acc: 8.3%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 30	acc: 83.3%
* class: 7 (cracked)	total: 36	correct: 27	acc: 75.0%
* class: 8 (crosshatched)	total: 36	correct: 13	acc: 36.1%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 19	acc: 52.8%
* class: 11 (fibrous)	total: 36	correct: 25	acc: 69.4%
* class: 12 (flecked)	total: 36	correct: 10	acc: 27.8%
* class: 13 (freckled)	total: 36	correct: 29	acc: 80.6%
* class: 14 (frilly)	total: 36	correct: 26	acc: 72.2%
* class: 15 (gauzy)	total: 36	correct: 15	acc: 41.7%
* class: 16 (grid)	total: 36	correct: 17	acc: 47.2%
* class: 17 (grooved)	total: 36	correct: 16	acc: 44.4%
* class: 18 (honeycombed)	total: 36	correct: 25	acc: 69.4%
* class: 19 (interlaced)	total: 36	correct: 24	acc: 66.7%
* class: 20 (knitted)	total: 36	correct: 26	acc: 72.2%
* class: 21 (lacelike)	total: 36	correct: 36	acc: 100.0%
* class: 22 (lined)	total: 36	correct: 9	acc: 25.0%
* class: 23 (marbled)	total: 36	correct: 25	acc: 69.4%
* class: 24 (matted)	total: 36	correct: 22	acc: 61.1%
* class: 25 (meshed)	total: 36	correct: 16	acc: 44.4%
* class: 26 (paisley)	total: 36	correct: 33	acc: 91.7%
* class: 27 (perforated)	total: 36	correct: 20	acc: 55.6%
* class: 28 (pitted)	total: 36	correct: 9	acc: 25.0%
* class: 29 (pleated)	total: 36	correct: 15	acc: 41.7%
* class: 30 (polka-dotted)	total: 36	correct: 22	acc: 61.1%
* class: 31 (porous)	total: 36	correct: 9	acc: 25.0%
* class: 32 (potholed)	total: 36	correct: 33	acc: 91.7%
* class: 33 (scaly)	total: 36	correct: 20	acc: 55.6%
* class: 34 (smeared)	total: 36	correct: 15	acc: 41.7%
* class: 35 (spiralled)	total: 36	correct: 21	acc: 58.3%
* class: 36 (sprinkled)	total: 36	correct: 15	acc: 41.7%
* class: 37 (stained)	total: 36	correct: 10	acc: 27.8%
* class: 38 (stratified)	total: 36	correct: 22	acc: 61.1%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 30	acc: 83.3%
* class: 41 (swirly)	total: 36	correct: 22	acc: 61.1%
* class: 42 (veined)	total: 36	correct: 21	acc: 58.3%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 20	acc: 55.6%
* class: 45 (wrinkled)	total: 36	correct: 20	acc: 55.6%
* class: 46 (zigzagged)	total: 36	correct: 29	acc: 80.6%
* average: 58.6%
Elapsed: 0:16:23
Run this job and save the output to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_8-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.265 (1.267) data 0.000 (0.436) loss 3.7733 (3.6981) acc 6.2500 (13.1250) lr 1.0000e-05 eta 0:24:10
epoch [1/50] batch [10/23] time 0.150 (0.742) data 0.000 (0.218) loss 3.7326 (3.7305) acc 9.3750 (10.9375) lr 1.0000e-05 eta 0:14:06
epoch [1/50] batch [15/23] time 0.149 (0.544) data 0.000 (0.146) loss 3.6150 (3.7045) acc 18.7500 (11.6667) lr 1.0000e-05 eta 0:10:17
epoch [1/50] batch [20/23] time 0.148 (0.445) data 0.000 (0.109) loss 3.4956 (3.6842) acc 12.5000 (11.7188) lr 1.0000e-05 eta 0:08:23
epoch [2/50] batch [5/23] time 0.210 (0.508) data 0.000 (0.287) loss 3.4537 (3.6911) acc 15.6250 (10.6250) lr 2.0000e-03 eta 0:09:30
epoch [2/50] batch [10/23] time 0.202 (0.365) data 0.000 (0.145) loss 3.6169 (3.6621) acc 6.2500 (13.7500) lr 2.0000e-03 eta 0:06:47
epoch [2/50] batch [15/23] time 0.197 (0.310) data 0.000 (0.097) loss 3.5862 (3.6317) acc 12.5000 (14.3750) lr 2.0000e-03 eta 0:05:44
epoch [2/50] batch [20/23] time 0.201 (0.282) data 0.000 (0.073) loss 3.2333 (3.6018) acc 25.0000 (16.4062) lr 2.0000e-03 eta 0:05:12
epoch [3/50] batch [5/23] time 0.280 (0.479) data 0.000 (0.241) loss 3.4900 (3.3060) acc 21.8750 (24.3750) lr 1.9980e-03 eta 0:08:46
epoch [3/50] batch [10/23] time 0.199 (0.343) data 0.000 (0.121) loss 3.4169 (3.4164) acc 21.8750 (23.7500) lr 1.9980e-03 eta 0:06:15
epoch [3/50] batch [15/23] time 0.200 (0.296) data 0.000 (0.081) loss 3.1457 (3.3936) acc 37.5000 (25.8333) lr 1.9980e-03 eta 0:05:22
epoch [3/50] batch [20/23] time 0.216 (0.273) data 0.000 (0.060) loss 3.7210 (3.4577) acc 15.6250 (23.1250) lr 1.9980e-03 eta 0:04:55
epoch [4/50] batch [5/23] time 0.195 (0.516) data 0.000 (0.288) loss 3.3764 (3.4582) acc 28.1250 (21.8750) lr 1.9921e-03 eta 0:09:15
epoch [4/50] batch [10/23] time 0.199 (0.362) data 0.000 (0.144) loss 3.6032 (3.4614) acc 18.7500 (20.3125) lr 1.9921e-03 eta 0:06:27
epoch [4/50] batch [15/23] time 0.202 (0.308) data 0.000 (0.096) loss 3.2477 (3.3919) acc 28.1250 (23.9583) lr 1.9921e-03 eta 0:05:28
epoch [4/50] batch [20/23] time 0.193 (0.284) data 0.000 (0.072) loss 3.6133 (3.4049) acc 21.8750 (24.0625) lr 1.9921e-03 eta 0:05:01
epoch [5/50] batch [5/23] time 0.202 (0.500) data 0.000 (0.254) loss 3.6291 (3.3161) acc 18.7500 (25.6250) lr 1.9823e-03 eta 0:08:46
epoch [5/50] batch [10/23] time 0.201 (0.352) data 0.000 (0.127) loss 3.3406 (3.3770) acc 18.7500 (23.7500) lr 1.9823e-03 eta 0:06:09
epoch [5/50] batch [15/23] time 0.198 (0.302) data 0.000 (0.085) loss 3.2399 (3.3353) acc 28.1250 (24.5833) lr 1.9823e-03 eta 0:05:14
epoch [5/50] batch [20/23] time 0.202 (0.280) data 0.000 (0.064) loss 3.4195 (3.3318) acc 15.6250 (25.0000) lr 1.9823e-03 eta 0:04:50
epoch [6/50] batch [5/23] time 0.206 (0.496) data 0.000 (0.277) loss 3.2344 (3.3800) acc 34.3750 (23.7500) lr 1.9686e-03 eta 0:08:30
epoch [6/50] batch [10/23] time 0.199 (0.353) data 0.000 (0.139) loss 3.5477 (3.2806) acc 25.0000 (25.0000) lr 1.9686e-03 eta 0:06:01
epoch [6/50] batch [15/23] time 0.264 (0.308) data 0.000 (0.093) loss 3.3280 (3.3149) acc 28.1250 (25.4167) lr 1.9686e-03 eta 0:05:13
epoch [6/50] batch [20/23] time 0.202 (0.282) data 0.000 (0.070) loss 3.3190 (3.3339) acc 28.1250 (25.1562) lr 1.9686e-03 eta 0:04:45
epoch [7/50] batch [5/23] time 0.203 (0.508) data 0.000 (0.291) loss 3.4442 (3.2706) acc 18.7500 (24.3750) lr 1.9511e-03 eta 0:08:31
epoch [7/50] batch [10/23] time 0.203 (0.360) data 0.000 (0.146) loss 3.3217 (3.2924) acc 31.2500 (25.0000) lr 1.9511e-03 eta 0:06:01
epoch [7/50] batch [15/23] time 0.203 (0.312) data 0.000 (0.097) loss 3.0265 (3.2526) acc 34.3750 (25.4167) lr 1.9511e-03 eta 0:05:10
epoch [7/50] batch [20/23] time 0.204 (0.284) data 0.000 (0.073) loss 3.2372 (3.2282) acc 28.1250 (26.4062) lr 1.9511e-03 eta 0:04:42
epoch [8/50] batch [5/23] time 0.213 (0.488) data 0.000 (0.262) loss 3.2484 (3.2897) acc 25.0000 (23.1250) lr 1.9298e-03 eta 0:07:59
epoch [8/50] batch [10/23] time 0.203 (0.357) data 0.000 (0.131) loss 3.6413 (3.3019) acc 25.0000 (24.3750) lr 1.9298e-03 eta 0:05:49
epoch [8/50] batch [15/23] time 0.199 (0.304) data 0.000 (0.087) loss 2.8047 (3.2878) acc 34.3750 (25.0000) lr 1.9298e-03 eta 0:04:56
epoch [8/50] batch [20/23] time 0.202 (0.278) data 0.000 (0.066) loss 3.1104 (3.2506) acc 25.0000 (25.7812) lr 1.9298e-03 eta 0:04:29
epoch [9/50] batch [5/23] time 0.199 (0.503) data 0.000 (0.272) loss 3.0201 (3.0906) acc 34.3750 (33.7500) lr 1.9048e-03 eta 0:08:03
epoch [9/50] batch [10/23] time 0.201 (0.365) data 0.000 (0.136) loss 3.2794 (3.1919) acc 28.1250 (30.3125) lr 1.9048e-03 eta 0:05:48
epoch [9/50] batch [15/23] time 0.203 (0.311) data 0.000 (0.091) loss 2.8464 (3.1797) acc 40.6250 (28.9583) lr 1.9048e-03 eta 0:04:55
epoch [9/50] batch [20/23] time 0.197 (0.283) data 0.000 (0.068) loss 3.0704 (3.1987) acc 34.3750 (28.4375) lr 1.9048e-03 eta 0:04:27
epoch [10/50] batch [5/23] time 0.203 (0.496) data 0.000 (0.263) loss 3.4257 (3.0417) acc 21.8750 (31.2500) lr 1.8763e-03 eta 0:07:45
epoch [10/50] batch [10/23] time 0.198 (0.351) data 0.000 (0.132) loss 3.2462 (3.1203) acc 21.8750 (28.7500) lr 1.8763e-03 eta 0:05:27
epoch [10/50] batch [15/23] time 0.198 (0.301) data 0.000 (0.088) loss 3.2343 (3.0889) acc 25.0000 (30.0000) lr 1.8763e-03 eta 0:04:39
epoch [10/50] batch [20/23] time 0.265 (0.279) data 0.000 (0.066) loss 3.2926 (3.1207) acc 28.1250 (29.0625) lr 1.8763e-03 eta 0:04:17
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 1.048  alpha2: 0.374 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.53 <<<
epoch [11/50] batch [5/23] time 0.145 (1.263) data 0.000 (0.339) loss 2.0414 (2.1756) acc 56.2500 (60.4201) lr 1.8443e-03 eta 0:19:15
epoch [11/50] batch [10/23] time 0.176 (0.931) data 0.000 (0.170) loss 2.1349 (2.1666) acc 60.8696 (57.6457) lr 1.8443e-03 eta 0:14:06
epoch [11/50] batch [15/23] time 1.222 (0.750) data 0.000 (0.113) loss 2.2431 (2.1470) acc 55.5556 (56.6342) lr 1.8443e-03 eta 0:11:18
epoch [11/50] batch [20/23] time 1.144 (0.696) data 0.000 (0.085) loss 2.0837 (2.1232) acc 57.0000 (56.8624) lr 1.8443e-03 eta 0:10:26
>>> alpha1: 0.781  alpha2: 0.328 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.51 <<<
epoch [12/50] batch [5/23] time 0.177 (0.458) data 0.000 (0.275) loss 1.4477 (1.5360) acc 72.8261 (63.2476) lr 1.8090e-03 eta 0:06:48
epoch [12/50] batch [10/23] time 0.185 (0.466) data 0.000 (0.138) loss 1.2874 (1.5171) acc 68.0851 (64.3374) lr 1.8090e-03 eta 0:06:53
epoch [12/50] batch [15/23] time 0.184 (0.371) data 0.000 (0.092) loss 1.4826 (1.4705) acc 63.2979 (64.5157) lr 1.8090e-03 eta 0:05:26
epoch [12/50] batch [20/23] time 0.190 (0.325) data 0.000 (0.069) loss 1.1838 (1.4537) acc 79.0816 (65.6306) lr 1.8090e-03 eta 0:04:45
>>> alpha1: 0.685  alpha2: 0.265 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.49 <<<
epoch [13/50] batch [5/23] time 0.181 (0.468) data 0.000 (0.283) loss 1.1820 (1.1625) acc 77.1277 (72.0235) lr 1.7705e-03 eta 0:06:46
epoch [13/50] batch [10/23] time 0.197 (0.331) data 0.000 (0.142) loss 0.9959 (1.2554) acc 71.0784 (67.9633) lr 1.7705e-03 eta 0:04:46
epoch [13/50] batch [15/23] time 0.175 (0.327) data 0.000 (0.095) loss 1.1078 (1.2051) acc 71.5909 (67.8116) lr 1.7705e-03 eta 0:04:40
epoch [13/50] batch [20/23] time 0.170 (0.287) data 0.000 (0.071) loss 1.4608 (1.2680) acc 64.7727 (66.4505) lr 1.7705e-03 eta 0:04:05
>>> alpha1: 0.631  alpha2: 0.239 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.52 <<<
epoch [14/50] batch [5/23] time 1.772 (1.159) data 0.000 (0.286) loss 1.1283 (1.1806) acc 76.9231 (69.2653) lr 1.7290e-03 eta 0:16:20
epoch [14/50] batch [10/23] time 0.143 (0.652) data 0.000 (0.143) loss 1.0336 (1.1667) acc 66.8367 (70.1637) lr 1.7290e-03 eta 0:09:08
epoch [14/50] batch [15/23] time 0.152 (0.486) data 0.000 (0.096) loss 1.0836 (1.1549) acc 68.2692 (69.9997) lr 1.7290e-03 eta 0:06:46
epoch [14/50] batch [20/23] time 0.256 (0.489) data 0.000 (0.072) loss 1.3111 (1.1722) acc 60.0962 (69.0953) lr 1.7290e-03 eta 0:06:46
>>> alpha1: 0.596  alpha2: 0.226 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.53 <<<
epoch [15/50] batch [5/23] time 0.203 (0.461) data 0.001 (0.260) loss 1.2883 (1.0910) acc 60.2941 (66.8449) lr 1.6845e-03 eta 0:06:19
epoch [15/50] batch [10/23] time 1.363 (0.450) data 0.000 (0.130) loss 0.9008 (1.0193) acc 78.9474 (71.3888) lr 1.6845e-03 eta 0:06:07
epoch [15/50] batch [15/23] time 0.193 (0.361) data 0.000 (0.087) loss 1.1795 (1.0731) acc 66.0000 (69.2270) lr 1.6845e-03 eta 0:04:53
epoch [15/50] batch [20/23] time 0.190 (0.323) data 0.000 (0.065) loss 1.3009 (1.0845) acc 65.5000 (69.0383) lr 1.6845e-03 eta 0:04:20
>>> alpha1: 0.511  alpha2: 0.152 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.34 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.53 <<<
epoch [16/50] batch [5/23] time 0.203 (0.524) data 0.000 (0.324) loss 0.7655 (0.9444) acc 80.7692 (72.7928) lr 1.6374e-03 eta 0:06:59
epoch [16/50] batch [10/23] time 0.184 (0.366) data 0.000 (0.162) loss 1.1419 (1.0127) acc 61.9792 (70.0689) lr 1.6374e-03 eta 0:04:50
epoch [16/50] batch [15/23] time 0.187 (0.307) data 0.000 (0.108) loss 0.9615 (1.0556) acc 74.4898 (69.0551) lr 1.6374e-03 eta 0:04:02
epoch [16/50] batch [20/23] time 0.194 (0.279) data 0.000 (0.081) loss 1.1826 (1.0735) acc 69.6078 (69.0568) lr 1.6374e-03 eta 0:03:38
>>> alpha1: 0.409  alpha2: 0.076 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.51 <<<
epoch [17/50] batch [5/23] time 0.221 (0.488) data 0.000 (0.285) loss 0.8730 (0.8647) acc 75.4630 (73.7433) lr 1.5878e-03 eta 0:06:19
epoch [17/50] batch [10/23] time 0.195 (0.348) data 0.000 (0.143) loss 0.9054 (0.8700) acc 78.4314 (76.4610) lr 1.5878e-03 eta 0:04:28
epoch [17/50] batch [15/23] time 0.192 (0.371) data 0.000 (0.095) loss 1.1851 (0.9267) acc 62.7551 (74.5048) lr 1.5878e-03 eta 0:04:44
epoch [17/50] batch [20/23] time 0.202 (0.331) data 0.000 (0.071) loss 0.8881 (0.9247) acc 82.5472 (74.7896) lr 1.5878e-03 eta 0:04:12
>>> alpha1: 0.365  alpha2: 0.053 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.34 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.50 <<<
epoch [18/50] batch [5/23] time 0.260 (0.449) data 0.000 (0.247) loss 0.8557 (0.7220) acc 81.5217 (80.9121) lr 1.5358e-03 eta 0:05:38
epoch [18/50] batch [10/23] time 0.189 (0.320) data 0.000 (0.124) loss 1.0768 (0.7853) acc 65.6250 (78.1071) lr 1.5358e-03 eta 0:03:59
epoch [18/50] batch [15/23] time 0.181 (0.276) data 0.000 (0.083) loss 0.7986 (0.7831) acc 77.2222 (78.0763) lr 1.5358e-03 eta 0:03:25
epoch [18/50] batch [20/23] time 0.185 (0.254) data 0.000 (0.062) loss 1.0228 (0.8026) acc 72.3958 (76.9005) lr 1.5358e-03 eta 0:03:07
>>> alpha1: 0.327  alpha2: 0.017 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [19/50] batch [5/23] time 0.195 (0.501) data 0.000 (0.303) loss 0.7589 (0.6808) acc 76.9608 (81.3351) lr 1.4818e-03 eta 0:06:06
epoch [19/50] batch [10/23] time 0.184 (0.345) data 0.000 (0.152) loss 0.7685 (0.7419) acc 78.7234 (78.9316) lr 1.4818e-03 eta 0:04:10
epoch [19/50] batch [15/23] time 0.189 (0.296) data 0.000 (0.101) loss 0.8103 (0.7708) acc 70.4082 (77.7825) lr 1.4818e-03 eta 0:03:33
epoch [19/50] batch [20/23] time 0.192 (0.269) data 0.000 (0.076) loss 0.9549 (0.8150) acc 67.0000 (76.5258) lr 1.4818e-03 eta 0:03:12
>>> alpha1: 0.285  alpha2: -0.016 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.48 <<<
epoch [20/50] batch [5/23] time 1.322 (0.741) data 0.000 (0.312) loss 0.7789 (0.7236) acc 70.5357 (75.0655) lr 1.4258e-03 eta 0:08:44
epoch [20/50] batch [10/23] time 0.197 (0.467) data 0.000 (0.156) loss 1.0535 (0.7653) acc 72.5000 (76.6328) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [15/23] time 0.194 (0.377) data 0.000 (0.104) loss 0.7901 (0.7810) acc 82.3529 (76.9366) lr 1.4258e-03 eta 0:04:22
epoch [20/50] batch [20/23] time 0.212 (0.336) data 0.000 (0.078) loss 0.8942 (0.7748) acc 76.5000 (77.7600) lr 1.4258e-03 eta 0:03:52
>>> alpha1: 0.252  alpha2: -0.026 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [21/50] batch [5/23] time 0.140 (0.540) data 0.000 (0.277) loss 0.7355 (0.6165) acc 79.6875 (82.5310) lr 1.3681e-03 eta 0:06:09
epoch [21/50] batch [10/23] time 0.157 (0.344) data 0.000 (0.139) loss 0.8031 (0.6966) acc 70.6731 (79.0416) lr 1.3681e-03 eta 0:03:53
epoch [21/50] batch [15/23] time 1.730 (0.384) data 0.000 (0.093) loss 0.7953 (0.6981) acc 71.9828 (78.5241) lr 1.3681e-03 eta 0:04:18
epoch [21/50] batch [20/23] time 0.267 (0.350) data 0.000 (0.069) loss 0.6951 (0.6926) acc 79.2453 (78.8803) lr 1.3681e-03 eta 0:03:54
>>> alpha1: 0.235  alpha2: -0.035 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.48 <<<
epoch [22/50] batch [5/23] time 0.202 (0.494) data 0.000 (0.292) loss 0.5062 (0.6012) acc 86.3208 (83.2156) lr 1.3090e-03 eta 0:05:26
epoch [22/50] batch [10/23] time 0.187 (0.344) data 0.000 (0.148) loss 0.6744 (0.6133) acc 75.0000 (81.7052) lr 1.3090e-03 eta 0:03:45
epoch [22/50] batch [15/23] time 0.181 (0.292) data 0.000 (0.099) loss 0.5120 (0.6358) acc 84.0425 (80.9092) lr 1.3090e-03 eta 0:03:10
epoch [22/50] batch [20/23] time 0.186 (0.270) data 0.000 (0.074) loss 0.9760 (0.6797) acc 69.1489 (79.7873) lr 1.3090e-03 eta 0:02:54
>>> alpha1: 0.222  alpha2: -0.037 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.50 <<<
epoch [23/50] batch [5/23] time 0.205 (0.489) data 0.000 (0.271) loss 0.5141 (0.6406) acc 77.4510 (80.4692) lr 1.2487e-03 eta 0:05:12
epoch [23/50] batch [10/23] time 0.200 (0.344) data 0.000 (0.136) loss 0.8629 (0.6504) acc 69.6808 (80.0777) lr 1.2487e-03 eta 0:03:37
epoch [23/50] batch [15/23] time 0.211 (0.294) data 0.000 (0.091) loss 0.6229 (0.6746) acc 76.8182 (79.1185) lr 1.2487e-03 eta 0:03:04
epoch [23/50] batch [20/23] time 0.266 (0.273) data 0.000 (0.068) loss 0.7213 (0.6807) acc 76.9608 (78.6737) lr 1.2487e-03 eta 0:02:50
>>> alpha1: 0.203  alpha2: -0.044 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.49 <<<
epoch [24/50] batch [5/23] time 0.185 (0.500) data 0.000 (0.289) loss 0.6930 (0.6890) acc 70.8333 (76.0845) lr 1.1874e-03 eta 0:05:07
epoch [24/50] batch [10/23] time 0.194 (0.349) data 0.000 (0.145) loss 0.5523 (0.6426) acc 84.6939 (79.7419) lr 1.1874e-03 eta 0:03:33
epoch [24/50] batch [15/23] time 0.192 (0.295) data 0.000 (0.096) loss 0.5526 (0.6157) acc 85.5000 (81.0610) lr 1.1874e-03 eta 0:02:58
epoch [24/50] batch [20/23] time 0.227 (0.273) data 0.000 (0.072) loss 0.4387 (0.6325) acc 89.0909 (80.9453) lr 1.1874e-03 eta 0:02:43
>>> alpha1: 0.194  alpha2: -0.032 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.51 <<<
epoch [25/50] batch [5/23] time 0.196 (0.474) data 0.000 (0.256) loss 0.6072 (0.5144) acc 81.3726 (83.9068) lr 1.1253e-03 eta 0:04:41
epoch [25/50] batch [10/23] time 0.177 (0.329) data 0.000 (0.128) loss 0.6781 (0.5644) acc 86.4130 (84.2792) lr 1.1253e-03 eta 0:03:13
epoch [25/50] batch [15/23] time 0.190 (0.284) data 0.000 (0.086) loss 0.7030 (0.5608) acc 81.6327 (83.1148) lr 1.1253e-03 eta 0:02:45
epoch [25/50] batch [20/23] time 0.188 (0.260) data 0.000 (0.064) loss 0.7177 (0.5997) acc 81.1225 (82.1246) lr 1.1253e-03 eta 0:02:30
>>> alpha1: 0.191  alpha2: -0.026 <<<
>>> noisy rate: 0.48 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [26/50] batch [5/23] time 0.207 (0.490) data 0.000 (0.280) loss 0.4561 (0.4457) acc 86.5000 (86.4109) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [10/23] time 0.196 (0.348) data 0.000 (0.140) loss 0.5681 (0.5049) acc 82.6923 (85.2629) lr 1.0628e-03 eta 0:03:16
epoch [26/50] batch [15/23] time 0.191 (0.297) data 0.000 (0.094) loss 0.6184 (0.5177) acc 79.5000 (84.9585) lr 1.0628e-03 eta 0:02:46
epoch [26/50] batch [20/23] time 0.176 (0.268) data 0.000 (0.070) loss 0.5717 (0.5456) acc 85.7955 (83.3386) lr 1.0628e-03 eta 0:02:28
>>> alpha1: 0.184  alpha2: -0.022 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.49 <<<
epoch [27/50] batch [5/23] time 0.198 (0.481) data 0.000 (0.283) loss 0.4907 (0.5530) acc 88.4615 (84.0298) lr 1.0000e-03 eta 0:04:23
epoch [27/50] batch [10/23] time 0.204 (0.340) data 0.000 (0.142) loss 0.4335 (0.5304) acc 80.1887 (84.6691) lr 1.0000e-03 eta 0:03:04
epoch [27/50] batch [15/23] time 0.187 (0.294) data 0.000 (0.094) loss 0.5851 (0.5330) acc 82.8125 (84.3032) lr 1.0000e-03 eta 0:02:38
epoch [27/50] batch [20/23] time 0.195 (0.270) data 0.000 (0.071) loss 0.6438 (0.5458) acc 88.7255 (84.5193) lr 1.0000e-03 eta 0:02:23
>>> alpha1: 0.181  alpha2: -0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.51 <<<
epoch [28/50] batch [5/23] time 0.322 (0.642) data 0.000 (0.349) loss 0.4485 (0.4826) acc 86.6071 (86.1290) lr 9.3721e-04 eta 0:05:36
epoch [28/50] batch [10/23] time 0.311 (0.469) data 0.000 (0.175) loss 0.4404 (0.4984) acc 90.2778 (86.6500) lr 9.3721e-04 eta 0:04:03
epoch [28/50] batch [15/23] time 0.288 (0.411) data 0.000 (0.116) loss 0.6335 (0.4992) acc 82.0000 (85.6580) lr 9.3721e-04 eta 0:03:31
epoch [28/50] batch [20/23] time 0.144 (0.362) data 0.000 (0.087) loss 0.4963 (0.5255) acc 91.3265 (85.1467) lr 9.3721e-04 eta 0:03:04
>>> alpha1: 0.177  alpha2: -0.011 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.50 <<<
epoch [29/50] batch [5/23] time 0.202 (0.459) data 0.000 (0.239) loss 0.5155 (0.4600) acc 81.2500 (87.5334) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [10/23] time 0.192 (0.328) data 0.000 (0.121) loss 0.4177 (0.4576) acc 91.0000 (87.2208) lr 8.7467e-04 eta 0:02:42
epoch [29/50] batch [15/23] time 0.198 (0.286) data 0.000 (0.081) loss 0.4637 (0.4769) acc 85.5769 (86.1021) lr 8.7467e-04 eta 0:02:20
epoch [29/50] batch [20/23] time 0.191 (0.266) data 0.000 (0.061) loss 0.4677 (0.4995) acc 88.2653 (85.6838) lr 8.7467e-04 eta 0:02:09
>>> alpha1: 0.175  alpha2: -0.008 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [30/50] batch [5/23] time 0.215 (0.464) data 0.000 (0.252) loss 0.5521 (0.5021) acc 82.6531 (86.1445) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [10/23] time 0.172 (0.335) data 0.000 (0.126) loss 0.6908 (0.5125) acc 80.8139 (85.1735) lr 8.1262e-04 eta 0:02:38
epoch [30/50] batch [15/23] time 0.195 (0.289) data 0.000 (0.084) loss 0.4384 (0.5071) acc 91.1765 (85.1882) lr 8.1262e-04 eta 0:02:15
epoch [30/50] batch [20/23] time 0.198 (0.267) data 0.000 (0.063) loss 0.5223 (0.5019) acc 87.0192 (85.3954) lr 8.1262e-04 eta 0:02:03
>>> alpha1: 0.170  alpha2: -0.006 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.48 <<<
epoch [31/50] batch [5/23] time 0.187 (0.490) data 0.000 (0.278) loss 0.5951 (0.4142) acc 85.6383 (89.1075) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [10/23] time 0.195 (0.344) data 0.000 (0.139) loss 0.5556 (0.4813) acc 90.5556 (86.7114) lr 7.5131e-04 eta 0:02:34
epoch [31/50] batch [15/23] time 0.192 (0.298) data 0.000 (0.093) loss 0.4100 (0.4808) acc 82.6531 (85.6860) lr 7.5131e-04 eta 0:02:12
epoch [31/50] batch [20/23] time 0.193 (0.272) data 0.000 (0.070) loss 0.6268 (0.4881) acc 80.5000 (85.4352) lr 7.5131e-04 eta 0:01:59
>>> alpha1: 0.167  alpha2: 0.000 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.47 <<<
epoch [32/50] batch [5/23] time 0.162 (0.448) data 0.000 (0.297) loss 0.5643 (0.5223) acc 84.5000 (84.0944) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [10/23] time 0.172 (0.315) data 0.000 (0.149) loss 0.3595 (0.4654) acc 86.5385 (85.7332) lr 6.9098e-04 eta 0:02:14
epoch [32/50] batch [15/23] time 0.187 (0.273) data 0.000 (0.099) loss 0.4161 (0.4624) acc 86.5000 (85.8871) lr 6.9098e-04 eta 0:01:54
epoch [32/50] batch [20/23] time 0.182 (0.251) data 0.000 (0.075) loss 0.5206 (0.4715) acc 88.0208 (85.8009) lr 6.9098e-04 eta 0:01:44
>>> alpha1: 0.164  alpha2: 0.005 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.46 <<<
epoch [33/50] batch [5/23] time 0.206 (0.503) data 0.000 (0.288) loss 0.4335 (0.4171) acc 89.2157 (88.3470) lr 6.3188e-04 eta 0:03:25
epoch [33/50] batch [10/23] time 0.195 (0.351) data 0.000 (0.144) loss 0.3389 (0.4217) acc 90.1961 (87.8118) lr 6.3188e-04 eta 0:02:21
epoch [33/50] batch [15/23] time 0.206 (0.300) data 0.000 (0.096) loss 0.5361 (0.4375) acc 86.3208 (86.4718) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [20/23] time 0.196 (0.276) data 0.000 (0.072) loss 0.3992 (0.4678) acc 85.5000 (85.4308) lr 6.3188e-04 eta 0:01:48
>>> alpha1: 0.162  alpha2: 0.004 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.46 <<<
epoch [34/50] batch [5/23] time 0.205 (0.490) data 0.000 (0.287) loss 0.6570 (0.4705) acc 82.3529 (86.7926) lr 5.7422e-04 eta 0:03:09
epoch [34/50] batch [10/23] time 0.242 (0.348) data 0.000 (0.143) loss 0.4048 (0.4491) acc 86.1111 (87.0025) lr 5.7422e-04 eta 0:02:12
epoch [34/50] batch [15/23] time 0.190 (0.298) data 0.000 (0.096) loss 0.4489 (0.4317) acc 85.2041 (87.3901) lr 5.7422e-04 eta 0:01:51
epoch [34/50] batch [20/23] time 0.186 (0.271) data 0.000 (0.072) loss 0.4244 (0.4422) acc 81.7708 (86.8326) lr 5.7422e-04 eta 0:01:40
>>> alpha1: 0.155  alpha2: -0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.47 <<<
epoch [35/50] batch [5/23] time 0.193 (0.510) data 0.000 (0.312) loss 0.2476 (0.4198) acc 95.0980 (88.1421) lr 5.1825e-04 eta 0:03:05
epoch [35/50] batch [10/23] time 0.181 (0.355) data 0.000 (0.156) loss 0.4009 (0.4483) acc 91.4894 (87.4438) lr 5.1825e-04 eta 0:02:07
epoch [35/50] batch [15/23] time 0.160 (0.287) data 0.000 (0.104) loss 0.4191 (0.4397) acc 90.6250 (87.1473) lr 5.1825e-04 eta 0:01:41
epoch [35/50] batch [20/23] time 0.156 (0.252) data 0.000 (0.078) loss 0.3032 (0.4270) acc 95.5882 (87.3860) lr 5.1825e-04 eta 0:01:27
>>> alpha1: 0.151  alpha2: -0.000 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [36/50] batch [5/23] time 0.212 (0.470) data 0.000 (0.271) loss 0.4235 (0.3707) acc 91.8367 (89.0888) lr 4.6417e-04 eta 0:02:39
epoch [36/50] batch [10/23] time 0.221 (0.341) data 0.000 (0.136) loss 0.5180 (0.4246) acc 85.0962 (87.7117) lr 4.6417e-04 eta 0:01:54
epoch [36/50] batch [15/23] time 0.189 (0.292) data 0.000 (0.090) loss 0.4163 (0.4276) acc 90.3061 (87.3606) lr 4.6417e-04 eta 0:01:36
epoch [36/50] batch [20/23] time 0.176 (0.268) data 0.000 (0.068) loss 0.4899 (0.4325) acc 88.5870 (87.3378) lr 4.6417e-04 eta 0:01:26
>>> alpha1: 0.149  alpha2: -0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.49 <<<
epoch [37/50] batch [5/23] time 0.271 (0.746) data 0.000 (0.310) loss 0.3984 (0.3859) acc 90.0000 (89.7137) lr 4.1221e-04 eta 0:03:56
epoch [37/50] batch [10/23] time 0.206 (0.472) data 0.000 (0.155) loss 0.4637 (0.4258) acc 88.7255 (87.6389) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [15/23] time 0.204 (0.382) data 0.000 (0.103) loss 0.2730 (0.3927) acc 94.3396 (88.5372) lr 4.1221e-04 eta 0:01:57
epoch [37/50] batch [20/23] time 0.202 (0.337) data 0.000 (0.078) loss 0.6414 (0.4088) acc 76.4151 (87.8519) lr 4.1221e-04 eta 0:01:41
>>> alpha1: 0.149  alpha2: -0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [38/50] batch [5/23] time 0.226 (0.488) data 0.000 (0.281) loss 0.4005 (0.5354) acc 88.1818 (86.1728) lr 3.6258e-04 eta 0:02:23
epoch [38/50] batch [10/23] time 0.198 (0.350) data 0.000 (0.141) loss 0.4145 (0.4655) acc 88.2979 (87.7178) lr 3.6258e-04 eta 0:01:41
epoch [38/50] batch [15/23] time 0.187 (0.299) data 0.000 (0.094) loss 0.4381 (0.4582) acc 92.1875 (88.3410) lr 3.6258e-04 eta 0:01:24
epoch [38/50] batch [20/23] time 0.185 (0.271) data 0.000 (0.071) loss 0.4765 (0.4625) acc 86.7021 (87.9319) lr 3.6258e-04 eta 0:01:15
>>> alpha1: 0.149  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [39/50] batch [5/23] time 0.244 (0.561) data 0.000 (0.304) loss 0.3383 (0.3519) acc 91.5000 (91.0625) lr 3.1545e-04 eta 0:02:32
epoch [39/50] batch [10/23] time 0.245 (0.406) data 0.000 (0.152) loss 0.4057 (0.4301) acc 90.0000 (89.2017) lr 3.1545e-04 eta 0:01:47
epoch [39/50] batch [15/23] time 0.148 (0.326) data 0.000 (0.102) loss 0.2789 (0.4417) acc 93.1373 (89.4271) lr 3.1545e-04 eta 0:01:25
epoch [39/50] batch [20/23] time 0.149 (0.282) data 0.000 (0.076) loss 0.4161 (0.4334) acc 91.5000 (89.2296) lr 3.1545e-04 eta 0:01:12
>>> alpha1: 0.149  alpha2: 0.005 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [40/50] batch [5/23] time 0.178 (0.472) data 0.000 (0.264) loss 0.3828 (0.3940) acc 91.8478 (87.0444) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [10/23] time 0.216 (0.339) data 0.000 (0.132) loss 0.3456 (0.3644) acc 93.8596 (89.6550) lr 2.7103e-04 eta 0:01:22
epoch [40/50] batch [15/23] time 0.191 (0.291) data 0.000 (0.088) loss 0.3473 (0.3776) acc 84.1837 (88.5147) lr 2.7103e-04 eta 0:01:09
epoch [40/50] batch [20/23] time 0.201 (0.271) data 0.000 (0.066) loss 0.3555 (0.3908) acc 91.9811 (88.4806) lr 2.7103e-04 eta 0:01:03
>>> alpha1: 0.148  alpha2: -0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [41/50] batch [5/23] time 0.212 (0.469) data 0.000 (0.244) loss 0.2331 (0.3902) acc 88.8889 (86.5356) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [10/23] time 0.189 (0.335) data 0.000 (0.122) loss 0.4176 (0.4080) acc 84.3750 (88.3560) lr 2.2949e-04 eta 0:01:13
epoch [41/50] batch [15/23] time 0.208 (0.291) data 0.000 (0.081) loss 0.5218 (0.3937) acc 88.4259 (89.5853) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [20/23] time 0.194 (0.271) data 0.000 (0.061) loss 0.5193 (0.3932) acc 88.0000 (89.6251) lr 2.2949e-04 eta 0:00:56
>>> alpha1: 0.145  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.48 <<<
epoch [42/50] batch [5/23] time 0.186 (0.470) data 0.000 (0.269) loss 0.5195 (0.3870) acc 85.2041 (89.1780) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [10/23] time 0.201 (0.337) data 0.000 (0.136) loss 0.4858 (0.4143) acc 91.9811 (88.5634) lr 1.9098e-04 eta 0:01:06
epoch [42/50] batch [15/23] time 0.197 (0.296) data 0.000 (0.091) loss 0.2894 (0.4040) acc 93.2692 (89.0735) lr 1.9098e-04 eta 0:00:56
epoch [42/50] batch [20/23] time 0.188 (0.272) data 0.000 (0.068) loss 0.2599 (0.4076) acc 90.8163 (88.6185) lr 1.9098e-04 eta 0:00:50
>>> alpha1: 0.142  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.47 <<<
epoch [43/50] batch [5/23] time 0.192 (0.471) data 0.001 (0.275) loss 0.3048 (0.3429) acc 95.5882 (93.4158) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [10/23] time 0.220 (0.339) data 0.000 (0.138) loss 0.4842 (0.3803) acc 84.3137 (91.0828) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [15/23] time 0.200 (0.295) data 0.000 (0.092) loss 0.3170 (0.3700) acc 95.7547 (90.6624) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [20/23] time 0.193 (0.270) data 0.000 (0.069) loss 0.3487 (0.3841) acc 92.7885 (90.0186) lr 1.5567e-04 eta 0:00:44
>>> alpha1: 0.140  alpha2: 0.000 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.48 <<<
epoch [44/50] batch [5/23] time 0.199 (0.500) data 0.000 (0.293) loss 0.3736 (0.5976) acc 87.2549 (87.1350) lr 1.2369e-04 eta 0:01:17
epoch [44/50] batch [10/23] time 0.177 (0.355) data 0.000 (0.147) loss 0.4534 (0.4627) acc 85.6383 (89.3782) lr 1.2369e-04 eta 0:00:53
epoch [44/50] batch [15/23] time 0.200 (0.301) data 0.000 (0.098) loss 0.3664 (0.4263) acc 90.4546 (89.4647) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [20/23] time 0.210 (0.275) data 0.000 (0.073) loss 0.5046 (0.4234) acc 85.0877 (88.5740) lr 1.2369e-04 eta 0:00:38
>>> alpha1: 0.139  alpha2: 0.003 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [45/50] batch [5/23] time 0.205 (0.465) data 0.000 (0.264) loss 0.4243 (0.3936) acc 83.7963 (89.3175) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [10/23] time 0.194 (0.333) data 0.000 (0.132) loss 0.2887 (0.3590) acc 93.5000 (91.0241) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [15/23] time 0.190 (0.291) data 0.000 (0.088) loss 0.3322 (0.3637) acc 92.7083 (90.2574) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [20/23] time 0.197 (0.266) data 0.000 (0.066) loss 0.3669 (0.3716) acc 87.0370 (90.2566) lr 9.5173e-05 eta 0:00:31
>>> alpha1: 0.136  alpha2: 0.004 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [46/50] batch [5/23] time 0.201 (0.531) data 0.000 (0.321) loss 0.4366 (0.4101) acc 85.8491 (88.0456) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [10/23] time 0.184 (0.360) data 0.001 (0.161) loss 0.4631 (0.4259) acc 88.7755 (88.1051) lr 7.0224e-05 eta 0:00:37
epoch [46/50] batch [15/23] time 0.202 (0.311) data 0.000 (0.107) loss 0.5224 (0.3938) acc 83.9623 (89.3451) lr 7.0224e-05 eta 0:00:31
epoch [46/50] batch [20/23] time 0.200 (0.283) data 0.000 (0.081) loss 0.1711 (0.3830) acc 99.0909 (90.0516) lr 7.0224e-05 eta 0:00:26
>>> alpha1: 0.138  alpha2: 0.004 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.46 <<<
epoch [47/50] batch [5/23] time 0.192 (0.490) data 0.000 (0.292) loss 0.4237 (0.3703) acc 85.2941 (87.2995) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [10/23] time 0.249 (0.346) data 0.000 (0.146) loss 0.4244 (0.4043) acc 85.3774 (87.5129) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [15/23] time 0.192 (0.295) data 0.000 (0.098) loss 0.3441 (0.3932) acc 91.1765 (88.5750) lr 4.8943e-05 eta 0:00:22
epoch [47/50] batch [20/23] time 0.190 (0.270) data 0.000 (0.073) loss 0.3199 (0.3690) acc 89.7059 (89.5159) lr 4.8943e-05 eta 0:00:19
>>> alpha1: 0.137  alpha2: 0.005 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.46 <<<
epoch [48/50] batch [5/23] time 0.194 (0.440) data 0.000 (0.243) loss 0.3636 (0.3486) acc 88.4615 (91.0467) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.198 (0.315) data 0.000 (0.122) loss 0.3828 (0.3777) acc 90.7407 (89.2929) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [15/23] time 0.180 (0.278) data 0.000 (0.081) loss 0.3077 (0.3930) acc 91.6667 (88.6600) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [20/23] time 0.198 (0.255) data 0.000 (0.061) loss 0.2833 (0.3911) acc 90.8654 (88.9958) lr 3.1417e-05 eta 0:00:12
>>> alpha1: 0.140  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.48 <<<
epoch [49/50] batch [5/23] time 0.243 (0.442) data 0.000 (0.232) loss 0.5036 (0.4253) acc 90.5000 (89.9622) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.211 (0.323) data 0.000 (0.116) loss 0.3725 (0.4188) acc 85.6383 (88.8786) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.206 (0.281) data 0.000 (0.078) loss 0.4003 (0.4110) acc 90.5660 (88.9820) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [20/23] time 0.196 (0.260) data 0.000 (0.058) loss 0.2162 (0.3831) acc 88.2353 (89.7081) lr 1.7713e-05 eta 0:00:06
>>> alpha1: 0.140  alpha2: -0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.47 <<<
epoch [50/50] batch [5/23] time 0.177 (0.429) data 0.000 (0.266) loss 0.4728 (0.3919) acc 87.0000 (89.5367) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.297 (0.366) data 0.001 (0.133) loss 0.3142 (0.3630) acc 93.0000 (90.5240) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.278 (0.340) data 0.000 (0.089) loss 0.4178 (0.3817) acc 91.1458 (90.2783) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.298 (0.326) data 0.000 (0.067) loss 0.3711 (0.3734) acc 88.4615 (90.3036) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.35, 0.35, 0.35, 0.33, 0.35, 0.34, 0.33, 0.34, 0.32, 0.31, 0.32, 0.33, 0.32, 0.33, 0.33, 0.32, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31]
* matched noise rate: [0.15, 0.15, 0.14, 0.19, 0.19, 0.19, 0.2, 0.18, 0.2, 0.21, 0.2, 0.19, 0.18, 0.18, 0.18, 0.17, 0.18, 0.19, 0.19, 0.2, 0.19, 0.19, 0.19, 0.18, 0.18, 0.2, 0.18, 0.19, 0.2, 0.2, 0.2, 0.21, 0.21, 0.19, 0.2, 0.2, 0.2, 0.18, 0.18, 0.18]
* unmatched noise rate: [0.53, 0.51, 0.49, 0.52, 0.53, 0.53, 0.51, 0.5, 0.45, 0.48, 0.48, 0.48, 0.5, 0.49, 0.51, 0.49, 0.49, 0.51, 0.5, 0.47, 0.48, 0.47, 0.46, 0.46, 0.47, 0.45, 0.49, 0.44, 0.45, 0.47, 0.49, 0.48, 0.47, 0.48, 0.47, 0.49, 0.46, 0.46, 0.48, 0.47]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:03<00:50,  3.14s/it] 12%|█▏        | 2/17 [00:03<00:20,  1.36s/it] 18%|█▊        | 3/17 [00:03<00:11,  1.25it/s] 24%|██▎       | 4/17 [00:03<00:06,  1.88it/s] 29%|██▉       | 5/17 [00:03<00:04,  2.56it/s] 35%|███▌      | 6/17 [00:03<00:03,  3.41it/s] 41%|████      | 7/17 [00:03<00:02,  4.21it/s] 47%|████▋     | 8/17 [00:04<00:01,  4.96it/s] 53%|█████▎    | 9/17 [00:04<00:01,  5.50it/s] 59%|█████▉    | 10/17 [00:04<00:01,  6.26it/s] 65%|██████▍   | 11/17 [00:04<00:00,  6.46it/s] 71%|███████   | 12/17 [00:04<00:00,  7.19it/s] 76%|███████▋  | 13/17 [00:04<00:00,  7.25it/s] 82%|████████▏ | 14/17 [00:04<00:00,  7.65it/s] 88%|████████▊ | 15/17 [00:04<00:00,  7.48it/s] 94%|█████████▍| 16/17 [00:04<00:00,  7.95it/s]100%|██████████| 17/17 [00:05<00:00,  2.62it/s]100%|██████████| 17/17 [00:06<00:00,  2.77it/s]
=> result
* total: 1,692
* correct: 919
* accuracy: 54.3%
* error: 45.7%
* macro_f1: 53.0%
=> per-class result
* class: 0 (banded)	total: 36	correct: 27	acc: 75.0%
* class: 1 (blotchy)	total: 36	correct: 8	acc: 22.2%
* class: 2 (braided)	total: 36	correct: 16	acc: 44.4%
* class: 3 (bubbly)	total: 36	correct: 29	acc: 80.6%
* class: 4 (bumpy)	total: 36	correct: 9	acc: 25.0%
* class: 5 (chequered)	total: 36	correct: 35	acc: 97.2%
* class: 6 (cobwebbed)	total: 36	correct: 29	acc: 80.6%
* class: 7 (cracked)	total: 36	correct: 30	acc: 83.3%
* class: 8 (crosshatched)	total: 36	correct: 10	acc: 27.8%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 12	acc: 33.3%
* class: 11 (fibrous)	total: 36	correct: 26	acc: 72.2%
* class: 12 (flecked)	total: 36	correct: 10	acc: 27.8%
* class: 13 (freckled)	total: 36	correct: 28	acc: 77.8%
* class: 14 (frilly)	total: 36	correct: 20	acc: 55.6%
* class: 15 (gauzy)	total: 36	correct: 21	acc: 58.3%
* class: 16 (grid)	total: 36	correct: 18	acc: 50.0%
* class: 17 (grooved)	total: 36	correct: 14	acc: 38.9%
* class: 18 (honeycombed)	total: 36	correct: 26	acc: 72.2%
* class: 19 (interlaced)	total: 36	correct: 18	acc: 50.0%
* class: 20 (knitted)	total: 36	correct: 28	acc: 77.8%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 15	acc: 41.7%
* class: 23 (marbled)	total: 36	correct: 14	acc: 38.9%
* class: 24 (matted)	total: 36	correct: 16	acc: 44.4%
* class: 25 (meshed)	total: 36	correct: 18	acc: 50.0%
* class: 26 (paisley)	total: 36	correct: 34	acc: 94.4%
* class: 27 (perforated)	total: 36	correct: 20	acc: 55.6%
* class: 28 (pitted)	total: 36	correct: 1	acc: 2.8%
* class: 29 (pleated)	total: 36	correct: 9	acc: 25.0%
* class: 30 (polka-dotted)	total: 36	correct: 12	acc: 33.3%
* class: 31 (porous)	total: 36	correct: 15	acc: 41.7%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 18	acc: 50.0%
* class: 34 (smeared)	total: 36	correct: 9	acc: 25.0%
* class: 35 (spiralled)	total: 36	correct: 16	acc: 44.4%
* class: 36 (sprinkled)	total: 36	correct: 12	acc: 33.3%
* class: 37 (stained)	total: 36	correct: 9	acc: 25.0%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 30	acc: 83.3%
* class: 40 (studded)	total: 36	correct: 21	acc: 58.3%
* class: 41 (swirly)	total: 36	correct: 24	acc: 66.7%
* class: 42 (veined)	total: 36	correct: 16	acc: 44.4%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 24	acc: 66.7%
* class: 45 (wrinkled)	total: 36	correct: 21	acc: 58.3%
* class: 46 (zigzagged)	total: 36	correct: 33	acc: 91.7%
* average: 54.3%
Elapsed: 0:16:13
Run this job and save the output to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_8-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.196 (1.049) data 0.000 (0.338) loss 3.7600 (3.6805) acc 18.7500 (16.2500) lr 1.0000e-05 eta 0:20:01
epoch [1/50] batch [10/23] time 0.209 (0.628) data 0.000 (0.169) loss 3.6003 (3.6871) acc 18.7500 (14.0625) lr 1.0000e-05 eta 0:11:55
epoch [1/50] batch [15/23] time 0.240 (0.490) data 0.000 (0.113) loss 3.7003 (3.6969) acc 12.5000 (13.9583) lr 1.0000e-05 eta 0:09:16
epoch [1/50] batch [20/23] time 0.202 (0.418) data 0.000 (0.085) loss 3.7955 (3.6911) acc 9.3750 (13.4375) lr 1.0000e-05 eta 0:07:52
epoch [2/50] batch [5/23] time 0.213 (0.499) data 0.000 (0.282) loss 3.3760 (3.6205) acc 28.1250 (16.2500) lr 2.0000e-03 eta 0:09:19
epoch [2/50] batch [10/23] time 0.196 (0.348) data 0.000 (0.141) loss 3.6490 (3.6496) acc 15.6250 (15.3125) lr 2.0000e-03 eta 0:06:29
epoch [2/50] batch [15/23] time 0.200 (0.303) data 0.000 (0.094) loss 3.7591 (3.6338) acc 9.3750 (15.0000) lr 2.0000e-03 eta 0:05:37
epoch [2/50] batch [20/23] time 0.197 (0.277) data 0.000 (0.071) loss 3.4398 (3.6019) acc 25.0000 (15.4688) lr 2.0000e-03 eta 0:05:06
epoch [3/50] batch [5/23] time 0.200 (0.439) data 0.000 (0.224) loss 3.8964 (3.6693) acc 12.5000 (15.0000) lr 1.9980e-03 eta 0:08:02
epoch [3/50] batch [10/23] time 0.197 (0.323) data 0.000 (0.112) loss 3.4116 (3.5766) acc 34.3750 (19.3750) lr 1.9980e-03 eta 0:05:53
epoch [3/50] batch [15/23] time 0.204 (0.288) data 0.000 (0.075) loss 3.2330 (3.4816) acc 21.8750 (20.8333) lr 1.9980e-03 eta 0:05:13
epoch [3/50] batch [20/23] time 0.198 (0.266) data 0.000 (0.056) loss 3.7651 (3.5112) acc 9.3750 (19.5312) lr 1.9980e-03 eta 0:04:48
epoch [4/50] batch [5/23] time 0.227 (0.501) data 0.000 (0.257) loss 3.5439 (3.5328) acc 28.1250 (18.1250) lr 1.9921e-03 eta 0:08:58
epoch [4/50] batch [10/23] time 0.197 (0.351) data 0.000 (0.129) loss 3.2635 (3.4860) acc 31.2500 (20.9375) lr 1.9921e-03 eta 0:06:15
epoch [4/50] batch [15/23] time 0.195 (0.304) data 0.000 (0.086) loss 2.9648 (3.4256) acc 34.3750 (22.2917) lr 1.9921e-03 eta 0:05:24
epoch [4/50] batch [20/23] time 0.201 (0.278) data 0.000 (0.065) loss 3.2661 (3.4159) acc 21.8750 (21.7188) lr 1.9921e-03 eta 0:04:54
epoch [5/50] batch [5/23] time 0.203 (0.483) data 0.000 (0.266) loss 3.4159 (3.2940) acc 18.7500 (23.7500) lr 1.9823e-03 eta 0:08:28
epoch [5/50] batch [10/23] time 0.210 (0.356) data 0.000 (0.134) loss 3.6254 (3.3269) acc 15.6250 (24.3750) lr 1.9823e-03 eta 0:06:13
epoch [5/50] batch [15/23] time 0.204 (0.305) data 0.000 (0.089) loss 3.0572 (3.2993) acc 34.3750 (25.0000) lr 1.9823e-03 eta 0:05:18
epoch [5/50] batch [20/23] time 0.201 (0.279) data 0.000 (0.067) loss 3.7258 (3.3639) acc 21.8750 (23.4375) lr 1.9823e-03 eta 0:04:49
epoch [6/50] batch [5/23] time 0.215 (0.500) data 0.000 (0.286) loss 3.2742 (3.3650) acc 34.3750 (28.1250) lr 1.9686e-03 eta 0:08:35
epoch [6/50] batch [10/23] time 0.201 (0.360) data 0.000 (0.143) loss 3.1244 (3.3474) acc 34.3750 (28.4375) lr 1.9686e-03 eta 0:06:08
epoch [6/50] batch [15/23] time 0.201 (0.307) data 0.000 (0.096) loss 2.9487 (3.3271) acc 25.0000 (26.8750) lr 1.9686e-03 eta 0:05:12
epoch [6/50] batch [20/23] time 0.201 (0.280) data 0.000 (0.072) loss 3.4319 (3.3389) acc 18.7500 (25.7812) lr 1.9686e-03 eta 0:04:44
epoch [7/50] batch [5/23] time 0.200 (0.502) data 0.000 (0.275) loss 3.0328 (3.1748) acc 37.5000 (28.1250) lr 1.9511e-03 eta 0:08:25
epoch [7/50] batch [10/23] time 0.207 (0.357) data 0.000 (0.138) loss 3.2325 (3.2461) acc 28.1250 (27.1875) lr 1.9511e-03 eta 0:05:57
epoch [7/50] batch [15/23] time 0.200 (0.305) data 0.000 (0.092) loss 2.6325 (3.1744) acc 34.3750 (28.1250) lr 1.9511e-03 eta 0:05:03
epoch [7/50] batch [20/23] time 0.200 (0.282) data 0.000 (0.069) loss 3.8472 (3.2515) acc 18.7500 (27.1875) lr 1.9511e-03 eta 0:04:39
epoch [8/50] batch [5/23] time 0.147 (0.433) data 0.000 (0.229) loss 3.0453 (3.2183) acc 34.3750 (26.2500) lr 1.9298e-03 eta 0:07:05
epoch [8/50] batch [10/23] time 0.160 (0.300) data 0.000 (0.115) loss 3.1883 (3.1810) acc 37.5000 (28.4375) lr 1.9298e-03 eta 0:04:54
epoch [8/50] batch [15/23] time 0.322 (0.282) data 0.000 (0.077) loss 3.9360 (3.1949) acc 9.3750 (27.2917) lr 1.9298e-03 eta 0:04:35
epoch [8/50] batch [20/23] time 0.317 (0.289) data 0.000 (0.057) loss 3.5126 (3.2181) acc 18.7500 (26.4062) lr 1.9298e-03 eta 0:04:40
epoch [9/50] batch [5/23] time 0.152 (0.488) data 0.000 (0.286) loss 3.0809 (3.2438) acc 40.6250 (28.7500) lr 1.9048e-03 eta 0:07:48
epoch [9/50] batch [10/23] time 0.161 (0.328) data 0.000 (0.146) loss 2.8979 (3.2092) acc 31.2500 (27.5000) lr 1.9048e-03 eta 0:05:13
epoch [9/50] batch [15/23] time 0.277 (0.298) data 0.000 (0.097) loss 2.7813 (3.1558) acc 31.2500 (28.3333) lr 1.9048e-03 eta 0:04:43
epoch [9/50] batch [20/23] time 0.258 (0.289) data 0.000 (0.073) loss 3.1130 (3.1722) acc 21.8750 (27.3438) lr 1.9048e-03 eta 0:04:33
epoch [10/50] batch [5/23] time 0.153 (0.516) data 0.000 (0.335) loss 3.2916 (3.2391) acc 21.8750 (26.2500) lr 1.8763e-03 eta 0:08:04
epoch [10/50] batch [10/23] time 0.196 (0.355) data 0.000 (0.168) loss 3.3360 (3.1835) acc 28.1250 (28.1250) lr 1.8763e-03 eta 0:05:31
epoch [10/50] batch [15/23] time 0.190 (0.298) data 0.000 (0.112) loss 3.1568 (3.2190) acc 40.6250 (28.7500) lr 1.8763e-03 eta 0:04:36
epoch [10/50] batch [20/23] time 0.193 (0.272) data 0.000 (0.084) loss 3.2973 (3.2098) acc 25.0000 (29.3750) lr 1.8763e-03 eta 0:04:11
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.993  alpha2: 0.376 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.48 <<<
epoch [11/50] batch [5/23] time 1.200 (1.248) data 0.000 (0.257) loss 2.4007 (2.4172) acc 56.1224 (56.8211) lr 1.8443e-03 eta 0:19:02
epoch [11/50] batch [10/23] time 1.215 (0.902) data 0.000 (0.129) loss 1.8620 (2.3187) acc 74.0000 (55.9605) lr 1.8443e-03 eta 0:13:40
epoch [11/50] batch [15/23] time 1.125 (0.860) data 0.000 (0.086) loss 2.0044 (2.2381) acc 55.4348 (56.3784) lr 1.8443e-03 eta 0:12:58
epoch [11/50] batch [20/23] time 0.182 (0.736) data 0.000 (0.065) loss 2.1452 (2.2329) acc 58.6957 (56.1828) lr 1.8443e-03 eta 0:11:02
>>> alpha1: 0.744  alpha2: 0.334 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.44 <<<
epoch [12/50] batch [5/23] time 0.185 (0.484) data 0.000 (0.291) loss 1.3259 (1.4129) acc 61.4583 (67.5495) lr 1.8090e-03 eta 0:07:11
epoch [12/50] batch [10/23] time 0.174 (0.341) data 0.000 (0.146) loss 1.4165 (1.3707) acc 64.7727 (67.9663) lr 1.8090e-03 eta 0:05:02
epoch [12/50] batch [15/23] time 1.058 (0.345) data 0.000 (0.097) loss 1.7574 (1.4430) acc 53.5714 (64.3203) lr 1.8090e-03 eta 0:05:04
epoch [12/50] batch [20/23] time 0.187 (0.343) data 0.000 (0.073) loss 1.3656 (1.4306) acc 75.0000 (65.1306) lr 1.8090e-03 eta 0:05:00
>>> alpha1: 0.717  alpha2: 0.288 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [13/50] batch [5/23] time 0.156 (0.437) data 0.000 (0.284) loss 1.0672 (1.3074) acc 73.4043 (69.5798) lr 1.7705e-03 eta 0:06:19
epoch [13/50] batch [10/23] time 0.142 (0.288) data 0.000 (0.142) loss 1.4089 (1.3386) acc 62.7660 (67.4674) lr 1.7705e-03 eta 0:04:08
epoch [13/50] batch [15/23] time 0.124 (0.237) data 0.000 (0.096) loss 1.6961 (1.3460) acc 53.2895 (65.4629) lr 1.7705e-03 eta 0:03:23
epoch [13/50] batch [20/23] time 0.134 (0.242) data 0.000 (0.072) loss 1.2132 (1.3587) acc 69.3182 (64.9805) lr 1.7705e-03 eta 0:03:26
>>> alpha1: 0.664  alpha2: 0.259 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [14/50] batch [5/23] time 0.136 (0.569) data 0.000 (0.306) loss 1.0977 (1.3227) acc 71.1956 (65.9106) lr 1.7290e-03 eta 0:08:01
epoch [14/50] batch [10/23] time 0.151 (0.354) data 0.001 (0.153) loss 1.1885 (1.4098) acc 61.9565 (64.7575) lr 1.7290e-03 eta 0:04:57
epoch [14/50] batch [15/23] time 0.133 (0.277) data 0.000 (0.102) loss 1.2208 (1.3500) acc 66.8478 (65.7004) lr 1.7290e-03 eta 0:03:51
epoch [14/50] batch [20/23] time 0.126 (0.240) data 0.000 (0.077) loss 1.3268 (1.3166) acc 64.5349 (65.6465) lr 1.7290e-03 eta 0:03:19
>>> alpha1: 0.618  alpha2: 0.237 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.42 <<<
epoch [15/50] batch [5/23] time 0.154 (0.454) data 0.017 (0.312) loss 1.1734 (1.0898) acc 70.2128 (75.2036) lr 1.6845e-03 eta 0:06:13
epoch [15/50] batch [10/23] time 0.136 (0.362) data 0.000 (0.156) loss 0.9670 (1.1439) acc 67.5532 (70.1339) lr 1.6845e-03 eta 0:04:55
epoch [15/50] batch [15/23] time 0.134 (0.287) data 0.000 (0.104) loss 1.1358 (1.1082) acc 71.6667 (70.2149) lr 1.6845e-03 eta 0:03:53
epoch [15/50] batch [20/23] time 0.140 (0.249) data 0.000 (0.078) loss 0.8172 (1.0849) acc 78.0612 (70.3908) lr 1.6845e-03 eta 0:03:20
>>> alpha1: 0.537  alpha2: 0.178 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.45 <<<
epoch [16/50] batch [5/23] time 0.132 (0.636) data 0.001 (0.353) loss 1.1608 (0.9296) acc 71.0227 (73.3280) lr 1.6374e-03 eta 0:08:28
epoch [16/50] batch [10/23] time 0.141 (0.390) data 0.000 (0.177) loss 1.0898 (0.9735) acc 74.4792 (73.5928) lr 1.6374e-03 eta 0:05:10
epoch [16/50] batch [15/23] time 0.146 (0.308) data 0.000 (0.118) loss 1.3243 (1.0158) acc 61.0000 (71.2985) lr 1.6374e-03 eta 0:04:03
epoch [16/50] batch [20/23] time 0.144 (0.268) data 0.000 (0.088) loss 1.0231 (0.9854) acc 69.8980 (71.3719) lr 1.6374e-03 eta 0:03:30
>>> alpha1: 0.439  alpha2: 0.116 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.42 <<<
epoch [17/50] batch [5/23] time 0.148 (0.546) data 0.000 (0.251) loss 1.2808 (0.9698) acc 73.0769 (76.5235) lr 1.5878e-03 eta 0:07:04
epoch [17/50] batch [10/23] time 0.141 (0.346) data 0.001 (0.126) loss 0.7398 (0.9583) acc 77.0000 (74.2764) lr 1.5878e-03 eta 0:04:27
epoch [17/50] batch [15/23] time 0.147 (0.279) data 0.000 (0.084) loss 0.9320 (0.9908) acc 72.5962 (72.8939) lr 1.5878e-03 eta 0:03:33
epoch [17/50] batch [20/23] time 0.148 (0.281) data 0.000 (0.063) loss 0.7445 (0.9590) acc 78.3654 (73.3223) lr 1.5878e-03 eta 0:03:34
>>> alpha1: 0.382  alpha2: 0.092 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.43 <<<
epoch [18/50] batch [5/23] time 0.154 (0.471) data 0.000 (0.312) loss 1.0026 (0.8237) acc 73.9362 (77.2505) lr 1.5358e-03 eta 0:05:54
epoch [18/50] batch [10/23] time 0.147 (0.311) data 0.000 (0.156) loss 0.7150 (0.8534) acc 78.0000 (75.3587) lr 1.5358e-03 eta 0:03:53
epoch [18/50] batch [15/23] time 0.143 (0.255) data 0.000 (0.104) loss 0.8880 (0.8267) acc 57.1429 (74.9422) lr 1.5358e-03 eta 0:03:09
epoch [18/50] batch [20/23] time 0.131 (0.226) data 0.000 (0.078) loss 0.6029 (0.8365) acc 78.9773 (75.3155) lr 1.5358e-03 eta 0:02:47
>>> alpha1: 0.344  alpha2: 0.075 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.40 <<<
epoch [19/50] batch [5/23] time 0.146 (0.393) data 0.000 (0.251) loss 0.7313 (0.6759) acc 80.6122 (81.3987) lr 1.4818e-03 eta 0:04:47
epoch [19/50] batch [10/23] time 0.132 (0.265) data 0.000 (0.126) loss 0.6886 (0.7878) acc 81.9767 (76.4479) lr 1.4818e-03 eta 0:03:12
epoch [19/50] batch [15/23] time 0.141 (0.222) data 0.000 (0.084) loss 0.7793 (0.7893) acc 77.5510 (76.2364) lr 1.4818e-03 eta 0:02:40
epoch [19/50] batch [20/23] time 0.128 (0.199) data 0.000 (0.063) loss 0.5565 (0.7893) acc 82.3864 (76.5249) lr 1.4818e-03 eta 0:02:22
>>> alpha1: 0.321  alpha2: 0.067 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.37 <<<
epoch [20/50] batch [5/23] time 0.143 (0.406) data 0.000 (0.261) loss 0.6729 (0.6700) acc 86.9048 (83.4709) lr 1.4258e-03 eta 0:04:47
epoch [20/50] batch [10/23] time 0.142 (0.273) data 0.000 (0.131) loss 0.8252 (0.7628) acc 73.2955 (79.1207) lr 1.4258e-03 eta 0:03:11
epoch [20/50] batch [15/23] time 0.135 (0.227) data 0.000 (0.087) loss 0.7995 (0.7629) acc 77.9070 (78.7168) lr 1.4258e-03 eta 0:02:38
epoch [20/50] batch [20/23] time 0.127 (0.203) data 0.000 (0.065) loss 0.8895 (0.7516) acc 66.4773 (78.2570) lr 1.4258e-03 eta 0:02:20
>>> alpha1: 0.288  alpha2: 0.049 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.42 <<<
epoch [21/50] batch [5/23] time 0.144 (0.600) data 0.000 (0.301) loss 1.0412 (0.7006) acc 71.4286 (80.2808) lr 1.3681e-03 eta 0:06:50
epoch [21/50] batch [10/23] time 0.139 (0.370) data 0.000 (0.150) loss 1.1192 (0.6883) acc 62.7660 (79.7139) lr 1.3681e-03 eta 0:04:11
epoch [21/50] batch [15/23] time 0.151 (0.296) data 0.000 (0.100) loss 0.6423 (0.7283) acc 84.9057 (78.3650) lr 1.3681e-03 eta 0:03:19
epoch [21/50] batch [20/23] time 0.162 (0.298) data 0.000 (0.075) loss 1.1188 (0.7496) acc 66.6667 (77.1122) lr 1.3681e-03 eta 0:03:19
>>> alpha1: 0.276  alpha2: 0.047 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [22/50] batch [5/23] time 0.139 (0.478) data 0.000 (0.320) loss 0.6420 (0.6877) acc 79.6875 (80.5555) lr 1.3090e-03 eta 0:05:16
epoch [22/50] batch [10/23] time 0.149 (0.316) data 0.001 (0.160) loss 0.8483 (0.7204) acc 69.6078 (78.8295) lr 1.3090e-03 eta 0:03:27
epoch [22/50] batch [15/23] time 0.157 (0.261) data 0.000 (0.107) loss 0.4823 (0.6996) acc 89.8148 (80.1081) lr 1.3090e-03 eta 0:02:50
epoch [22/50] batch [20/23] time 0.138 (0.232) data 0.000 (0.080) loss 0.8670 (0.7108) acc 73.4375 (79.0064) lr 1.3090e-03 eta 0:02:30
>>> alpha1: 0.264  alpha2: 0.038 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.39 <<<
epoch [23/50] batch [5/23] time 0.158 (0.453) data 0.000 (0.302) loss 0.7817 (0.6648) acc 76.5000 (81.8785) lr 1.2487e-03 eta 0:04:49
epoch [23/50] batch [10/23] time 0.147 (0.299) data 0.000 (0.151) loss 0.6924 (0.7123) acc 77.0408 (80.0070) lr 1.2487e-03 eta 0:03:09
epoch [23/50] batch [15/23] time 0.155 (0.246) data 0.000 (0.101) loss 0.6219 (0.6875) acc 80.9091 (80.0793) lr 1.2487e-03 eta 0:02:34
epoch [23/50] batch [20/23] time 0.137 (0.219) data 0.001 (0.076) loss 0.5787 (0.6722) acc 77.1739 (79.5241) lr 1.2487e-03 eta 0:02:16
>>> alpha1: 0.257  alpha2: 0.036 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.43 <<<
epoch [24/50] batch [5/23] time 0.149 (0.468) data 0.000 (0.312) loss 0.5214 (0.5887) acc 89.6226 (85.5365) lr 1.1874e-03 eta 0:04:48
epoch [24/50] batch [10/23] time 0.146 (0.308) data 0.000 (0.156) loss 0.6405 (0.6293) acc 84.1837 (83.0307) lr 1.1874e-03 eta 0:03:08
epoch [24/50] batch [15/23] time 0.149 (0.254) data 0.000 (0.104) loss 0.4870 (0.6351) acc 84.6154 (82.4180) lr 1.1874e-03 eta 0:02:33
epoch [24/50] batch [20/23] time 0.150 (0.226) data 0.000 (0.078) loss 0.8875 (0.6469) acc 73.0769 (82.1730) lr 1.1874e-03 eta 0:02:16
>>> alpha1: 0.244  alpha2: 0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.38 <<<
epoch [25/50] batch [5/23] time 0.165 (0.494) data 0.000 (0.332) loss 0.5099 (0.5763) acc 81.8627 (83.9567) lr 1.1253e-03 eta 0:04:52
epoch [25/50] batch [10/23] time 0.140 (0.313) data 0.000 (0.166) loss 0.4835 (0.6377) acc 82.8125 (82.4552) lr 1.1253e-03 eta 0:03:04
epoch [25/50] batch [15/23] time 0.136 (0.255) data 0.000 (0.112) loss 0.6727 (0.6339) acc 82.8125 (82.7500) lr 1.1253e-03 eta 0:02:28
epoch [25/50] batch [20/23] time 0.133 (0.226) data 0.000 (0.084) loss 0.6893 (0.6296) acc 77.7778 (82.5963) lr 1.1253e-03 eta 0:02:10
>>> alpha1: 0.234  alpha2: 0.015 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.40 <<<
epoch [26/50] batch [5/23] time 0.157 (0.416) data 0.001 (0.258) loss 0.6690 (0.5793) acc 82.8125 (83.5779) lr 1.0628e-03 eta 0:03:56
epoch [26/50] batch [10/23] time 0.138 (0.279) data 0.001 (0.129) loss 0.5739 (0.5589) acc 82.2917 (84.3450) lr 1.0628e-03 eta 0:02:37
epoch [26/50] batch [15/23] time 0.137 (0.232) data 0.000 (0.086) loss 0.6983 (0.5934) acc 82.9787 (83.6279) lr 1.0628e-03 eta 0:02:10
epoch [26/50] batch [20/23] time 0.143 (0.208) data 0.001 (0.065) loss 0.4428 (0.5876) acc 89.2157 (83.7988) lr 1.0628e-03 eta 0:01:55
>>> alpha1: 0.219  alpha2: 0.008 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.40 <<<
epoch [27/50] batch [5/23] time 0.139 (0.396) data 0.000 (0.255) loss 0.6984 (0.5902) acc 78.6458 (82.0261) lr 1.0000e-03 eta 0:03:36
epoch [27/50] batch [10/23] time 0.156 (0.269) data 0.000 (0.128) loss 0.3648 (0.5662) acc 89.7959 (83.3030) lr 1.0000e-03 eta 0:02:25
epoch [27/50] batch [15/23] time 0.144 (0.224) data 0.000 (0.085) loss 0.4497 (0.5624) acc 90.3061 (83.8102) lr 1.0000e-03 eta 0:02:00
epoch [27/50] batch [20/23] time 0.135 (0.203) data 0.001 (0.064) loss 0.5287 (0.5684) acc 82.2222 (83.8737) lr 1.0000e-03 eta 0:01:48
>>> alpha1: 0.205  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.42 <<<
epoch [28/50] batch [5/23] time 0.181 (0.444) data 0.000 (0.284) loss 0.6183 (0.5733) acc 81.3830 (82.2086) lr 9.3721e-04 eta 0:03:52
epoch [28/50] batch [10/23] time 0.144 (0.294) data 0.000 (0.142) loss 0.7029 (0.5807) acc 77.5510 (81.5987) lr 9.3721e-04 eta 0:02:32
epoch [28/50] batch [15/23] time 0.134 (0.242) data 0.000 (0.095) loss 0.5328 (0.5748) acc 83.1522 (82.0233) lr 9.3721e-04 eta 0:02:04
epoch [28/50] batch [20/23] time 0.153 (0.218) data 0.000 (0.071) loss 0.4828 (0.5658) acc 84.9057 (83.0585) lr 9.3721e-04 eta 0:01:50
>>> alpha1: 0.191  alpha2: -0.010 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.37 <<<
epoch [29/50] batch [5/23] time 0.157 (0.458) data 0.000 (0.313) loss 0.5254 (0.4479) acc 88.8298 (87.1768) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [10/23] time 0.141 (0.300) data 0.000 (0.157) loss 0.5231 (0.5148) acc 85.6383 (86.3068) lr 8.7467e-04 eta 0:02:28
epoch [29/50] batch [15/23] time 0.122 (0.243) data 0.000 (0.105) loss 0.6149 (0.5519) acc 81.5476 (85.2340) lr 8.7467e-04 eta 0:01:59
epoch [29/50] batch [20/23] time 0.137 (0.214) data 0.000 (0.079) loss 0.7459 (0.5642) acc 81.3830 (84.6973) lr 8.7467e-04 eta 0:01:44
>>> alpha1: 0.182  alpha2: -0.008 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.38 <<<
epoch [30/50] batch [5/23] time 0.134 (0.424) data 0.000 (0.277) loss 0.5479 (0.4354) acc 89.4445 (88.8874) lr 8.1262e-04 eta 0:03:22
epoch [30/50] batch [10/23] time 0.117 (0.279) data 0.000 (0.139) loss 0.5908 (0.5057) acc 76.2500 (85.8889) lr 8.1262e-04 eta 0:02:11
epoch [30/50] batch [15/23] time 0.127 (0.231) data 0.000 (0.092) loss 0.5858 (0.5134) acc 87.2093 (85.9456) lr 8.1262e-04 eta 0:01:48
epoch [30/50] batch [20/23] time 0.136 (0.207) data 0.000 (0.069) loss 0.4391 (0.5151) acc 87.7660 (85.8747) lr 8.1262e-04 eta 0:01:35
>>> alpha1: 0.175  alpha2: -0.009 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [31/50] batch [5/23] time 0.174 (0.426) data 0.000 (0.262) loss 0.4493 (0.4650) acc 86.8421 (85.3254) lr 7.5131e-04 eta 0:03:13
epoch [31/50] batch [10/23] time 0.155 (0.286) data 0.000 (0.131) loss 0.4858 (0.4885) acc 89.0909 (85.5236) lr 7.5131e-04 eta 0:02:08
epoch [31/50] batch [15/23] time 0.147 (0.239) data 0.000 (0.087) loss 0.3762 (0.4889) acc 91.3462 (86.2557) lr 7.5131e-04 eta 0:01:46
epoch [31/50] batch [20/23] time 0.156 (0.215) data 0.000 (0.066) loss 0.5002 (0.4975) acc 87.9630 (85.8972) lr 7.5131e-04 eta 0:01:34
>>> alpha1: 0.172  alpha2: -0.004 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [32/50] batch [5/23] time 0.152 (0.472) data 0.000 (0.312) loss 0.6319 (0.4735) acc 81.7308 (86.5062) lr 6.9098e-04 eta 0:03:24
epoch [32/50] batch [10/23] time 0.159 (0.384) data 0.000 (0.156) loss 0.7835 (0.4946) acc 81.5789 (85.9556) lr 6.9098e-04 eta 0:02:44
epoch [32/50] batch [15/23] time 0.145 (0.305) data 0.000 (0.104) loss 0.5799 (0.5100) acc 82.0000 (86.0622) lr 6.9098e-04 eta 0:02:08
epoch [32/50] batch [20/23] time 0.135 (0.263) data 0.000 (0.078) loss 0.6264 (0.5106) acc 79.2553 (86.5309) lr 6.9098e-04 eta 0:01:49
>>> alpha1: 0.168  alpha2: -0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [33/50] batch [5/23] time 0.146 (0.444) data 0.003 (0.278) loss 0.6211 (0.4801) acc 84.6939 (85.3758) lr 6.3188e-04 eta 0:03:01
epoch [33/50] batch [10/23] time 0.147 (0.295) data 0.001 (0.139) loss 0.3746 (0.4553) acc 87.7551 (87.0242) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [15/23] time 0.143 (0.244) data 0.000 (0.093) loss 0.6559 (0.4904) acc 85.0000 (86.6969) lr 6.3188e-04 eta 0:01:37
epoch [33/50] batch [20/23] time 0.155 (0.221) data 0.000 (0.070) loss 0.4420 (0.5042) acc 84.7222 (85.7448) lr 6.3188e-04 eta 0:01:26
>>> alpha1: 0.166  alpha2: 0.003 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [34/50] batch [5/23] time 0.157 (0.452) data 0.000 (0.294) loss 0.6368 (0.4836) acc 86.2745 (86.0426) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [10/23] time 0.157 (0.305) data 0.000 (0.147) loss 0.4545 (0.4634) acc 84.0000 (86.5175) lr 5.7422e-04 eta 0:01:56
epoch [34/50] batch [15/23] time 0.146 (0.252) data 0.000 (0.098) loss 0.3163 (0.4581) acc 92.3077 (87.0607) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [20/23] time 0.133 (0.225) data 0.000 (0.074) loss 0.6074 (0.4861) acc 82.7778 (85.7395) lr 5.7422e-04 eta 0:01:23
>>> alpha1: 0.163  alpha2: 0.002 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [35/50] batch [5/23] time 0.165 (0.463) data 0.000 (0.302) loss 0.4551 (0.4344) acc 89.9038 (88.1395) lr 5.1825e-04 eta 0:02:48
epoch [35/50] batch [10/23] time 0.139 (0.302) data 0.000 (0.151) loss 0.4604 (0.4294) acc 83.3333 (86.5195) lr 5.1825e-04 eta 0:01:48
epoch [35/50] batch [15/23] time 0.148 (0.251) data 0.000 (0.101) loss 0.5477 (0.4366) acc 88.2353 (87.4655) lr 5.1825e-04 eta 0:01:28
epoch [35/50] batch [20/23] time 0.155 (0.224) data 0.000 (0.076) loss 0.3163 (0.4309) acc 88.1818 (87.8718) lr 5.1825e-04 eta 0:01:18
>>> alpha1: 0.159  alpha2: -0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.44 <<<
epoch [36/50] batch [5/23] time 0.143 (0.458) data 0.000 (0.294) loss 0.3281 (0.4588) acc 91.8367 (86.3396) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [10/23] time 0.138 (0.300) data 0.000 (0.147) loss 0.3897 (0.6720) acc 89.8936 (85.1189) lr 4.6417e-04 eta 0:01:40
epoch [36/50] batch [15/23] time 0.156 (0.250) data 0.000 (0.098) loss 0.4611 (0.6093) acc 88.1818 (85.5499) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [20/23] time 0.135 (0.223) data 0.000 (0.074) loss 0.5235 (0.5679) acc 84.0425 (86.0988) lr 4.6417e-04 eta 0:01:12
>>> alpha1: 0.157  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.41 <<<
epoch [37/50] batch [5/23] time 0.136 (0.441) data 0.000 (0.290) loss 0.6297 (0.4706) acc 83.6956 (87.6587) lr 4.1221e-04 eta 0:02:19
epoch [37/50] batch [10/23] time 0.145 (0.293) data 0.000 (0.145) loss 0.4382 (0.4737) acc 86.2245 (86.7450) lr 4.1221e-04 eta 0:01:31
epoch [37/50] batch [15/23] time 0.136 (0.241) data 0.000 (0.097) loss 0.3636 (0.4565) acc 93.7500 (87.6331) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [20/23] time 0.146 (0.217) data 0.000 (0.073) loss 0.6040 (0.4606) acc 86.0000 (87.6841) lr 4.1221e-04 eta 0:01:05
>>> alpha1: 0.156  alpha2: 0.003 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.43 <<<
epoch [38/50] batch [5/23] time 0.158 (0.485) data 0.000 (0.333) loss 0.3600 (0.3935) acc 87.9630 (86.9121) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [10/23] time 0.150 (0.316) data 0.000 (0.167) loss 0.6470 (0.4216) acc 81.3726 (87.9023) lr 3.6258e-04 eta 0:01:31
epoch [38/50] batch [15/23] time 0.146 (0.259) data 0.000 (0.111) loss 0.4604 (0.4298) acc 86.5385 (87.7842) lr 3.6258e-04 eta 0:01:13
epoch [38/50] batch [20/23] time 0.142 (0.230) data 0.000 (0.083) loss 0.5158 (0.4470) acc 84.3137 (87.2015) lr 3.6258e-04 eta 0:01:04
>>> alpha1: 0.152  alpha2: 0.010 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.44 <<<
epoch [39/50] batch [5/23] time 0.171 (0.416) data 0.000 (0.264) loss 0.5117 (0.4399) acc 86.5000 (86.3317) lr 3.1545e-04 eta 0:01:52
epoch [39/50] batch [10/23] time 0.150 (0.283) data 0.000 (0.132) loss 0.4135 (0.4251) acc 88.4615 (88.2065) lr 3.1545e-04 eta 0:01:15
epoch [39/50] batch [15/23] time 0.153 (0.240) data 0.000 (0.088) loss 0.4552 (0.4128) acc 87.7273 (89.1173) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [20/23] time 0.140 (0.216) data 0.000 (0.066) loss 0.5002 (0.4222) acc 88.0208 (88.1944) lr 3.1545e-04 eta 0:00:55
>>> alpha1: 0.148  alpha2: 0.012 <<<
>>> noisy rate: 0.48 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.42 <<<
epoch [40/50] batch [5/23] time 0.144 (0.481) data 0.000 (0.329) loss 0.4960 (0.4099) acc 89.7059 (90.6763) lr 2.7103e-04 eta 0:01:59
epoch [40/50] batch [10/23] time 0.156 (0.314) data 0.000 (0.165) loss 0.3491 (0.3907) acc 92.7273 (89.6789) lr 2.7103e-04 eta 0:01:16
epoch [40/50] batch [15/23] time 0.131 (0.258) data 0.000 (0.110) loss 0.4836 (0.3921) acc 85.5556 (89.7797) lr 2.7103e-04 eta 0:01:01
epoch [40/50] batch [20/23] time 0.147 (0.230) data 0.000 (0.083) loss 0.4261 (0.4030) acc 90.0000 (89.7512) lr 2.7103e-04 eta 0:00:53
>>> alpha1: 0.151  alpha2: 0.015 <<<
>>> noisy rate: 0.48 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.44 <<<
epoch [41/50] batch [5/23] time 0.166 (0.420) data 0.001 (0.263) loss 0.3730 (0.3733) acc 88.5417 (90.0751) lr 2.2949e-04 eta 0:01:34
epoch [41/50] batch [10/23] time 0.146 (0.286) data 0.001 (0.132) loss 0.4114 (0.3841) acc 90.6863 (88.8132) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [15/23] time 0.148 (0.239) data 0.000 (0.088) loss 0.5071 (0.3914) acc 86.7647 (89.4048) lr 2.2949e-04 eta 0:00:51
epoch [41/50] batch [20/23] time 0.133 (0.215) data 0.001 (0.066) loss 0.4905 (0.3834) acc 85.8696 (89.9238) lr 2.2949e-04 eta 0:00:45
>>> alpha1: 0.151  alpha2: 0.017 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.45 <<<
epoch [42/50] batch [5/23] time 0.157 (0.472) data 0.000 (0.319) loss 0.3540 (0.4032) acc 92.0000 (90.6448) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [10/23] time 0.163 (0.312) data 0.000 (0.160) loss 0.2909 (0.4034) acc 94.6078 (90.2171) lr 1.9098e-04 eta 0:01:01
epoch [42/50] batch [15/23] time 0.145 (0.255) data 0.001 (0.106) loss 0.4130 (0.4103) acc 87.7451 (89.3433) lr 1.9098e-04 eta 0:00:48
epoch [42/50] batch [20/23] time 0.152 (0.228) data 0.000 (0.080) loss 0.4479 (0.4121) acc 88.2075 (88.8202) lr 1.9098e-04 eta 0:00:42
>>> alpha1: 0.150  alpha2: 0.018 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.44 <<<
epoch [43/50] batch [5/23] time 0.181 (0.433) data 0.000 (0.275) loss 0.2374 (0.3191) acc 93.8679 (90.0263) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [10/23] time 0.149 (0.295) data 0.000 (0.138) loss 0.4034 (0.3663) acc 86.7647 (89.3333) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [15/23] time 0.149 (0.244) data 0.000 (0.092) loss 0.3546 (0.3861) acc 91.8269 (89.6889) lr 1.5567e-04 eta 0:00:41
epoch [43/50] batch [20/23] time 0.144 (0.218) data 0.001 (0.069) loss 0.3868 (0.3905) acc 93.0000 (89.8576) lr 1.5567e-04 eta 0:00:35
>>> alpha1: 0.150  alpha2: 0.014 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.43 <<<
epoch [44/50] batch [5/23] time 0.137 (0.435) data 0.000 (0.283) loss 0.2766 (0.4316) acc 93.0851 (89.0851) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [10/23] time 0.146 (0.291) data 0.000 (0.142) loss 0.4030 (0.6563) acc 92.5000 (85.9687) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [15/23] time 0.147 (0.242) data 0.000 (0.094) loss 0.2298 (0.5513) acc 94.0000 (86.3131) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [20/23] time 0.141 (0.217) data 0.000 (0.071) loss 0.3686 (0.5060) acc 95.8333 (87.4279) lr 1.2369e-04 eta 0:00:30
>>> alpha1: 0.152  alpha2: 0.020 <<<
>>> noisy rate: 0.48 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.39 <<<
epoch [45/50] batch [5/23] time 0.156 (0.449) data 0.000 (0.314) loss 0.2873 (0.4107) acc 94.4445 (88.6760) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [10/23] time 0.134 (0.294) data 0.000 (0.157) loss 0.5410 (0.4292) acc 89.1304 (89.7426) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [15/23] time 0.123 (0.243) data 0.000 (0.105) loss 0.5976 (0.4164) acc 76.1905 (89.2880) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [20/23] time 0.142 (0.217) data 0.000 (0.079) loss 0.3279 (0.4074) acc 94.5000 (89.8701) lr 9.5173e-05 eta 0:00:25
>>> alpha1: 0.148  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.39 <<<
epoch [46/50] batch [5/23] time 0.167 (0.415) data 0.000 (0.260) loss 0.3765 (0.3741) acc 84.6939 (90.3532) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [10/23] time 0.145 (0.279) data 0.000 (0.130) loss 0.3872 (0.3698) acc 89.7059 (90.5298) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [15/23] time 0.144 (0.232) data 0.000 (0.087) loss 0.4175 (0.3718) acc 88.7755 (90.4927) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/23] time 0.144 (0.209) data 0.000 (0.065) loss 0.3662 (0.3839) acc 87.5000 (90.3002) lr 7.0224e-05 eta 0:00:19
>>> alpha1: 0.147  alpha2: 0.011 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.38 <<<
epoch [47/50] batch [5/23] time 0.154 (0.426) data 0.000 (0.283) loss 0.3192 (0.4414) acc 92.9348 (91.2846) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [10/23] time 0.136 (0.286) data 0.000 (0.142) loss 0.5598 (0.4133) acc 85.8696 (90.7515) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/23] time 0.131 (0.237) data 0.000 (0.095) loss 0.3564 (0.4054) acc 89.7727 (90.0463) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.143 (0.213) data 0.000 (0.071) loss 0.4412 (0.4169) acc 84.0000 (89.1454) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.146  alpha2: 0.012 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [48/50] batch [5/23] time 0.158 (0.408) data 0.000 (0.253) loss 0.6457 (0.4223) acc 82.6087 (88.4424) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [10/23] time 0.174 (0.286) data 0.000 (0.127) loss 0.3535 (0.3933) acc 92.9245 (89.9197) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [15/23] time 0.142 (0.236) data 0.000 (0.085) loss 0.4112 (0.4018) acc 85.2041 (87.9761) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/23] time 0.147 (0.213) data 0.000 (0.064) loss 0.4619 (0.3891) acc 87.5000 (88.8049) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.145  alpha2: 0.010 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.39 <<<
epoch [49/50] batch [5/23] time 0.129 (0.419) data 0.000 (0.282) loss 0.4223 (0.4285) acc 86.9318 (88.2869) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.126 (0.281) data 0.000 (0.141) loss 0.4091 (0.4198) acc 89.5349 (89.4956) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.147 (0.234) data 0.000 (0.094) loss 0.3253 (0.3912) acc 92.4528 (89.5282) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.154 (0.210) data 0.000 (0.071) loss 0.3863 (0.3936) acc 91.8182 (89.4938) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.143  alpha2: 0.001 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [50/50] batch [5/23] time 0.149 (0.409) data 0.000 (0.259) loss 0.3554 (0.3366) acc 94.4445 (92.5733) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.147 (0.278) data 0.000 (0.129) loss 0.2822 (0.3445) acc 95.1923 (91.6866) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/23] time 0.143 (0.234) data 0.000 (0.086) loss 0.3143 (0.3561) acc 96.4286 (91.3802) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/23] time 0.142 (0.210) data 0.000 (0.065) loss 0.3923 (0.3671) acc 91.0000 (90.6838) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.33, 0.32, 0.3, 0.3, 0.29, 0.3, 0.29, 0.3, 0.3, 0.28, 0.28, 0.29, 0.29, 0.29, 0.28, 0.3, 0.3, 0.3, 0.29, 0.3, 0.29, 0.3, 0.3, 0.3, 0.3, 0.3, 0.29, 0.3, 0.3, 0.29, 0.29, 0.3, 0.29, 0.3, 0.3, 0.3, 0.3, 0.3, 0.29, 0.3]
* matched noise rate: [0.13, 0.15, 0.12, 0.12, 0.14, 0.18, 0.18, 0.17, 0.16, 0.16, 0.18, 0.19, 0.18, 0.17, 0.17, 0.18, 0.18, 0.18, 0.19, 0.2, 0.19, 0.2, 0.2, 0.2, 0.19, 0.2, 0.19, 0.2, 0.2, 0.2, 0.18, 0.18, 0.17, 0.17, 0.19, 0.19, 0.2, 0.19, 0.2, 0.19]
* unmatched noise rate: [0.48, 0.44, 0.4, 0.41, 0.42, 0.45, 0.42, 0.43, 0.4, 0.37, 0.42, 0.43, 0.39, 0.43, 0.38, 0.4, 0.4, 0.42, 0.37, 0.38, 0.44, 0.47, 0.45, 0.45, 0.43, 0.44, 0.41, 0.43, 0.44, 0.42, 0.44, 0.45, 0.44, 0.43, 0.39, 0.39, 0.38, 0.43, 0.39, 0.43]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:35,  2.23s/it] 18%|█▊        | 3/17 [00:02<00:08,  1.59it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.93it/s] 41%|████      | 7/17 [00:02<00:02,  4.43it/s] 53%|█████▎    | 9/17 [00:02<00:01,  6.01it/s] 65%|██████▍   | 11/17 [00:02<00:00,  7.58it/s] 76%|███████▋  | 13/17 [00:03<00:00,  9.03it/s] 88%|████████▊ | 15/17 [00:03<00:00, 10.31it/s]100%|██████████| 17/17 [00:03<00:00,  6.86it/s]100%|██████████| 17/17 [00:03<00:00,  4.38it/s]
=> result
* total: 1,692
* correct: 974
* accuracy: 57.6%
* error: 42.4%
* macro_f1: 55.9%
=> per-class result
* class: 0 (banded)	total: 36	correct: 24	acc: 66.7%
* class: 1 (blotchy)	total: 36	correct: 6	acc: 16.7%
* class: 2 (braided)	total: 36	correct: 12	acc: 33.3%
* class: 3 (bubbly)	total: 36	correct: 31	acc: 86.1%
* class: 4 (bumpy)	total: 36	correct: 8	acc: 22.2%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 30	acc: 83.3%
* class: 7 (cracked)	total: 36	correct: 21	acc: 58.3%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 13	acc: 36.1%
* class: 11 (fibrous)	total: 36	correct: 31	acc: 86.1%
* class: 12 (flecked)	total: 36	correct: 16	acc: 44.4%
* class: 13 (freckled)	total: 36	correct: 28	acc: 77.8%
* class: 14 (frilly)	total: 36	correct: 30	acc: 83.3%
* class: 15 (gauzy)	total: 36	correct: 22	acc: 61.1%
* class: 16 (grid)	total: 36	correct: 11	acc: 30.6%
* class: 17 (grooved)	total: 36	correct: 13	acc: 36.1%
* class: 18 (honeycombed)	total: 36	correct: 22	acc: 61.1%
* class: 19 (interlaced)	total: 36	correct: 25	acc: 69.4%
* class: 20 (knitted)	total: 36	correct: 33	acc: 91.7%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 16	acc: 44.4%
* class: 23 (marbled)	total: 36	correct: 17	acc: 47.2%
* class: 24 (matted)	total: 36	correct: 17	acc: 47.2%
* class: 25 (meshed)	total: 36	correct: 18	acc: 50.0%
* class: 26 (paisley)	total: 36	correct: 35	acc: 97.2%
* class: 27 (perforated)	total: 36	correct: 19	acc: 52.8%
* class: 28 (pitted)	total: 36	correct: 13	acc: 36.1%
* class: 29 (pleated)	total: 36	correct: 16	acc: 44.4%
* class: 30 (polka-dotted)	total: 36	correct: 33	acc: 91.7%
* class: 31 (porous)	total: 36	correct: 4	acc: 11.1%
* class: 32 (potholed)	total: 36	correct: 28	acc: 77.8%
* class: 33 (scaly)	total: 36	correct: 20	acc: 55.6%
* class: 34 (smeared)	total: 36	correct: 16	acc: 44.4%
* class: 35 (spiralled)	total: 36	correct: 24	acc: 66.7%
* class: 36 (sprinkled)	total: 36	correct: 17	acc: 47.2%
* class: 37 (stained)	total: 36	correct: 16	acc: 44.4%
* class: 38 (stratified)	total: 36	correct: 27	acc: 75.0%
* class: 39 (striped)	total: 36	correct: 27	acc: 75.0%
* class: 40 (studded)	total: 36	correct: 27	acc: 75.0%
* class: 41 (swirly)	total: 36	correct: 20	acc: 55.6%
* class: 42 (veined)	total: 36	correct: 21	acc: 58.3%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 14	acc: 38.9%
* class: 45 (wrinkled)	total: 36	correct: 23	acc: 63.9%
* class: 46 (zigzagged)	total: 36	correct: 27	acc: 75.0%
* average: 57.6%
Elapsed: 0:13:12
Run this job and save the output to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_8-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.146 (0.926) data 0.000 (0.260) loss 3.6152 (3.7776) acc 15.6250 (12.5000) lr 1.0000e-05 eta 0:17:40
epoch [1/50] batch [10/23] time 0.178 (0.546) data 0.000 (0.130) loss 3.6310 (3.7617) acc 15.6250 (11.5625) lr 1.0000e-05 eta 0:10:21
epoch [1/50] batch [15/23] time 0.146 (0.413) data 0.000 (0.087) loss 3.5357 (3.7237) acc 21.8750 (11.4583) lr 1.0000e-05 eta 0:07:48
epoch [1/50] batch [20/23] time 0.149 (0.346) data 0.000 (0.065) loss 3.5592 (3.7009) acc 18.7500 (11.4062) lr 1.0000e-05 eta 0:06:31
epoch [2/50] batch [5/23] time 0.150 (0.448) data 0.000 (0.271) loss 3.4459 (3.5725) acc 18.7500 (10.0000) lr 2.0000e-03 eta 0:08:22
epoch [2/50] batch [10/23] time 0.160 (0.306) data 0.000 (0.136) loss 3.3867 (3.5568) acc 21.8750 (12.8125) lr 2.0000e-03 eta 0:05:41
epoch [2/50] batch [15/23] time 0.145 (0.252) data 0.000 (0.091) loss 3.4111 (3.5442) acc 18.7500 (14.7917) lr 2.0000e-03 eta 0:04:40
epoch [2/50] batch [20/23] time 0.148 (0.226) data 0.000 (0.068) loss 3.5647 (3.5087) acc 15.6250 (15.4688) lr 2.0000e-03 eta 0:04:10
epoch [3/50] batch [5/23] time 0.200 (0.459) data 0.000 (0.269) loss 3.6726 (3.3881) acc 9.3750 (17.5000) lr 1.9980e-03 eta 0:08:24
epoch [3/50] batch [10/23] time 0.149 (0.311) data 0.000 (0.135) loss 3.6269 (3.4992) acc 21.8750 (16.2500) lr 1.9980e-03 eta 0:05:40
epoch [3/50] batch [15/23] time 0.149 (0.257) data 0.000 (0.090) loss 3.2442 (3.4402) acc 28.1250 (19.3750) lr 1.9980e-03 eta 0:04:39
epoch [3/50] batch [20/23] time 0.148 (0.230) data 0.000 (0.068) loss 3.5376 (3.4281) acc 15.6250 (19.5312) lr 1.9980e-03 eta 0:04:08
epoch [4/50] batch [5/23] time 0.188 (0.442) data 0.000 (0.257) loss 3.1884 (3.2478) acc 18.7500 (23.7500) lr 1.9921e-03 eta 0:07:56
epoch [4/50] batch [10/23] time 0.144 (0.303) data 0.000 (0.129) loss 3.5644 (3.3081) acc 25.0000 (23.4375) lr 1.9921e-03 eta 0:05:24
epoch [4/50] batch [15/23] time 0.144 (0.250) data 0.000 (0.086) loss 3.3832 (3.3307) acc 15.6250 (21.8750) lr 1.9921e-03 eta 0:04:26
epoch [4/50] batch [20/23] time 0.144 (0.223) data 0.000 (0.064) loss 3.3148 (3.3450) acc 25.0000 (21.4062) lr 1.9921e-03 eta 0:03:56
epoch [5/50] batch [5/23] time 0.156 (0.454) data 0.000 (0.276) loss 3.3117 (3.2076) acc 21.8750 (26.2500) lr 1.9823e-03 eta 0:07:58
epoch [5/50] batch [10/23] time 0.158 (0.307) data 0.000 (0.138) loss 3.3199 (3.2810) acc 25.0000 (25.9375) lr 1.9823e-03 eta 0:05:22
epoch [5/50] batch [15/23] time 0.146 (0.254) data 0.000 (0.092) loss 3.2777 (3.3356) acc 18.7500 (23.1250) lr 1.9823e-03 eta 0:04:24
epoch [5/50] batch [20/23] time 0.148 (0.227) data 0.000 (0.069) loss 3.6651 (3.3337) acc 9.3750 (22.0312) lr 1.9823e-03 eta 0:03:55
epoch [6/50] batch [5/23] time 0.234 (0.441) data 0.000 (0.254) loss 3.3544 (3.3631) acc 21.8750 (21.8750) lr 1.9686e-03 eta 0:07:33
epoch [6/50] batch [10/23] time 0.163 (0.302) data 0.000 (0.127) loss 3.2499 (3.2906) acc 28.1250 (23.1250) lr 1.9686e-03 eta 0:05:09
epoch [6/50] batch [15/23] time 0.145 (0.250) data 0.000 (0.085) loss 3.2082 (3.2926) acc 28.1250 (22.2917) lr 1.9686e-03 eta 0:04:15
epoch [6/50] batch [20/23] time 0.149 (0.224) data 0.000 (0.064) loss 3.6917 (3.2807) acc 12.5000 (22.6562) lr 1.9686e-03 eta 0:03:47
epoch [7/50] batch [5/23] time 0.160 (0.447) data 0.000 (0.262) loss 3.7954 (3.3201) acc 18.7500 (25.6250) lr 1.9511e-03 eta 0:07:30
epoch [7/50] batch [10/23] time 0.164 (0.306) data 0.000 (0.131) loss 3.5874 (3.2978) acc 18.7500 (24.6875) lr 1.9511e-03 eta 0:05:06
epoch [7/50] batch [15/23] time 0.147 (0.253) data 0.000 (0.087) loss 3.0870 (3.2555) acc 21.8750 (24.1667) lr 1.9511e-03 eta 0:04:12
epoch [7/50] batch [20/23] time 0.146 (0.226) data 0.000 (0.066) loss 2.9638 (3.2492) acc 37.5000 (24.8438) lr 1.9511e-03 eta 0:03:44
epoch [8/50] batch [5/23] time 0.172 (0.474) data 0.000 (0.287) loss 3.3984 (3.3423) acc 15.6250 (17.5000) lr 1.9298e-03 eta 0:07:46
epoch [8/50] batch [10/23] time 0.153 (0.316) data 0.000 (0.144) loss 3.3349 (3.2721) acc 18.7500 (22.8125) lr 1.9298e-03 eta 0:05:09
epoch [8/50] batch [15/23] time 0.145 (0.259) data 0.000 (0.096) loss 3.6831 (3.2065) acc 18.7500 (24.3750) lr 1.9298e-03 eta 0:04:12
epoch [8/50] batch [20/23] time 0.149 (0.231) data 0.000 (0.072) loss 3.0729 (3.1922) acc 37.5000 (25.9375) lr 1.9298e-03 eta 0:03:44
epoch [9/50] batch [5/23] time 0.146 (0.474) data 0.000 (0.297) loss 3.4060 (3.1575) acc 15.6250 (30.0000) lr 1.9048e-03 eta 0:07:35
epoch [9/50] batch [10/23] time 0.151 (0.319) data 0.000 (0.148) loss 2.7801 (3.1386) acc 40.6250 (29.3750) lr 1.9048e-03 eta 0:05:04
epoch [9/50] batch [15/23] time 0.146 (0.262) data 0.000 (0.099) loss 3.0009 (3.1130) acc 21.8750 (28.5417) lr 1.9048e-03 eta 0:04:08
epoch [9/50] batch [20/23] time 0.148 (0.233) data 0.000 (0.074) loss 3.4945 (3.1497) acc 12.5000 (27.3438) lr 1.9048e-03 eta 0:03:40
epoch [10/50] batch [5/23] time 0.160 (0.431) data 0.000 (0.247) loss 3.0721 (3.1340) acc 31.2500 (30.0000) lr 1.8763e-03 eta 0:06:43
epoch [10/50] batch [10/23] time 0.150 (0.300) data 0.000 (0.124) loss 3.3117 (3.2056) acc 25.0000 (27.5000) lr 1.8763e-03 eta 0:04:40
epoch [10/50] batch [15/23] time 0.146 (0.249) data 0.000 (0.082) loss 3.1621 (3.1943) acc 28.1250 (27.5000) lr 1.8763e-03 eta 0:03:50
epoch [10/50] batch [20/23] time 0.146 (0.223) data 0.000 (0.062) loss 2.9300 (3.1386) acc 31.2500 (28.2812) lr 1.8763e-03 eta 0:03:25
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.894  alpha2: 0.357 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.52 <<<
epoch [11/50] batch [5/23] time 0.699 (0.888) data 0.001 (0.242) loss 1.8481 (1.8618) acc 55.1136 (65.7549) lr 1.8443e-03 eta 0:13:32
epoch [11/50] batch [10/23] time 0.136 (0.768) data 0.001 (0.121) loss 1.9114 (1.9618) acc 50.0000 (61.4789) lr 1.8443e-03 eta 0:11:39
epoch [11/50] batch [15/23] time 0.136 (0.596) data 0.000 (0.081) loss 1.7067 (1.9469) acc 66.1458 (61.7367) lr 1.8443e-03 eta 0:08:59
epoch [11/50] batch [20/23] time 0.134 (0.516) data 0.000 (0.061) loss 1.7533 (1.8768) acc 53.8043 (62.2246) lr 1.8443e-03 eta 0:07:44
>>> alpha1: 0.705  alpha2: 0.308 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.54 <<<
epoch [12/50] batch [5/23] time 0.130 (0.560) data 0.000 (0.279) loss 1.2116 (1.3393) acc 73.3333 (70.0171) lr 1.8090e-03 eta 0:08:19
epoch [12/50] batch [10/23] time 0.131 (0.348) data 0.000 (0.140) loss 1.1377 (1.3691) acc 61.1111 (65.4726) lr 1.8090e-03 eta 0:05:08
epoch [12/50] batch [15/23] time 0.136 (0.276) data 0.000 (0.093) loss 1.3619 (1.4073) acc 65.6250 (64.7235) lr 1.8090e-03 eta 0:04:03
epoch [12/50] batch [20/23] time 0.127 (0.267) data 0.000 (0.070) loss 1.0442 (1.3712) acc 72.6190 (64.5170) lr 1.8090e-03 eta 0:03:54
>>> alpha1: 0.682  alpha2: 0.277 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.48 <<<
epoch [13/50] batch [5/23] time 0.127 (0.516) data 0.000 (0.264) loss 1.3529 (1.4549) acc 63.3721 (62.1774) lr 1.7705e-03 eta 0:07:28
epoch [13/50] batch [10/23] time 0.134 (0.327) data 0.000 (0.132) loss 1.2581 (1.3546) acc 66.6667 (64.0775) lr 1.7705e-03 eta 0:04:42
epoch [13/50] batch [15/23] time 0.132 (0.261) data 0.000 (0.088) loss 1.4959 (1.3905) acc 57.0652 (62.4047) lr 1.7705e-03 eta 0:03:43
epoch [13/50] batch [20/23] time 0.135 (0.228) data 0.000 (0.066) loss 0.8289 (1.3590) acc 79.3478 (63.0763) lr 1.7705e-03 eta 0:03:15
>>> alpha1: 0.662  alpha2: 0.242 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.44 <<<
epoch [14/50] batch [5/23] time 0.132 (0.436) data 0.000 (0.294) loss 1.1423 (1.1381) acc 65.7609 (68.4877) lr 1.7290e-03 eta 0:06:08
epoch [14/50] batch [10/23] time 0.133 (0.286) data 0.000 (0.147) loss 0.8921 (1.2157) acc 76.7045 (66.7010) lr 1.7290e-03 eta 0:04:00
epoch [14/50] batch [15/23] time 0.144 (0.235) data 0.001 (0.098) loss 0.9679 (1.2591) acc 73.4694 (65.6492) lr 1.7290e-03 eta 0:03:16
epoch [14/50] batch [20/23] time 0.137 (0.236) data 0.000 (0.074) loss 1.4382 (1.2861) acc 59.0426 (63.8883) lr 1.7290e-03 eta 0:03:16
>>> alpha1: 0.646  alpha2: 0.254 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.44 <<<
epoch [15/50] batch [5/23] time 0.153 (0.432) data 0.000 (0.288) loss 1.1351 (1.1938) acc 63.5417 (69.0030) lr 1.6845e-03 eta 0:05:55
epoch [15/50] batch [10/23] time 0.127 (0.284) data 0.000 (0.144) loss 1.2014 (1.2098) acc 61.9048 (67.1129) lr 1.6845e-03 eta 0:03:52
epoch [15/50] batch [15/23] time 0.132 (0.234) data 0.000 (0.096) loss 1.4705 (1.2473) acc 60.5556 (66.5390) lr 1.6845e-03 eta 0:03:09
epoch [15/50] batch [20/23] time 0.136 (0.244) data 0.000 (0.072) loss 1.2931 (1.2152) acc 71.2766 (67.1690) lr 1.6845e-03 eta 0:03:17
>>> alpha1: 0.557  alpha2: 0.194 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [16/50] batch [5/23] time 0.154 (0.411) data 0.000 (0.262) loss 1.0143 (1.0911) acc 73.0000 (70.3125) lr 1.6374e-03 eta 0:05:29
epoch [16/50] batch [10/23] time 0.135 (0.350) data 0.000 (0.131) loss 1.0816 (1.0990) acc 69.0217 (68.8982) lr 1.6374e-03 eta 0:04:38
epoch [16/50] batch [15/23] time 0.143 (0.328) data 0.000 (0.088) loss 1.3218 (1.1048) acc 68.5000 (69.3312) lr 1.6374e-03 eta 0:04:18
epoch [16/50] batch [20/23] time 0.136 (0.280) data 0.000 (0.066) loss 1.1961 (1.1404) acc 61.4583 (67.6945) lr 1.6374e-03 eta 0:03:40
>>> alpha1: 0.491  alpha2: 0.165 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [17/50] batch [5/23] time 0.135 (0.474) data 0.000 (0.327) loss 1.1282 (0.9299) acc 63.8298 (71.7019) lr 1.5878e-03 eta 0:06:08
epoch [17/50] batch [10/23] time 0.137 (0.310) data 0.000 (0.164) loss 1.2171 (0.9964) acc 56.6327 (70.8008) lr 1.5878e-03 eta 0:03:59
epoch [17/50] batch [15/23] time 0.143 (0.254) data 0.000 (0.109) loss 1.1761 (1.0445) acc 64.2857 (68.6791) lr 1.5878e-03 eta 0:03:15
epoch [17/50] batch [20/23] time 0.144 (0.226) data 0.000 (0.082) loss 0.9491 (1.0316) acc 67.0000 (68.3910) lr 1.5878e-03 eta 0:02:52
>>> alpha1: 0.455  alpha2: 0.146 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.46 <<<
epoch [18/50] batch [5/23] time 0.156 (0.452) data 0.000 (0.294) loss 0.9525 (1.2003) acc 70.1923 (70.4953) lr 1.5358e-03 eta 0:05:40
epoch [18/50] batch [10/23] time 0.148 (0.298) data 0.000 (0.147) loss 1.0684 (1.1309) acc 70.9184 (69.8436) lr 1.5358e-03 eta 0:03:43
epoch [18/50] batch [15/23] time 0.141 (0.247) data 0.000 (0.098) loss 1.3542 (1.1426) acc 58.6735 (68.7614) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [20/23] time 0.129 (0.220) data 0.001 (0.074) loss 1.0550 (1.1174) acc 67.2222 (68.7238) lr 1.5358e-03 eta 0:02:42
>>> alpha1: 0.414  alpha2: 0.133 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.47 <<<
epoch [19/50] batch [5/23] time 0.153 (0.458) data 0.000 (0.305) loss 0.9831 (0.9477) acc 81.7708 (75.4841) lr 1.4818e-03 eta 0:05:34
epoch [19/50] batch [10/23] time 0.145 (0.302) data 0.000 (0.153) loss 0.9352 (0.9157) acc 67.5000 (73.9776) lr 1.4818e-03 eta 0:03:39
epoch [19/50] batch [15/23] time 0.140 (0.247) data 0.000 (0.102) loss 1.0752 (0.9241) acc 63.0000 (73.1805) lr 1.4818e-03 eta 0:02:58
epoch [19/50] batch [20/23] time 0.145 (0.220) data 0.000 (0.076) loss 0.9363 (0.9368) acc 71.1538 (72.3194) lr 1.4818e-03 eta 0:02:37
>>> alpha1: 0.369  alpha2: 0.098 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.47 <<<
epoch [20/50] batch [5/23] time 0.154 (0.450) data 0.000 (0.303) loss 0.5183 (0.7582) acc 81.6327 (78.1669) lr 1.4258e-03 eta 0:05:18
epoch [20/50] batch [10/23] time 0.151 (0.298) data 0.000 (0.152) loss 0.8906 (0.8134) acc 75.5556 (76.9862) lr 1.4258e-03 eta 0:03:29
epoch [20/50] batch [15/23] time 0.142 (0.246) data 0.000 (0.101) loss 0.8159 (0.8568) acc 72.3958 (74.8820) lr 1.4258e-03 eta 0:02:51
epoch [20/50] batch [20/23] time 0.142 (0.220) data 0.000 (0.076) loss 0.7035 (0.8576) acc 77.5000 (74.7664) lr 1.4258e-03 eta 0:02:32
>>> alpha1: 0.338  alpha2: 0.073 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.48 <<<
epoch [21/50] batch [5/23] time 0.157 (0.441) data 0.000 (0.286) loss 0.9593 (0.8179) acc 71.7593 (75.5192) lr 1.3681e-03 eta 0:05:01
epoch [21/50] batch [10/23] time 0.154 (0.296) data 0.000 (0.143) loss 0.9156 (0.7885) acc 66.9811 (73.8488) lr 1.3681e-03 eta 0:03:21
epoch [21/50] batch [15/23] time 0.142 (0.292) data 0.000 (0.096) loss 0.7392 (0.7828) acc 82.6531 (74.0223) lr 1.3681e-03 eta 0:03:17
epoch [21/50] batch [20/23] time 0.143 (0.254) data 0.000 (0.072) loss 0.7487 (0.8129) acc 81.8627 (72.4243) lr 1.3681e-03 eta 0:02:50
>>> alpha1: 0.327  alpha2: 0.067 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.49 <<<
epoch [22/50] batch [5/23] time 0.134 (0.432) data 0.000 (0.285) loss 0.8632 (0.8353) acc 71.1111 (73.9695) lr 1.3090e-03 eta 0:04:45
epoch [22/50] batch [10/23] time 0.164 (0.294) data 0.000 (0.143) loss 0.9246 (0.8117) acc 74.4898 (75.1640) lr 1.3090e-03 eta 0:03:13
epoch [22/50] batch [15/23] time 0.139 (0.243) data 0.000 (0.095) loss 0.7374 (0.7919) acc 81.1225 (76.4603) lr 1.3090e-03 eta 0:02:38
epoch [22/50] batch [20/23] time 0.137 (0.218) data 0.000 (0.071) loss 1.0761 (0.8008) acc 70.3125 (75.7792) lr 1.3090e-03 eta 0:02:20
>>> alpha1: 0.314  alpha2: 0.062 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.47 <<<
epoch [23/50] batch [5/23] time 0.144 (0.456) data 0.000 (0.299) loss 0.6806 (0.8050) acc 74.5098 (72.4133) lr 1.2487e-03 eta 0:04:51
epoch [23/50] batch [10/23] time 0.150 (0.303) data 0.000 (0.150) loss 0.7607 (0.8073) acc 77.3148 (74.9478) lr 1.2487e-03 eta 0:03:12
epoch [23/50] batch [15/23] time 0.144 (0.249) data 0.000 (0.100) loss 0.8047 (0.8016) acc 79.0000 (75.6947) lr 1.2487e-03 eta 0:02:36
epoch [23/50] batch [20/23] time 0.137 (0.221) data 0.000 (0.075) loss 0.6311 (0.7985) acc 83.6735 (75.9112) lr 1.2487e-03 eta 0:02:17
>>> alpha1: 0.293  alpha2: 0.033 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.44 <<<
epoch [24/50] batch [5/23] time 0.145 (0.422) data 0.000 (0.270) loss 0.7470 (0.8157) acc 78.1250 (76.2397) lr 1.1874e-03 eta 0:04:20
epoch [24/50] batch [10/23] time 0.151 (0.284) data 0.000 (0.135) loss 0.7295 (0.7579) acc 74.0566 (77.7232) lr 1.1874e-03 eta 0:02:53
epoch [24/50] batch [15/23] time 0.141 (0.235) data 0.000 (0.090) loss 0.8520 (0.7545) acc 73.5000 (77.4555) lr 1.1874e-03 eta 0:02:22
epoch [24/50] batch [20/23] time 0.141 (0.210) data 0.000 (0.068) loss 0.9670 (0.7600) acc 70.0980 (77.0820) lr 1.1874e-03 eta 0:02:06
>>> alpha1: 0.278  alpha2: 0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.45 <<<
epoch [25/50] batch [5/23] time 0.130 (0.415) data 0.001 (0.273) loss 0.7590 (0.7061) acc 75.0000 (76.6515) lr 1.1253e-03 eta 0:04:05
epoch [25/50] batch [10/23] time 0.146 (0.282) data 0.000 (0.137) loss 0.7733 (0.7294) acc 76.0638 (76.1929) lr 1.1253e-03 eta 0:02:45
epoch [25/50] batch [15/23] time 0.148 (0.236) data 0.000 (0.091) loss 0.4209 (0.7044) acc 81.6038 (77.7884) lr 1.1253e-03 eta 0:02:17
epoch [25/50] batch [20/23] time 0.149 (0.212) data 0.000 (0.069) loss 0.6399 (0.7202) acc 77.8302 (77.6318) lr 1.1253e-03 eta 0:02:02
>>> alpha1: 0.262  alpha2: 0.025 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.45 <<<
epoch [26/50] batch [5/23] time 0.169 (0.451) data 0.000 (0.295) loss 0.6348 (0.7670) acc 84.8039 (76.9844) lr 1.0628e-03 eta 0:04:17
epoch [26/50] batch [10/23] time 0.157 (0.300) data 0.000 (0.147) loss 0.6732 (0.7130) acc 80.3922 (77.5027) lr 1.0628e-03 eta 0:02:49
epoch [26/50] batch [15/23] time 0.137 (0.248) data 0.000 (0.098) loss 0.9802 (0.7041) acc 75.0000 (78.4751) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [20/23] time 0.134 (0.256) data 0.000 (0.074) loss 0.6209 (0.7054) acc 84.5745 (78.3530) lr 1.0628e-03 eta 0:02:21
>>> alpha1: 0.251  alpha2: 0.028 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.45 <<<
epoch [27/50] batch [5/23] time 0.180 (0.440) data 0.000 (0.283) loss 0.6081 (0.6387) acc 82.5472 (80.6033) lr 1.0000e-03 eta 0:04:00
epoch [27/50] batch [10/23] time 0.134 (0.368) data 0.000 (0.142) loss 0.7771 (0.6896) acc 75.5319 (79.4931) lr 1.0000e-03 eta 0:03:19
epoch [27/50] batch [15/23] time 0.146 (0.294) data 0.000 (0.095) loss 0.7663 (0.7203) acc 73.0392 (78.3567) lr 1.0000e-03 eta 0:02:37
epoch [27/50] batch [20/23] time 0.143 (0.257) data 0.000 (0.071) loss 0.8772 (0.7177) acc 74.0196 (77.9084) lr 1.0000e-03 eta 0:02:16
>>> alpha1: 0.241  alpha2: 0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.48 <<<
epoch [28/50] batch [5/23] time 0.163 (0.464) data 0.000 (0.301) loss 2.3515 (1.0086) acc 58.9623 (75.6257) lr 9.3721e-04 eta 0:04:02
epoch [28/50] batch [10/23] time 0.141 (0.305) data 0.000 (0.151) loss 0.5962 (0.9562) acc 82.0000 (77.7111) lr 9.3721e-04 eta 0:02:38
epoch [28/50] batch [15/23] time 0.138 (0.253) data 0.000 (0.101) loss 0.8129 (0.8852) acc 74.4792 (77.1886) lr 9.3721e-04 eta 0:02:09
epoch [28/50] batch [20/23] time 0.152 (0.226) data 0.001 (0.076) loss 0.5572 (0.8344) acc 86.1111 (77.6994) lr 9.3721e-04 eta 0:01:54
>>> alpha1: 0.230  alpha2: 0.025 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [29/50] batch [5/23] time 0.154 (0.546) data 0.000 (0.238) loss 0.7474 (0.6005) acc 81.6038 (83.8996) lr 8.7467e-04 eta 0:04:33
epoch [29/50] batch [10/23] time 0.148 (0.349) data 0.001 (0.119) loss 0.6346 (0.6174) acc 86.2745 (81.9154) lr 8.7467e-04 eta 0:02:53
epoch [29/50] batch [15/23] time 0.149 (0.282) data 0.000 (0.080) loss 0.5776 (0.6489) acc 82.5472 (81.4196) lr 8.7467e-04 eta 0:02:18
epoch [29/50] batch [20/23] time 0.140 (0.248) data 0.000 (0.060) loss 0.5664 (0.6438) acc 83.1633 (81.2287) lr 8.7467e-04 eta 0:02:00
>>> alpha1: 0.227  alpha2: 0.026 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [30/50] batch [5/23] time 0.145 (0.470) data 0.000 (0.316) loss 0.5893 (0.6227) acc 79.9020 (81.8888) lr 8.1262e-04 eta 0:03:44
epoch [30/50] batch [10/23] time 0.141 (0.312) data 0.000 (0.158) loss 0.7192 (0.6513) acc 76.5306 (80.4556) lr 8.1262e-04 eta 0:02:27
epoch [30/50] batch [15/23] time 0.148 (0.258) data 0.000 (0.106) loss 0.5241 (0.6062) acc 80.2885 (82.1101) lr 8.1262e-04 eta 0:02:00
epoch [30/50] batch [20/23] time 0.141 (0.231) data 0.000 (0.079) loss 0.6287 (0.6464) acc 83.5000 (80.9329) lr 8.1262e-04 eta 0:01:46
>>> alpha1: 0.225  alpha2: 0.027 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.50 <<<
epoch [31/50] batch [5/23] time 0.150 (0.462) data 0.000 (0.301) loss 0.7690 (0.6295) acc 79.8077 (81.2932) lr 7.5131e-04 eta 0:03:30
epoch [31/50] batch [10/23] time 0.154 (0.312) data 0.000 (0.151) loss 0.4225 (0.6021) acc 83.7963 (81.0300) lr 7.5131e-04 eta 0:02:20
epoch [31/50] batch [15/23] time 0.153 (0.258) data 0.000 (0.101) loss 0.5283 (0.6069) acc 88.2075 (81.0500) lr 7.5131e-04 eta 0:01:54
epoch [31/50] batch [20/23] time 0.154 (0.232) data 0.000 (0.076) loss 0.5512 (0.6056) acc 82.8704 (80.7858) lr 7.5131e-04 eta 0:01:42
>>> alpha1: 0.221  alpha2: 0.029 <<<
>>> noisy rate: 0.48 --> refined noisy rate: 0.27 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.42 <<<
epoch [32/50] batch [5/23] time 0.169 (0.473) data 0.000 (0.309) loss 0.4009 (0.5149) acc 91.0377 (87.4217) lr 6.9098e-04 eta 0:03:24
epoch [32/50] batch [10/23] time 0.143 (0.311) data 0.000 (0.155) loss 0.6485 (0.5428) acc 78.5000 (84.4716) lr 6.9098e-04 eta 0:02:12
epoch [32/50] batch [15/23] time 0.141 (0.256) data 0.000 (0.103) loss 0.6262 (0.5534) acc 74.0000 (83.7515) lr 6.9098e-04 eta 0:01:48
epoch [32/50] batch [20/23] time 0.144 (0.228) data 0.000 (0.078) loss 0.6003 (0.5705) acc 86.0000 (83.6192) lr 6.9098e-04 eta 0:01:35
>>> alpha1: 0.213  alpha2: 0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.43 <<<
epoch [33/50] batch [5/23] time 0.157 (0.459) data 0.000 (0.309) loss 0.4932 (0.5839) acc 81.0185 (81.9345) lr 6.3188e-04 eta 0:03:07
epoch [33/50] batch [10/23] time 0.151 (0.304) data 0.000 (0.155) loss 0.5024 (0.5955) acc 88.2653 (83.5889) lr 6.3188e-04 eta 0:02:02
epoch [33/50] batch [15/23] time 0.157 (0.251) data 0.000 (0.103) loss 0.5554 (0.5889) acc 83.3333 (83.5695) lr 6.3188e-04 eta 0:01:40
epoch [33/50] batch [20/23] time 0.149 (0.225) data 0.000 (0.078) loss 0.6254 (0.5944) acc 87.0192 (83.2989) lr 6.3188e-04 eta 0:01:28
>>> alpha1: 0.214  alpha2: 0.024 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.42 <<<
epoch [34/50] batch [5/23] time 0.165 (0.421) data 0.010 (0.274) loss 0.5011 (0.5516) acc 76.4423 (82.9373) lr 5.7422e-04 eta 0:02:42
epoch [34/50] batch [10/23] time 0.147 (0.286) data 0.000 (0.138) loss 0.3933 (0.5301) acc 90.3846 (83.6774) lr 5.7422e-04 eta 0:01:49
epoch [34/50] batch [15/23] time 0.139 (0.237) data 0.000 (0.092) loss 0.5482 (0.5350) acc 81.7708 (83.6910) lr 5.7422e-04 eta 0:01:29
epoch [34/50] batch [20/23] time 0.133 (0.212) data 0.000 (0.069) loss 0.5734 (0.5329) acc 83.5106 (84.2477) lr 5.7422e-04 eta 0:01:18
>>> alpha1: 0.207  alpha2: 0.024 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.46 <<<
epoch [35/50] batch [5/23] time 0.160 (0.419) data 0.000 (0.256) loss 0.7902 (0.5936) acc 83.3333 (84.9799) lr 5.1825e-04 eta 0:02:31
epoch [35/50] batch [10/23] time 0.151 (0.284) data 0.000 (0.128) loss 0.6653 (0.5742) acc 77.3585 (84.6383) lr 5.1825e-04 eta 0:01:41
epoch [35/50] batch [15/23] time 0.156 (0.242) data 0.000 (0.086) loss 0.3038 (0.5532) acc 85.4546 (84.1550) lr 5.1825e-04 eta 0:01:25
epoch [35/50] batch [20/23] time 0.158 (0.219) data 0.001 (0.064) loss 0.5984 (0.5483) acc 72.7679 (83.6571) lr 5.1825e-04 eta 0:01:16
>>> alpha1: 0.200  alpha2: 0.024 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.50 <<<
epoch [36/50] batch [5/23] time 0.172 (0.460) data 0.000 (0.288) loss 0.5052 (0.5391) acc 91.9643 (85.7205) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [10/23] time 0.167 (0.311) data 0.001 (0.144) loss 0.7689 (0.5446) acc 79.6296 (84.0514) lr 4.6417e-04 eta 0:01:44
epoch [36/50] batch [15/23] time 0.148 (0.258) data 0.000 (0.096) loss 0.4628 (0.6869) acc 88.2075 (82.3658) lr 4.6417e-04 eta 0:01:25
epoch [36/50] batch [20/23] time 0.151 (0.232) data 0.000 (0.072) loss 0.6705 (0.6508) acc 78.7037 (82.5587) lr 4.6417e-04 eta 0:01:15
>>> alpha1: 0.199  alpha2: 0.028 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.45 <<<
epoch [37/50] batch [5/23] time 0.143 (0.406) data 0.000 (0.249) loss 0.4170 (0.4658) acc 87.0000 (85.2258) lr 4.1221e-04 eta 0:02:08
epoch [37/50] batch [10/23] time 0.153 (0.283) data 0.000 (0.124) loss 0.4255 (0.4663) acc 82.3529 (85.0910) lr 4.1221e-04 eta 0:01:28
epoch [37/50] batch [15/23] time 0.140 (0.237) data 0.000 (0.083) loss 0.4523 (0.4805) acc 88.5000 (85.4662) lr 4.1221e-04 eta 0:01:12
epoch [37/50] batch [20/23] time 0.149 (0.216) data 0.000 (0.062) loss 0.4795 (0.5098) acc 88.4259 (84.8612) lr 4.1221e-04 eta 0:01:05
>>> alpha1: 0.198  alpha2: 0.030 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.47 <<<
epoch [38/50] batch [5/23] time 0.143 (0.440) data 0.000 (0.287) loss 0.2704 (0.5150) acc 90.5000 (85.2930) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [10/23] time 0.158 (0.297) data 0.000 (0.144) loss 0.3599 (0.5044) acc 93.3962 (85.7606) lr 3.6258e-04 eta 0:01:25
epoch [38/50] batch [15/23] time 0.156 (0.248) data 0.000 (0.096) loss 0.6357 (0.5091) acc 80.0000 (86.0931) lr 3.6258e-04 eta 0:01:10
epoch [38/50] batch [20/23] time 0.155 (0.223) data 0.000 (0.072) loss 0.5088 (0.5117) acc 80.1887 (85.4312) lr 3.6258e-04 eta 0:01:02
>>> alpha1: 0.193  alpha2: 0.024 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.47 <<<
epoch [39/50] batch [5/23] time 0.155 (0.545) data 0.000 (0.383) loss 0.3258 (0.4380) acc 91.8182 (89.4509) lr 3.1545e-04 eta 0:02:27
epoch [39/50] batch [10/23] time 0.157 (0.351) data 0.000 (0.192) loss 0.5123 (0.4636) acc 88.7755 (87.3434) lr 3.1545e-04 eta 0:01:33
epoch [39/50] batch [15/23] time 0.165 (0.285) data 0.000 (0.128) loss 0.5689 (0.4778) acc 87.0536 (87.8139) lr 3.1545e-04 eta 0:01:14
epoch [39/50] batch [20/23] time 0.147 (0.251) data 0.000 (0.096) loss 0.6904 (0.4986) acc 77.9412 (86.4027) lr 3.1545e-04 eta 0:01:04
>>> alpha1: 0.190  alpha2: 0.026 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.47 <<<
epoch [40/50] batch [5/23] time 0.145 (0.459) data 0.000 (0.293) loss 0.5932 (0.5564) acc 86.0000 (84.5642) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [10/23] time 0.143 (0.312) data 0.000 (0.147) loss 0.2709 (0.5496) acc 92.1569 (85.6300) lr 2.7103e-04 eta 0:01:15
epoch [40/50] batch [15/23] time 0.156 (0.260) data 0.000 (0.098) loss 0.4971 (0.5290) acc 80.6604 (85.8823) lr 2.7103e-04 eta 0:01:01
epoch [40/50] batch [20/23] time 0.158 (0.267) data 0.000 (0.074) loss 0.5546 (0.5314) acc 88.1818 (85.0779) lr 2.7103e-04 eta 0:01:02
>>> alpha1: 0.189  alpha2: 0.029 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.46 <<<
epoch [41/50] batch [5/23] time 0.163 (0.426) data 0.000 (0.265) loss 0.4900 (0.4530) acc 84.9057 (89.7259) lr 2.2949e-04 eta 0:01:35
epoch [41/50] batch [10/23] time 0.161 (0.302) data 0.001 (0.133) loss 0.4478 (0.4452) acc 81.7308 (87.2346) lr 2.2949e-04 eta 0:01:06
epoch [41/50] batch [15/23] time 0.150 (0.251) data 0.000 (0.088) loss 0.5295 (0.5433) acc 86.7924 (86.3812) lr 2.2949e-04 eta 0:00:53
epoch [41/50] batch [20/23] time 0.146 (0.225) data 0.000 (0.066) loss 0.7996 (0.5503) acc 82.8431 (85.5418) lr 2.2949e-04 eta 0:00:47
>>> alpha1: 0.184  alpha2: 0.024 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [42/50] batch [5/23] time 0.150 (0.448) data 0.000 (0.278) loss 0.5634 (0.5103) acc 89.8936 (87.6565) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [10/23] time 0.171 (0.310) data 0.000 (0.139) loss 0.5119 (0.5028) acc 86.5741 (87.4411) lr 1.9098e-04 eta 0:01:01
epoch [42/50] batch [15/23] time 0.175 (0.263) data 0.000 (0.093) loss 0.4316 (0.4730) acc 83.4821 (87.6799) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [20/23] time 0.155 (0.235) data 0.001 (0.070) loss 0.3542 (0.4796) acc 77.3148 (87.0338) lr 1.9098e-04 eta 0:00:43
>>> alpha1: 0.183  alpha2: 0.032 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.47 <<<
epoch [43/50] batch [5/23] time 0.148 (0.407) data 0.000 (0.252) loss 0.5164 (0.5080) acc 78.4314 (83.2030) lr 1.5567e-04 eta 0:01:12
epoch [43/50] batch [10/23] time 0.149 (0.287) data 0.000 (0.126) loss 0.4453 (0.4826) acc 87.7358 (85.2505) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [15/23] time 0.156 (0.241) data 0.000 (0.084) loss 0.3228 (0.4667) acc 90.0000 (85.5034) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [20/23] time 0.141 (0.218) data 0.001 (0.063) loss 0.4475 (0.4691) acc 90.0000 (86.2526) lr 1.5567e-04 eta 0:00:35
>>> alpha1: 0.182  alpha2: 0.040 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.46 <<<
epoch [44/50] batch [5/23] time 0.164 (0.423) data 0.000 (0.263) loss 0.2556 (0.4327) acc 89.3519 (86.3186) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [10/23] time 0.145 (0.289) data 0.000 (0.132) loss 0.3872 (0.5264) acc 90.5000 (86.2662) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [15/23] time 0.150 (0.243) data 0.000 (0.088) loss 0.5977 (0.5011) acc 79.3269 (85.8355) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [20/23] time 0.144 (0.219) data 0.000 (0.066) loss 0.5213 (0.5079) acc 88.0000 (85.8524) lr 1.2369e-04 eta 0:00:30
>>> alpha1: 0.182  alpha2: 0.036 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.46 <<<
epoch [45/50] batch [5/23] time 0.162 (0.419) data 0.000 (0.259) loss 0.4320 (0.4211) acc 86.7647 (89.3400) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [10/23] time 0.148 (0.286) data 0.000 (0.129) loss 0.6077 (0.4592) acc 87.2549 (87.6691) lr 9.5173e-05 eta 0:00:36
epoch [45/50] batch [15/23] time 0.135 (0.239) data 0.000 (0.087) loss 0.5576 (0.4544) acc 79.7872 (87.7249) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [20/23] time 0.146 (0.216) data 0.000 (0.065) loss 0.5429 (0.4712) acc 89.2157 (87.4511) lr 9.5173e-05 eta 0:00:25
>>> alpha1: 0.183  alpha2: 0.036 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.53 <<<
epoch [46/50] batch [5/23] time 0.162 (0.452) data 0.000 (0.282) loss 0.4645 (0.5015) acc 91.2281 (83.0123) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [10/23] time 0.148 (0.306) data 0.000 (0.141) loss 0.5552 (0.4906) acc 84.8039 (86.0524) lr 7.0224e-05 eta 0:00:32
epoch [46/50] batch [15/23] time 0.149 (0.255) data 0.000 (0.094) loss 0.5032 (0.4876) acc 87.7358 (86.3721) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [20/23] time 0.154 (0.229) data 0.000 (0.071) loss 0.3995 (0.4669) acc 87.0536 (86.8705) lr 7.0224e-05 eta 0:00:21
>>> alpha1: 0.179  alpha2: 0.025 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.45 <<<
epoch [47/50] batch [5/23] time 0.142 (0.458) data 0.000 (0.305) loss 0.3652 (0.4555) acc 90.9574 (86.2420) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [10/23] time 0.136 (0.301) data 0.001 (0.153) loss 0.6331 (0.4850) acc 84.0425 (85.7632) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [15/23] time 0.137 (0.249) data 0.000 (0.102) loss 0.6425 (0.4925) acc 76.5625 (85.4786) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [20/23] time 0.142 (0.222) data 0.000 (0.077) loss 0.5206 (0.4864) acc 84.0000 (85.7573) lr 4.8943e-05 eta 0:00:16
>>> alpha1: 0.178  alpha2: 0.025 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.45 <<<
epoch [48/50] batch [5/23] time 0.147 (0.436) data 0.000 (0.278) loss 0.5466 (0.4875) acc 88.7255 (85.7135) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [10/23] time 0.153 (0.291) data 0.000 (0.139) loss 0.4121 (0.4756) acc 83.9623 (85.8583) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.138 (0.241) data 0.000 (0.093) loss 0.3532 (0.4666) acc 88.0208 (86.1089) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.153 (0.218) data 0.000 (0.070) loss 0.5141 (0.4720) acc 83.9623 (85.9704) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.177  alpha2: 0.019 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.27 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [49/50] batch [5/23] time 0.151 (0.419) data 0.000 (0.279) loss 0.2890 (0.4537) acc 89.7959 (87.7578) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.132 (0.278) data 0.000 (0.140) loss 0.4358 (0.4664) acc 91.4634 (89.0300) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.129 (0.230) data 0.000 (0.093) loss 0.4866 (0.4843) acc 85.7955 (88.1030) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.128 (0.205) data 0.000 (0.070) loss 0.4614 (0.4807) acc 89.2045 (87.7714) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.176  alpha2: 0.023 <<<
>>> noisy rate: 0.49 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.44 <<<
epoch [50/50] batch [5/23] time 0.148 (0.485) data 0.000 (0.328) loss 0.5917 (0.4570) acc 83.0189 (87.2478) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.135 (0.314) data 0.000 (0.164) loss 0.6051 (0.4467) acc 72.9167 (86.9161) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.140 (0.258) data 0.001 (0.110) loss 0.3120 (0.4309) acc 93.3673 (87.4093) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.139 (0.229) data 0.001 (0.082) loss 0.4377 (0.4355) acc 92.3469 (87.2205) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.37, 0.37, 0.35, 0.32, 0.32, 0.31, 0.31, 0.3, 0.3, 0.32, 0.29, 0.31, 0.3, 0.3, 0.3, 0.3, 0.29, 0.29, 0.29, 0.29, 0.29, 0.27, 0.28, 0.28, 0.28, 0.29, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.29, 0.28, 0.28, 0.27, 0.29]
* matched noise rate: [0.18, 0.16, 0.15, 0.13, 0.16, 0.17, 0.17, 0.16, 0.15, 0.16, 0.18, 0.16, 0.17, 0.16, 0.17, 0.17, 0.19, 0.18, 0.17, 0.17, 0.18, 0.17, 0.18, 0.15, 0.18, 0.21, 0.19, 0.16, 0.17, 0.19, 0.18, 0.17, 0.19, 0.17, 0.16, 0.19, 0.16, 0.16, 0.12, 0.16]
* unmatched noise rate: [0.52, 0.54, 0.48, 0.44, 0.44, 0.48, 0.48, 0.46, 0.47, 0.47, 0.48, 0.49, 0.47, 0.44, 0.45, 0.45, 0.45, 0.48, 0.48, 0.5, 0.5, 0.42, 0.43, 0.42, 0.46, 0.5, 0.45, 0.47, 0.47, 0.47, 0.46, 0.48, 0.47, 0.46, 0.46, 0.53, 0.45, 0.45, 0.4, 0.44]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:39,  2.49s/it] 18%|█▊        | 3/17 [00:02<00:09,  1.43it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.67it/s] 41%|████      | 7/17 [00:02<00:02,  4.01it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.52it/s] 65%|██████▍   | 11/17 [00:03<00:00,  7.05it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.52it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.85it/s]100%|██████████| 17/17 [00:04<00:00,  5.16it/s]100%|██████████| 17/17 [00:04<00:00,  3.80it/s]
=> result
* total: 1,692
* correct: 955
* accuracy: 56.4%
* error: 43.6%
* macro_f1: 55.3%
=> per-class result
* class: 0 (banded)	total: 36	correct: 24	acc: 66.7%
* class: 1 (blotchy)	total: 36	correct: 11	acc: 30.6%
* class: 2 (braided)	total: 36	correct: 12	acc: 33.3%
* class: 3 (bubbly)	total: 36	correct: 31	acc: 86.1%
* class: 4 (bumpy)	total: 36	correct: 3	acc: 8.3%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 31	acc: 86.1%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 11	acc: 30.6%
* class: 9 (crystalline)	total: 36	correct: 32	acc: 88.9%
* class: 10 (dotted)	total: 36	correct: 27	acc: 75.0%
* class: 11 (fibrous)	total: 36	correct: 23	acc: 63.9%
* class: 12 (flecked)	total: 36	correct: 15	acc: 41.7%
* class: 13 (freckled)	total: 36	correct: 23	acc: 63.9%
* class: 14 (frilly)	total: 36	correct: 27	acc: 75.0%
* class: 15 (gauzy)	total: 36	correct: 21	acc: 58.3%
* class: 16 (grid)	total: 36	correct: 13	acc: 36.1%
* class: 17 (grooved)	total: 36	correct: 19	acc: 52.8%
* class: 18 (honeycombed)	total: 36	correct: 22	acc: 61.1%
* class: 19 (interlaced)	total: 36	correct: 16	acc: 44.4%
* class: 20 (knitted)	total: 36	correct: 28	acc: 77.8%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 16	acc: 44.4%
* class: 23 (marbled)	total: 36	correct: 25	acc: 69.4%
* class: 24 (matted)	total: 36	correct: 19	acc: 52.8%
* class: 25 (meshed)	total: 36	correct: 17	acc: 47.2%
* class: 26 (paisley)	total: 36	correct: 31	acc: 86.1%
* class: 27 (perforated)	total: 36	correct: 24	acc: 66.7%
* class: 28 (pitted)	total: 36	correct: 11	acc: 30.6%
* class: 29 (pleated)	total: 36	correct: 14	acc: 38.9%
* class: 30 (polka-dotted)	total: 36	correct: 19	acc: 52.8%
* class: 31 (porous)	total: 36	correct: 14	acc: 38.9%
* class: 32 (potholed)	total: 36	correct: 28	acc: 77.8%
* class: 33 (scaly)	total: 36	correct: 17	acc: 47.2%
* class: 34 (smeared)	total: 36	correct: 14	acc: 38.9%
* class: 35 (spiralled)	total: 36	correct: 16	acc: 44.4%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 4	acc: 11.1%
* class: 38 (stratified)	total: 36	correct: 27	acc: 75.0%
* class: 39 (striped)	total: 36	correct: 30	acc: 83.3%
* class: 40 (studded)	total: 36	correct: 28	acc: 77.8%
* class: 41 (swirly)	total: 36	correct: 23	acc: 63.9%
* class: 42 (veined)	total: 36	correct: 20	acc: 55.6%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 17	acc: 47.2%
* class: 45 (wrinkled)	total: 36	correct: 25	acc: 69.4%
* class: 46 (zigzagged)	total: 36	correct: 28	acc: 77.8%
* average: 56.4%
Elapsed: 0:12:57
Run this job and save the output to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_10-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.147 (0.919) data 0.000 (0.353) loss 3.7257 (3.7734) acc 6.2500 (9.3750) lr 1.0000e-05 eta 0:17:32
epoch [1/50] batch [10/23] time 0.169 (0.539) data 0.000 (0.177) loss 3.8105 (3.7862) acc 6.2500 (8.7500) lr 1.0000e-05 eta 0:10:14
epoch [1/50] batch [15/23] time 0.167 (0.414) data 0.000 (0.118) loss 3.6826 (3.7654) acc 15.6250 (8.9583) lr 1.0000e-05 eta 0:07:49
epoch [1/50] batch [20/23] time 0.161 (0.351) data 0.000 (0.088) loss 3.5032 (3.7465) acc 9.3750 (9.0625) lr 1.0000e-05 eta 0:06:36
epoch [2/50] batch [5/23] time 0.172 (0.450) data 0.000 (0.246) loss 3.8166 (3.8683) acc 3.1250 (6.8750) lr 2.0000e-03 eta 0:08:24
epoch [2/50] batch [10/23] time 0.153 (0.312) data 0.000 (0.123) loss 3.7717 (3.7721) acc 15.6250 (10.6250) lr 2.0000e-03 eta 0:05:48
epoch [2/50] batch [15/23] time 0.146 (0.256) data 0.000 (0.082) loss 3.6653 (3.7451) acc 12.5000 (11.2500) lr 2.0000e-03 eta 0:04:45
epoch [2/50] batch [20/23] time 0.148 (0.229) data 0.000 (0.062) loss 3.6285 (3.7309) acc 12.5000 (11.7188) lr 2.0000e-03 eta 0:04:13
epoch [3/50] batch [5/23] time 0.155 (0.432) data 0.000 (0.247) loss 3.3795 (3.4958) acc 25.0000 (19.3750) lr 1.9980e-03 eta 0:07:54
epoch [3/50] batch [10/23] time 0.165 (0.302) data 0.000 (0.124) loss 3.8974 (3.5535) acc 6.2500 (17.1875) lr 1.9980e-03 eta 0:05:30
epoch [3/50] batch [15/23] time 0.147 (0.251) data 0.000 (0.082) loss 3.5794 (3.5860) acc 18.7500 (16.0417) lr 1.9980e-03 eta 0:04:32
epoch [3/50] batch [20/23] time 0.146 (0.225) data 0.000 (0.062) loss 3.7521 (3.6240) acc 15.6250 (15.0000) lr 1.9980e-03 eta 0:04:03
epoch [4/50] batch [5/23] time 0.190 (0.436) data 0.000 (0.250) loss 3.5138 (3.6145) acc 28.1250 (18.1250) lr 1.9921e-03 eta 0:07:48
epoch [4/50] batch [10/23] time 0.147 (0.298) data 0.000 (0.125) loss 3.6438 (3.6056) acc 15.6250 (16.8750) lr 1.9921e-03 eta 0:05:18
epoch [4/50] batch [15/23] time 0.148 (0.248) data 0.000 (0.083) loss 3.4841 (3.5851) acc 15.6250 (16.4583) lr 1.9921e-03 eta 0:04:24
epoch [4/50] batch [20/23] time 0.149 (0.223) data 0.000 (0.063) loss 3.7217 (3.5893) acc 15.6250 (16.4062) lr 1.9921e-03 eta 0:03:56
epoch [5/50] batch [5/23] time 0.152 (0.451) data 0.000 (0.271) loss 3.5952 (3.5629) acc 18.7500 (19.3750) lr 1.9823e-03 eta 0:07:54
epoch [5/50] batch [10/23] time 0.160 (0.309) data 0.000 (0.136) loss 3.5238 (3.5704) acc 12.5000 (16.5625) lr 1.9823e-03 eta 0:05:23
epoch [5/50] batch [15/23] time 0.147 (0.256) data 0.000 (0.091) loss 3.4072 (3.5771) acc 21.8750 (16.8750) lr 1.9823e-03 eta 0:04:26
epoch [5/50] batch [20/23] time 0.146 (0.229) data 0.000 (0.068) loss 3.5948 (3.5677) acc 18.7500 (17.5000) lr 1.9823e-03 eta 0:03:57
epoch [6/50] batch [5/23] time 0.163 (0.439) data 0.000 (0.253) loss 3.1643 (3.4274) acc 28.1250 (21.2500) lr 1.9686e-03 eta 0:07:32
epoch [6/50] batch [10/23] time 0.163 (0.298) data 0.000 (0.127) loss 3.5469 (3.4546) acc 18.7500 (20.0000) lr 1.9686e-03 eta 0:05:05
epoch [6/50] batch [15/23] time 0.147 (0.248) data 0.000 (0.085) loss 3.6636 (3.4970) acc 15.6250 (18.5417) lr 1.9686e-03 eta 0:04:13
epoch [6/50] batch [20/23] time 0.148 (0.223) data 0.000 (0.063) loss 3.5211 (3.5043) acc 18.7500 (19.0625) lr 1.9686e-03 eta 0:03:46
epoch [7/50] batch [5/23] time 0.180 (0.404) data 0.000 (0.219) loss 3.6296 (3.4985) acc 12.5000 (18.1250) lr 1.9511e-03 eta 0:06:47
epoch [7/50] batch [10/23] time 0.159 (0.282) data 0.000 (0.110) loss 3.2401 (3.4894) acc 28.1250 (18.4375) lr 1.9511e-03 eta 0:04:42
epoch [7/50] batch [15/23] time 0.148 (0.237) data 0.000 (0.073) loss 3.4135 (3.4754) acc 18.7500 (19.7917) lr 1.9511e-03 eta 0:03:56
epoch [7/50] batch [20/23] time 0.150 (0.215) data 0.000 (0.055) loss 3.6297 (3.4763) acc 18.7500 (20.6250) lr 1.9511e-03 eta 0:03:33
epoch [8/50] batch [5/23] time 0.152 (0.477) data 0.000 (0.304) loss 3.5265 (3.4787) acc 9.3750 (15.6250) lr 1.9298e-03 eta 0:07:49
epoch [8/50] batch [10/23] time 0.160 (0.320) data 0.000 (0.152) loss 3.7031 (3.4930) acc 9.3750 (15.6250) lr 1.9298e-03 eta 0:05:13
epoch [8/50] batch [15/23] time 0.151 (0.264) data 0.000 (0.102) loss 3.0186 (3.4765) acc 31.2500 (16.8750) lr 1.9298e-03 eta 0:04:16
epoch [8/50] batch [20/23] time 0.151 (0.235) data 0.000 (0.076) loss 3.3964 (3.4578) acc 15.6250 (18.1250) lr 1.9298e-03 eta 0:03:48
epoch [9/50] batch [5/23] time 0.166 (0.478) data 0.000 (0.289) loss 3.3015 (3.3744) acc 21.8750 (22.5000) lr 1.9048e-03 eta 0:07:39
epoch [9/50] batch [10/23] time 0.171 (0.326) data 0.000 (0.145) loss 3.3126 (3.3999) acc 18.7500 (22.1875) lr 1.9048e-03 eta 0:05:12
epoch [9/50] batch [15/23] time 0.151 (0.268) data 0.000 (0.097) loss 3.0173 (3.3978) acc 34.3750 (21.6667) lr 1.9048e-03 eta 0:04:14
epoch [9/50] batch [20/23] time 0.148 (0.238) data 0.000 (0.073) loss 3.3520 (3.4321) acc 25.0000 (20.4688) lr 1.9048e-03 eta 0:03:45
epoch [10/50] batch [5/23] time 0.150 (0.498) data 0.000 (0.312) loss 3.3956 (3.4658) acc 12.5000 (14.3750) lr 1.8763e-03 eta 0:07:47
epoch [10/50] batch [10/23] time 0.170 (0.334) data 0.000 (0.156) loss 3.3114 (3.4465) acc 21.8750 (18.1250) lr 1.8763e-03 eta 0:05:11
epoch [10/50] batch [15/23] time 0.149 (0.272) data 0.000 (0.104) loss 3.3920 (3.3728) acc 25.0000 (20.6250) lr 1.8763e-03 eta 0:04:12
epoch [10/50] batch [20/23] time 0.145 (0.241) data 0.000 (0.078) loss 3.5764 (3.4395) acc 21.8750 (20.0000) lr 1.8763e-03 eta 0:03:42
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 1.183  alpha2: 0.517 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.63 <<<
epoch [11/50] batch [5/23] time 0.853 (0.949) data 0.000 (0.271) loss 2.6578 (2.9155) acc 77.8302 (73.1090) lr 1.8443e-03 eta 0:14:28
epoch [11/50] batch [10/23] time 0.142 (0.715) data 0.000 (0.136) loss 2.6753 (2.8282) acc 70.5000 (70.5974) lr 1.8443e-03 eta 0:10:50
epoch [11/50] batch [15/23] time 0.146 (0.650) data 0.000 (0.091) loss 2.7863 (2.7822) acc 56.9149 (69.6210) lr 1.8443e-03 eta 0:09:48
epoch [11/50] batch [20/23] time 0.133 (0.556) data 0.000 (0.068) loss 2.8031 (2.7590) acc 73.9130 (71.2312) lr 1.8443e-03 eta 0:08:20
>>> alpha1: 0.926  alpha2: 0.517 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.60 <<<
epoch [12/50] batch [5/23] time 0.131 (0.438) data 0.000 (0.295) loss 1.7405 (1.7694) acc 78.8889 (73.1203) lr 1.8090e-03 eta 0:06:30
epoch [12/50] batch [10/23] time 0.132 (0.291) data 0.000 (0.148) loss 1.9384 (1.8313) acc 71.6667 (69.6915) lr 1.8090e-03 eta 0:04:18
epoch [12/50] batch [15/23] time 0.134 (0.277) data 0.000 (0.099) loss 1.7595 (1.8150) acc 71.1956 (68.8531) lr 1.8090e-03 eta 0:04:04
epoch [12/50] batch [20/23] time 0.119 (0.270) data 0.000 (0.074) loss 1.7701 (1.8327) acc 76.8293 (67.8606) lr 1.8090e-03 eta 0:03:56
>>> alpha1: 0.808  alpha2: 0.444 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.58 <<<
epoch [13/50] batch [5/23] time 0.138 (0.417) data 0.000 (0.272) loss 1.2809 (1.2902) acc 72.7273 (73.0157) lr 1.7705e-03 eta 0:06:02
epoch [13/50] batch [10/23] time 0.145 (0.278) data 0.000 (0.136) loss 1.3462 (1.3876) acc 71.1111 (71.3934) lr 1.7705e-03 eta 0:04:00
epoch [13/50] batch [15/23] time 0.642 (0.298) data 0.000 (0.091) loss 1.4167 (1.3132) acc 60.1351 (70.0381) lr 1.7705e-03 eta 0:04:15
epoch [13/50] batch [20/23] time 0.137 (0.284) data 0.000 (0.068) loss 0.9203 (1.3031) acc 81.1111 (70.0415) lr 1.7705e-03 eta 0:04:02
>>> alpha1: 0.710  alpha2: 0.368 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.30 & unmatched refined noisy rate: 0.66 <<<
epoch [14/50] batch [5/23] time 0.145 (0.433) data 0.000 (0.276) loss 0.9993 (0.9801) acc 78.0000 (77.2423) lr 1.7290e-03 eta 0:06:06
epoch [14/50] batch [10/23] time 0.135 (0.425) data 0.000 (0.138) loss 0.9363 (0.9961) acc 71.2766 (76.2933) lr 1.7290e-03 eta 0:05:57
epoch [14/50] batch [15/23] time 0.815 (0.378) data 0.000 (0.092) loss 0.8090 (1.0243) acc 75.9091 (74.7621) lr 1.7290e-03 eta 0:05:15
epoch [14/50] batch [20/23] time 0.141 (0.320) data 0.000 (0.069) loss 1.2630 (1.0437) acc 72.0000 (74.3528) lr 1.7290e-03 eta 0:04:25
>>> alpha1: 0.664  alpha2: 0.327 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.54 <<<
epoch [15/50] batch [5/23] time 0.170 (0.392) data 0.000 (0.254) loss 0.7837 (0.8564) acc 74.4898 (73.6559) lr 1.6845e-03 eta 0:05:22
epoch [15/50] batch [10/23] time 0.135 (0.261) data 0.000 (0.127) loss 0.9293 (0.8812) acc 75.0000 (75.2797) lr 1.6845e-03 eta 0:03:33
epoch [15/50] batch [15/23] time 0.128 (0.216) data 0.000 (0.085) loss 0.9957 (0.9310) acc 74.4186 (73.5077) lr 1.6845e-03 eta 0:02:55
epoch [15/50] batch [20/23] time 0.130 (0.195) data 0.000 (0.064) loss 1.0298 (0.9395) acc 75.0000 (74.3827) lr 1.6845e-03 eta 0:02:37
>>> alpha1: 0.569  alpha2: 0.221 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.56 <<<
epoch [16/50] batch [5/23] time 0.141 (0.397) data 0.001 (0.256) loss 0.6062 (0.7514) acc 83.8542 (80.8859) lr 1.6374e-03 eta 0:05:17
epoch [16/50] batch [10/23] time 0.138 (0.266) data 0.000 (0.128) loss 0.8076 (0.8336) acc 74.4186 (76.8967) lr 1.6374e-03 eta 0:03:31
epoch [16/50] batch [15/23] time 0.126 (0.222) data 0.000 (0.086) loss 0.7477 (0.8496) acc 78.4884 (75.5742) lr 1.6374e-03 eta 0:02:55
epoch [16/50] batch [20/23] time 0.130 (0.199) data 0.000 (0.064) loss 0.6937 (0.8736) acc 79.0698 (75.4554) lr 1.6374e-03 eta 0:02:36
>>> alpha1: 0.368  alpha2: 0.098 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.60 <<<
epoch [17/50] batch [5/23] time 0.125 (0.472) data 0.000 (0.328) loss 0.6221 (0.6410) acc 87.1951 (82.5062) lr 1.5878e-03 eta 0:06:06
epoch [17/50] batch [10/23] time 0.134 (0.307) data 0.000 (0.164) loss 0.7626 (0.6730) acc 78.3333 (81.4620) lr 1.5878e-03 eta 0:03:56
epoch [17/50] batch [15/23] time 0.139 (0.250) data 0.000 (0.109) loss 0.7165 (0.7262) acc 77.5510 (79.2271) lr 1.5878e-03 eta 0:03:11
epoch [17/50] batch [20/23] time 0.144 (0.223) data 0.000 (0.082) loss 0.5834 (0.7202) acc 88.2353 (79.8129) lr 1.5878e-03 eta 0:02:49
>>> alpha1: 0.301  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.57 <<<
epoch [18/50] batch [5/23] time 0.130 (0.403) data 0.000 (0.259) loss 0.7380 (0.6154) acc 83.3333 (85.8255) lr 1.5358e-03 eta 0:05:04
epoch [18/50] batch [10/23] time 0.129 (0.272) data 0.001 (0.130) loss 0.6622 (0.6352) acc 82.9545 (83.8620) lr 1.5358e-03 eta 0:03:23
epoch [18/50] batch [15/23] time 0.116 (0.225) data 0.000 (0.087) loss 0.5263 (0.6115) acc 83.3333 (83.7741) lr 1.5358e-03 eta 0:02:47
epoch [18/50] batch [20/23] time 0.131 (0.200) data 0.000 (0.065) loss 0.7650 (0.6413) acc 80.4348 (82.4847) lr 1.5358e-03 eta 0:02:28
>>> alpha1: 0.253  alpha2: 0.027 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.60 <<<
epoch [19/50] batch [5/23] time 0.139 (0.436) data 0.001 (0.287) loss 0.5592 (0.4838) acc 86.1702 (88.0436) lr 1.4818e-03 eta 0:05:18
epoch [19/50] batch [10/23] time 0.134 (0.290) data 0.000 (0.144) loss 0.7553 (0.5738) acc 76.0638 (82.9849) lr 1.4818e-03 eta 0:03:30
epoch [19/50] batch [15/23] time 0.126 (0.238) data 0.000 (0.096) loss 1.0446 (0.5984) acc 59.0909 (81.9446) lr 1.4818e-03 eta 0:02:51
epoch [19/50] batch [20/23] time 0.139 (0.212) data 0.000 (0.072) loss 0.7418 (0.6145) acc 78.0612 (81.6642) lr 1.4818e-03 eta 0:02:31
>>> alpha1: 0.226  alpha2: 0.021 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.62 <<<
epoch [20/50] batch [5/23] time 0.162 (0.422) data 0.010 (0.277) loss 0.5775 (0.5495) acc 81.1225 (82.7450) lr 1.4258e-03 eta 0:04:58
epoch [20/50] batch [10/23] time 0.134 (0.281) data 0.000 (0.138) loss 0.7004 (0.5773) acc 79.7872 (82.2226) lr 1.4258e-03 eta 0:03:17
epoch [20/50] batch [15/23] time 0.138 (0.236) data 0.000 (0.092) loss 0.5474 (0.5895) acc 84.8958 (82.0008) lr 1.4258e-03 eta 0:02:44
epoch [20/50] batch [20/23] time 0.136 (0.212) data 0.000 (0.069) loss 0.7690 (0.5977) acc 72.8723 (81.3577) lr 1.4258e-03 eta 0:02:26
>>> alpha1: 0.211  alpha2: 0.020 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.61 <<<
epoch [21/50] batch [5/23] time 0.127 (0.450) data 0.000 (0.299) loss 0.5790 (0.5667) acc 73.2558 (82.8072) lr 1.3681e-03 eta 0:05:08
epoch [21/50] batch [10/23] time 0.155 (0.297) data 0.000 (0.150) loss 0.6136 (0.5767) acc 82.8704 (83.2951) lr 1.3681e-03 eta 0:03:21
epoch [21/50] batch [15/23] time 0.146 (0.246) data 0.000 (0.100) loss 0.4567 (0.5798) acc 85.7843 (82.3043) lr 1.3681e-03 eta 0:02:45
epoch [21/50] batch [20/23] time 0.137 (0.219) data 0.000 (0.075) loss 0.6579 (0.5882) acc 78.5714 (82.0028) lr 1.3681e-03 eta 0:02:26
>>> alpha1: 0.199  alpha2: 0.022 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.62 <<<
epoch [22/50] batch [5/23] time 0.146 (0.440) data 0.000 (0.286) loss 0.4681 (0.5701) acc 87.7451 (84.6537) lr 1.3090e-03 eta 0:04:51
epoch [22/50] batch [10/23] time 0.142 (0.298) data 0.000 (0.143) loss 0.5922 (0.5446) acc 81.9149 (83.8971) lr 1.3090e-03 eta 0:03:15
epoch [22/50] batch [15/23] time 0.141 (0.247) data 0.000 (0.095) loss 0.5831 (0.5432) acc 83.1633 (83.5032) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [20/23] time 0.134 (0.220) data 0.000 (0.072) loss 0.8455 (0.5561) acc 75.5319 (84.1656) lr 1.3090e-03 eta 0:02:22
>>> alpha1: 0.188  alpha2: 0.022 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.66 <<<
epoch [23/50] batch [5/23] time 0.160 (0.590) data 0.000 (0.286) loss 0.5116 (0.4995) acc 87.2727 (87.4217) lr 1.2487e-03 eta 0:06:16
epoch [23/50] batch [10/23] time 0.142 (0.444) data 0.001 (0.143) loss 0.5033 (0.5145) acc 84.8958 (86.7138) lr 1.2487e-03 eta 0:04:41
epoch [23/50] batch [15/23] time 0.155 (0.346) data 0.000 (0.096) loss 0.4408 (0.5171) acc 83.0357 (85.3767) lr 1.2487e-03 eta 0:03:37
epoch [23/50] batch [20/23] time 0.153 (0.297) data 0.000 (0.072) loss 0.5202 (0.5407) acc 84.7222 (84.5898) lr 1.2487e-03 eta 0:03:05
>>> alpha1: 0.176  alpha2: 0.017 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.65 <<<
epoch [24/50] batch [5/23] time 0.127 (0.441) data 0.000 (0.294) loss 0.5097 (0.4375) acc 84.0909 (87.5077) lr 1.1874e-03 eta 0:04:31
epoch [24/50] batch [10/23] time 0.159 (0.295) data 0.000 (0.147) loss 0.5531 (0.4708) acc 82.9787 (86.9128) lr 1.1874e-03 eta 0:03:00
epoch [24/50] batch [15/23] time 0.152 (0.246) data 0.000 (0.098) loss 0.4276 (0.4702) acc 81.9444 (86.5178) lr 1.1874e-03 eta 0:02:29
epoch [24/50] batch [20/23] time 0.155 (0.220) data 0.000 (0.074) loss 0.5760 (0.5046) acc 83.7963 (85.1908) lr 1.1874e-03 eta 0:02:12
>>> alpha1: 0.166  alpha2: 0.020 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.63 <<<
epoch [25/50] batch [5/23] time 0.146 (0.418) data 0.000 (0.261) loss 0.5506 (0.4965) acc 78.8889 (85.9878) lr 1.1253e-03 eta 0:04:07
epoch [25/50] batch [10/23] time 0.150 (0.282) data 0.000 (0.132) loss 0.5173 (0.5023) acc 87.5000 (86.6538) lr 1.1253e-03 eta 0:02:45
epoch [25/50] batch [15/23] time 0.149 (0.238) data 0.000 (0.088) loss 0.5613 (0.4735) acc 90.3846 (87.0065) lr 1.1253e-03 eta 0:02:18
epoch [25/50] batch [20/23] time 0.147 (0.216) data 0.001 (0.066) loss 0.6823 (0.4721) acc 80.8824 (87.4091) lr 1.1253e-03 eta 0:02:04
>>> alpha1: 0.162  alpha2: 0.025 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [26/50] batch [5/23] time 0.164 (0.418) data 0.000 (0.255) loss 0.3535 (0.4170) acc 92.5926 (88.8789) lr 1.0628e-03 eta 0:03:58
epoch [26/50] batch [10/23] time 0.149 (0.287) data 0.000 (0.128) loss 0.4117 (0.4599) acc 91.8269 (88.6780) lr 1.0628e-03 eta 0:02:42
epoch [26/50] batch [15/23] time 0.138 (0.241) data 0.000 (0.085) loss 0.6534 (0.4589) acc 81.6327 (88.5354) lr 1.0628e-03 eta 0:02:14
epoch [26/50] batch [20/23] time 0.147 (0.217) data 0.000 (0.064) loss 0.4660 (0.4585) acc 89.9038 (87.5676) lr 1.0628e-03 eta 0:02:00
>>> alpha1: 0.155  alpha2: 0.027 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.65 <<<
epoch [27/50] batch [5/23] time 0.158 (0.466) data 0.000 (0.313) loss 0.3342 (0.4833) acc 90.7895 (83.6723) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [10/23] time 0.144 (0.308) data 0.000 (0.157) loss 0.3652 (0.4347) acc 87.2549 (86.4995) lr 1.0000e-03 eta 0:02:46
epoch [27/50] batch [15/23] time 0.152 (0.253) data 0.000 (0.105) loss 0.3152 (0.4308) acc 91.3462 (87.2129) lr 1.0000e-03 eta 0:02:15
epoch [27/50] batch [20/23] time 0.148 (0.226) data 0.000 (0.078) loss 0.4640 (0.4326) acc 87.0192 (87.4639) lr 1.0000e-03 eta 0:02:00
>>> alpha1: 0.153  alpha2: 0.026 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.64 <<<
epoch [28/50] batch [5/23] time 0.149 (0.456) data 0.001 (0.308) loss 0.3781 (0.4381) acc 90.8654 (87.1972) lr 9.3721e-04 eta 0:03:58
epoch [28/50] batch [10/23] time 0.140 (0.306) data 0.000 (0.154) loss 0.3622 (0.4021) acc 90.1042 (87.9929) lr 9.3721e-04 eta 0:02:38
epoch [28/50] batch [15/23] time 0.145 (0.254) data 0.000 (0.103) loss 0.3467 (0.4029) acc 90.1961 (88.3053) lr 9.3721e-04 eta 0:02:10
epoch [28/50] batch [20/23] time 0.143 (0.226) data 0.000 (0.077) loss 0.3854 (0.4214) acc 91.5000 (87.5986) lr 9.3721e-04 eta 0:01:54
>>> alpha1: 0.153  alpha2: 0.030 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.65 <<<
epoch [29/50] batch [5/23] time 0.157 (0.437) data 0.000 (0.280) loss 0.3972 (0.4018) acc 83.3333 (88.0046) lr 8.7467e-04 eta 0:03:39
epoch [29/50] batch [10/23] time 0.140 (0.293) data 0.000 (0.140) loss 0.6286 (0.4220) acc 86.7347 (87.7845) lr 8.7467e-04 eta 0:02:25
epoch [29/50] batch [15/23] time 0.144 (0.244) data 0.000 (0.094) loss 0.6193 (0.4355) acc 80.6122 (87.9375) lr 8.7467e-04 eta 0:01:59
epoch [29/50] batch [20/23] time 0.152 (0.221) data 0.000 (0.070) loss 0.3516 (0.4280) acc 89.6226 (88.0232) lr 8.7467e-04 eta 0:01:47
>>> alpha1: 0.152  alpha2: 0.028 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.64 <<<
epoch [30/50] batch [5/23] time 0.156 (0.409) data 0.000 (0.261) loss 0.3313 (0.4094) acc 88.5417 (89.0887) lr 8.1262e-04 eta 0:03:15
epoch [30/50] batch [10/23] time 0.160 (0.282) data 0.000 (0.131) loss 0.3913 (0.4377) acc 85.7843 (87.4106) lr 8.1262e-04 eta 0:02:13
epoch [30/50] batch [15/23] time 0.147 (0.237) data 0.000 (0.087) loss 0.4523 (0.4501) acc 87.2549 (87.9097) lr 8.1262e-04 eta 0:01:50
epoch [30/50] batch [20/23] time 0.161 (0.214) data 0.000 (0.066) loss 0.3372 (0.4425) acc 91.2281 (87.3894) lr 8.1262e-04 eta 0:01:39
>>> alpha1: 0.149  alpha2: 0.021 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.65 <<<
epoch [31/50] batch [5/23] time 0.160 (0.423) data 0.000 (0.267) loss 0.3619 (0.3898) acc 92.1569 (91.4250) lr 7.5131e-04 eta 0:03:12
epoch [31/50] batch [10/23] time 0.138 (0.283) data 0.000 (0.134) loss 0.4172 (0.4073) acc 90.2174 (88.6545) lr 7.5131e-04 eta 0:02:07
epoch [31/50] batch [15/23] time 0.135 (0.235) data 0.000 (0.089) loss 0.4258 (0.3960) acc 87.7778 (88.9390) lr 7.5131e-04 eta 0:01:44
epoch [31/50] batch [20/23] time 0.135 (0.213) data 0.000 (0.067) loss 0.2118 (0.3849) acc 91.4894 (89.3269) lr 7.5131e-04 eta 0:01:33
>>> alpha1: 0.149  alpha2: 0.019 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.65 <<<
epoch [32/50] batch [5/23] time 0.160 (0.425) data 0.000 (0.273) loss 0.3163 (0.3376) acc 87.0000 (89.9123) lr 6.9098e-04 eta 0:03:03
epoch [32/50] batch [10/23] time 0.153 (0.290) data 0.000 (0.137) loss 0.4633 (0.3643) acc 83.9623 (89.8901) lr 6.9098e-04 eta 0:02:04
epoch [32/50] batch [15/23] time 0.133 (0.242) data 0.000 (0.091) loss 0.3486 (0.3929) acc 86.1702 (89.1101) lr 6.9098e-04 eta 0:01:42
epoch [32/50] batch [20/23] time 0.148 (0.218) data 0.000 (0.068) loss 0.3846 (0.4007) acc 85.8491 (88.5787) lr 6.9098e-04 eta 0:01:30
>>> alpha1: 0.149  alpha2: 0.025 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.63 <<<
epoch [33/50] batch [5/23] time 0.158 (0.466) data 0.000 (0.311) loss 0.3844 (0.4149) acc 93.1818 (89.1804) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [10/23] time 0.144 (0.306) data 0.000 (0.156) loss 0.2753 (0.4100) acc 96.4286 (90.4424) lr 6.3188e-04 eta 0:02:03
epoch [33/50] batch [15/23] time 0.153 (0.255) data 0.000 (0.104) loss 0.3965 (0.3944) acc 82.5472 (89.6443) lr 6.3188e-04 eta 0:01:41
epoch [33/50] batch [20/23] time 0.145 (0.228) data 0.000 (0.078) loss 0.3782 (0.3968) acc 84.0000 (89.7894) lr 6.3188e-04 eta 0:01:29
>>> alpha1: 0.146  alpha2: 0.021 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.64 <<<
epoch [34/50] batch [5/23] time 0.152 (0.410) data 0.000 (0.253) loss 0.4446 (0.3147) acc 88.8889 (92.4225) lr 5.7422e-04 eta 0:02:38
epoch [34/50] batch [10/23] time 0.146 (0.285) data 0.000 (0.127) loss 0.5126 (0.3661) acc 83.8542 (90.6956) lr 5.7422e-04 eta 0:01:48
epoch [34/50] batch [15/23] time 0.132 (0.239) data 0.000 (0.085) loss 0.5171 (0.3803) acc 82.4468 (89.4863) lr 5.7422e-04 eta 0:01:29
epoch [34/50] batch [20/23] time 0.131 (0.214) data 0.000 (0.063) loss 0.5420 (0.3932) acc 83.3333 (89.4830) lr 5.7422e-04 eta 0:01:19
>>> alpha1: 0.142  alpha2: 0.022 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.64 <<<
epoch [35/50] batch [5/23] time 0.148 (0.402) data 0.000 (0.255) loss 0.4722 (0.3826) acc 91.8269 (90.0125) lr 5.1825e-04 eta 0:02:26
epoch [35/50] batch [10/23] time 0.147 (0.279) data 0.000 (0.128) loss 0.3950 (0.3922) acc 91.3462 (90.7432) lr 5.1825e-04 eta 0:01:39
epoch [35/50] batch [15/23] time 0.156 (0.237) data 0.000 (0.086) loss 0.3759 (0.4015) acc 89.5455 (89.4169) lr 5.1825e-04 eta 0:01:23
epoch [35/50] batch [20/23] time 0.154 (0.216) data 0.000 (0.064) loss 0.3012 (0.4028) acc 95.8333 (88.9748) lr 5.1825e-04 eta 0:01:15
>>> alpha1: 0.143  alpha2: 0.031 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.65 <<<
epoch [36/50] batch [5/23] time 0.152 (0.428) data 0.000 (0.271) loss 0.5682 (0.3947) acc 89.8936 (89.7608) lr 4.6417e-04 eta 0:02:25
epoch [36/50] batch [10/23] time 0.146 (0.289) data 0.000 (0.136) loss 0.5316 (0.4067) acc 87.0192 (90.5443) lr 4.6417e-04 eta 0:01:36
epoch [36/50] batch [15/23] time 0.154 (0.245) data 0.001 (0.091) loss 0.3998 (0.3971) acc 86.3208 (89.7608) lr 4.6417e-04 eta 0:01:20
epoch [36/50] batch [20/23] time 0.142 (0.221) data 0.000 (0.068) loss 0.3479 (0.3785) acc 92.8571 (89.8842) lr 4.6417e-04 eta 0:01:11
>>> alpha1: 0.141  alpha2: 0.034 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.66 <<<
epoch [37/50] batch [5/23] time 0.146 (0.431) data 0.000 (0.276) loss 0.3697 (0.3503) acc 92.3077 (92.6345) lr 4.1221e-04 eta 0:02:16
epoch [37/50] batch [10/23] time 0.157 (0.290) data 0.000 (0.138) loss 0.3388 (0.3725) acc 85.9091 (90.1755) lr 4.1221e-04 eta 0:01:30
epoch [37/50] batch [15/23] time 0.151 (0.244) data 0.000 (0.092) loss 0.2319 (0.3409) acc 95.2830 (90.8180) lr 4.1221e-04 eta 0:01:14
epoch [37/50] batch [20/23] time 0.156 (0.222) data 0.000 (0.069) loss 0.3239 (0.3529) acc 90.7407 (90.4533) lr 4.1221e-04 eta 0:01:06
>>> alpha1: 0.136  alpha2: 0.037 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.65 <<<
epoch [38/50] batch [5/23] time 0.167 (0.428) data 0.000 (0.266) loss 0.4503 (0.5198) acc 87.7273 (89.7782) lr 3.6258e-04 eta 0:02:05
epoch [38/50] batch [10/23] time 0.148 (0.289) data 0.000 (0.133) loss 0.5113 (0.4551) acc 84.8039 (88.9790) lr 3.6258e-04 eta 0:01:23
epoch [38/50] batch [15/23] time 0.146 (0.243) data 0.000 (0.089) loss 0.4241 (0.4212) acc 91.1765 (89.6629) lr 3.6258e-04 eta 0:01:09
epoch [38/50] batch [20/23] time 0.142 (0.219) data 0.000 (0.067) loss 0.4847 (0.4187) acc 85.7143 (88.8949) lr 3.6258e-04 eta 0:01:01
>>> alpha1: 0.136  alpha2: 0.040 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.65 <<<
epoch [39/50] batch [5/23] time 0.159 (0.442) data 0.000 (0.283) loss 0.5295 (0.3885) acc 87.0192 (90.2218) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [10/23] time 0.157 (0.295) data 0.000 (0.142) loss 0.2668 (0.3865) acc 93.6364 (89.3209) lr 3.1545e-04 eta 0:01:18
epoch [39/50] batch [15/23] time 0.160 (0.248) data 0.000 (0.095) loss 0.2980 (0.3815) acc 93.4211 (90.3492) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [20/23] time 0.159 (0.223) data 0.000 (0.071) loss 0.2677 (0.3628) acc 97.7679 (90.4939) lr 3.1545e-04 eta 0:00:57
>>> alpha1: 0.137  alpha2: 0.042 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.65 <<<
epoch [40/50] batch [5/23] time 0.152 (0.460) data 0.000 (0.295) loss 0.5958 (0.3925) acc 86.7924 (88.1281) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [10/23] time 0.149 (0.308) data 0.000 (0.148) loss 0.4522 (0.3595) acc 87.5000 (89.8657) lr 2.7103e-04 eta 0:01:14
epoch [40/50] batch [15/23] time 0.149 (0.255) data 0.000 (0.099) loss 0.3870 (0.3590) acc 83.4906 (89.2477) lr 2.7103e-04 eta 0:01:00
epoch [40/50] batch [20/23] time 0.150 (0.229) data 0.000 (0.074) loss 0.2601 (0.3536) acc 94.3396 (89.8057) lr 2.7103e-04 eta 0:00:53
>>> alpha1: 0.136  alpha2: 0.040 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.67 <<<
epoch [41/50] batch [5/23] time 0.153 (0.437) data 0.000 (0.277) loss 0.3875 (0.3388) acc 86.3208 (89.8362) lr 2.2949e-04 eta 0:01:38
epoch [41/50] batch [10/23] time 0.142 (0.370) data 0.000 (0.139) loss 0.4158 (0.3257) acc 85.7843 (91.0314) lr 2.2949e-04 eta 0:01:21
epoch [41/50] batch [15/23] time 0.148 (0.296) data 0.001 (0.093) loss 0.6634 (0.3552) acc 81.8627 (90.6770) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [20/23] time 0.149 (0.260) data 0.000 (0.070) loss 0.2660 (0.3375) acc 92.3077 (90.9573) lr 2.2949e-04 eta 0:00:54
>>> alpha1: 0.133  alpha2: 0.040 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.66 <<<
epoch [42/50] batch [5/23] time 0.159 (0.443) data 0.000 (0.279) loss 0.3651 (0.3599) acc 90.0943 (90.7923) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [10/23] time 0.150 (0.300) data 0.000 (0.141) loss 0.5016 (0.3308) acc 91.3462 (90.9606) lr 1.9098e-04 eta 0:00:59
epoch [42/50] batch [15/23] time 0.150 (0.249) data 0.000 (0.094) loss 0.4392 (0.3265) acc 88.2075 (91.8678) lr 1.9098e-04 eta 0:00:47
epoch [42/50] batch [20/23] time 0.147 (0.225) data 0.000 (0.071) loss 0.5399 (0.3359) acc 82.8431 (91.4189) lr 1.9098e-04 eta 0:00:42
>>> alpha1: 0.131  alpha2: 0.036 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.65 <<<
epoch [43/50] batch [5/23] time 0.163 (0.468) data 0.000 (0.304) loss 0.3208 (0.3411) acc 94.2308 (93.1771) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [10/23] time 0.170 (0.315) data 0.000 (0.152) loss 0.2040 (0.3331) acc 96.4286 (91.8441) lr 1.5567e-04 eta 0:00:54
epoch [43/50] batch [15/23] time 0.142 (0.258) data 0.000 (0.102) loss 0.4421 (0.3200) acc 91.5000 (92.1214) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [20/23] time 0.163 (0.232) data 0.000 (0.076) loss 0.2779 (0.3205) acc 92.6724 (91.5080) lr 1.5567e-04 eta 0:00:38
>>> alpha1: 0.129  alpha2: 0.032 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.63 <<<
epoch [44/50] batch [5/23] time 0.172 (0.429) data 0.000 (0.265) loss 0.1671 (0.3913) acc 94.8113 (92.9665) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [10/23] time 0.141 (0.289) data 0.000 (0.133) loss 0.3727 (0.3763) acc 87.5000 (91.0989) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [15/23] time 0.153 (0.242) data 0.000 (0.089) loss 0.4802 (0.3702) acc 91.0377 (91.2187) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [20/23] time 0.152 (0.219) data 0.000 (0.067) loss 0.2899 (0.3422) acc 93.3962 (91.1277) lr 1.2369e-04 eta 0:00:30
>>> alpha1: 0.127  alpha2: 0.032 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [45/50] batch [5/23] time 0.174 (0.448) data 0.000 (0.279) loss 0.2777 (0.3091) acc 90.1786 (91.5096) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [10/23] time 0.150 (0.300) data 0.000 (0.140) loss 0.3487 (0.3125) acc 90.1961 (90.9170) lr 9.5173e-05 eta 0:00:38
epoch [45/50] batch [15/23] time 0.134 (0.248) data 0.000 (0.093) loss 0.4219 (0.3296) acc 85.6383 (89.7393) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [20/23] time 0.151 (0.223) data 0.000 (0.070) loss 0.3917 (0.3357) acc 83.6364 (89.9302) lr 9.5173e-05 eta 0:00:26
>>> alpha1: 0.126  alpha2: 0.037 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.63 <<<
epoch [46/50] batch [5/23] time 0.149 (0.411) data 0.000 (0.268) loss 0.3407 (0.3120) acc 91.4894 (91.5544) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [10/23] time 0.141 (0.279) data 0.000 (0.134) loss 0.2908 (0.3275) acc 95.9184 (91.0076) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [15/23] time 0.148 (0.234) data 0.000 (0.090) loss 0.2758 (0.3305) acc 91.8367 (91.4208) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/23] time 0.135 (0.211) data 0.000 (0.067) loss 0.4407 (0.3392) acc 84.5745 (90.7028) lr 7.0224e-05 eta 0:00:20
>>> alpha1: 0.126  alpha2: 0.037 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.56 <<<
epoch [47/50] batch [5/23] time 0.137 (0.436) data 0.000 (0.290) loss 0.3306 (0.3204) acc 90.9574 (91.4415) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [10/23] time 0.146 (0.289) data 0.000 (0.145) loss 0.3055 (0.3370) acc 91.5000 (90.0341) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/23] time 0.134 (0.236) data 0.001 (0.097) loss 0.2356 (0.3312) acc 95.6522 (90.9567) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.132 (0.211) data 0.000 (0.073) loss 0.2826 (0.3191) acc 94.1489 (91.5329) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.125  alpha2: 0.040 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.62 <<<
epoch [48/50] batch [5/23] time 0.166 (0.441) data 0.000 (0.280) loss 0.3464 (0.3062) acc 88.1818 (92.0590) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.168 (0.299) data 0.000 (0.140) loss 0.2680 (0.3312) acc 95.4546 (91.4738) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.140 (0.247) data 0.000 (0.094) loss 0.1875 (0.3251) acc 93.2292 (91.0281) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.151 (0.221) data 0.000 (0.070) loss 0.3333 (0.3274) acc 90.2778 (91.3520) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.127  alpha2: 0.045 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.63 <<<
epoch [49/50] batch [5/23] time 0.165 (0.461) data 0.000 (0.301) loss 0.3504 (0.3905) acc 89.9038 (90.0971) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.142 (0.306) data 0.001 (0.151) loss 0.3894 (0.3381) acc 85.9375 (89.6968) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [15/23] time 0.145 (0.253) data 0.000 (0.101) loss 0.2435 (0.3181) acc 98.0000 (91.2387) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.146 (0.226) data 0.000 (0.075) loss 0.2890 (0.3239) acc 87.5000 (90.9042) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.128  alpha2: 0.047 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.63 <<<
epoch [50/50] batch [5/23] time 0.142 (0.418) data 0.000 (0.260) loss 0.4053 (0.3377) acc 85.7143 (90.5463) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.148 (0.288) data 0.000 (0.130) loss 0.2331 (0.3148) acc 92.6471 (90.5962) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/23] time 0.159 (0.242) data 0.000 (0.087) loss 0.2125 (0.3153) acc 94.1964 (91.5704) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/23] time 0.146 (0.219) data 0.000 (0.065) loss 0.2695 (0.3220) acc 88.2353 (91.3204) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.44, 0.43, 0.44, 0.44, 0.41, 0.43, 0.43, 0.43, 0.41, 0.41, 0.4, 0.4, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.4, 0.41, 0.41, 0.4, 0.41, 0.4, 0.41, 0.4, 0.41, 0.4, 0.41, 0.4, 0.41, 0.41, 0.41, 0.4, 0.41, 0.41, 0.41, 0.41, 0.4]
* matched noise rate: [0.23, 0.19, 0.18, 0.3, 0.17, 0.19, 0.25, 0.2, 0.22, 0.21, 0.24, 0.24, 0.28, 0.25, 0.26, 0.26, 0.24, 0.24, 0.25, 0.25, 0.22, 0.23, 0.24, 0.25, 0.27, 0.28, 0.27, 0.26, 0.26, 0.28, 0.27, 0.28, 0.28, 0.27, 0.26, 0.24, 0.21, 0.25, 0.25, 0.28]
* unmatched noise rate: [0.63, 0.6, 0.58, 0.66, 0.54, 0.56, 0.6, 0.57, 0.6, 0.62, 0.61, 0.62, 0.66, 0.65, 0.63, 0.64, 0.65, 0.64, 0.65, 0.64, 0.65, 0.65, 0.63, 0.64, 0.64, 0.65, 0.66, 0.65, 0.65, 0.65, 0.67, 0.66, 0.65, 0.63, 0.64, 0.63, 0.56, 0.62, 0.63, 0.63]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:39,  2.47s/it] 18%|█▊        | 3/17 [00:02<00:09,  1.41it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.60it/s] 41%|████      | 7/17 [00:02<00:02,  3.99it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.50it/s] 65%|██████▍   | 11/17 [00:03<00:00,  7.04it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.52it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.85it/s]100%|██████████| 17/17 [00:04<00:00,  5.86it/s]100%|██████████| 17/17 [00:04<00:00,  3.95it/s]
=> result
* total: 1,692
* correct: 832
* accuracy: 49.2%
* error: 50.8%
* macro_f1: 44.6%
=> per-class result
* class: 0 (banded)	total: 36	correct: 31	acc: 86.1%
* class: 1 (blotchy)	total: 36	correct: 0	acc: 0.0%
* class: 2 (braided)	total: 36	correct: 17	acc: 47.2%
* class: 3 (bubbly)	total: 36	correct: 31	acc: 86.1%
* class: 4 (bumpy)	total: 36	correct: 0	acc: 0.0%
* class: 5 (chequered)	total: 36	correct: 36	acc: 100.0%
* class: 6 (cobwebbed)	total: 36	correct: 26	acc: 72.2%
* class: 7 (cracked)	total: 36	correct: 30	acc: 83.3%
* class: 8 (crosshatched)	total: 36	correct: 10	acc: 27.8%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 5	acc: 13.9%
* class: 11 (fibrous)	total: 36	correct: 0	acc: 0.0%
* class: 12 (flecked)	total: 36	correct: 10	acc: 27.8%
* class: 13 (freckled)	total: 36	correct: 28	acc: 77.8%
* class: 14 (frilly)	total: 36	correct: 18	acc: 50.0%
* class: 15 (gauzy)	total: 36	correct: 21	acc: 58.3%
* class: 16 (grid)	total: 36	correct: 14	acc: 38.9%
* class: 17 (grooved)	total: 36	correct: 14	acc: 38.9%
* class: 18 (honeycombed)	total: 36	correct: 19	acc: 52.8%
* class: 19 (interlaced)	total: 36	correct: 0	acc: 0.0%
* class: 20 (knitted)	total: 36	correct: 36	acc: 100.0%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 6	acc: 16.7%
* class: 23 (marbled)	total: 36	correct: 3	acc: 8.3%
* class: 24 (matted)	total: 36	correct: 17	acc: 47.2%
* class: 25 (meshed)	total: 36	correct: 13	acc: 36.1%
* class: 26 (paisley)	total: 36	correct: 32	acc: 88.9%
* class: 27 (perforated)	total: 36	correct: 26	acc: 72.2%
* class: 28 (pitted)	total: 36	correct: 0	acc: 0.0%
* class: 29 (pleated)	total: 36	correct: 7	acc: 19.4%
* class: 30 (polka-dotted)	total: 36	correct: 20	acc: 55.6%
* class: 31 (porous)	total: 36	correct: 16	acc: 44.4%
* class: 32 (potholed)	total: 36	correct: 27	acc: 75.0%
* class: 33 (scaly)	total: 36	correct: 24	acc: 66.7%
* class: 34 (smeared)	total: 36	correct: 17	acc: 47.2%
* class: 35 (spiralled)	total: 36	correct: 16	acc: 44.4%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 0	acc: 0.0%
* class: 38 (stratified)	total: 36	correct: 29	acc: 80.6%
* class: 39 (striped)	total: 36	correct: 28	acc: 77.8%
* class: 40 (studded)	total: 36	correct: 31	acc: 86.1%
* class: 41 (swirly)	total: 36	correct: 24	acc: 66.7%
* class: 42 (veined)	total: 36	correct: 16	acc: 44.4%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 11	acc: 30.6%
* class: 45 (wrinkled)	total: 36	correct: 19	acc: 52.8%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 49.2%
Elapsed: 0:12:50
Run this job and save the output to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_10-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.149 (0.970) data 0.000 (0.282) loss 3.6863 (3.6798) acc 18.7500 (14.3750) lr 1.0000e-05 eta 0:18:30
epoch [1/50] batch [10/23] time 0.175 (0.570) data 0.000 (0.141) loss 3.7868 (3.7401) acc 9.3750 (11.5625) lr 1.0000e-05 eta 0:10:49
epoch [1/50] batch [15/23] time 0.164 (0.434) data 0.000 (0.094) loss 3.7125 (3.7424) acc 15.6250 (11.0417) lr 1.0000e-05 eta 0:08:12
epoch [1/50] batch [20/23] time 0.163 (0.367) data 0.000 (0.071) loss 3.8099 (3.7481) acc 3.1250 (10.0000) lr 1.0000e-05 eta 0:06:54
epoch [2/50] batch [5/23] time 0.152 (0.461) data 0.000 (0.274) loss 3.6008 (3.7498) acc 15.6250 (8.1250) lr 2.0000e-03 eta 0:08:37
epoch [2/50] batch [10/23] time 0.149 (0.316) data 0.000 (0.137) loss 3.6313 (3.7427) acc 15.6250 (9.3750) lr 2.0000e-03 eta 0:05:52
epoch [2/50] batch [15/23] time 0.149 (0.260) data 0.000 (0.091) loss 3.8641 (3.7412) acc 3.1250 (9.5833) lr 2.0000e-03 eta 0:04:48
epoch [2/50] batch [20/23] time 0.147 (0.232) data 0.000 (0.069) loss 3.6522 (3.7381) acc 15.6250 (9.8438) lr 2.0000e-03 eta 0:04:16
epoch [3/50] batch [5/23] time 0.168 (0.437) data 0.000 (0.253) loss 3.9015 (3.7050) acc 12.5000 (11.2500) lr 1.9980e-03 eta 0:07:59
epoch [3/50] batch [10/23] time 0.158 (0.297) data 0.000 (0.127) loss 3.7107 (3.6706) acc 18.7500 (13.7500) lr 1.9980e-03 eta 0:05:24
epoch [3/50] batch [15/23] time 0.147 (0.247) data 0.000 (0.085) loss 3.7026 (3.6466) acc 18.7500 (15.4167) lr 1.9980e-03 eta 0:04:28
epoch [3/50] batch [20/23] time 0.147 (0.222) data 0.000 (0.063) loss 3.5925 (3.6444) acc 9.3750 (14.8438) lr 1.9980e-03 eta 0:04:00
epoch [4/50] batch [5/23] time 0.148 (0.437) data 0.000 (0.261) loss 3.6085 (3.6157) acc 9.3750 (14.3750) lr 1.9921e-03 eta 0:07:50
epoch [4/50] batch [10/23] time 0.145 (0.299) data 0.000 (0.131) loss 3.4623 (3.6052) acc 21.8750 (14.0625) lr 1.9921e-03 eta 0:05:20
epoch [4/50] batch [15/23] time 0.149 (0.249) data 0.000 (0.087) loss 3.7149 (3.6021) acc 12.5000 (15.0000) lr 1.9921e-03 eta 0:04:25
epoch [4/50] batch [20/23] time 0.151 (0.224) data 0.000 (0.065) loss 3.4019 (3.6035) acc 18.7500 (15.4688) lr 1.9921e-03 eta 0:03:57
epoch [5/50] batch [5/23] time 0.151 (0.446) data 0.000 (0.264) loss 3.5318 (3.4662) acc 25.0000 (20.6250) lr 1.9823e-03 eta 0:07:49
epoch [5/50] batch [10/23] time 0.157 (0.307) data 0.000 (0.132) loss 3.7011 (3.5171) acc 9.3750 (18.7500) lr 1.9823e-03 eta 0:05:21
epoch [5/50] batch [15/23] time 0.147 (0.254) data 0.000 (0.088) loss 3.2110 (3.5184) acc 34.3750 (19.3750) lr 1.9823e-03 eta 0:04:25
epoch [5/50] batch [20/23] time 0.156 (0.229) data 0.000 (0.066) loss 3.5061 (3.5563) acc 12.5000 (17.3438) lr 1.9823e-03 eta 0:03:57
epoch [6/50] batch [5/23] time 0.222 (0.430) data 0.000 (0.239) loss 3.3429 (3.4424) acc 21.8750 (19.3750) lr 1.9686e-03 eta 0:07:23
epoch [6/50] batch [10/23] time 0.176 (0.298) data 0.000 (0.121) loss 3.3849 (3.5169) acc 15.6250 (17.1875) lr 1.9686e-03 eta 0:05:05
epoch [6/50] batch [15/23] time 0.150 (0.248) data 0.000 (0.081) loss 3.5018 (3.5234) acc 12.5000 (16.0417) lr 1.9686e-03 eta 0:04:12
epoch [6/50] batch [20/23] time 0.148 (0.223) data 0.000 (0.061) loss 3.8559 (3.5242) acc 12.5000 (17.3438) lr 1.9686e-03 eta 0:03:46
epoch [7/50] batch [5/23] time 0.152 (0.478) data 0.000 (0.289) loss 3.3118 (3.3816) acc 28.1250 (24.3750) lr 1.9511e-03 eta 0:08:01
epoch [7/50] batch [10/23] time 0.160 (0.323) data 0.000 (0.145) loss 3.4958 (3.4851) acc 12.5000 (19.6875) lr 1.9511e-03 eta 0:05:23
epoch [7/50] batch [15/23] time 0.151 (0.266) data 0.000 (0.097) loss 3.3576 (3.4474) acc 21.8750 (20.2083) lr 1.9511e-03 eta 0:04:25
epoch [7/50] batch [20/23] time 0.148 (0.237) data 0.000 (0.073) loss 3.5397 (3.4514) acc 15.6250 (19.8438) lr 1.9511e-03 eta 0:03:55
epoch [8/50] batch [5/23] time 0.175 (0.457) data 0.000 (0.266) loss 3.2408 (3.4179) acc 25.0000 (17.5000) lr 1.9298e-03 eta 0:07:30
epoch [8/50] batch [10/23] time 0.160 (0.310) data 0.000 (0.133) loss 3.3724 (3.4378) acc 21.8750 (18.1250) lr 1.9298e-03 eta 0:05:03
epoch [8/50] batch [15/23] time 0.148 (0.256) data 0.000 (0.089) loss 3.9546 (3.4620) acc 9.3750 (18.3333) lr 1.9298e-03 eta 0:04:09
epoch [8/50] batch [20/23] time 0.146 (0.229) data 0.000 (0.067) loss 3.6851 (3.4631) acc 15.6250 (18.1250) lr 1.9298e-03 eta 0:03:41
epoch [9/50] batch [5/23] time 0.153 (0.449) data 0.000 (0.262) loss 3.3491 (3.4485) acc 28.1250 (19.3750) lr 1.9048e-03 eta 0:07:11
epoch [9/50] batch [10/23] time 0.161 (0.307) data 0.000 (0.131) loss 3.1515 (3.4470) acc 28.1250 (21.2500) lr 1.9048e-03 eta 0:04:53
epoch [9/50] batch [15/23] time 0.147 (0.254) data 0.000 (0.087) loss 3.0039 (3.4107) acc 28.1250 (21.0417) lr 1.9048e-03 eta 0:04:01
epoch [9/50] batch [20/23] time 0.146 (0.227) data 0.000 (0.066) loss 3.2785 (3.4361) acc 12.5000 (20.3125) lr 1.9048e-03 eta 0:03:34
epoch [10/50] batch [5/23] time 0.151 (0.503) data 0.000 (0.324) loss 3.5541 (3.4630) acc 12.5000 (20.0000) lr 1.8763e-03 eta 0:07:51
epoch [10/50] batch [10/23] time 0.160 (0.335) data 0.000 (0.162) loss 3.1709 (3.3602) acc 28.1250 (22.1875) lr 1.8763e-03 eta 0:05:12
epoch [10/50] batch [15/23] time 0.147 (0.272) data 0.000 (0.108) loss 3.2933 (3.4099) acc 15.6250 (19.7917) lr 1.8763e-03 eta 0:04:12
epoch [10/50] batch [20/23] time 0.146 (0.241) data 0.000 (0.081) loss 3.5005 (3.4258) acc 18.7500 (19.0625) lr 1.8763e-03 eta 0:03:42
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.765  alpha2: 0.453 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.73 <<<
epoch [11/50] batch [5/23] time 0.803 (1.104) data 0.001 (0.267) loss 2.7065 (2.8468) acc 62.7551 (69.5847) lr 1.8443e-03 eta 0:16:50
epoch [11/50] batch [10/23] time 0.153 (0.767) data 0.000 (0.134) loss 2.3650 (2.7841) acc 68.2692 (65.0925) lr 1.8443e-03 eta 0:11:38
epoch [11/50] batch [15/23] time 0.729 (0.600) data 0.001 (0.089) loss 2.6180 (2.7607) acc 57.2222 (64.8595) lr 1.8443e-03 eta 0:09:03
epoch [11/50] batch [20/23] time 0.141 (0.486) data 0.000 (0.067) loss 2.6600 (2.7653) acc 61.0000 (64.2989) lr 1.8443e-03 eta 0:07:17
>>> alpha1: 0.849  alpha2: 0.473 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.56 <<<
epoch [12/50] batch [5/23] time 0.144 (0.892) data 0.000 (0.286) loss 1.5780 (1.9107) acc 66.3265 (66.5815) lr 1.8090e-03 eta 0:13:15
epoch [12/50] batch [10/23] time 0.130 (0.569) data 0.000 (0.143) loss 2.0290 (1.8842) acc 67.6136 (65.0188) lr 1.8090e-03 eta 0:08:24
epoch [12/50] batch [15/23] time 0.127 (0.458) data 0.000 (0.096) loss 2.1706 (1.9339) acc 61.9048 (63.0545) lr 1.8090e-03 eta 0:06:43
epoch [12/50] batch [20/23] time 0.130 (0.403) data 0.000 (0.072) loss 1.6472 (1.8937) acc 76.1111 (63.8104) lr 1.8090e-03 eta 0:05:53
>>> alpha1: 0.826  alpha2: 0.385 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.31 & unmatched refined noisy rate: 0.63 <<<
epoch [13/50] batch [5/23] time 0.851 (0.682) data 0.000 (0.248) loss 1.5795 (1.5793) acc 58.3333 (60.7959) lr 1.7705e-03 eta 0:09:52
epoch [13/50] batch [10/23] time 0.139 (0.415) data 0.000 (0.124) loss 1.9652 (1.6323) acc 48.4375 (60.0528) lr 1.7705e-03 eta 0:05:58
epoch [13/50] batch [15/23] time 0.143 (0.373) data 0.000 (0.083) loss 1.4063 (1.6253) acc 67.0000 (60.7952) lr 1.7705e-03 eta 0:05:20
epoch [13/50] batch [20/23] time 0.142 (0.317) data 0.000 (0.062) loss 1.3374 (1.6231) acc 69.8980 (60.4614) lr 1.7705e-03 eta 0:04:30
>>> alpha1: 0.764  alpha2: 0.326 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.30 & unmatched refined noisy rate: 0.64 <<<
epoch [14/50] batch [5/23] time 0.154 (0.414) data 0.000 (0.268) loss 1.2207 (1.3794) acc 68.2692 (67.8632) lr 1.7290e-03 eta 0:05:50
epoch [14/50] batch [10/23] time 0.136 (0.354) data 0.000 (0.134) loss 1.3989 (1.4412) acc 71.3542 (69.4729) lr 1.7290e-03 eta 0:04:58
epoch [14/50] batch [15/23] time 0.159 (0.287) data 0.000 (0.090) loss 1.1507 (1.4200) acc 72.7679 (67.8181) lr 1.7290e-03 eta 0:03:59
epoch [14/50] batch [20/23] time 0.737 (0.282) data 0.000 (0.067) loss 1.2543 (1.3969) acc 60.8696 (65.9467) lr 1.7290e-03 eta 0:03:54
>>> alpha1: 0.713  alpha2: 0.289 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.32 & unmatched refined noisy rate: 0.61 <<<
epoch [15/50] batch [5/23] time 0.162 (0.454) data 0.000 (0.297) loss 1.3216 (1.1789) acc 64.0000 (68.1156) lr 1.6845e-03 eta 0:06:13
epoch [15/50] batch [10/23] time 0.153 (0.305) data 0.000 (0.149) loss 1.2035 (1.2255) acc 60.3774 (67.0007) lr 1.6845e-03 eta 0:04:09
epoch [15/50] batch [15/23] time 0.151 (0.254) data 0.000 (0.099) loss 1.6307 (1.2078) acc 56.0185 (67.2283) lr 1.6845e-03 eta 0:03:26
epoch [15/50] batch [20/23] time 0.147 (0.264) data 0.001 (0.075) loss 1.0082 (1.2089) acc 76.4706 (67.5316) lr 1.6845e-03 eta 0:03:33
>>> alpha1: 0.649  alpha2: 0.221 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.61 <<<
epoch [16/50] batch [5/23] time 0.169 (0.422) data 0.000 (0.250) loss 1.2053 (1.2196) acc 75.9259 (71.7510) lr 1.6374e-03 eta 0:05:37
epoch [16/50] batch [10/23] time 0.148 (0.287) data 0.000 (0.125) loss 1.2703 (1.2929) acc 74.5000 (69.4538) lr 1.6374e-03 eta 0:03:47
epoch [16/50] batch [15/23] time 0.143 (0.238) data 0.000 (0.083) loss 1.5355 (1.2771) acc 61.7347 (67.9769) lr 1.6374e-03 eta 0:03:08
epoch [16/50] batch [20/23] time 0.152 (0.216) data 0.000 (0.063) loss 1.0499 (1.2404) acc 76.8868 (69.2286) lr 1.6374e-03 eta 0:02:49
>>> alpha1: 0.565  alpha2: 0.166 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.58 <<<
epoch [17/50] batch [5/23] time 0.162 (0.426) data 0.000 (0.268) loss 1.5783 (1.3588) acc 61.7647 (65.4862) lr 1.5878e-03 eta 0:05:31
epoch [17/50] batch [10/23] time 0.162 (0.288) data 0.000 (0.134) loss 1.0889 (1.2255) acc 73.0392 (67.3784) lr 1.5878e-03 eta 0:03:42
epoch [17/50] batch [15/23] time 0.142 (0.239) data 0.000 (0.090) loss 1.0631 (1.2066) acc 62.0000 (68.0541) lr 1.5878e-03 eta 0:03:03
epoch [17/50] batch [20/23] time 0.138 (0.214) data 0.000 (0.067) loss 0.7445 (1.1536) acc 86.2245 (69.5194) lr 1.5878e-03 eta 0:02:42
>>> alpha1: 0.495  alpha2: 0.134 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.55 <<<
epoch [18/50] batch [5/23] time 0.167 (0.425) data 0.000 (0.270) loss 1.0143 (1.0138) acc 79.3478 (72.5205) lr 1.5358e-03 eta 0:05:20
epoch [18/50] batch [10/23] time 0.157 (0.285) data 0.001 (0.135) loss 0.9200 (0.9967) acc 71.6667 (72.2553) lr 1.5358e-03 eta 0:03:33
epoch [18/50] batch [15/23] time 0.147 (0.236) data 0.000 (0.090) loss 0.7853 (0.9948) acc 65.8654 (71.3967) lr 1.5358e-03 eta 0:02:55
epoch [18/50] batch [20/23] time 0.126 (0.211) data 0.000 (0.068) loss 0.8268 (1.0226) acc 72.7273 (70.7642) lr 1.5358e-03 eta 0:02:36
>>> alpha1: 0.395  alpha2: 0.079 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.54 <<<
epoch [19/50] batch [5/23] time 0.162 (0.440) data 0.000 (0.281) loss 0.9232 (0.8202) acc 75.9804 (77.5557) lr 1.4818e-03 eta 0:05:21
epoch [19/50] batch [10/23] time 0.158 (0.298) data 0.000 (0.141) loss 1.1470 (0.9041) acc 75.4630 (74.9987) lr 1.4818e-03 eta 0:03:36
epoch [19/50] batch [15/23] time 0.150 (0.246) data 0.000 (0.094) loss 1.2910 (0.9354) acc 64.1509 (73.5869) lr 1.4818e-03 eta 0:02:57
epoch [19/50] batch [20/23] time 0.148 (0.221) data 0.001 (0.071) loss 0.9116 (0.9248) acc 66.8269 (74.0563) lr 1.4818e-03 eta 0:02:38
>>> alpha1: 0.372  alpha2: 0.065 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.57 <<<
epoch [20/50] batch [5/23] time 0.147 (0.488) data 0.000 (0.331) loss 0.6608 (0.8478) acc 84.6154 (76.8068) lr 1.4258e-03 eta 0:05:45
epoch [20/50] batch [10/23] time 0.147 (0.318) data 0.000 (0.166) loss 0.5642 (0.8285) acc 81.7308 (76.1494) lr 1.4258e-03 eta 0:03:43
epoch [20/50] batch [15/23] time 0.131 (0.259) data 0.000 (0.111) loss 0.8490 (0.8536) acc 69.4445 (75.4408) lr 1.4258e-03 eta 0:03:00
epoch [20/50] batch [20/23] time 0.145 (0.231) data 0.000 (0.083) loss 1.0213 (0.8668) acc 62.9808 (74.4633) lr 1.4258e-03 eta 0:02:39
>>> alpha1: 0.328  alpha2: 0.039 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.53 <<<
epoch [21/50] batch [5/23] time 0.166 (0.417) data 0.000 (0.260) loss 1.1086 (0.7030) acc 64.6739 (78.8422) lr 1.3681e-03 eta 0:04:45
epoch [21/50] batch [10/23] time 0.149 (0.280) data 0.000 (0.130) loss 0.7589 (0.7506) acc 79.2553 (78.1747) lr 1.3681e-03 eta 0:03:10
epoch [21/50] batch [15/23] time 0.147 (0.235) data 0.000 (0.087) loss 0.6909 (0.7728) acc 85.2941 (78.2409) lr 1.3681e-03 eta 0:02:38
epoch [21/50] batch [20/23] time 0.143 (0.211) data 0.000 (0.065) loss 0.8536 (0.7861) acc 77.0408 (77.6758) lr 1.3681e-03 eta 0:02:21
>>> alpha1: 0.290  alpha2: 0.025 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.55 <<<
epoch [22/50] batch [5/23] time 0.148 (0.482) data 0.000 (0.335) loss 0.7892 (0.6896) acc 77.0408 (79.7486) lr 1.3090e-03 eta 0:05:19
epoch [22/50] batch [10/23] time 0.182 (0.321) data 0.001 (0.168) loss 0.8114 (0.7289) acc 80.6122 (77.9884) lr 1.3090e-03 eta 0:03:31
epoch [22/50] batch [15/23] time 0.141 (0.262) data 0.000 (0.112) loss 1.0446 (0.7295) acc 67.8571 (77.7847) lr 1.3090e-03 eta 0:02:51
epoch [22/50] batch [20/23] time 0.137 (0.232) data 0.000 (0.084) loss 0.6419 (0.7382) acc 83.3333 (77.7663) lr 1.3090e-03 eta 0:02:29
>>> alpha1: 0.262  alpha2: 0.017 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.52 <<<
epoch [23/50] batch [5/23] time 0.157 (0.490) data 0.001 (0.342) loss 0.8082 (0.7548) acc 73.9583 (77.9887) lr 1.2487e-03 eta 0:05:13
epoch [23/50] batch [10/23] time 0.137 (0.314) data 0.000 (0.171) loss 0.7007 (0.7526) acc 77.8409 (77.5939) lr 1.2487e-03 eta 0:03:18
epoch [23/50] batch [15/23] time 0.135 (0.254) data 0.000 (0.114) loss 0.9159 (0.7031) acc 75.0000 (79.2292) lr 1.2487e-03 eta 0:02:39
epoch [23/50] batch [20/23] time 0.127 (0.223) data 0.001 (0.086) loss 0.7562 (0.6920) acc 76.1628 (78.9239) lr 1.2487e-03 eta 0:02:19
>>> alpha1: 0.238  alpha2: 0.012 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.51 <<<
epoch [24/50] batch [5/23] time 0.138 (0.468) data 0.001 (0.333) loss 0.6536 (0.6553) acc 87.5000 (83.4048) lr 1.1874e-03 eta 0:04:48
epoch [24/50] batch [10/23] time 0.139 (0.306) data 0.000 (0.167) loss 0.5322 (0.6376) acc 89.6739 (82.5747) lr 1.1874e-03 eta 0:03:06
epoch [24/50] batch [15/23] time 0.140 (0.249) data 0.001 (0.111) loss 0.6312 (0.6631) acc 75.0000 (81.4138) lr 1.1874e-03 eta 0:02:30
epoch [24/50] batch [20/23] time 0.134 (0.221) data 0.000 (0.084) loss 0.6163 (0.6383) acc 87.7778 (82.3137) lr 1.1874e-03 eta 0:02:12
>>> alpha1: 0.225  alpha2: 0.022 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.50 <<<
epoch [25/50] batch [5/23] time 0.159 (0.462) data 0.000 (0.311) loss 0.7267 (0.6739) acc 79.6875 (82.5229) lr 1.1253e-03 eta 0:04:34
epoch [25/50] batch [10/23] time 0.151 (0.299) data 0.000 (0.156) loss 0.4742 (0.6259) acc 83.0000 (82.7172) lr 1.1253e-03 eta 0:02:55
epoch [25/50] batch [15/23] time 0.130 (0.246) data 0.000 (0.104) loss 0.6828 (0.6654) acc 78.4884 (80.3272) lr 1.1253e-03 eta 0:02:23
epoch [25/50] batch [20/23] time 0.123 (0.218) data 0.000 (0.078) loss 0.6475 (0.6666) acc 79.3750 (80.6518) lr 1.1253e-03 eta 0:02:06
>>> alpha1: 0.212  alpha2: 0.014 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [26/50] batch [5/23] time 0.122 (0.473) data 0.000 (0.335) loss 0.5089 (0.5920) acc 89.0244 (83.3935) lr 1.0628e-03 eta 0:04:29
epoch [26/50] batch [10/23] time 0.147 (0.307) data 0.001 (0.168) loss 1.0228 (0.6325) acc 60.2273 (80.5201) lr 1.0628e-03 eta 0:02:53
epoch [26/50] batch [15/23] time 0.126 (0.249) data 0.000 (0.112) loss 0.6392 (0.6157) acc 84.8837 (81.3408) lr 1.0628e-03 eta 0:02:19
epoch [26/50] batch [20/23] time 0.129 (0.219) data 0.000 (0.084) loss 0.5643 (0.6274) acc 86.3636 (81.0211) lr 1.0628e-03 eta 0:02:01
>>> alpha1: 0.207  alpha2: 0.014 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [27/50] batch [5/23] time 0.168 (0.489) data 0.000 (0.346) loss 0.7146 (0.5891) acc 76.1628 (82.2789) lr 1.0000e-03 eta 0:04:27
epoch [27/50] batch [10/23] time 0.135 (0.311) data 0.001 (0.173) loss 0.4655 (0.5846) acc 83.1522 (81.2823) lr 1.0000e-03 eta 0:02:48
epoch [27/50] batch [15/23] time 0.143 (0.251) data 0.000 (0.116) loss 0.5234 (0.5657) acc 83.8235 (82.5212) lr 1.0000e-03 eta 0:02:14
epoch [27/50] batch [20/23] time 0.138 (0.222) data 0.001 (0.087) loss 0.6946 (0.5690) acc 80.4348 (83.2647) lr 1.0000e-03 eta 0:01:58
>>> alpha1: 0.201  alpha2: 0.013 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [28/50] batch [5/23] time 0.148 (0.484) data 0.000 (0.340) loss 0.4843 (0.5274) acc 90.2174 (84.1518) lr 9.3721e-04 eta 0:04:13
epoch [28/50] batch [10/23] time 0.130 (0.309) data 0.001 (0.170) loss 0.5539 (0.5313) acc 80.6818 (84.5760) lr 9.3721e-04 eta 0:02:40
epoch [28/50] batch [15/23] time 0.126 (0.249) data 0.000 (0.114) loss 0.6587 (0.5434) acc 76.7442 (84.9474) lr 9.3721e-04 eta 0:02:08
epoch [28/50] batch [20/23] time 0.124 (0.219) data 0.000 (0.085) loss 0.5858 (0.5615) acc 86.0465 (83.9957) lr 9.3721e-04 eta 0:01:51
>>> alpha1: 0.190  alpha2: 0.005 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.57 <<<
epoch [29/50] batch [5/23] time 0.152 (0.466) data 0.000 (0.307) loss 0.6285 (0.4805) acc 83.5000 (87.0489) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [10/23] time 0.148 (0.310) data 0.000 (0.154) loss 0.7768 (0.5375) acc 72.0000 (85.0579) lr 8.7467e-04 eta 0:02:33
epoch [29/50] batch [15/23] time 0.143 (0.254) data 0.000 (0.103) loss 0.7392 (0.5391) acc 82.1429 (85.2077) lr 8.7467e-04 eta 0:02:04
epoch [29/50] batch [20/23] time 0.153 (0.227) data 0.000 (0.077) loss 0.6626 (0.5513) acc 82.5472 (84.1972) lr 8.7467e-04 eta 0:01:50
>>> alpha1: 0.182  alpha2: -0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.56 <<<
epoch [30/50] batch [5/23] time 0.141 (0.524) data 0.001 (0.367) loss 0.4172 (0.5010) acc 90.4255 (86.8592) lr 8.1262e-04 eta 0:04:10
epoch [30/50] batch [10/23] time 0.138 (0.339) data 0.000 (0.184) loss 0.3506 (0.5272) acc 80.8511 (84.8451) lr 8.1262e-04 eta 0:02:40
epoch [30/50] batch [15/23] time 0.142 (0.276) data 0.000 (0.123) loss 0.5880 (0.5354) acc 87.5000 (85.1067) lr 8.1262e-04 eta 0:02:09
epoch [30/50] batch [20/23] time 0.149 (0.243) data 0.000 (0.092) loss 0.5148 (0.5361) acc 85.8491 (85.0751) lr 8.1262e-04 eta 0:01:52
>>> alpha1: 0.178  alpha2: -0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.58 <<<
epoch [31/50] batch [5/23] time 0.164 (0.438) data 0.000 (0.280) loss 0.2646 (0.4611) acc 96.8182 (87.4564) lr 7.5131e-04 eta 0:03:19
epoch [31/50] batch [10/23] time 0.160 (0.296) data 0.000 (0.140) loss 0.3656 (0.4458) acc 91.5179 (86.8720) lr 7.5131e-04 eta 0:02:13
epoch [31/50] batch [15/23] time 0.150 (0.246) data 0.000 (0.093) loss 0.4474 (0.4974) acc 84.1346 (86.0728) lr 7.5131e-04 eta 0:01:49
epoch [31/50] batch [20/23] time 0.146 (0.223) data 0.000 (0.070) loss 0.5933 (0.5102) acc 83.8235 (85.7436) lr 7.5131e-04 eta 0:01:38
>>> alpha1: 0.172  alpha2: -0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.59 <<<
epoch [32/50] batch [5/23] time 0.150 (0.461) data 0.000 (0.304) loss 0.4081 (0.4170) acc 91.1765 (87.6518) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [10/23] time 0.148 (0.310) data 0.000 (0.152) loss 0.4080 (0.4775) acc 93.1373 (85.4569) lr 6.9098e-04 eta 0:02:12
epoch [32/50] batch [15/23] time 0.145 (0.255) data 0.000 (0.102) loss 0.4536 (0.4911) acc 90.6863 (86.0920) lr 6.9098e-04 eta 0:01:47
epoch [32/50] batch [20/23] time 0.143 (0.228) data 0.001 (0.076) loss 0.4776 (0.5052) acc 83.6735 (85.6952) lr 6.9098e-04 eta 0:01:35
>>> alpha1: 0.165  alpha2: -0.003 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.58 <<<
epoch [33/50] batch [5/23] time 0.153 (0.460) data 0.000 (0.303) loss 0.7088 (0.5416) acc 75.5208 (83.4606) lr 6.3188e-04 eta 0:03:08
epoch [33/50] batch [10/23] time 0.138 (0.302) data 0.000 (0.152) loss 0.5358 (0.5171) acc 89.0625 (84.9786) lr 6.3188e-04 eta 0:02:02
epoch [33/50] batch [15/23] time 0.154 (0.252) data 0.000 (0.101) loss 0.4919 (0.5020) acc 86.7924 (85.2179) lr 6.3188e-04 eta 0:01:40
epoch [33/50] batch [20/23] time 0.146 (0.226) data 0.001 (0.076) loss 0.5156 (0.5034) acc 80.2885 (85.1572) lr 6.3188e-04 eta 0:01:28
>>> alpha1: 0.160  alpha2: -0.006 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.58 <<<
epoch [34/50] batch [5/23] time 0.180 (0.505) data 0.000 (0.346) loss 0.5266 (0.4481) acc 87.5000 (85.4502) lr 5.7422e-04 eta 0:03:14
epoch [34/50] batch [10/23] time 0.164 (0.332) data 0.000 (0.173) loss 0.3825 (0.4440) acc 83.9623 (86.6258) lr 5.7422e-04 eta 0:02:06
epoch [34/50] batch [15/23] time 0.141 (0.269) data 0.000 (0.116) loss 0.4645 (0.4496) acc 85.9375 (87.1419) lr 5.7422e-04 eta 0:01:41
epoch [34/50] batch [20/23] time 0.131 (0.238) data 0.000 (0.087) loss 0.5563 (0.4752) acc 80.2326 (86.0995) lr 5.7422e-04 eta 0:01:28
>>> alpha1: 0.157  alpha2: -0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.55 <<<
epoch [35/50] batch [5/23] time 0.147 (0.555) data 0.001 (0.397) loss 0.5258 (0.4733) acc 87.5000 (87.1127) lr 5.1825e-04 eta 0:03:21
epoch [35/50] batch [10/23] time 0.143 (0.349) data 0.000 (0.199) loss 0.5798 (0.4736) acc 77.4510 (85.1933) lr 5.1825e-04 eta 0:02:05
epoch [35/50] batch [15/23] time 0.129 (0.280) data 0.000 (0.133) loss 0.6745 (0.4708) acc 82.7778 (85.7575) lr 5.1825e-04 eta 0:01:38
epoch [35/50] batch [20/23] time 0.148 (0.246) data 0.001 (0.100) loss 0.4111 (0.4835) acc 83.8235 (85.2232) lr 5.1825e-04 eta 0:01:25
>>> alpha1: 0.158  alpha2: 0.006 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.57 <<<
epoch [36/50] batch [5/23] time 0.145 (0.447) data 0.000 (0.283) loss 0.4770 (0.4491) acc 82.1429 (86.3581) lr 4.6417e-04 eta 0:02:32
epoch [36/50] batch [10/23] time 0.145 (0.301) data 0.001 (0.142) loss 0.4232 (0.6236) acc 86.5000 (84.6566) lr 4.6417e-04 eta 0:01:40
epoch [36/50] batch [15/23] time 0.145 (0.248) data 0.000 (0.095) loss 0.3686 (0.5777) acc 91.8269 (85.6766) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [20/23] time 0.143 (0.222) data 0.000 (0.071) loss 0.5813 (0.5591) acc 81.8627 (85.1631) lr 4.6417e-04 eta 0:01:12
>>> alpha1: 0.155  alpha2: 0.012 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.50 <<<
epoch [37/50] batch [5/23] time 0.147 (0.473) data 0.000 (0.330) loss 0.5123 (0.4565) acc 86.0465 (89.9884) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [10/23] time 0.130 (0.307) data 0.000 (0.165) loss 0.4156 (0.4726) acc 85.7955 (88.1753) lr 4.1221e-04 eta 0:01:35
epoch [37/50] batch [15/23] time 0.131 (0.250) data 0.001 (0.110) loss 0.5624 (0.4514) acc 84.0909 (87.9270) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [20/23] time 0.134 (0.222) data 0.001 (0.083) loss 0.5692 (0.4566) acc 85.7955 (87.7772) lr 4.1221e-04 eta 0:01:07
>>> alpha1: 0.153  alpha2: 0.011 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.51 <<<
epoch [38/50] batch [5/23] time 0.131 (0.500) data 0.000 (0.357) loss 0.5533 (0.4139) acc 85.8696 (89.6571) lr 3.6258e-04 eta 0:02:26
epoch [38/50] batch [10/23] time 0.133 (0.319) data 0.000 (0.179) loss 0.3473 (0.4227) acc 90.7609 (88.9579) lr 3.6258e-04 eta 0:01:32
epoch [38/50] batch [15/23] time 0.127 (0.258) data 0.001 (0.119) loss 0.4057 (0.4501) acc 88.9535 (87.9066) lr 3.6258e-04 eta 0:01:13
epoch [38/50] batch [20/23] time 0.134 (0.226) data 0.001 (0.090) loss 0.5481 (0.4696) acc 85.7955 (87.4623) lr 3.6258e-04 eta 0:01:03
>>> alpha1: 0.155  alpha2: 0.013 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.58 <<<
epoch [39/50] batch [5/23] time 0.154 (0.461) data 0.000 (0.314) loss 0.4445 (0.4027) acc 92.9245 (90.4132) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [10/23] time 0.160 (0.308) data 0.000 (0.157) loss 0.4560 (0.4266) acc 90.1961 (88.5929) lr 3.1545e-04 eta 0:01:22
epoch [39/50] batch [15/23] time 0.151 (0.256) data 0.000 (0.105) loss 0.5892 (0.4266) acc 79.1667 (88.6305) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [20/23] time 0.128 (0.228) data 0.001 (0.079) loss 0.7101 (0.4526) acc 83.1395 (87.1056) lr 3.1545e-04 eta 0:00:58
>>> alpha1: 0.153  alpha2: 0.015 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.56 <<<
epoch [40/50] batch [5/23] time 0.169 (0.424) data 0.000 (0.270) loss 0.3494 (0.4885) acc 89.8148 (85.2869) lr 2.7103e-04 eta 0:01:45
epoch [40/50] batch [10/23] time 0.170 (0.290) data 0.000 (0.135) loss 0.4641 (0.4512) acc 84.0909 (87.4464) lr 2.7103e-04 eta 0:01:10
epoch [40/50] batch [15/23] time 0.153 (0.244) data 0.000 (0.090) loss 0.4755 (0.4403) acc 90.5660 (87.4054) lr 2.7103e-04 eta 0:00:58
epoch [40/50] batch [20/23] time 0.151 (0.221) data 0.000 (0.068) loss 0.3585 (0.4537) acc 93.0556 (87.7359) lr 2.7103e-04 eta 0:00:51
>>> alpha1: 0.149  alpha2: 0.009 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.57 <<<
epoch [41/50] batch [5/23] time 0.152 (0.433) data 0.000 (0.280) loss 0.5045 (0.4712) acc 80.8824 (85.6781) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [10/23] time 0.146 (0.292) data 0.001 (0.140) loss 0.2388 (0.4127) acc 96.0784 (86.7267) lr 2.2949e-04 eta 0:01:04
epoch [41/50] batch [15/23] time 0.151 (0.244) data 0.000 (0.094) loss 0.3860 (0.4127) acc 87.9808 (87.9179) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [20/23] time 0.140 (0.218) data 0.000 (0.070) loss 0.4978 (0.4204) acc 88.5417 (88.3351) lr 2.2949e-04 eta 0:00:45
>>> alpha1: 0.149  alpha2: 0.012 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [42/50] batch [5/23] time 0.166 (0.505) data 0.000 (0.351) loss 0.3900 (0.4227) acc 95.3488 (89.5545) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [10/23] time 0.155 (0.329) data 0.000 (0.176) loss 0.2792 (0.4270) acc 94.8113 (89.9612) lr 1.9098e-04 eta 0:01:04
epoch [42/50] batch [15/23] time 0.143 (0.267) data 0.000 (0.117) loss 0.3812 (0.4162) acc 84.8039 (89.0830) lr 1.9098e-04 eta 0:00:51
epoch [42/50] batch [20/23] time 0.144 (0.237) data 0.000 (0.088) loss 0.3875 (0.4095) acc 87.7551 (88.4300) lr 1.9098e-04 eta 0:00:44
>>> alpha1: 0.151  alpha2: 0.014 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.58 <<<
epoch [43/50] batch [5/23] time 0.165 (0.462) data 0.000 (0.305) loss 0.5231 (0.4266) acc 85.0000 (88.6254) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [10/23] time 0.153 (0.310) data 0.000 (0.153) loss 0.4097 (0.4191) acc 86.2745 (89.2093) lr 1.5567e-04 eta 0:00:53
epoch [43/50] batch [15/23] time 0.146 (0.256) data 0.000 (0.102) loss 0.3945 (0.3883) acc 90.3846 (90.2271) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [20/23] time 0.149 (0.227) data 0.000 (0.076) loss 0.3666 (0.4015) acc 91.8269 (90.1784) lr 1.5567e-04 eta 0:00:37
>>> alpha1: 0.148  alpha2: 0.016 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.57 <<<
epoch [44/50] batch [5/23] time 0.162 (0.604) data 0.001 (0.446) loss 0.4183 (0.4479) acc 86.0000 (87.9796) lr 1.2369e-04 eta 0:01:34
epoch [44/50] batch [10/23] time 0.144 (0.375) data 0.001 (0.223) loss 0.5239 (0.6189) acc 86.7021 (85.9190) lr 1.2369e-04 eta 0:00:56
epoch [44/50] batch [15/23] time 0.140 (0.299) data 0.000 (0.149) loss 0.4976 (0.5444) acc 86.4583 (86.6462) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [20/23] time 0.154 (0.261) data 0.000 (0.112) loss 0.4030 (0.5118) acc 90.5660 (87.5130) lr 1.2369e-04 eta 0:00:36
>>> alpha1: 0.149  alpha2: 0.018 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.57 <<<
epoch [45/50] batch [5/23] time 0.150 (0.838) data 0.000 (0.688) loss 0.3742 (0.3937) acc 87.2642 (87.6593) lr 9.5173e-05 eta 0:01:51
epoch [45/50] batch [10/23] time 0.148 (0.492) data 0.000 (0.344) loss 0.4219 (0.4029) acc 92.5000 (87.9220) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [15/23] time 0.133 (0.374) data 0.000 (0.229) loss 0.4530 (0.4128) acc 88.2979 (87.9751) lr 9.5173e-05 eta 0:00:46
epoch [45/50] batch [20/23] time 0.142 (0.316) data 0.000 (0.172) loss 0.4597 (0.4031) acc 92.6471 (89.0611) lr 9.5173e-05 eta 0:00:37
>>> alpha1: 0.147  alpha2: 0.018 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.56 <<<
epoch [46/50] batch [5/23] time 0.167 (0.481) data 0.001 (0.327) loss 0.4138 (0.4248) acc 82.2115 (87.0434) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [10/23] time 0.164 (0.319) data 0.000 (0.164) loss 0.3961 (0.4065) acc 91.6667 (88.4108) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [15/23] time 0.151 (0.260) data 0.000 (0.109) loss 0.4827 (0.4148) acc 81.6038 (88.6569) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [20/23] time 0.145 (0.231) data 0.000 (0.082) loss 0.4415 (0.4252) acc 83.3333 (88.5976) lr 7.0224e-05 eta 0:00:21
>>> alpha1: 0.148  alpha2: 0.018 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.55 <<<
epoch [47/50] batch [5/23] time 0.164 (0.443) data 0.001 (0.281) loss 0.3201 (0.3722) acc 91.3636 (90.3852) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [10/23] time 0.128 (0.294) data 0.000 (0.141) loss 0.3636 (0.3870) acc 86.9318 (89.1131) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [15/23] time 0.131 (0.243) data 0.000 (0.094) loss 0.4281 (0.3961) acc 88.8889 (89.3982) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.141 (0.217) data 0.000 (0.070) loss 0.3154 (0.3924) acc 89.7059 (89.4810) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.147  alpha2: 0.020 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.54 <<<
epoch [48/50] batch [5/23] time 0.148 (0.451) data 0.000 (0.305) loss 0.4845 (0.4609) acc 90.1042 (88.9155) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.147 (0.303) data 0.000 (0.153) loss 0.6054 (0.4530) acc 81.0000 (89.0020) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.155 (0.250) data 0.000 (0.102) loss 0.2973 (0.4144) acc 91.6667 (89.6307) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.139 (0.224) data 0.000 (0.077) loss 0.4373 (0.3945) acc 85.2041 (89.6764) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.147  alpha2: 0.019 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.55 <<<
epoch [49/50] batch [5/23] time 0.142 (0.459) data 0.000 (0.310) loss 0.4956 (0.4336) acc 88.7755 (88.7740) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.140 (0.305) data 0.000 (0.155) loss 0.3299 (0.4035) acc 95.2128 (89.2278) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.145 (0.250) data 0.000 (0.104) loss 0.3037 (0.3987) acc 94.2308 (89.3404) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.150 (0.223) data 0.000 (0.078) loss 0.3883 (0.4075) acc 91.0377 (88.7105) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.149  alpha2: 0.018 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [50/50] batch [5/23] time 0.146 (0.454) data 0.000 (0.302) loss 0.3272 (0.3735) acc 94.8980 (90.7929) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.191 (0.310) data 0.001 (0.153) loss 0.2714 (0.4027) acc 95.7547 (89.2434) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.152 (0.256) data 0.000 (0.102) loss 0.2801 (0.4064) acc 95.5882 (89.2162) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.139 (0.227) data 0.000 (0.077) loss 0.4014 (0.3920) acc 88.8298 (88.8195) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.53, 0.44, 0.43, 0.43, 0.43, 0.41, 0.41, 0.39, 0.38, 0.39, 0.38, 0.38, 0.38, 0.39, 0.38, 0.38, 0.38, 0.38, 0.38, 0.38, 0.38, 0.39, 0.38, 0.38, 0.38, 0.38, 0.38, 0.39, 0.39, 0.39, 0.39, 0.38, 0.39, 0.39, 0.38, 0.39, 0.39, 0.38, 0.39, 0.39]
* matched noise rate: [0.4, 0.19, 0.31, 0.3, 0.32, 0.28, 0.25, 0.23, 0.26, 0.27, 0.25, 0.23, 0.18, 0.21, 0.21, 0.2, 0.2, 0.2, 0.25, 0.26, 0.25, 0.25, 0.24, 0.24, 0.24, 0.25, 0.22, 0.22, 0.26, 0.28, 0.24, 0.24, 0.25, 0.26, 0.24, 0.26, 0.27, 0.26, 0.25, 0.24]
* unmatched noise rate: [0.73, 0.56, 0.63, 0.64, 0.61, 0.61, 0.58, 0.55, 0.54, 0.57, 0.53, 0.55, 0.52, 0.51, 0.5, 0.49, 0.5, 0.49, 0.57, 0.56, 0.58, 0.59, 0.58, 0.58, 0.55, 0.57, 0.5, 0.51, 0.58, 0.56, 0.57, 0.56, 0.58, 0.57, 0.57, 0.56, 0.55, 0.54, 0.55, 0.56]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:39,  2.49s/it] 18%|█▊        | 3/17 [00:02<00:09,  1.44it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.62it/s] 41%|████      | 7/17 [00:02<00:02,  3.87it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.28it/s] 65%|██████▍   | 11/17 [00:03<00:00,  6.79it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.26it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.61it/s]100%|██████████| 17/17 [00:04<00:00,  5.26it/s]100%|██████████| 17/17 [00:04<00:00,  3.78it/s]
=> result
* total: 1,692
* correct: 903
* accuracy: 53.4%
* error: 46.6%
* macro_f1: 50.8%
=> per-class result
* class: 0 (banded)	total: 36	correct: 19	acc: 52.8%
* class: 1 (blotchy)	total: 36	correct: 2	acc: 5.6%
* class: 2 (braided)	total: 36	correct: 13	acc: 36.1%
* class: 3 (bubbly)	total: 36	correct: 31	acc: 86.1%
* class: 4 (bumpy)	total: 36	correct: 0	acc: 0.0%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 33	acc: 91.7%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 32	acc: 88.9%
* class: 10 (dotted)	total: 36	correct: 8	acc: 22.2%
* class: 11 (fibrous)	total: 36	correct: 25	acc: 69.4%
* class: 12 (flecked)	total: 36	correct: 13	acc: 36.1%
* class: 13 (freckled)	total: 36	correct: 24	acc: 66.7%
* class: 14 (frilly)	total: 36	correct: 24	acc: 66.7%
* class: 15 (gauzy)	total: 36	correct: 22	acc: 61.1%
* class: 16 (grid)	total: 36	correct: 12	acc: 33.3%
* class: 17 (grooved)	total: 36	correct: 12	acc: 33.3%
* class: 18 (honeycombed)	total: 36	correct: 12	acc: 33.3%
* class: 19 (interlaced)	total: 36	correct: 9	acc: 25.0%
* class: 20 (knitted)	total: 36	correct: 35	acc: 97.2%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 22	acc: 61.1%
* class: 23 (marbled)	total: 36	correct: 21	acc: 58.3%
* class: 24 (matted)	total: 36	correct: 16	acc: 44.4%
* class: 25 (meshed)	total: 36	correct: 22	acc: 61.1%
* class: 26 (paisley)	total: 36	correct: 35	acc: 97.2%
* class: 27 (perforated)	total: 36	correct: 21	acc: 58.3%
* class: 28 (pitted)	total: 36	correct: 16	acc: 44.4%
* class: 29 (pleated)	total: 36	correct: 11	acc: 30.6%
* class: 30 (polka-dotted)	total: 36	correct: 35	acc: 97.2%
* class: 31 (porous)	total: 36	correct: 7	acc: 19.4%
* class: 32 (potholed)	total: 36	correct: 29	acc: 80.6%
* class: 33 (scaly)	total: 36	correct: 16	acc: 44.4%
* class: 34 (smeared)	total: 36	correct: 14	acc: 38.9%
* class: 35 (spiralled)	total: 36	correct: 21	acc: 58.3%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 0	acc: 0.0%
* class: 38 (stratified)	total: 36	correct: 27	acc: 75.0%
* class: 39 (striped)	total: 36	correct: 28	acc: 77.8%
* class: 40 (studded)	total: 36	correct: 29	acc: 80.6%
* class: 41 (swirly)	total: 36	correct: 17	acc: 47.2%
* class: 42 (veined)	total: 36	correct: 17	acc: 47.2%
* class: 43 (waffled)	total: 36	correct: 24	acc: 66.7%
* class: 44 (woven)	total: 36	correct: 12	acc: 33.3%
* class: 45 (wrinkled)	total: 36	correct: 23	acc: 63.9%
* class: 46 (zigzagged)	total: 36	correct: 29	acc: 80.6%
* average: 53.4%
Elapsed: 0:13:22
Run this job and save the output to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_10-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.147 (0.889) data 0.000 (0.339) loss 3.5969 (3.8815) acc 18.7500 (8.7500) lr 1.0000e-05 eta 0:16:57
epoch [1/50] batch [10/23] time 0.161 (0.523) data 0.000 (0.170) loss 3.6457 (3.8145) acc 15.6250 (8.4375) lr 1.0000e-05 eta 0:09:56
epoch [1/50] batch [15/23] time 0.145 (0.397) data 0.000 (0.113) loss 3.6857 (3.7848) acc 18.7500 (9.3750) lr 1.0000e-05 eta 0:07:31
epoch [1/50] batch [20/23] time 0.145 (0.335) data 0.000 (0.085) loss 3.6197 (3.7652) acc 15.6250 (9.0625) lr 1.0000e-05 eta 0:06:18
epoch [2/50] batch [5/23] time 0.151 (0.450) data 0.000 (0.264) loss 3.5086 (3.7239) acc 21.8750 (10.0000) lr 2.0000e-03 eta 0:08:24
epoch [2/50] batch [10/23] time 0.152 (0.309) data 0.000 (0.132) loss 3.4871 (3.7248) acc 18.7500 (10.3125) lr 2.0000e-03 eta 0:05:44
epoch [2/50] batch [15/23] time 0.153 (0.257) data 0.000 (0.088) loss 3.7759 (3.7292) acc 15.6250 (12.0833) lr 2.0000e-03 eta 0:04:46
epoch [2/50] batch [20/23] time 0.150 (0.231) data 0.000 (0.066) loss 4.0263 (3.7210) acc 6.2500 (12.0312) lr 2.0000e-03 eta 0:04:15
epoch [3/50] batch [5/23] time 0.148 (0.456) data 0.000 (0.268) loss 3.6660 (3.6560) acc 9.3750 (12.5000) lr 1.9980e-03 eta 0:08:21
epoch [3/50] batch [10/23] time 0.150 (0.312) data 0.000 (0.134) loss 3.6050 (3.6895) acc 15.6250 (12.5000) lr 1.9980e-03 eta 0:05:41
epoch [3/50] batch [15/23] time 0.147 (0.259) data 0.000 (0.090) loss 3.6415 (3.6882) acc 9.3750 (12.2917) lr 1.9980e-03 eta 0:04:42
epoch [3/50] batch [20/23] time 0.150 (0.232) data 0.000 (0.067) loss 3.7362 (3.6882) acc 12.5000 (12.6562) lr 1.9980e-03 eta 0:04:11
epoch [4/50] batch [5/23] time 0.151 (0.427) data 0.000 (0.254) loss 3.4536 (3.5011) acc 28.1250 (18.1250) lr 1.9921e-03 eta 0:07:39
epoch [4/50] batch [10/23] time 0.169 (0.301) data 0.000 (0.128) loss 3.8197 (3.5683) acc 18.7500 (15.6250) lr 1.9921e-03 eta 0:05:22
epoch [4/50] batch [15/23] time 0.151 (0.251) data 0.000 (0.085) loss 3.7524 (3.6054) acc 12.5000 (14.5833) lr 1.9921e-03 eta 0:04:27
epoch [4/50] batch [20/23] time 0.150 (0.226) data 0.000 (0.064) loss 3.7207 (3.6205) acc 12.5000 (14.0625) lr 1.9921e-03 eta 0:03:59
epoch [5/50] batch [5/23] time 0.153 (0.469) data 0.000 (0.291) loss 3.7522 (3.5481) acc 6.2500 (16.2500) lr 1.9823e-03 eta 0:08:13
epoch [5/50] batch [10/23] time 0.154 (0.315) data 0.000 (0.145) loss 3.8856 (3.6499) acc 12.5000 (13.7500) lr 1.9823e-03 eta 0:05:30
epoch [5/50] batch [15/23] time 0.149 (0.260) data 0.000 (0.097) loss 3.5505 (3.6494) acc 15.6250 (13.7500) lr 1.9823e-03 eta 0:04:31
epoch [5/50] batch [20/23] time 0.145 (0.232) data 0.000 (0.073) loss 3.7886 (3.6394) acc 6.2500 (14.0625) lr 1.9823e-03 eta 0:04:00
epoch [6/50] batch [5/23] time 0.183 (0.416) data 0.000 (0.219) loss 3.4917 (3.5540) acc 18.7500 (19.3750) lr 1.9686e-03 eta 0:07:08
epoch [6/50] batch [10/23] time 0.162 (0.295) data 0.000 (0.110) loss 3.5309 (3.5387) acc 12.5000 (18.1250) lr 1.9686e-03 eta 0:05:02
epoch [6/50] batch [15/23] time 0.150 (0.246) data 0.000 (0.073) loss 3.4981 (3.5470) acc 15.6250 (16.2500) lr 1.9686e-03 eta 0:04:11
epoch [6/50] batch [20/23] time 0.148 (0.222) data 0.000 (0.055) loss 3.4209 (3.5297) acc 12.5000 (17.1875) lr 1.9686e-03 eta 0:03:45
epoch [7/50] batch [5/23] time 0.153 (0.464) data 0.000 (0.290) loss 3.5201 (3.5277) acc 21.8750 (18.7500) lr 1.9511e-03 eta 0:07:47
epoch [7/50] batch [10/23] time 0.164 (0.317) data 0.000 (0.145) loss 3.4460 (3.4913) acc 21.8750 (18.1250) lr 1.9511e-03 eta 0:05:17
epoch [7/50] batch [15/23] time 0.149 (0.260) data 0.000 (0.097) loss 3.4258 (3.5062) acc 18.7500 (17.2917) lr 1.9511e-03 eta 0:04:19
epoch [7/50] batch [20/23] time 0.149 (0.232) data 0.000 (0.073) loss 3.3243 (3.5164) acc 21.8750 (17.3438) lr 1.9511e-03 eta 0:03:50
epoch [8/50] batch [5/23] time 0.170 (0.432) data 0.000 (0.243) loss 3.7569 (3.6103) acc 3.1250 (14.3750) lr 1.9298e-03 eta 0:07:05
epoch [8/50] batch [10/23] time 0.161 (0.296) data 0.000 (0.122) loss 3.3962 (3.5408) acc 18.7500 (14.3750) lr 1.9298e-03 eta 0:04:50
epoch [8/50] batch [15/23] time 0.147 (0.247) data 0.000 (0.081) loss 3.9173 (3.4782) acc 9.3750 (17.0833) lr 1.9298e-03 eta 0:04:00
epoch [8/50] batch [20/23] time 0.146 (0.222) data 0.000 (0.061) loss 3.1751 (3.4567) acc 21.8750 (17.1875) lr 1.9298e-03 eta 0:03:34
epoch [9/50] batch [5/23] time 0.178 (0.419) data 0.000 (0.231) loss 3.3961 (3.3553) acc 6.2500 (18.7500) lr 1.9048e-03 eta 0:06:42
epoch [9/50] batch [10/23] time 0.164 (0.294) data 0.000 (0.116) loss 2.7673 (3.3093) acc 37.5000 (21.5625) lr 1.9048e-03 eta 0:04:40
epoch [9/50] batch [15/23] time 0.146 (0.245) data 0.000 (0.077) loss 3.7518 (3.3910) acc 15.6250 (20.4167) lr 1.9048e-03 eta 0:03:52
epoch [9/50] batch [20/23] time 0.149 (0.220) data 0.000 (0.058) loss 3.7615 (3.4171) acc 12.5000 (19.5312) lr 1.9048e-03 eta 0:03:28
epoch [10/50] batch [5/23] time 0.151 (0.427) data 0.000 (0.248) loss 3.5780 (3.4160) acc 12.5000 (17.5000) lr 1.8763e-03 eta 0:06:40
epoch [10/50] batch [10/23] time 0.174 (0.300) data 0.000 (0.124) loss 3.6450 (3.4408) acc 9.3750 (16.8750) lr 1.8763e-03 eta 0:04:40
epoch [10/50] batch [15/23] time 0.155 (0.251) data 0.000 (0.083) loss 3.4443 (3.4129) acc 15.6250 (17.5000) lr 1.8763e-03 eta 0:03:52
epoch [10/50] batch [20/23] time 0.152 (0.227) data 0.000 (0.062) loss 3.1928 (3.3951) acc 28.1250 (18.2812) lr 1.8763e-03 eta 0:03:29
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 1.062  alpha2: 0.461 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.63 <<<
epoch [11/50] batch [5/23] time 0.673 (0.908) data 0.000 (0.292) loss 2.3796 (2.4059) acc 70.8333 (67.5626) lr 1.8443e-03 eta 0:13:51
epoch [11/50] batch [10/23] time 0.735 (0.647) data 0.000 (0.146) loss 2.5318 (2.4401) acc 61.1111 (64.3694) lr 1.8443e-03 eta 0:09:48
epoch [11/50] batch [15/23] time 0.132 (0.523) data 0.000 (0.098) loss 2.4120 (2.4455) acc 57.7778 (61.4268) lr 1.8443e-03 eta 0:07:53
epoch [11/50] batch [20/23] time 0.143 (0.455) data 0.000 (0.073) loss 2.3438 (2.4231) acc 54.5918 (62.1537) lr 1.8443e-03 eta 0:06:49
>>> alpha1: 0.821  alpha2: 0.395 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.62 <<<
epoch [12/50] batch [5/23] time 0.139 (0.418) data 0.000 (0.275) loss 1.6402 (1.7732) acc 70.3488 (68.3162) lr 1.8090e-03 eta 0:06:12
epoch [12/50] batch [10/23] time 0.146 (0.287) data 0.000 (0.138) loss 1.4581 (1.7280) acc 63.9423 (65.5760) lr 1.8090e-03 eta 0:04:14
epoch [12/50] batch [15/23] time 0.138 (0.283) data 0.000 (0.092) loss 1.5949 (1.7453) acc 69.7917 (63.5567) lr 1.8090e-03 eta 0:04:09
epoch [12/50] batch [20/23] time 0.127 (0.275) data 0.000 (0.069) loss 1.7692 (1.7268) acc 65.4762 (64.3131) lr 1.8090e-03 eta 0:04:00
>>> alpha1: 0.803  alpha2: 0.326 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.57 <<<
epoch [13/50] batch [5/23] time 0.133 (0.437) data 0.000 (0.298) loss 1.4955 (1.6099) acc 59.6591 (61.8776) lr 1.7705e-03 eta 0:06:19
epoch [13/50] batch [10/23] time 0.146 (0.290) data 0.000 (0.149) loss 1.6034 (1.5611) acc 54.6512 (63.4251) lr 1.7705e-03 eta 0:04:10
epoch [13/50] batch [15/23] time 0.131 (0.277) data 0.000 (0.100) loss 1.4180 (1.5159) acc 65.5556 (64.6078) lr 1.7705e-03 eta 0:03:57
epoch [13/50] batch [20/23] time 0.120 (0.240) data 0.000 (0.075) loss 1.0266 (1.5011) acc 74.3902 (64.4947) lr 1.7705e-03 eta 0:03:24
>>> alpha1: 0.739  alpha2: 0.275 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.53 <<<
epoch [14/50] batch [5/23] time 0.142 (0.390) data 0.001 (0.249) loss 1.4896 (1.2304) acc 57.9787 (71.8571) lr 1.7290e-03 eta 0:05:30
epoch [14/50] batch [10/23] time 0.137 (0.266) data 0.000 (0.125) loss 1.2033 (1.2091) acc 70.1220 (71.2220) lr 1.7290e-03 eta 0:03:44
epoch [14/50] batch [15/23] time 0.141 (0.221) data 0.000 (0.083) loss 0.9662 (1.2871) acc 84.1837 (68.9832) lr 1.7290e-03 eta 0:03:04
epoch [14/50] batch [20/23] time 0.140 (0.225) data 0.000 (0.062) loss 1.6872 (1.3414) acc 55.6818 (66.9663) lr 1.7290e-03 eta 0:03:06
>>> alpha1: 0.694  alpha2: 0.246 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.61 <<<
epoch [15/50] batch [5/23] time 0.139 (0.571) data 0.000 (0.272) loss 1.1222 (1.2153) acc 68.7500 (69.4224) lr 1.6845e-03 eta 0:07:49
epoch [15/50] batch [10/23] time 0.144 (0.357) data 0.000 (0.136) loss 0.9121 (1.1928) acc 74.4898 (69.4596) lr 1.6845e-03 eta 0:04:52
epoch [15/50] batch [15/23] time 0.137 (0.284) data 0.000 (0.091) loss 1.1513 (1.1902) acc 69.2708 (69.6985) lr 1.6845e-03 eta 0:03:50
epoch [15/50] batch [20/23] time 0.140 (0.248) data 0.000 (0.068) loss 1.3435 (1.2027) acc 74.4792 (70.2272) lr 1.6845e-03 eta 0:03:20
>>> alpha1: 0.569  alpha2: 0.163 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.40 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.58 <<<
epoch [16/50] batch [5/23] time 0.137 (0.432) data 0.000 (0.288) loss 0.9366 (1.1505) acc 76.5957 (65.5842) lr 1.6374e-03 eta 0:05:45
epoch [16/50] batch [10/23] time 0.132 (0.288) data 0.000 (0.144) loss 0.8183 (1.0797) acc 78.8043 (70.0845) lr 1.6374e-03 eta 0:03:48
epoch [16/50] batch [15/23] time 0.148 (0.240) data 0.001 (0.096) loss 0.9036 (1.0862) acc 80.7692 (70.4721) lr 1.6374e-03 eta 0:03:09
epoch [16/50] batch [20/23] time 0.143 (0.215) data 0.000 (0.072) loss 0.9218 (1.0709) acc 71.4286 (70.8342) lr 1.6374e-03 eta 0:02:48
>>> alpha1: 0.491  alpha2: 0.121 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [17/50] batch [5/23] time 0.168 (0.449) data 0.017 (0.303) loss 0.8777 (0.8927) acc 81.6327 (76.0034) lr 1.5878e-03 eta 0:05:48
epoch [17/50] batch [10/23] time 0.153 (0.369) data 0.000 (0.152) loss 0.8169 (0.9193) acc 73.1132 (74.8840) lr 1.5878e-03 eta 0:04:44
epoch [17/50] batch [15/23] time 0.148 (0.294) data 0.000 (0.101) loss 0.7771 (0.9536) acc 83.4906 (74.0019) lr 1.5878e-03 eta 0:03:45
epoch [17/50] batch [20/23] time 0.139 (0.290) data 0.000 (0.076) loss 0.8565 (0.9524) acc 72.4490 (73.8757) lr 1.5878e-03 eta 0:03:41
>>> alpha1: 0.362  alpha2: 0.061 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.54 <<<
epoch [18/50] batch [5/23] time 0.131 (0.445) data 0.000 (0.299) loss 1.0040 (1.0633) acc 73.8636 (77.6519) lr 1.5358e-03 eta 0:05:35
epoch [18/50] batch [10/23] time 0.149 (0.296) data 0.000 (0.150) loss 0.7425 (0.9365) acc 82.9787 (76.5213) lr 1.5358e-03 eta 0:03:42
epoch [18/50] batch [15/23] time 0.144 (0.246) data 0.000 (0.100) loss 0.7855 (0.8888) acc 79.0000 (77.4184) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [20/23] time 0.128 (0.219) data 0.000 (0.075) loss 0.9057 (0.8967) acc 77.9070 (76.3925) lr 1.5358e-03 eta 0:02:41
>>> alpha1: 0.337  alpha2: 0.051 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.58 <<<
epoch [19/50] batch [5/23] time 0.191 (0.403) data 0.017 (0.249) loss 0.9026 (0.7247) acc 74.5370 (80.8898) lr 1.4818e-03 eta 0:04:54
epoch [19/50] batch [10/23] time 0.144 (0.278) data 0.000 (0.125) loss 0.5516 (0.7211) acc 87.0000 (80.3285) lr 1.4818e-03 eta 0:03:21
epoch [19/50] batch [15/23] time 0.151 (0.235) data 0.000 (0.083) loss 1.1067 (0.7850) acc 69.8113 (79.1565) lr 1.4818e-03 eta 0:02:49
epoch [19/50] batch [20/23] time 0.156 (0.213) data 0.000 (0.063) loss 0.6955 (0.7635) acc 82.0755 (79.4940) lr 1.4818e-03 eta 0:02:32
>>> alpha1: 0.308  alpha2: 0.045 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.56 <<<
epoch [20/50] batch [5/23] time 0.168 (0.401) data 0.000 (0.248) loss 0.4660 (0.5825) acc 84.6154 (85.4791) lr 1.4258e-03 eta 0:04:43
epoch [20/50] batch [10/23] time 0.135 (0.272) data 0.000 (0.124) loss 0.5874 (0.6309) acc 87.2340 (83.7632) lr 1.4258e-03 eta 0:03:11
epoch [20/50] batch [15/23] time 0.134 (0.228) data 0.000 (0.083) loss 0.7092 (0.6419) acc 71.7391 (82.1282) lr 1.4258e-03 eta 0:02:39
epoch [20/50] batch [20/23] time 0.144 (0.205) data 0.001 (0.062) loss 0.6313 (0.6836) acc 83.0000 (80.5462) lr 1.4258e-03 eta 0:02:22
>>> alpha1: 0.275  alpha2: 0.033 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.51 <<<
epoch [21/50] batch [5/23] time 0.177 (0.429) data 0.001 (0.274) loss 0.4276 (0.6241) acc 94.3182 (84.8636) lr 1.3681e-03 eta 0:04:53
epoch [21/50] batch [10/23] time 0.136 (0.284) data 0.000 (0.137) loss 0.5349 (0.6601) acc 83.8889 (81.7908) lr 1.3681e-03 eta 0:03:13
epoch [21/50] batch [15/23] time 0.124 (0.231) data 0.000 (0.092) loss 0.7888 (0.6867) acc 80.2326 (81.6153) lr 1.3681e-03 eta 0:02:36
epoch [21/50] batch [20/23] time 0.131 (0.207) data 0.001 (0.069) loss 0.5823 (0.6734) acc 85.1190 (81.6450) lr 1.3681e-03 eta 0:02:18
>>> alpha1: 0.253  alpha2: 0.025 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.51 <<<
epoch [22/50] batch [5/23] time 0.133 (0.464) data 0.001 (0.326) loss 0.6150 (0.6327) acc 88.5870 (86.7428) lr 1.3090e-03 eta 0:05:06
epoch [22/50] batch [10/23] time 0.139 (0.303) data 0.000 (0.163) loss 0.4945 (0.6476) acc 89.2857 (84.7926) lr 1.3090e-03 eta 0:03:19
epoch [22/50] batch [15/23] time 0.135 (0.247) data 0.000 (0.109) loss 0.4283 (0.6479) acc 88.8889 (83.0181) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [20/23] time 0.132 (0.219) data 0.000 (0.082) loss 0.9349 (0.6770) acc 67.3913 (81.2419) lr 1.3090e-03 eta 0:02:21
>>> alpha1: 0.232  alpha2: 0.011 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.55 <<<
epoch [23/50] batch [5/23] time 0.172 (0.384) data 0.000 (0.234) loss 0.3598 (0.5784) acc 93.0000 (84.1385) lr 1.2487e-03 eta 0:04:05
epoch [23/50] batch [10/23] time 0.163 (0.266) data 0.000 (0.117) loss 0.6562 (0.6094) acc 80.8824 (82.9358) lr 1.2487e-03 eta 0:02:48
epoch [23/50] batch [15/23] time 0.144 (0.222) data 0.000 (0.078) loss 0.4467 (0.6234) acc 83.5000 (82.4221) lr 1.2487e-03 eta 0:02:19
epoch [23/50] batch [20/23] time 0.145 (0.238) data 0.001 (0.059) loss 0.6618 (0.6471) acc 80.0000 (80.6165) lr 1.2487e-03 eta 0:02:28
>>> alpha1: 0.220  alpha2: 0.006 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [24/50] batch [5/23] time 0.164 (0.463) data 0.000 (0.310) loss 0.5628 (0.6251) acc 79.4118 (82.3493) lr 1.1874e-03 eta 0:04:45
epoch [24/50] batch [10/23] time 0.146 (0.307) data 0.000 (0.155) loss 0.6074 (0.6184) acc 78.5714 (82.1978) lr 1.1874e-03 eta 0:03:07
epoch [24/50] batch [15/23] time 0.154 (0.253) data 0.000 (0.104) loss 0.6981 (0.6187) acc 84.7222 (82.4572) lr 1.1874e-03 eta 0:02:33
epoch [24/50] batch [20/23] time 0.134 (0.224) data 0.001 (0.078) loss 0.6024 (0.6232) acc 88.0435 (82.4876) lr 1.1874e-03 eta 0:02:14
>>> alpha1: 0.209  alpha2: -0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.55 <<<
epoch [25/50] batch [5/23] time 0.141 (0.426) data 0.000 (0.279) loss 0.6178 (0.5459) acc 78.5000 (86.2308) lr 1.1253e-03 eta 0:04:12
epoch [25/50] batch [10/23] time 0.138 (0.288) data 0.000 (0.140) loss 0.6451 (0.5670) acc 78.1915 (84.5434) lr 1.1253e-03 eta 0:02:49
epoch [25/50] batch [15/23] time 0.138 (0.238) data 0.000 (0.093) loss 0.5256 (0.5947) acc 78.0612 (82.8656) lr 1.1253e-03 eta 0:02:18
epoch [25/50] batch [20/23] time 0.143 (0.214) data 0.000 (0.070) loss 0.6749 (0.5785) acc 80.3922 (83.6318) lr 1.1253e-03 eta 0:02:03
>>> alpha1: 0.200  alpha2: -0.007 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [26/50] batch [5/23] time 0.145 (0.496) data 0.000 (0.335) loss 0.5678 (0.6425) acc 87.2449 (81.1962) lr 1.0628e-03 eta 0:04:42
epoch [26/50] batch [10/23] time 0.147 (0.395) data 0.000 (0.168) loss 0.5001 (0.5850) acc 86.2745 (84.0322) lr 1.0628e-03 eta 0:03:43
epoch [26/50] batch [15/23] time 0.142 (0.312) data 0.000 (0.112) loss 0.5717 (0.5668) acc 82.6531 (84.2448) lr 1.0628e-03 eta 0:02:54
epoch [26/50] batch [20/23] time 0.134 (0.270) data 0.000 (0.084) loss 0.6980 (0.5508) acc 77.7174 (84.4014) lr 1.0628e-03 eta 0:02:29
>>> alpha1: 0.183  alpha2: -0.011 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [27/50] batch [5/23] time 0.163 (0.462) data 0.000 (0.313) loss 0.5042 (0.5143) acc 89.2157 (85.7927) lr 1.0000e-03 eta 0:04:12
epoch [27/50] batch [10/23] time 0.146 (0.384) data 0.000 (0.157) loss 0.5319 (0.5186) acc 87.7451 (85.6280) lr 1.0000e-03 eta 0:03:28
epoch [27/50] batch [15/23] time 0.144 (0.304) data 0.001 (0.105) loss 0.5648 (0.5310) acc 79.5918 (84.3851) lr 1.0000e-03 eta 0:02:43
epoch [27/50] batch [20/23] time 0.151 (0.266) data 0.000 (0.079) loss 0.5413 (0.5333) acc 81.6038 (84.5658) lr 1.0000e-03 eta 0:02:21
>>> alpha1: 0.180  alpha2: -0.001 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.59 <<<
epoch [28/50] batch [5/23] time 0.176 (0.463) data 0.013 (0.302) loss 2.4413 (0.8800) acc 60.5769 (81.3433) lr 9.3721e-04 eta 0:04:02
epoch [28/50] batch [10/23] time 0.153 (0.306) data 0.001 (0.152) loss 0.3827 (0.8280) acc 90.5660 (82.3394) lr 9.3721e-04 eta 0:02:39
epoch [28/50] batch [15/23] time 0.139 (0.254) data 0.000 (0.101) loss 0.7762 (0.7545) acc 77.6596 (82.2073) lr 9.3721e-04 eta 0:02:10
epoch [28/50] batch [20/23] time 0.153 (0.226) data 0.001 (0.076) loss 0.5115 (0.7084) acc 88.2075 (82.4778) lr 9.3721e-04 eta 0:01:55
>>> alpha1: 0.179  alpha2: 0.011 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.56 <<<
epoch [29/50] batch [5/23] time 0.163 (0.462) data 0.001 (0.305) loss 0.5662 (0.4836) acc 84.1346 (87.7217) lr 8.7467e-04 eta 0:03:51
epoch [29/50] batch [10/23] time 0.145 (0.306) data 0.000 (0.153) loss 0.4256 (0.4896) acc 90.0000 (86.8313) lr 8.7467e-04 eta 0:02:31
epoch [29/50] batch [15/23] time 0.136 (0.251) data 0.000 (0.102) loss 0.5871 (0.4898) acc 79.8913 (86.3749) lr 8.7467e-04 eta 0:02:03
epoch [29/50] batch [20/23] time 0.144 (0.223) data 0.000 (0.076) loss 0.5096 (0.5078) acc 82.6531 (85.5348) lr 8.7467e-04 eta 0:01:48
>>> alpha1: 0.172  alpha2: 0.009 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.57 <<<
epoch [30/50] batch [5/23] time 0.169 (0.428) data 0.001 (0.272) loss 0.3168 (0.4658) acc 92.5000 (87.1228) lr 8.1262e-04 eta 0:03:24
epoch [30/50] batch [10/23] time 0.171 (0.295) data 0.001 (0.136) loss 0.5880 (0.5078) acc 78.9216 (84.3096) lr 8.1262e-04 eta 0:02:19
epoch [30/50] batch [15/23] time 0.137 (0.243) data 0.000 (0.091) loss 0.5023 (0.5000) acc 78.8043 (85.0789) lr 8.1262e-04 eta 0:01:53
epoch [30/50] batch [20/23] time 0.143 (0.218) data 0.001 (0.068) loss 0.5112 (0.5085) acc 88.2653 (85.7000) lr 8.1262e-04 eta 0:01:41
>>> alpha1: 0.165  alpha2: 0.011 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.53 <<<
epoch [31/50] batch [5/23] time 0.156 (0.431) data 0.000 (0.281) loss 0.5068 (0.4359) acc 89.5833 (88.2689) lr 7.5131e-04 eta 0:03:16
epoch [31/50] batch [10/23] time 0.143 (0.288) data 0.000 (0.141) loss 0.3314 (0.4331) acc 92.3469 (87.9741) lr 7.5131e-04 eta 0:02:09
epoch [31/50] batch [15/23] time 0.137 (0.239) data 0.000 (0.094) loss 0.4945 (0.4462) acc 80.2083 (86.6531) lr 7.5131e-04 eta 0:01:46
epoch [31/50] batch [20/23] time 0.129 (0.214) data 0.000 (0.071) loss 0.6311 (0.4602) acc 84.6591 (86.7336) lr 7.5131e-04 eta 0:01:34
>>> alpha1: 0.164  alpha2: 0.019 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.53 <<<
epoch [32/50] batch [5/23] time 0.142 (0.414) data 0.000 (0.265) loss 0.4804 (0.4519) acc 83.5106 (88.2155) lr 6.9098e-04 eta 0:02:59
epoch [32/50] batch [10/23] time 0.139 (0.279) data 0.000 (0.133) loss 0.4709 (0.4398) acc 89.7959 (88.9556) lr 6.9098e-04 eta 0:01:59
epoch [32/50] batch [15/23] time 0.139 (0.233) data 0.000 (0.089) loss 0.4414 (0.4516) acc 80.6122 (87.5486) lr 6.9098e-04 eta 0:01:38
epoch [32/50] batch [20/23] time 0.151 (0.209) data 0.000 (0.067) loss 0.4521 (0.4422) acc 89.8148 (87.9099) lr 6.9098e-04 eta 0:01:27
>>> alpha1: 0.161  alpha2: 0.016 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.57 <<<
epoch [33/50] batch [5/23] time 0.147 (0.434) data 0.000 (0.280) loss 0.3812 (0.4134) acc 87.7451 (88.5563) lr 6.3188e-04 eta 0:02:57
epoch [33/50] batch [10/23] time 0.151 (0.295) data 0.000 (0.140) loss 0.3783 (0.4678) acc 92.4528 (87.0367) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [15/23] time 0.137 (0.244) data 0.000 (0.094) loss 0.5266 (0.4554) acc 82.8125 (87.2556) lr 6.3188e-04 eta 0:01:37
epoch [33/50] batch [20/23] time 0.139 (0.220) data 0.000 (0.070) loss 0.4715 (0.4636) acc 89.2857 (87.4160) lr 6.3188e-04 eta 0:01:26
>>> alpha1: 0.156  alpha2: 0.014 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.57 <<<
epoch [34/50] batch [5/23] time 0.144 (0.421) data 0.000 (0.272) loss 0.5145 (0.5056) acc 84.6939 (85.2803) lr 5.7422e-04 eta 0:02:42
epoch [34/50] batch [10/23] time 0.151 (0.285) data 0.000 (0.136) loss 0.3392 (0.4405) acc 92.1296 (87.6702) lr 5.7422e-04 eta 0:01:48
epoch [34/50] batch [15/23] time 0.144 (0.239) data 0.000 (0.091) loss 0.3611 (0.4530) acc 90.1961 (87.6814) lr 5.7422e-04 eta 0:01:29
epoch [34/50] batch [20/23] time 0.138 (0.214) data 0.000 (0.068) loss 0.5710 (0.4556) acc 88.2653 (87.6008) lr 5.7422e-04 eta 0:01:19
>>> alpha1: 0.153  alpha2: 0.013 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.52 <<<
epoch [35/50] batch [5/23] time 0.155 (0.419) data 0.000 (0.273) loss 0.4323 (0.4050) acc 91.6667 (89.9560) lr 5.1825e-04 eta 0:02:32
epoch [35/50] batch [10/23] time 0.141 (0.282) data 0.000 (0.137) loss 0.5082 (0.4156) acc 86.9318 (89.2438) lr 5.1825e-04 eta 0:01:40
epoch [35/50] batch [15/23] time 0.159 (0.235) data 0.000 (0.091) loss 0.2924 (0.4363) acc 91.8182 (88.4138) lr 5.1825e-04 eta 0:01:23
epoch [35/50] batch [20/23] time 0.146 (0.213) data 0.000 (0.068) loss 0.5400 (0.4473) acc 75.4902 (87.7247) lr 5.1825e-04 eta 0:01:14
>>> alpha1: 0.151  alpha2: 0.013 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.53 <<<
epoch [36/50] batch [5/23] time 0.152 (0.411) data 0.000 (0.261) loss 0.3684 (0.3646) acc 94.8980 (90.0088) lr 4.6417e-04 eta 0:02:19
epoch [36/50] batch [10/23] time 0.152 (0.280) data 0.001 (0.131) loss 0.3553 (0.3661) acc 91.9811 (90.2316) lr 4.6417e-04 eta 0:01:33
epoch [36/50] batch [15/23] time 0.141 (0.233) data 0.000 (0.087) loss 0.3769 (0.5371) acc 92.7083 (86.9243) lr 4.6417e-04 eta 0:01:17
epoch [36/50] batch [20/23] time 0.124 (0.210) data 0.000 (0.066) loss 0.6428 (0.5269) acc 83.9286 (87.0145) lr 4.6417e-04 eta 0:01:08
>>> alpha1: 0.151  alpha2: 0.014 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.56 <<<
epoch [37/50] batch [5/23] time 0.154 (0.407) data 0.000 (0.250) loss 0.4538 (0.4045) acc 87.5000 (87.8155) lr 4.1221e-04 eta 0:02:08
epoch [37/50] batch [10/23] time 0.141 (0.278) data 0.000 (0.125) loss 0.3995 (0.4182) acc 87.5000 (87.0904) lr 4.1221e-04 eta 0:01:26
epoch [37/50] batch [15/23] time 0.136 (0.234) data 0.000 (0.084) loss 0.6499 (0.4171) acc 80.3191 (87.1048) lr 4.1221e-04 eta 0:01:11
epoch [37/50] batch [20/23] time 0.144 (0.212) data 0.000 (0.063) loss 0.3952 (0.4318) acc 88.5000 (86.9819) lr 4.1221e-04 eta 0:01:03
>>> alpha1: 0.147  alpha2: 0.016 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.54 <<<
epoch [38/50] batch [5/23] time 0.158 (0.419) data 0.000 (0.270) loss 0.2980 (0.3716) acc 95.9184 (91.0834) lr 3.6258e-04 eta 0:02:03
epoch [38/50] batch [10/23] time 0.154 (0.283) data 0.000 (0.137) loss 0.3043 (0.3939) acc 95.9091 (90.2331) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [15/23] time 0.148 (0.237) data 0.001 (0.091) loss 0.4344 (0.4071) acc 93.2692 (90.6635) lr 3.6258e-04 eta 0:01:07
epoch [38/50] batch [20/23] time 0.130 (0.212) data 0.001 (0.068) loss 0.4697 (0.4204) acc 86.6667 (89.7901) lr 3.6258e-04 eta 0:00:59
>>> alpha1: 0.142  alpha2: 0.012 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.55 <<<
epoch [39/50] batch [5/23] time 0.154 (0.379) data 0.001 (0.222) loss 0.3269 (0.4068) acc 90.8654 (89.5647) lr 3.1545e-04 eta 0:01:42
epoch [39/50] batch [10/23] time 0.151 (0.267) data 0.000 (0.111) loss 0.3738 (0.4126) acc 92.9245 (88.9614) lr 3.1545e-04 eta 0:01:11
epoch [39/50] batch [15/23] time 0.140 (0.225) data 0.001 (0.074) loss 0.7452 (0.4288) acc 83.6956 (88.8966) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [20/23] time 0.134 (0.203) data 0.000 (0.056) loss 0.4799 (0.4039) acc 82.7778 (88.6386) lr 3.1545e-04 eta 0:00:52
>>> alpha1: 0.143  alpha2: 0.013 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.56 <<<
epoch [40/50] batch [5/23] time 0.135 (0.456) data 0.001 (0.310) loss 0.5447 (0.4373) acc 88.5870 (89.3010) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [10/23] time 0.157 (0.303) data 0.000 (0.155) loss 0.3690 (0.4051) acc 91.8367 (89.3015) lr 2.7103e-04 eta 0:01:13
epoch [40/50] batch [15/23] time 0.152 (0.248) data 0.000 (0.104) loss 0.4666 (0.4165) acc 83.9623 (88.5545) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [20/23] time 0.143 (0.222) data 0.002 (0.078) loss 0.3329 (0.4116) acc 91.0000 (88.3190) lr 2.7103e-04 eta 0:00:51
>>> alpha1: 0.144  alpha2: 0.015 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.55 <<<
epoch [41/50] batch [5/23] time 0.170 (0.414) data 0.000 (0.255) loss 0.3700 (0.3986) acc 88.4615 (88.3210) lr 2.2949e-04 eta 0:01:33
epoch [41/50] batch [10/23] time 0.141 (0.277) data 0.000 (0.128) loss 0.3550 (0.3982) acc 89.0000 (88.0796) lr 2.2949e-04 eta 0:01:00
epoch [41/50] batch [15/23] time 0.136 (0.231) data 0.000 (0.085) loss 0.3583 (0.4812) acc 91.4894 (87.4013) lr 2.2949e-04 eta 0:00:49
epoch [41/50] batch [20/23] time 0.141 (0.208) data 0.000 (0.064) loss 0.3380 (0.4655) acc 92.3469 (87.7629) lr 2.2949e-04 eta 0:00:43
>>> alpha1: 0.143  alpha2: 0.008 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.53 <<<
epoch [42/50] batch [5/23] time 0.145 (0.406) data 0.001 (0.255) loss 0.4849 (0.3483) acc 87.7778 (91.9197) lr 1.9098e-04 eta 0:01:22
epoch [42/50] batch [10/23] time 0.165 (0.277) data 0.000 (0.128) loss 0.3989 (0.3745) acc 92.5000 (91.5172) lr 1.9098e-04 eta 0:00:54
epoch [42/50] batch [15/23] time 0.138 (0.231) data 0.000 (0.085) loss 0.2946 (0.3678) acc 94.2708 (91.5129) lr 1.9098e-04 eta 0:00:44
epoch [42/50] batch [20/23] time 0.138 (0.235) data 0.000 (0.064) loss 0.3383 (0.3842) acc 88.0208 (90.4846) lr 1.9098e-04 eta 0:00:43
>>> alpha1: 0.143  alpha2: 0.007 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.56 <<<
epoch [43/50] batch [5/23] time 0.151 (0.401) data 0.000 (0.246) loss 0.4315 (0.3812) acc 83.8889 (89.7435) lr 1.5567e-04 eta 0:01:11
epoch [43/50] batch [10/23] time 0.149 (0.277) data 0.000 (0.123) loss 0.3855 (0.3965) acc 91.6667 (89.2109) lr 1.5567e-04 eta 0:00:48
epoch [43/50] batch [15/23] time 0.133 (0.232) data 0.000 (0.082) loss 0.3844 (0.3897) acc 92.3913 (90.0336) lr 1.5567e-04 eta 0:00:39
epoch [43/50] batch [20/23] time 0.150 (0.211) data 0.000 (0.062) loss 0.2707 (0.3813) acc 92.9245 (89.9740) lr 1.5567e-04 eta 0:00:34
>>> alpha1: 0.143  alpha2: 0.005 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.59 <<<
epoch [44/50] batch [5/23] time 0.162 (0.440) data 0.000 (0.285) loss 0.4971 (0.3800) acc 84.8039 (88.4222) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [10/23] time 0.152 (0.298) data 0.000 (0.143) loss 0.3977 (0.4253) acc 90.0943 (89.6458) lr 1.2369e-04 eta 0:00:45
epoch [44/50] batch [15/23] time 0.157 (0.248) data 0.000 (0.095) loss 0.3768 (0.4192) acc 93.3036 (89.7885) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [20/23] time 0.146 (0.223) data 0.000 (0.071) loss 0.3975 (0.4139) acc 93.6274 (89.9257) lr 1.2369e-04 eta 0:00:31
>>> alpha1: 0.139  alpha2: 0.003 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.58 <<<
epoch [45/50] batch [5/23] time 0.164 (0.468) data 0.001 (0.307) loss 0.3353 (0.3685) acc 91.8269 (89.5956) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [10/23] time 0.144 (0.313) data 0.000 (0.154) loss 0.4951 (0.3879) acc 87.2549 (89.6105) lr 9.5173e-05 eta 0:00:40
epoch [45/50] batch [15/23] time 0.152 (0.258) data 0.000 (0.103) loss 0.3706 (0.3974) acc 89.6226 (89.3012) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [20/23] time 0.144 (0.229) data 0.000 (0.077) loss 0.5312 (0.4070) acc 87.7451 (89.1212) lr 9.5173e-05 eta 0:00:27
>>> alpha1: 0.139  alpha2: 0.004 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.58 <<<
epoch [46/50] batch [5/23] time 0.159 (0.396) data 0.000 (0.231) loss 0.4513 (0.3909) acc 85.6481 (88.6924) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [10/23] time 0.165 (0.277) data 0.000 (0.116) loss 0.5120 (0.4138) acc 92.1569 (89.3892) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [15/23] time 0.146 (0.234) data 0.000 (0.077) loss 0.4152 (0.4114) acc 92.6471 (89.9336) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/23] time 0.141 (0.213) data 0.000 (0.058) loss 0.4072 (0.3876) acc 83.3333 (90.1047) lr 7.0224e-05 eta 0:00:20
>>> alpha1: 0.137  alpha2: 0.007 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.60 <<<
epoch [47/50] batch [5/23] time 0.152 (0.406) data 0.000 (0.248) loss 0.2750 (0.3706) acc 89.0000 (88.1460) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [10/23] time 0.162 (0.283) data 0.000 (0.124) loss 0.4450 (0.3804) acc 88.4259 (88.3490) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/23] time 0.155 (0.238) data 0.000 (0.083) loss 0.3316 (0.3942) acc 89.3519 (88.7611) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.159 (0.215) data 0.000 (0.062) loss 0.1468 (0.3955) acc 96.4912 (88.6636) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.137  alpha2: 0.009 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.57 <<<
epoch [48/50] batch [5/23] time 0.152 (0.463) data 0.000 (0.307) loss 0.3437 (0.3227) acc 93.5185 (92.1195) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [10/23] time 0.144 (0.302) data 0.000 (0.154) loss 0.4233 (0.3731) acc 87.2549 (90.8163) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.138 (0.252) data 0.000 (0.103) loss 0.4993 (0.3971) acc 84.5745 (89.4372) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.149 (0.225) data 0.000 (0.077) loss 0.4487 (0.4011) acc 91.0377 (89.0948) lr 3.1417e-05 eta 0:00:11
>>> alpha1: 0.135  alpha2: 0.011 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.59 <<<
epoch [49/50] batch [5/23] time 0.172 (0.414) data 0.000 (0.250) loss 0.2971 (0.4096) acc 88.2075 (89.3460) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [10/23] time 0.144 (0.282) data 0.000 (0.125) loss 0.3472 (0.3977) acc 90.3061 (89.5570) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.152 (0.237) data 0.000 (0.084) loss 0.3947 (0.3966) acc 83.7963 (89.4553) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.142 (0.215) data 0.000 (0.063) loss 0.4708 (0.4012) acc 87.5000 (88.7775) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.137  alpha2: 0.019 <<<
>>> noisy rate: 0.61 --> refined noisy rate: 0.38 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.50 <<<
epoch [50/50] batch [5/23] time 0.146 (0.477) data 0.000 (0.326) loss 0.5097 (0.3662) acc 88.8298 (89.3818) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [10/23] time 0.133 (0.306) data 0.000 (0.163) loss 0.3399 (0.3838) acc 89.4445 (89.5162) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/23] time 0.137 (0.250) data 0.000 (0.109) loss 0.4372 (0.3833) acc 91.4773 (89.7221) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.136 (0.221) data 0.001 (0.082) loss 0.4283 (0.3840) acc 88.5417 (89.7679) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.45, 0.45, 0.43, 0.41, 0.4, 0.4, 0.38, 0.38, 0.38, 0.38, 0.39, 0.38, 0.38, 0.39, 0.38, 0.38, 0.38, 0.39, 0.38, 0.37, 0.37, 0.38, 0.38, 0.38, 0.37, 0.38, 0.38, 0.37, 0.39, 0.39, 0.38, 0.38, 0.38, 0.38, 0.38, 0.38, 0.39, 0.38, 0.39, 0.38]
* matched noise rate: [0.24, 0.24, 0.21, 0.2, 0.22, 0.23, 0.24, 0.24, 0.24, 0.22, 0.2, 0.21, 0.24, 0.24, 0.25, 0.24, 0.24, 0.24, 0.23, 0.22, 0.23, 0.24, 0.25, 0.24, 0.24, 0.24, 0.24, 0.23, 0.24, 0.25, 0.24, 0.24, 0.25, 0.25, 0.26, 0.26, 0.26, 0.25, 0.26, 0.23]
* unmatched noise rate: [0.63, 0.62, 0.57, 0.53, 0.61, 0.58, 0.56, 0.54, 0.58, 0.56, 0.51, 0.51, 0.55, 0.56, 0.55, 0.56, 0.56, 0.59, 0.56, 0.57, 0.53, 0.53, 0.57, 0.57, 0.52, 0.53, 0.56, 0.54, 0.55, 0.56, 0.55, 0.53, 0.56, 0.59, 0.58, 0.58, 0.6, 0.57, 0.59, 0.5]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:39,  2.46s/it] 18%|█▊        | 3/17 [00:02<00:09,  1.45it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.70it/s] 41%|████      | 7/17 [00:02<00:02,  4.01it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.52it/s] 65%|██████▍   | 11/17 [00:03<00:00,  7.06it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.54it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.88it/s]100%|██████████| 17/17 [00:04<00:00,  4.91it/s]100%|██████████| 17/17 [00:04<00:00,  3.81it/s]
=> result
* total: 1,692
* correct: 886
* accuracy: 52.4%
* error: 47.6%
* macro_f1: 49.6%
=> per-class result
* class: 0 (banded)	total: 36	correct: 20	acc: 55.6%
* class: 1 (blotchy)	total: 36	correct: 5	acc: 13.9%
* class: 2 (braided)	total: 36	correct: 14	acc: 38.9%
* class: 3 (bubbly)	total: 36	correct: 32	acc: 88.9%
* class: 4 (bumpy)	total: 36	correct: 5	acc: 13.9%
* class: 5 (chequered)	total: 36	correct: 31	acc: 86.1%
* class: 6 (cobwebbed)	total: 36	correct: 28	acc: 77.8%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 12	acc: 33.3%
* class: 9 (crystalline)	total: 36	correct: 33	acc: 91.7%
* class: 10 (dotted)	total: 36	correct: 10	acc: 27.8%
* class: 11 (fibrous)	total: 36	correct: 28	acc: 77.8%
* class: 12 (flecked)	total: 36	correct: 1	acc: 2.8%
* class: 13 (freckled)	total: 36	correct: 26	acc: 72.2%
* class: 14 (frilly)	total: 36	correct: 27	acc: 75.0%
* class: 15 (gauzy)	total: 36	correct: 19	acc: 52.8%
* class: 16 (grid)	total: 36	correct: 18	acc: 50.0%
* class: 17 (grooved)	total: 36	correct: 17	acc: 47.2%
* class: 18 (honeycombed)	total: 36	correct: 11	acc: 30.6%
* class: 19 (interlaced)	total: 36	correct: 3	acc: 8.3%
* class: 20 (knitted)	total: 36	correct: 34	acc: 94.4%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 21	acc: 58.3%
* class: 23 (marbled)	total: 36	correct: 20	acc: 55.6%
* class: 24 (matted)	total: 36	correct: 6	acc: 16.7%
* class: 25 (meshed)	total: 36	correct: 17	acc: 47.2%
* class: 26 (paisley)	total: 36	correct: 35	acc: 97.2%
* class: 27 (perforated)	total: 36	correct: 26	acc: 72.2%
* class: 28 (pitted)	total: 36	correct: 5	acc: 13.9%
* class: 29 (pleated)	total: 36	correct: 17	acc: 47.2%
* class: 30 (polka-dotted)	total: 36	correct: 30	acc: 83.3%
* class: 31 (porous)	total: 36	correct: 12	acc: 33.3%
* class: 32 (potholed)	total: 36	correct: 31	acc: 86.1%
* class: 33 (scaly)	total: 36	correct: 18	acc: 50.0%
* class: 34 (smeared)	total: 36	correct: 12	acc: 33.3%
* class: 35 (spiralled)	total: 36	correct: 15	acc: 41.7%
* class: 36 (sprinkled)	total: 36	correct: 11	acc: 30.6%
* class: 37 (stained)	total: 36	correct: 6	acc: 16.7%
* class: 38 (stratified)	total: 36	correct: 26	acc: 72.2%
* class: 39 (striped)	total: 36	correct: 26	acc: 72.2%
* class: 40 (studded)	total: 36	correct: 27	acc: 75.0%
* class: 41 (swirly)	total: 36	correct: 21	acc: 58.3%
* class: 42 (veined)	total: 36	correct: 17	acc: 47.2%
* class: 43 (waffled)	total: 36	correct: 25	acc: 69.4%
* class: 44 (woven)	total: 36	correct: 12	acc: 33.3%
* class: 45 (wrinkled)	total: 36	correct: 20	acc: 55.6%
* class: 46 (zigzagged)	total: 36	correct: 32	acc: 88.9%
* average: 52.4%
Elapsed: 0:12:55
Run this job and save the output to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_12-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.154 (0.978) data 0.000 (0.398) loss 3.8002 (3.7853) acc 12.5000 (8.7500) lr 1.0000e-05 eta 0:18:39
epoch [1/50] batch [10/23] time 0.158 (0.568) data 0.000 (0.199) loss 3.8766 (3.8197) acc 3.1250 (6.8750) lr 1.0000e-05 eta 0:10:47
epoch [1/50] batch [15/23] time 0.150 (0.428) data 0.000 (0.133) loss 3.7624 (3.8248) acc 12.5000 (6.6667) lr 1.0000e-05 eta 0:08:05
epoch [1/50] batch [20/23] time 0.145 (0.358) data 0.000 (0.100) loss 3.8028 (3.8245) acc 6.2500 (6.5625) lr 1.0000e-05 eta 0:06:44
epoch [2/50] batch [5/23] time 0.163 (0.450) data 0.000 (0.259) loss 3.8210 (3.8522) acc 0.0000 (5.0000) lr 2.0000e-03 eta 0:08:25
epoch [2/50] batch [10/23] time 0.156 (0.306) data 0.000 (0.130) loss 3.9118 (3.8481) acc 3.1250 (5.6250) lr 2.0000e-03 eta 0:05:41
epoch [2/50] batch [15/23] time 0.145 (0.253) data 0.000 (0.087) loss 3.8700 (3.8348) acc 3.1250 (5.8333) lr 2.0000e-03 eta 0:04:40
epoch [2/50] batch [20/23] time 0.145 (0.226) data 0.000 (0.065) loss 3.6376 (3.8157) acc 6.2500 (6.0938) lr 2.0000e-03 eta 0:04:09
epoch [3/50] batch [5/23] time 0.188 (0.431) data 0.000 (0.249) loss 3.8190 (3.7240) acc 9.3750 (10.0000) lr 1.9980e-03 eta 0:07:54
epoch [3/50] batch [10/23] time 0.148 (0.298) data 0.000 (0.125) loss 3.9161 (3.7771) acc 0.0000 (8.1250) lr 1.9980e-03 eta 0:05:26
epoch [3/50] batch [15/23] time 0.147 (0.248) data 0.000 (0.083) loss 3.7093 (3.7658) acc 12.5000 (8.5417) lr 1.9980e-03 eta 0:04:30
epoch [3/50] batch [20/23] time 0.150 (0.223) data 0.000 (0.062) loss 3.7591 (3.7800) acc 9.3750 (7.8125) lr 1.9980e-03 eta 0:04:01
epoch [4/50] batch [5/23] time 0.150 (0.475) data 0.000 (0.302) loss 3.7550 (3.7094) acc 9.3750 (11.8750) lr 1.9921e-03 eta 0:08:30
epoch [4/50] batch [10/23] time 0.172 (0.322) data 0.000 (0.151) loss 3.6921 (3.7121) acc 12.5000 (10.9375) lr 1.9921e-03 eta 0:05:44
epoch [4/50] batch [15/23] time 0.146 (0.263) data 0.000 (0.101) loss 3.7870 (3.7488) acc 9.3750 (10.0000) lr 1.9921e-03 eta 0:04:40
epoch [4/50] batch [20/23] time 0.151 (0.235) data 0.000 (0.076) loss 3.7900 (3.7624) acc 12.5000 (9.6875) lr 1.9921e-03 eta 0:04:09
epoch [5/50] batch [5/23] time 0.149 (0.468) data 0.000 (0.281) loss 3.7428 (3.7096) acc 15.6250 (14.3750) lr 1.9823e-03 eta 0:08:12
epoch [5/50] batch [10/23] time 0.169 (0.318) data 0.000 (0.141) loss 3.7615 (3.7209) acc 6.2500 (11.8750) lr 1.9823e-03 eta 0:05:33
epoch [5/50] batch [15/23] time 0.148 (0.261) data 0.000 (0.094) loss 3.7160 (3.7269) acc 9.3750 (11.0417) lr 1.9823e-03 eta 0:04:32
epoch [5/50] batch [20/23] time 0.150 (0.233) data 0.000 (0.071) loss 3.7772 (3.7341) acc 9.3750 (10.6250) lr 1.9823e-03 eta 0:04:01
epoch [6/50] batch [5/23] time 0.149 (0.454) data 0.000 (0.278) loss 3.6049 (3.6920) acc 15.6250 (13.1250) lr 1.9686e-03 eta 0:07:48
epoch [6/50] batch [10/23] time 0.149 (0.310) data 0.000 (0.139) loss 3.7949 (3.6690) acc 12.5000 (13.1250) lr 1.9686e-03 eta 0:05:18
epoch [6/50] batch [15/23] time 0.150 (0.256) data 0.000 (0.093) loss 3.7328 (3.7073) acc 6.2500 (11.6667) lr 1.9686e-03 eta 0:04:21
epoch [6/50] batch [20/23] time 0.151 (0.229) data 0.000 (0.070) loss 3.7494 (3.7151) acc 6.2500 (10.7812) lr 1.9686e-03 eta 0:03:52
epoch [7/50] batch [5/23] time 0.148 (0.461) data 0.000 (0.286) loss 3.6281 (3.6894) acc 9.3750 (11.2500) lr 1.9511e-03 eta 0:07:44
epoch [7/50] batch [10/23] time 0.161 (0.313) data 0.000 (0.143) loss 3.8603 (3.6758) acc 12.5000 (12.1875) lr 1.9511e-03 eta 0:05:13
epoch [7/50] batch [15/23] time 0.145 (0.259) data 0.000 (0.096) loss 3.5836 (3.6862) acc 15.6250 (11.6667) lr 1.9511e-03 eta 0:04:17
epoch [7/50] batch [20/23] time 0.147 (0.231) data 0.000 (0.072) loss 3.6996 (3.6663) acc 6.2500 (12.5000) lr 1.9511e-03 eta 0:03:48
epoch [8/50] batch [5/23] time 0.181 (0.459) data 0.000 (0.272) loss 3.5833 (3.6936) acc 3.1250 (8.7500) lr 1.9298e-03 eta 0:07:31
epoch [8/50] batch [10/23] time 0.155 (0.311) data 0.000 (0.136) loss 3.5158 (3.6444) acc 12.5000 (10.9375) lr 1.9298e-03 eta 0:05:04
epoch [8/50] batch [15/23] time 0.150 (0.257) data 0.000 (0.091) loss 3.6851 (3.6671) acc 12.5000 (10.2083) lr 1.9298e-03 eta 0:04:10
epoch [8/50] batch [20/23] time 0.148 (0.230) data 0.000 (0.068) loss 3.6580 (3.6542) acc 9.3750 (11.2500) lr 1.9298e-03 eta 0:03:42
epoch [9/50] batch [5/23] time 0.174 (0.421) data 0.000 (0.244) loss 3.5643 (3.5339) acc 12.5000 (15.0000) lr 1.9048e-03 eta 0:06:44
epoch [9/50] batch [10/23] time 0.150 (0.293) data 0.000 (0.124) loss 3.3298 (3.5408) acc 21.8750 (14.6875) lr 1.9048e-03 eta 0:04:40
epoch [9/50] batch [15/23] time 0.145 (0.244) data 0.000 (0.083) loss 3.7138 (3.5912) acc 6.2500 (12.0833) lr 1.9048e-03 eta 0:03:52
epoch [9/50] batch [20/23] time 0.145 (0.219) data 0.000 (0.062) loss 3.4903 (3.6024) acc 12.5000 (12.1875) lr 1.9048e-03 eta 0:03:27
epoch [10/50] batch [5/23] time 0.148 (0.462) data 0.000 (0.292) loss 3.6671 (3.6108) acc 6.2500 (13.1250) lr 1.8763e-03 eta 0:07:13
epoch [10/50] batch [10/23] time 0.147 (0.314) data 0.000 (0.146) loss 3.6485 (3.5906) acc 12.5000 (14.0625) lr 1.8763e-03 eta 0:04:52
epoch [10/50] batch [15/23] time 0.145 (0.259) data 0.000 (0.098) loss 3.6852 (3.5657) acc 9.3750 (13.7500) lr 1.8763e-03 eta 0:04:00
epoch [10/50] batch [20/23] time 0.144 (0.231) data 0.000 (0.073) loss 3.7076 (3.6017) acc 6.2500 (12.3438) lr 1.8763e-03 eta 0:03:32
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.077  alpha2: 0.541 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.63 --> matched refined noisy rate: 0.39 & unmatched refined noisy rate: 0.88 <<<
epoch [11/50] batch [5/23] time 0.156 (0.993) data 0.000 (0.331) loss 3.3527 (3.2176) acc 69.7115 (68.8115) lr 1.8443e-03 eta 0:15:08
epoch [11/50] batch [10/23] time 0.133 (0.689) data 0.000 (0.166) loss 3.1794 (3.2180) acc 60.5556 (64.1300) lr 1.8443e-03 eta 0:10:26
epoch [11/50] batch [15/23] time 0.138 (0.544) data 0.000 (0.111) loss 2.9733 (3.1914) acc 67.0213 (63.9071) lr 1.8443e-03 eta 0:08:12
epoch [11/50] batch [20/23] time 0.154 (0.477) data 0.000 (0.083) loss 3.2675 (3.1992) acc 70.4082 (64.5595) lr 1.8443e-03 eta 0:07:09
>>> alpha1: 0.994  alpha2: 0.550 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.72 <<<
epoch [12/50] batch [5/23] time 0.123 (0.664) data 0.000 (0.301) loss 2.6116 (2.3415) acc 56.0976 (60.2544) lr 1.8090e-03 eta 0:09:52
epoch [12/50] batch [10/23] time 0.108 (0.506) data 0.000 (0.151) loss 2.5164 (2.3629) acc 43.7500 (55.9605) lr 1.8090e-03 eta 0:07:29
epoch [12/50] batch [15/23] time 0.127 (0.380) data 0.000 (0.101) loss 2.4556 (2.2975) acc 58.5227 (57.7687) lr 1.8090e-03 eta 0:05:34
epoch [12/50] batch [20/23] time 0.124 (0.372) data 0.000 (0.076) loss 2.0117 (2.3340) acc 57.9268 (58.3250) lr 1.8090e-03 eta 0:05:26
>>> alpha1: 0.877  alpha2: 0.497 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.57 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.77 <<<
epoch [13/50] batch [5/23] time 0.141 (0.603) data 0.000 (0.306) loss 1.9994 (1.8555) acc 65.1042 (61.7468) lr 1.7705e-03 eta 0:08:43
epoch [13/50] batch [10/23] time 0.136 (0.372) data 0.000 (0.153) loss 1.9064 (1.8248) acc 54.8913 (60.2107) lr 1.7705e-03 eta 0:05:21
epoch [13/50] batch [15/23] time 0.140 (0.294) data 0.000 (0.102) loss 1.7855 (1.7749) acc 62.5000 (60.3658) lr 1.7705e-03 eta 0:04:12
epoch [13/50] batch [20/23] time 0.148 (0.257) data 0.000 (0.077) loss 2.0847 (1.8158) acc 50.9804 (59.1972) lr 1.7705e-03 eta 0:03:39
>>> alpha1: 0.837  alpha2: 0.441 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.55 --> matched refined noisy rate: 0.34 & unmatched refined noisy rate: 0.78 <<<
epoch [14/50] batch [5/23] time 0.135 (0.399) data 0.001 (0.255) loss 1.6429 (1.6183) acc 68.3333 (65.1506) lr 1.7290e-03 eta 0:05:37
epoch [14/50] batch [10/23] time 0.153 (0.272) data 0.000 (0.128) loss 1.5951 (1.6344) acc 55.7292 (63.9565) lr 1.7290e-03 eta 0:03:48
epoch [14/50] batch [15/23] time 0.131 (0.229) data 0.000 (0.085) loss 1.6725 (1.6187) acc 44.0217 (62.6024) lr 1.7290e-03 eta 0:03:11
epoch [14/50] batch [20/23] time 0.144 (0.206) data 0.000 (0.064) loss 1.7235 (1.6201) acc 65.8654 (62.5088) lr 1.7290e-03 eta 0:02:51
>>> alpha1: 0.772  alpha2: 0.402 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.34 & unmatched refined noisy rate: 0.74 <<<
epoch [15/50] batch [5/23] time 0.162 (0.440) data 0.000 (0.289) loss 1.9808 (1.4680) acc 50.9804 (61.5815) lr 1.6845e-03 eta 0:06:01
epoch [15/50] batch [10/23] time 0.159 (0.295) data 0.000 (0.146) loss 1.3666 (1.4492) acc 62.5000 (61.6891) lr 1.6845e-03 eta 0:04:01
epoch [15/50] batch [15/23] time 0.128 (0.290) data 0.000 (0.097) loss 1.6857 (1.4381) acc 59.0909 (62.5093) lr 1.6845e-03 eta 0:03:55
epoch [15/50] batch [20/23] time 0.134 (0.289) data 0.000 (0.073) loss 1.7367 (1.4785) acc 49.4565 (61.3742) lr 1.6845e-03 eta 0:03:53
>>> alpha1: 0.713  alpha2: 0.307 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.51 --> matched refined noisy rate: 0.30 & unmatched refined noisy rate: 0.72 <<<
epoch [16/50] batch [5/23] time 0.146 (0.444) data 0.000 (0.297) loss 1.2060 (1.2842) acc 72.5962 (65.8130) lr 1.6374e-03 eta 0:05:55
epoch [16/50] batch [10/23] time 0.148 (0.298) data 0.000 (0.149) loss 1.2087 (1.2810) acc 61.5000 (65.7702) lr 1.6374e-03 eta 0:03:56
epoch [16/50] batch [15/23] time 0.147 (0.244) data 0.000 (0.099) loss 1.2471 (1.3314) acc 57.2115 (63.4906) lr 1.6374e-03 eta 0:03:12
epoch [16/50] batch [20/23] time 0.140 (0.218) data 0.000 (0.075) loss 1.3969 (1.3874) acc 56.6327 (62.2619) lr 1.6374e-03 eta 0:02:51
>>> alpha1: 0.642  alpha2: 0.242 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.48 --> matched refined noisy rate: 0.29 & unmatched refined noisy rate: 0.70 <<<
epoch [17/50] batch [5/23] time 0.131 (0.426) data 0.000 (0.284) loss 1.4118 (1.2408) acc 59.8837 (68.7216) lr 1.5878e-03 eta 0:05:31
epoch [17/50] batch [10/23] time 0.852 (0.358) data 0.000 (0.142) loss 1.4756 (1.2866) acc 55.6604 (67.6014) lr 1.5878e-03 eta 0:04:36
epoch [17/50] batch [15/23] time 0.156 (0.287) data 0.000 (0.095) loss 1.3119 (1.2597) acc 66.3636 (68.1533) lr 1.5878e-03 eta 0:03:40
epoch [17/50] batch [20/23] time 0.130 (0.251) data 0.000 (0.071) loss 1.4134 (1.2660) acc 72.2222 (67.9103) lr 1.5878e-03 eta 0:03:11
>>> alpha1: 0.582  alpha2: 0.214 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.49 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.70 <<<
epoch [18/50] batch [5/23] time 0.132 (0.416) data 0.000 (0.280) loss 1.1916 (1.1344) acc 72.6190 (69.6072) lr 1.5358e-03 eta 0:05:13
epoch [18/50] batch [10/23] time 0.135 (0.280) data 0.000 (0.140) loss 1.2232 (1.1579) acc 73.9130 (69.4978) lr 1.5358e-03 eta 0:03:29
epoch [18/50] batch [15/23] time 0.128 (0.234) data 0.000 (0.093) loss 1.4281 (1.2045) acc 69.3182 (68.4647) lr 1.5358e-03 eta 0:02:53
epoch [18/50] batch [20/23] time 0.136 (0.210) data 0.000 (0.070) loss 1.0203 (1.1787) acc 74.4681 (67.8870) lr 1.5358e-03 eta 0:02:35
>>> alpha1: 0.569  alpha2: 0.194 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.47 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.68 <<<
epoch [19/50] batch [5/23] time 0.180 (0.455) data 0.000 (0.305) loss 1.1201 (1.0703) acc 65.0943 (71.5167) lr 1.4818e-03 eta 0:05:32
epoch [19/50] batch [10/23] time 0.142 (0.301) data 0.001 (0.153) loss 1.4605 (1.1528) acc 60.7143 (67.9188) lr 1.4818e-03 eta 0:03:38
epoch [19/50] batch [15/23] time 0.134 (0.247) data 0.001 (0.102) loss 1.1625 (1.1497) acc 72.2826 (67.8815) lr 1.4818e-03 eta 0:02:58
epoch [19/50] batch [20/23] time 0.132 (0.220) data 0.000 (0.077) loss 1.4425 (1.1728) acc 57.0652 (67.3481) lr 1.4818e-03 eta 0:02:37
>>> alpha1: 0.550  alpha2: 0.178 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.65 <<<
epoch [20/50] batch [5/23] time 0.152 (0.392) data 0.000 (0.250) loss 0.8393 (1.0086) acc 74.5098 (70.2533) lr 1.4258e-03 eta 0:04:37
epoch [20/50] batch [10/23] time 0.144 (0.268) data 0.000 (0.125) loss 1.0461 (1.0397) acc 67.8571 (69.2008) lr 1.4258e-03 eta 0:03:08
epoch [20/50] batch [15/23] time 0.138 (0.224) data 0.000 (0.084) loss 1.0171 (1.0999) acc 73.4375 (68.6773) lr 1.4258e-03 eta 0:02:36
epoch [20/50] batch [20/23] time 0.137 (0.202) data 0.000 (0.063) loss 1.1006 (1.1156) acc 72.4490 (69.4273) lr 1.4258e-03 eta 0:02:19
>>> alpha1: 0.525  alpha2: 0.165 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.31 & unmatched refined noisy rate: 0.68 <<<
epoch [21/50] batch [5/23] time 0.161 (0.459) data 0.000 (0.298) loss 1.1241 (1.0606) acc 62.2641 (71.4917) lr 1.3681e-03 eta 0:05:14
epoch [21/50] batch [10/23] time 0.170 (0.308) data 0.000 (0.149) loss 0.9705 (1.1024) acc 72.1154 (70.7931) lr 1.3681e-03 eta 0:03:29
epoch [21/50] batch [15/23] time 0.140 (0.255) data 0.000 (0.099) loss 1.3717 (1.1453) acc 66.6667 (68.5205) lr 1.3681e-03 eta 0:02:52
epoch [21/50] batch [20/23] time 0.157 (0.229) data 0.000 (0.075) loss 0.9473 (1.1198) acc 77.2727 (69.6561) lr 1.3681e-03 eta 0:02:33
>>> alpha1: 0.504  alpha2: 0.153 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.60 <<<
epoch [22/50] batch [5/23] time 0.131 (0.608) data 0.000 (0.354) loss 0.7419 (0.9721) acc 76.7442 (72.3329) lr 1.3090e-03 eta 0:06:42
epoch [22/50] batch [10/23] time 0.127 (0.370) data 0.000 (0.177) loss 0.9406 (1.0208) acc 71.5116 (71.8613) lr 1.3090e-03 eta 0:04:03
epoch [22/50] batch [15/23] time 0.125 (0.291) data 0.000 (0.118) loss 1.3819 (1.0807) acc 57.1429 (69.7540) lr 1.3090e-03 eta 0:03:09
epoch [22/50] batch [20/23] time 0.131 (0.251) data 0.000 (0.089) loss 1.1101 (1.1072) acc 75.5682 (70.6534) lr 1.3090e-03 eta 0:02:42
>>> alpha1: 0.481  alpha2: 0.148 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.66 <<<
epoch [23/50] batch [5/23] time 0.176 (0.426) data 0.000 (0.273) loss 1.0313 (0.9941) acc 67.3077 (75.9961) lr 1.2487e-03 eta 0:04:32
epoch [23/50] batch [10/23] time 0.148 (0.285) data 0.000 (0.137) loss 1.2075 (1.0523) acc 66.1111 (72.0400) lr 1.2487e-03 eta 0:03:00
epoch [23/50] batch [15/23] time 0.142 (0.236) data 0.000 (0.091) loss 0.9767 (1.0681) acc 63.2653 (71.3458) lr 1.2487e-03 eta 0:02:28
epoch [23/50] batch [20/23] time 0.143 (0.211) data 0.000 (0.069) loss 1.1665 (1.0730) acc 72.5490 (71.1873) lr 1.2487e-03 eta 0:02:11
>>> alpha1: 0.478  alpha2: 0.147 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.66 <<<
epoch [24/50] batch [5/23] time 0.159 (0.414) data 0.000 (0.266) loss 0.5520 (0.9494) acc 77.4510 (74.5945) lr 1.1874e-03 eta 0:04:15
epoch [24/50] batch [10/23] time 0.158 (0.279) data 0.000 (0.133) loss 0.7199 (0.9535) acc 79.4118 (74.3881) lr 1.1874e-03 eta 0:02:50
epoch [24/50] batch [15/23] time 0.137 (0.231) data 0.000 (0.089) loss 1.0569 (1.0101) acc 70.8333 (73.4251) lr 1.1874e-03 eta 0:02:19
epoch [24/50] batch [20/23] time 0.124 (0.207) data 0.000 (0.067) loss 1.4831 (1.0570) acc 52.9762 (71.6535) lr 1.1874e-03 eta 0:02:04
>>> alpha1: 0.442  alpha2: 0.132 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.62 <<<
epoch [25/50] batch [5/23] time 0.163 (0.399) data 0.000 (0.257) loss 0.7416 (0.8858) acc 77.6042 (73.4175) lr 1.1253e-03 eta 0:03:56
epoch [25/50] batch [10/23] time 0.139 (0.268) data 0.001 (0.129) loss 0.7350 (0.9575) acc 85.3261 (73.4222) lr 1.1253e-03 eta 0:02:37
epoch [25/50] batch [15/23] time 0.130 (0.224) data 0.000 (0.086) loss 1.0697 (0.9799) acc 66.8605 (71.6210) lr 1.1253e-03 eta 0:02:10
epoch [25/50] batch [20/23] time 0.135 (0.201) data 0.000 (0.065) loss 1.1172 (1.0107) acc 66.8478 (71.2310) lr 1.1253e-03 eta 0:01:56
>>> alpha1: 0.422  alpha2: 0.123 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.65 <<<
epoch [26/50] batch [5/23] time 0.162 (0.444) data 0.000 (0.296) loss 0.9049 (0.9878) acc 77.7778 (70.6257) lr 1.0628e-03 eta 0:04:13
epoch [26/50] batch [10/23] time 0.143 (0.295) data 0.000 (0.148) loss 0.7186 (0.9249) acc 77.7778 (73.8410) lr 1.0628e-03 eta 0:02:46
epoch [26/50] batch [15/23] time 0.134 (0.242) data 0.000 (0.099) loss 1.1241 (0.9235) acc 72.8261 (73.9561) lr 1.0628e-03 eta 0:02:15
epoch [26/50] batch [20/23] time 0.145 (0.216) data 0.000 (0.074) loss 0.9552 (0.9475) acc 74.0000 (72.1943) lr 1.0628e-03 eta 0:01:59
>>> alpha1: 0.400  alpha2: 0.118 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.46 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [27/50] batch [5/23] time 0.149 (0.481) data 0.000 (0.338) loss 1.0196 (0.9186) acc 71.7391 (73.0838) lr 1.0000e-03 eta 0:04:23
epoch [27/50] batch [10/23] time 0.153 (0.311) data 0.000 (0.169) loss 0.6886 (0.9501) acc 76.4151 (73.2258) lr 1.0000e-03 eta 0:02:48
epoch [27/50] batch [15/23] time 0.129 (0.252) data 0.000 (0.113) loss 1.0138 (0.9140) acc 65.0000 (73.7717) lr 1.0000e-03 eta 0:02:15
epoch [27/50] batch [20/23] time 0.141 (0.223) data 0.000 (0.085) loss 0.6576 (0.9237) acc 81.8627 (72.8602) lr 1.0000e-03 eta 0:01:58
>>> alpha1: 0.379  alpha2: 0.099 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.64 <<<
epoch [28/50] batch [5/23] time 0.146 (0.499) data 0.000 (0.350) loss 0.5962 (0.7744) acc 85.2941 (76.6143) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [10/23] time 0.148 (0.320) data 0.000 (0.175) loss 0.6543 (0.8101) acc 75.4902 (74.6405) lr 9.3721e-04 eta 0:02:46
epoch [28/50] batch [15/23] time 0.128 (0.260) data 0.000 (0.117) loss 1.0404 (0.8114) acc 71.0227 (74.9636) lr 9.3721e-04 eta 0:02:13
epoch [28/50] batch [20/23] time 0.144 (0.229) data 0.000 (0.088) loss 1.0270 (0.8638) acc 73.0000 (74.0647) lr 9.3721e-04 eta 0:01:56
>>> alpha1: 0.362  alpha2: 0.095 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.64 <<<
epoch [29/50] batch [5/23] time 0.141 (0.497) data 0.000 (0.331) loss 0.8231 (0.8650) acc 79.6875 (75.5612) lr 8.7467e-04 eta 0:04:08
epoch [29/50] batch [10/23] time 0.129 (0.318) data 0.000 (0.166) loss 1.1427 (0.8486) acc 65.9091 (75.8417) lr 8.7467e-04 eta 0:02:37
epoch [29/50] batch [15/23] time 0.142 (0.258) data 0.000 (0.111) loss 0.8687 (0.8460) acc 75.9804 (75.1283) lr 8.7467e-04 eta 0:02:06
epoch [29/50] batch [20/23] time 0.137 (0.228) data 0.001 (0.083) loss 0.9220 (0.8370) acc 68.0851 (75.1375) lr 8.7467e-04 eta 0:01:50
>>> alpha1: 0.341  alpha2: 0.082 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.65 <<<
epoch [30/50] batch [5/23] time 0.140 (0.427) data 0.000 (0.282) loss 0.6203 (0.7827) acc 83.6735 (76.8918) lr 8.1262e-04 eta 0:03:24
epoch [30/50] batch [10/23] time 0.135 (0.285) data 0.001 (0.141) loss 0.9097 (0.8147) acc 80.0000 (77.7021) lr 8.1262e-04 eta 0:02:14
epoch [30/50] batch [15/23] time 0.140 (0.238) data 0.000 (0.094) loss 1.0465 (0.8478) acc 72.4490 (76.3448) lr 8.1262e-04 eta 0:01:51
epoch [30/50] batch [20/23] time 0.138 (0.214) data 0.000 (0.071) loss 0.8550 (0.8349) acc 72.9167 (75.6605) lr 8.1262e-04 eta 0:01:38
>>> alpha1: 0.332  alpha2: 0.085 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.62 <<<
epoch [31/50] batch [5/23] time 0.155 (0.428) data 0.000 (0.278) loss 0.8380 (0.7419) acc 80.8824 (79.2928) lr 7.5131e-04 eta 0:03:14
epoch [31/50] batch [10/23] time 0.133 (0.284) data 0.000 (0.139) loss 0.9481 (0.7451) acc 71.1111 (79.0821) lr 7.5131e-04 eta 0:02:07
epoch [31/50] batch [15/23] time 0.129 (0.235) data 0.000 (0.093) loss 0.8566 (0.7571) acc 78.4884 (77.8962) lr 7.5131e-04 eta 0:01:44
epoch [31/50] batch [20/23] time 0.137 (0.210) data 0.000 (0.070) loss 0.8927 (0.7780) acc 64.3617 (76.8769) lr 7.5131e-04 eta 0:01:32
>>> alpha1: 0.310  alpha2: 0.069 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.63 <<<
epoch [32/50] batch [5/23] time 0.150 (0.403) data 0.011 (0.254) loss 0.6180 (0.7486) acc 80.2083 (76.7525) lr 6.9098e-04 eta 0:02:53
epoch [32/50] batch [10/23] time 0.145 (0.277) data 0.000 (0.127) loss 0.8128 (0.7796) acc 69.7917 (76.4764) lr 6.9098e-04 eta 0:01:58
epoch [32/50] batch [15/23] time 0.139 (0.231) data 0.000 (0.085) loss 0.7675 (0.7785) acc 73.9796 (75.8708) lr 6.9098e-04 eta 0:01:37
epoch [32/50] batch [20/23] time 0.147 (0.210) data 0.001 (0.064) loss 0.7191 (0.7511) acc 79.4118 (77.0009) lr 6.9098e-04 eta 0:01:27
>>> alpha1: 0.303  alpha2: 0.075 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.62 <<<
epoch [33/50] batch [5/23] time 0.144 (0.468) data 0.000 (0.320) loss 0.8525 (0.7493) acc 72.5490 (79.7449) lr 6.3188e-04 eta 0:03:11
epoch [33/50] batch [10/23] time 0.144 (0.302) data 0.000 (0.160) loss 0.7563 (0.7453) acc 77.0000 (79.8168) lr 6.3188e-04 eta 0:02:01
epoch [33/50] batch [15/23] time 0.141 (0.248) data 0.000 (0.107) loss 0.7161 (0.7331) acc 81.0000 (79.2772) lr 6.3188e-04 eta 0:01:39
epoch [33/50] batch [20/23] time 0.131 (0.219) data 0.000 (0.080) loss 0.7514 (0.7535) acc 78.8043 (78.1695) lr 6.3188e-04 eta 0:01:26
>>> alpha1: 0.295  alpha2: 0.077 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.61 <<<
epoch [34/50] batch [5/23] time 0.171 (0.452) data 0.015 (0.309) loss 0.6542 (0.6775) acc 85.0962 (81.2048) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [10/23] time 0.140 (0.296) data 0.000 (0.155) loss 0.5716 (0.7366) acc 84.5000 (78.6528) lr 5.7422e-04 eta 0:01:52
epoch [34/50] batch [15/23] time 0.139 (0.244) data 0.000 (0.103) loss 0.5825 (0.7320) acc 83.3333 (78.1270) lr 5.7422e-04 eta 0:01:31
epoch [34/50] batch [20/23] time 0.142 (0.218) data 0.000 (0.078) loss 0.7725 (0.7391) acc 79.0816 (78.5722) lr 5.7422e-04 eta 0:01:20
>>> alpha1: 0.289  alpha2: 0.083 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.66 <<<
epoch [35/50] batch [5/23] time 0.153 (0.448) data 0.000 (0.301) loss 0.4741 (0.6112) acc 91.0377 (83.6950) lr 5.1825e-04 eta 0:02:42
epoch [35/50] batch [10/23] time 0.153 (0.300) data 0.000 (0.151) loss 0.6089 (0.6688) acc 84.1346 (81.5941) lr 5.1825e-04 eta 0:01:47
epoch [35/50] batch [15/23] time 0.153 (0.248) data 0.000 (0.101) loss 0.6018 (0.6736) acc 83.9623 (80.5369) lr 5.1825e-04 eta 0:01:27
epoch [35/50] batch [20/23] time 0.138 (0.221) data 0.000 (0.076) loss 0.8912 (0.7121) acc 72.3404 (78.5763) lr 5.1825e-04 eta 0:01:16
>>> alpha1: 0.282  alpha2: 0.087 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.67 <<<
epoch [36/50] batch [5/23] time 0.159 (0.422) data 0.000 (0.268) loss 0.9060 (0.7245) acc 77.4038 (76.4233) lr 4.6417e-04 eta 0:02:23
epoch [36/50] batch [10/23] time 0.132 (0.285) data 0.000 (0.136) loss 0.8058 (0.7117) acc 71.1956 (78.8589) lr 4.6417e-04 eta 0:01:35
epoch [36/50] batch [15/23] time 0.138 (0.239) data 0.001 (0.091) loss 0.6930 (0.7093) acc 78.1915 (78.7860) lr 4.6417e-04 eta 0:01:18
epoch [36/50] batch [20/23] time 0.147 (0.249) data 0.000 (0.068) loss 0.8565 (0.7210) acc 77.4510 (78.6509) lr 4.6417e-04 eta 0:01:21
>>> alpha1: 0.280  alpha2: 0.084 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.67 <<<
epoch [37/50] batch [5/23] time 0.137 (0.431) data 0.000 (0.283) loss 0.7414 (0.6729) acc 80.4348 (81.4673) lr 4.1221e-04 eta 0:02:16
epoch [37/50] batch [10/23] time 0.146 (0.288) data 0.000 (0.143) loss 0.8618 (0.7207) acc 71.0784 (78.7558) lr 4.1221e-04 eta 0:01:29
epoch [37/50] batch [15/23] time 0.150 (0.239) data 0.000 (0.096) loss 0.4881 (0.6906) acc 87.5000 (79.6324) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [20/23] time 0.141 (0.216) data 0.000 (0.072) loss 0.6286 (0.6631) acc 83.3333 (80.5577) lr 4.1221e-04 eta 0:01:05
>>> alpha1: 0.275  alpha2: 0.081 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.66 <<<
epoch [38/50] batch [5/23] time 0.153 (0.421) data 0.000 (0.268) loss 0.5491 (0.8451) acc 86.7924 (79.7109) lr 3.6258e-04 eta 0:02:03
epoch [38/50] batch [10/23] time 0.171 (0.288) data 0.000 (0.134) loss 0.7874 (0.7490) acc 75.9804 (81.5587) lr 3.6258e-04 eta 0:01:23
epoch [38/50] batch [15/23] time 0.137 (0.240) data 0.000 (0.090) loss 0.6127 (0.7054) acc 83.1522 (81.8695) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [20/23] time 0.130 (0.215) data 0.000 (0.067) loss 0.7096 (0.7099) acc 72.7273 (80.5458) lr 3.6258e-04 eta 0:01:00
>>> alpha1: 0.272  alpha2: 0.083 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.65 <<<
epoch [39/50] batch [5/23] time 0.162 (0.430) data 0.000 (0.280) loss 0.5403 (0.5989) acc 87.0000 (85.5069) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [10/23] time 0.144 (0.291) data 0.000 (0.140) loss 0.6778 (0.6272) acc 84.5000 (83.5824) lr 3.1545e-04 eta 0:01:17
epoch [39/50] batch [15/23] time 0.152 (0.244) data 0.000 (0.094) loss 0.8182 (0.6800) acc 69.8113 (81.1887) lr 3.1545e-04 eta 0:01:03
epoch [39/50] batch [20/23] time 0.136 (0.219) data 0.000 (0.070) loss 1.0349 (0.6928) acc 66.1458 (80.4334) lr 3.1545e-04 eta 0:00:56
>>> alpha1: 0.265  alpha2: 0.076 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.65 <<<
epoch [40/50] batch [5/23] time 0.160 (0.421) data 0.001 (0.271) loss 0.8786 (0.6250) acc 75.0000 (79.8381) lr 2.7103e-04 eta 0:01:44
epoch [40/50] batch [10/23] time 0.141 (0.283) data 0.000 (0.136) loss 0.7812 (0.6420) acc 77.6042 (80.4297) lr 2.7103e-04 eta 0:01:08
epoch [40/50] batch [15/23] time 0.143 (0.236) data 0.000 (0.091) loss 0.6719 (0.6481) acc 78.0612 (80.7066) lr 2.7103e-04 eta 0:00:56
epoch [40/50] batch [20/23] time 0.142 (0.213) data 0.000 (0.068) loss 0.9593 (0.6564) acc 80.5000 (81.1474) lr 2.7103e-04 eta 0:00:49
>>> alpha1: 0.258  alpha2: 0.071 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.68 <<<
epoch [41/50] batch [5/23] time 0.154 (0.433) data 0.000 (0.281) loss 0.5139 (0.5770) acc 80.0000 (82.2414) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [10/23] time 0.141 (0.289) data 0.000 (0.141) loss 0.6946 (0.6142) acc 79.0816 (82.4018) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [15/23] time 0.145 (0.242) data 0.000 (0.094) loss 0.7120 (0.6322) acc 82.5000 (82.2035) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [20/23] time 0.155 (0.219) data 0.000 (0.071) loss 0.8112 (0.6360) acc 81.0185 (82.0633) lr 2.2949e-04 eta 0:00:45
>>> alpha1: 0.251  alpha2: 0.076 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.65 <<<
epoch [42/50] batch [5/23] time 0.154 (0.433) data 0.000 (0.276) loss 0.7058 (0.6701) acc 84.9057 (81.6200) lr 1.9098e-04 eta 0:01:27
epoch [42/50] batch [10/23] time 0.169 (0.296) data 0.000 (0.138) loss 0.7368 (0.6316) acc 83.0000 (81.4527) lr 1.9098e-04 eta 0:00:58
epoch [42/50] batch [15/23] time 0.142 (0.245) data 0.000 (0.092) loss 0.9300 (0.6889) acc 72.9592 (79.5080) lr 1.9098e-04 eta 0:00:47
epoch [42/50] batch [20/23] time 0.141 (0.220) data 0.000 (0.069) loss 0.4759 (0.6636) acc 84.0000 (80.1856) lr 1.9098e-04 eta 0:00:41
>>> alpha1: 0.248  alpha2: 0.076 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.27 & unmatched refined noisy rate: 0.63 <<<
epoch [43/50] batch [5/23] time 0.179 (0.420) data 0.000 (0.264) loss 0.5783 (0.5735) acc 84.1346 (85.1811) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [10/23] time 0.143 (0.284) data 0.001 (0.132) loss 0.5624 (0.5837) acc 86.7347 (83.7065) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [15/23] time 0.127 (0.237) data 0.000 (0.088) loss 0.6791 (0.5745) acc 81.5476 (83.8541) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [20/23] time 0.135 (0.214) data 0.000 (0.066) loss 0.6259 (0.5892) acc 79.7872 (83.1816) lr 1.5567e-04 eta 0:00:35
>>> alpha1: 0.241  alpha2: 0.073 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [44/50] batch [5/23] time 0.159 (0.375) data 0.000 (0.229) loss 0.4622 (0.8151) acc 87.5000 (82.0329) lr 1.2369e-04 eta 0:00:58
epoch [44/50] batch [10/23] time 0.149 (0.261) data 0.000 (0.115) loss 0.7040 (0.7302) acc 83.1731 (82.2995) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [15/23] time 0.140 (0.221) data 0.001 (0.077) loss 0.4145 (0.6613) acc 84.1837 (83.3530) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/23] time 0.141 (0.203) data 0.000 (0.058) loss 0.7384 (0.6346) acc 80.1020 (82.4696) lr 1.2369e-04 eta 0:00:28
>>> alpha1: 0.234  alpha2: 0.068 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.63 <<<
epoch [45/50] batch [5/23] time 0.152 (0.403) data 0.000 (0.250) loss 0.6323 (0.5112) acc 78.6458 (84.6388) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [10/23] time 0.135 (0.275) data 0.000 (0.125) loss 0.7515 (0.5750) acc 78.3333 (83.0408) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [15/23] time 0.149 (0.232) data 0.000 (0.084) loss 0.4162 (0.5665) acc 87.7358 (81.9897) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/23] time 0.132 (0.207) data 0.000 (0.063) loss 0.7630 (0.6065) acc 79.3478 (81.4200) lr 9.5173e-05 eta 0:00:24
>>> alpha1: 0.236  alpha2: 0.074 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [46/50] batch [5/23] time 0.152 (0.428) data 0.000 (0.275) loss 0.5290 (0.5805) acc 87.2340 (86.0138) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [10/23] time 0.138 (0.288) data 0.000 (0.138) loss 0.4744 (0.5622) acc 88.5417 (84.7017) lr 7.0224e-05 eta 0:00:30
epoch [46/50] batch [15/23] time 0.146 (0.241) data 0.000 (0.092) loss 0.5122 (0.5778) acc 88.7255 (84.7014) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [20/23] time 0.142 (0.216) data 0.000 (0.069) loss 0.6069 (0.5912) acc 82.1429 (84.1014) lr 7.0224e-05 eta 0:00:20
>>> alpha1: 0.239  alpha2: 0.076 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.64 <<<
epoch [47/50] batch [5/23] time 0.147 (0.423) data 0.001 (0.271) loss 0.7225 (0.7268) acc 79.5000 (76.9062) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [10/23] time 0.155 (0.284) data 0.000 (0.136) loss 0.4474 (0.6556) acc 90.5660 (79.9413) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/23] time 0.139 (0.237) data 0.000 (0.091) loss 0.4968 (0.6140) acc 85.4167 (81.8017) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.146 (0.214) data 0.000 (0.068) loss 0.4761 (0.6162) acc 92.1569 (82.1159) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.231  alpha2: 0.071 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.66 <<<
epoch [48/50] batch [5/23] time 0.160 (0.449) data 0.000 (0.292) loss 0.5489 (0.4908) acc 79.9020 (85.9126) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [10/23] time 0.139 (0.298) data 0.000 (0.146) loss 0.9409 (0.5405) acc 67.0213 (84.9704) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.138 (0.248) data 0.000 (0.098) loss 0.6599 (0.5434) acc 82.4468 (84.7744) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.147 (0.222) data 0.001 (0.073) loss 0.5159 (0.5877) acc 86.0577 (83.4863) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.231  alpha2: 0.070 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.44 --> matched refined noisy rate: 0.25 & unmatched refined noisy rate: 0.64 <<<
epoch [49/50] batch [5/23] time 0.149 (0.461) data 0.001 (0.310) loss 0.4832 (0.5651) acc 90.0000 (85.7175) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [10/23] time 0.138 (0.305) data 0.000 (0.155) loss 0.7154 (0.5968) acc 77.6596 (82.5728) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.137 (0.251) data 0.000 (0.104) loss 0.5766 (0.5885) acc 86.7021 (83.1388) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.135 (0.224) data 0.001 (0.078) loss 0.8038 (0.5965) acc 71.7391 (82.9475) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.232  alpha2: 0.070 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.65 <<<
epoch [50/50] batch [5/23] time 0.154 (0.417) data 0.000 (0.263) loss 0.5315 (0.5545) acc 85.2041 (84.0617) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.138 (0.283) data 0.000 (0.132) loss 0.4790 (0.5572) acc 88.5417 (84.3143) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/23] time 0.142 (0.236) data 0.000 (0.088) loss 0.4545 (0.5532) acc 87.2449 (85.1232) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/23] time 0.160 (0.212) data 0.000 (0.066) loss 0.4707 (0.5553) acc 84.8214 (84.3975) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.63, 0.58, 0.57, 0.55, 0.53, 0.51, 0.48, 0.49, 0.47, 0.46, 0.46, 0.46, 0.46, 0.46, 0.46, 0.46, 0.46, 0.45, 0.45, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.44, 0.43, 0.44, 0.44, 0.44, 0.44, 0.44, 0.43, 0.43, 0.43, 0.44, 0.44, 0.43]
* matched noise rate: [0.39, 0.27, 0.37, 0.34, 0.34, 0.3, 0.29, 0.25, 0.27, 0.25, 0.31, 0.22, 0.26, 0.24, 0.23, 0.24, 0.26, 0.23, 0.27, 0.26, 0.25, 0.27, 0.26, 0.26, 0.26, 0.26, 0.24, 0.26, 0.28, 0.27, 0.28, 0.28, 0.27, 0.26, 0.26, 0.26, 0.26, 0.28, 0.25, 0.24]
* unmatched noise rate: [0.88, 0.72, 0.77, 0.78, 0.74, 0.72, 0.7, 0.7, 0.68, 0.65, 0.68, 0.6, 0.66, 0.66, 0.62, 0.65, 0.64, 0.64, 0.64, 0.65, 0.62, 0.63, 0.62, 0.61, 0.66, 0.67, 0.67, 0.66, 0.65, 0.65, 0.68, 0.65, 0.63, 0.64, 0.63, 0.64, 0.64, 0.66, 0.64, 0.65]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:41,  2.58s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.37it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.53it/s] 41%|████      | 7/17 [00:03<00:02,  3.90it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.39it/s] 65%|██████▍   | 11/17 [00:03<00:00,  6.92it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.39it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.73it/s]100%|██████████| 17/17 [00:04<00:00,  5.16it/s]100%|██████████| 17/17 [00:04<00:00,  3.73it/s]
=> result
* total: 1,692
* correct: 824
* accuracy: 48.7%
* error: 51.3%
* macro_f1: 46.6%
=> per-class result
* class: 0 (banded)	total: 36	correct: 26	acc: 72.2%
* class: 1 (blotchy)	total: 36	correct: 0	acc: 0.0%
* class: 2 (braided)	total: 36	correct: 11	acc: 30.6%
* class: 3 (bubbly)	total: 36	correct: 28	acc: 77.8%
* class: 4 (bumpy)	total: 36	correct: 0	acc: 0.0%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 27	acc: 75.0%
* class: 7 (cracked)	total: 36	correct: 23	acc: 63.9%
* class: 8 (crosshatched)	total: 36	correct: 10	acc: 27.8%
* class: 9 (crystalline)	total: 36	correct: 27	acc: 75.0%
* class: 10 (dotted)	total: 36	correct: 4	acc: 11.1%
* class: 11 (fibrous)	total: 36	correct: 21	acc: 58.3%
* class: 12 (flecked)	total: 36	correct: 8	acc: 22.2%
* class: 13 (freckled)	total: 36	correct: 32	acc: 88.9%
* class: 14 (frilly)	total: 36	correct: 18	acc: 50.0%
* class: 15 (gauzy)	total: 36	correct: 12	acc: 33.3%
* class: 16 (grid)	total: 36	correct: 9	acc: 25.0%
* class: 17 (grooved)	total: 36	correct: 3	acc: 8.3%
* class: 18 (honeycombed)	total: 36	correct: 20	acc: 55.6%
* class: 19 (interlaced)	total: 36	correct: 22	acc: 61.1%
* class: 20 (knitted)	total: 36	correct: 30	acc: 83.3%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 9	acc: 25.0%
* class: 23 (marbled)	total: 36	correct: 13	acc: 36.1%
* class: 24 (matted)	total: 36	correct: 22	acc: 61.1%
* class: 25 (meshed)	total: 36	correct: 17	acc: 47.2%
* class: 26 (paisley)	total: 36	correct: 30	acc: 83.3%
* class: 27 (perforated)	total: 36	correct: 27	acc: 75.0%
* class: 28 (pitted)	total: 36	correct: 2	acc: 5.6%
* class: 29 (pleated)	total: 36	correct: 22	acc: 61.1%
* class: 30 (polka-dotted)	total: 36	correct: 22	acc: 61.1%
* class: 31 (porous)	total: 36	correct: 8	acc: 22.2%
* class: 32 (potholed)	total: 36	correct: 35	acc: 97.2%
* class: 33 (scaly)	total: 36	correct: 21	acc: 58.3%
* class: 34 (smeared)	total: 36	correct: 12	acc: 33.3%
* class: 35 (spiralled)	total: 36	correct: 16	acc: 44.4%
* class: 36 (sprinkled)	total: 36	correct: 11	acc: 30.6%
* class: 37 (stained)	total: 36	correct: 0	acc: 0.0%
* class: 38 (stratified)	total: 36	correct: 26	acc: 72.2%
* class: 39 (striped)	total: 36	correct: 25	acc: 69.4%
* class: 40 (studded)	total: 36	correct: 21	acc: 58.3%
* class: 41 (swirly)	total: 36	correct: 17	acc: 47.2%
* class: 42 (veined)	total: 36	correct: 17	acc: 47.2%
* class: 43 (waffled)	total: 36	correct: 24	acc: 66.7%
* class: 44 (woven)	total: 36	correct: 15	acc: 41.7%
* class: 45 (wrinkled)	total: 36	correct: 18	acc: 50.0%
* class: 46 (zigzagged)	total: 36	correct: 30	acc: 83.3%
* average: 48.7%
Elapsed: 0:12:57
Run this job and save the output to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_12-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.149 (1.012) data 0.000 (0.295) loss 3.8567 (3.7736) acc 9.3750 (10.6250) lr 1.0000e-05 eta 0:19:18
epoch [1/50] batch [10/23] time 0.152 (0.585) data 0.000 (0.148) loss 3.8205 (3.8022) acc 12.5000 (8.7500) lr 1.0000e-05 eta 0:11:07
epoch [1/50] batch [15/23] time 0.147 (0.440) data 0.000 (0.099) loss 3.8855 (3.8279) acc 12.5000 (7.9167) lr 1.0000e-05 eta 0:08:19
epoch [1/50] batch [20/23] time 0.148 (0.367) data 0.000 (0.074) loss 3.8779 (3.8281) acc 3.1250 (7.5000) lr 1.0000e-05 eta 0:06:55
epoch [2/50] batch [5/23] time 0.151 (0.502) data 0.000 (0.319) loss 3.7542 (3.8495) acc 6.2500 (6.2500) lr 2.0000e-03 eta 0:09:22
epoch [2/50] batch [10/23] time 0.178 (0.337) data 0.000 (0.160) loss 3.8000 (3.8359) acc 9.3750 (6.2500) lr 2.0000e-03 eta 0:06:16
epoch [2/50] batch [15/23] time 0.149 (0.274) data 0.000 (0.106) loss 3.8797 (3.8301) acc 0.0000 (6.4583) lr 2.0000e-03 eta 0:05:04
epoch [2/50] batch [20/23] time 0.147 (0.242) data 0.000 (0.080) loss 3.7482 (3.8214) acc 9.3750 (7.0312) lr 2.0000e-03 eta 0:04:28
epoch [3/50] batch [5/23] time 0.150 (0.514) data 0.000 (0.329) loss 3.7527 (3.7517) acc 15.6250 (7.5000) lr 1.9980e-03 eta 0:09:24
epoch [3/50] batch [10/23] time 0.162 (0.342) data 0.000 (0.164) loss 3.7838 (3.7759) acc 15.6250 (9.0625) lr 1.9980e-03 eta 0:06:14
epoch [3/50] batch [15/23] time 0.150 (0.278) data 0.000 (0.110) loss 3.8656 (3.7732) acc 6.2500 (9.7917) lr 1.9980e-03 eta 0:05:03
epoch [3/50] batch [20/23] time 0.148 (0.246) data 0.000 (0.082) loss 3.7443 (3.7678) acc 6.2500 (9.3750) lr 1.9980e-03 eta 0:04:26
epoch [4/50] batch [5/23] time 0.176 (0.459) data 0.012 (0.278) loss 3.7687 (3.7834) acc 12.5000 (6.8750) lr 1.9921e-03 eta 0:08:13
epoch [4/50] batch [10/23] time 0.161 (0.313) data 0.000 (0.139) loss 3.6445 (3.7581) acc 15.6250 (8.4375) lr 1.9921e-03 eta 0:05:35
epoch [4/50] batch [15/23] time 0.148 (0.259) data 0.000 (0.093) loss 3.6402 (3.7438) acc 18.7500 (9.3750) lr 1.9921e-03 eta 0:04:35
epoch [4/50] batch [20/23] time 0.147 (0.231) data 0.000 (0.070) loss 3.6283 (3.7477) acc 15.6250 (9.6875) lr 1.9921e-03 eta 0:04:05
epoch [5/50] batch [5/23] time 0.150 (0.531) data 0.000 (0.355) loss 3.6768 (3.6920) acc 15.6250 (15.6250) lr 1.9823e-03 eta 0:09:19
epoch [5/50] batch [10/23] time 0.162 (0.351) data 0.000 (0.178) loss 3.7009 (3.6862) acc 9.3750 (15.0000) lr 1.9823e-03 eta 0:06:08
epoch [5/50] batch [15/23] time 0.146 (0.283) data 0.000 (0.119) loss 3.6615 (3.7199) acc 18.7500 (13.3333) lr 1.9823e-03 eta 0:04:55
epoch [5/50] batch [20/23] time 0.146 (0.249) data 0.000 (0.089) loss 3.7391 (3.7198) acc 6.2500 (12.1875) lr 1.9823e-03 eta 0:04:18
epoch [6/50] batch [5/23] time 0.158 (0.466) data 0.000 (0.292) loss 3.6015 (3.6878) acc 18.7500 (13.7500) lr 1.9686e-03 eta 0:08:00
epoch [6/50] batch [10/23] time 0.153 (0.312) data 0.000 (0.146) loss 3.6505 (3.7077) acc 15.6250 (12.1875) lr 1.9686e-03 eta 0:05:20
epoch [6/50] batch [15/23] time 0.150 (0.259) data 0.000 (0.098) loss 3.7406 (3.7129) acc 6.2500 (11.2500) lr 1.9686e-03 eta 0:04:23
epoch [6/50] batch [20/23] time 0.152 (0.231) data 0.000 (0.073) loss 3.9551 (3.7049) acc 3.1250 (11.0938) lr 1.9686e-03 eta 0:03:54
epoch [7/50] batch [5/23] time 0.151 (0.474) data 0.000 (0.295) loss 3.5181 (3.6506) acc 18.7500 (11.8750) lr 1.9511e-03 eta 0:07:57
epoch [7/50] batch [10/23] time 0.152 (0.319) data 0.000 (0.148) loss 3.6222 (3.6862) acc 6.2500 (10.6250) lr 1.9511e-03 eta 0:05:19
epoch [7/50] batch [15/23] time 0.146 (0.263) data 0.000 (0.098) loss 3.5788 (3.6446) acc 12.5000 (12.5000) lr 1.9511e-03 eta 0:04:22
epoch [7/50] batch [20/23] time 0.151 (0.234) data 0.000 (0.074) loss 3.5778 (3.6698) acc 12.5000 (12.3438) lr 1.9511e-03 eta 0:03:52
epoch [8/50] batch [5/23] time 0.152 (0.470) data 0.000 (0.287) loss 3.5190 (3.6568) acc 18.7500 (15.0000) lr 1.9298e-03 eta 0:07:42
epoch [8/50] batch [10/23] time 0.185 (0.319) data 0.000 (0.144) loss 3.7217 (3.6751) acc 6.2500 (12.8125) lr 1.9298e-03 eta 0:05:12
epoch [8/50] batch [15/23] time 0.147 (0.262) data 0.000 (0.096) loss 3.7867 (3.6768) acc 6.2500 (12.5000) lr 1.9298e-03 eta 0:04:15
epoch [8/50] batch [20/23] time 0.151 (0.234) data 0.000 (0.072) loss 4.0693 (3.6845) acc 3.1250 (11.8750) lr 1.9298e-03 eta 0:03:46
epoch [9/50] batch [5/23] time 0.166 (0.497) data 0.000 (0.310) loss 3.6006 (3.6919) acc 15.6250 (10.6250) lr 1.9048e-03 eta 0:07:57
epoch [9/50] batch [10/23] time 0.157 (0.332) data 0.000 (0.155) loss 3.2418 (3.6135) acc 25.0000 (13.4375) lr 1.9048e-03 eta 0:05:17
epoch [9/50] batch [15/23] time 0.149 (0.271) data 0.000 (0.104) loss 3.5540 (3.6356) acc 15.6250 (12.9167) lr 1.9048e-03 eta 0:04:17
epoch [9/50] batch [20/23] time 0.148 (0.240) data 0.000 (0.078) loss 3.3963 (3.6384) acc 15.6250 (12.5000) lr 1.9048e-03 eta 0:03:47
epoch [10/50] batch [5/23] time 0.166 (0.446) data 0.000 (0.266) loss 3.7533 (3.6663) acc 9.3750 (11.2500) lr 1.8763e-03 eta 0:06:58
epoch [10/50] batch [10/23] time 0.151 (0.305) data 0.000 (0.133) loss 3.4673 (3.5914) acc 15.6250 (14.3750) lr 1.8763e-03 eta 0:04:44
epoch [10/50] batch [15/23] time 0.151 (0.254) data 0.000 (0.089) loss 3.7994 (3.6297) acc 9.3750 (12.2917) lr 1.8763e-03 eta 0:03:55
epoch [10/50] batch [20/23] time 0.151 (0.229) data 0.000 (0.067) loss 3.6967 (3.6562) acc 6.2500 (11.0938) lr 1.8763e-03 eta 0:03:30
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 1.247  alpha2: 0.699 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.74 --> matched refined noisy rate: 0.57 & unmatched refined noisy rate: 0.92 <<<
epoch [11/50] batch [5/23] time 0.756 (1.120) data 0.000 (0.340) loss 3.3912 (3.4273) acc 55.3191 (55.1387) lr 1.8443e-03 eta 0:17:04
epoch [11/50] batch [10/23] time 0.734 (0.760) data 0.000 (0.170) loss 3.4688 (3.4133) acc 51.1111 (57.0856) lr 1.8443e-03 eta 0:11:31
epoch [11/50] batch [15/23] time 0.787 (0.649) data 0.001 (0.114) loss 3.0686 (3.3966) acc 51.7857 (55.4853) lr 1.8443e-03 eta 0:09:46
epoch [11/50] batch [20/23] time 0.144 (0.521) data 0.000 (0.085) loss 3.2846 (3.3721) acc 59.5000 (55.2711) lr 1.8443e-03 eta 0:07:48
>>> alpha1: 0.806  alpha2: 0.646 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.76 --> matched refined noisy rate: 0.55 & unmatched refined noisy rate: 0.88 <<<
epoch [12/50] batch [5/23] time 0.120 (0.755) data 0.000 (0.277) loss 2.8769 (2.7886) acc 62.1951 (67.2861) lr 1.8090e-03 eta 0:11:13
epoch [12/50] batch [10/23] time 0.726 (0.503) data 0.001 (0.139) loss 2.8131 (2.6599) acc 58.7209 (67.0696) lr 1.8090e-03 eta 0:07:26
epoch [12/50] batch [15/23] time 0.665 (0.414) data 0.001 (0.092) loss 2.5702 (2.5386) acc 60.2564 (66.4978) lr 1.8090e-03 eta 0:06:05
epoch [12/50] batch [20/23] time 0.119 (0.343) data 0.000 (0.069) loss 2.5651 (2.5163) acc 68.9024 (65.7084) lr 1.8090e-03 eta 0:05:00
>>> alpha1: 0.915  alpha2: 0.615 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.78 --> matched refined noisy rate: 0.58 & unmatched refined noisy rate: 0.92 <<<
epoch [13/50] batch [5/23] time 0.143 (0.427) data 0.000 (0.284) loss 2.2276 (2.0525) acc 49.4318 (64.6113) lr 1.7705e-03 eta 0:06:10
epoch [13/50] batch [10/23] time 0.811 (0.350) data 0.001 (0.142) loss 1.6380 (1.9836) acc 70.5882 (64.4042) lr 1.7705e-03 eta 0:05:02
epoch [13/50] batch [15/23] time 0.133 (0.276) data 0.000 (0.095) loss 2.2714 (1.9815) acc 63.3333 (63.4021) lr 1.7705e-03 eta 0:03:57
epoch [13/50] batch [20/23] time 0.114 (0.241) data 0.000 (0.071) loss 1.9155 (1.9368) acc 69.2308 (64.9828) lr 1.7705e-03 eta 0:03:25
>>> alpha1: 0.938  alpha2: 0.540 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.75 --> matched refined noisy rate: 0.56 & unmatched refined noisy rate: 0.90 <<<
epoch [14/50] batch [5/23] time 0.159 (0.457) data 0.000 (0.308) loss 1.5479 (1.3258) acc 68.2292 (73.8133) lr 1.7290e-03 eta 0:06:26
epoch [14/50] batch [10/23] time 0.129 (0.296) data 0.000 (0.156) loss 1.9125 (1.6484) acc 56.2500 (66.0880) lr 1.7290e-03 eta 0:04:09
epoch [14/50] batch [15/23] time 0.128 (0.243) data 0.001 (0.104) loss 1.5890 (1.6624) acc 63.6905 (64.1063) lr 1.7290e-03 eta 0:03:23
epoch [14/50] batch [20/23] time 0.144 (0.216) data 0.000 (0.078) loss 1.1135 (1.6129) acc 81.1225 (63.8267) lr 1.7290e-03 eta 0:02:59
>>> alpha1: 0.949  alpha2: 0.473 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.72 --> matched refined noisy rate: 0.53 & unmatched refined noisy rate: 0.87 <<<
epoch [15/50] batch [5/23] time 0.139 (0.418) data 0.000 (0.271) loss 1.5170 (1.1846) acc 67.0455 (72.9412) lr 1.6845e-03 eta 0:05:44
epoch [15/50] batch [10/23] time 0.148 (0.277) data 0.001 (0.136) loss 1.1990 (1.2710) acc 72.9167 (70.2645) lr 1.6845e-03 eta 0:03:46
epoch [15/50] batch [15/23] time 0.133 (0.228) data 0.000 (0.091) loss 1.4922 (1.2641) acc 78.2609 (70.2380) lr 1.6845e-03 eta 0:03:05
epoch [15/50] batch [20/23] time 0.143 (0.205) data 0.000 (0.068) loss 1.2887 (1.2728) acc 73.4694 (69.5294) lr 1.6845e-03 eta 0:02:45
>>> alpha1: 0.824  alpha2: 0.367 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.72 --> matched refined noisy rate: 0.51 & unmatched refined noisy rate: 0.89 <<<
epoch [16/50] batch [5/23] time 0.136 (0.429) data 0.000 (0.282) loss 0.9003 (0.9796) acc 82.4468 (76.4657) lr 1.6374e-03 eta 0:05:43
epoch [16/50] batch [10/23] time 0.157 (0.286) data 0.000 (0.141) loss 1.0784 (1.0598) acc 75.0000 (74.8084) lr 1.6374e-03 eta 0:03:47
epoch [16/50] batch [15/23] time 0.134 (0.233) data 0.000 (0.094) loss 1.0234 (1.0478) acc 79.7872 (74.6907) lr 1.6374e-03 eta 0:03:04
epoch [16/50] batch [20/23] time 0.150 (0.210) data 0.000 (0.071) loss 1.0717 (1.0570) acc 68.7500 (73.9363) lr 1.6374e-03 eta 0:02:44
>>> alpha1: 0.773  alpha2: 0.280 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.70 --> matched refined noisy rate: 0.49 & unmatched refined noisy rate: 0.88 <<<
epoch [17/50] batch [5/23] time 0.156 (0.432) data 0.000 (0.286) loss 1.6932 (1.1145) acc 55.3191 (74.6016) lr 1.5878e-03 eta 0:05:36
epoch [17/50] batch [10/23] time 0.131 (0.286) data 0.001 (0.143) loss 0.9014 (0.9932) acc 82.7778 (75.5430) lr 1.5878e-03 eta 0:03:40
epoch [17/50] batch [15/23] time 0.133 (0.236) data 0.000 (0.096) loss 0.9828 (1.0018) acc 67.2222 (75.7692) lr 1.5878e-03 eta 0:03:01
epoch [17/50] batch [20/23] time 0.124 (0.210) data 0.000 (0.072) loss 1.1585 (1.0157) acc 75.5952 (75.5953) lr 1.5878e-03 eta 0:02:40
>>> alpha1: 0.727  alpha2: 0.209 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.68 --> matched refined noisy rate: 0.55 & unmatched refined noisy rate: 0.89 <<<
epoch [18/50] batch [5/23] time 0.141 (0.590) data 0.000 (0.283) loss 1.3081 (1.1703) acc 63.2653 (67.6261) lr 1.5358e-03 eta 0:07:24
epoch [18/50] batch [10/23] time 0.166 (0.443) data 0.001 (0.142) loss 1.2148 (1.1439) acc 65.7407 (69.9548) lr 1.5358e-03 eta 0:05:31
epoch [18/50] batch [15/23] time 0.159 (0.391) data 0.000 (0.095) loss 1.1133 (1.1033) acc 60.2679 (69.4119) lr 1.5358e-03 eta 0:04:50
epoch [18/50] batch [20/23] time 0.142 (0.362) data 0.001 (0.071) loss 0.9578 (1.0806) acc 70.0000 (70.7174) lr 1.5358e-03 eta 0:04:27
>>> alpha1: 0.673  alpha2: 0.184 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.69 --> matched refined noisy rate: 0.47 & unmatched refined noisy rate: 0.88 <<<
epoch [19/50] batch [5/23] time 0.154 (0.437) data 0.000 (0.294) loss 0.9307 (0.9676) acc 79.0816 (76.9512) lr 1.4818e-03 eta 0:05:19
epoch [19/50] batch [10/23] time 0.141 (0.289) data 0.000 (0.147) loss 1.2431 (1.0129) acc 63.5870 (74.3366) lr 1.4818e-03 eta 0:03:29
epoch [19/50] batch [15/23] time 0.138 (0.239) data 0.000 (0.098) loss 0.7681 (1.0438) acc 79.6875 (72.6679) lr 1.4818e-03 eta 0:02:52
epoch [19/50] batch [20/23] time 0.137 (0.214) data 0.000 (0.074) loss 0.9845 (1.0272) acc 61.2245 (72.7488) lr 1.4818e-03 eta 0:02:33
>>> alpha1: 0.645  alpha2: 0.184 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.67 --> matched refined noisy rate: 0.45 & unmatched refined noisy rate: 0.88 <<<
epoch [20/50] batch [5/23] time 0.140 (0.433) data 0.000 (0.290) loss 1.1795 (1.0130) acc 69.6429 (73.6192) lr 1.4258e-03 eta 0:05:06
epoch [20/50] batch [10/23] time 0.139 (0.288) data 0.000 (0.145) loss 0.8950 (1.0279) acc 70.4082 (72.6812) lr 1.4258e-03 eta 0:03:22
epoch [20/50] batch [15/23] time 0.133 (0.237) data 0.000 (0.097) loss 1.0145 (1.0060) acc 66.4894 (73.5753) lr 1.4258e-03 eta 0:02:45
epoch [20/50] batch [20/23] time 0.129 (0.212) data 0.000 (0.073) loss 0.8998 (1.0053) acc 70.0000 (73.2202) lr 1.4258e-03 eta 0:02:26
>>> alpha1: 0.652  alpha2: 0.188 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.66 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.89 <<<
epoch [21/50] batch [5/23] time 0.148 (0.430) data 0.011 (0.278) loss 1.1199 (0.9787) acc 76.1111 (73.9706) lr 1.3681e-03 eta 0:04:54
epoch [21/50] batch [10/23] time 0.136 (0.286) data 0.000 (0.139) loss 1.1064 (1.0511) acc 73.9362 (70.8471) lr 1.3681e-03 eta 0:03:14
epoch [21/50] batch [15/23] time 0.137 (0.236) data 0.000 (0.093) loss 1.0617 (1.0504) acc 71.8750 (71.9811) lr 1.3681e-03 eta 0:02:39
epoch [21/50] batch [20/23] time 0.152 (0.212) data 0.000 (0.070) loss 0.7764 (1.0137) acc 82.5472 (72.3132) lr 1.3681e-03 eta 0:02:22
>>> alpha1: 0.683  alpha2: 0.201 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.65 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.87 <<<
epoch [22/50] batch [5/23] time 0.158 (0.431) data 0.011 (0.282) loss 0.8675 (0.9252) acc 80.5000 (76.6802) lr 1.3090e-03 eta 0:04:45
epoch [22/50] batch [10/23] time 0.137 (0.287) data 0.000 (0.141) loss 1.5435 (1.0465) acc 64.6739 (71.4153) lr 1.3090e-03 eta 0:03:08
epoch [22/50] batch [15/23] time 0.154 (0.238) data 0.000 (0.094) loss 1.0728 (1.1057) acc 65.5660 (69.7368) lr 1.3090e-03 eta 0:02:35
epoch [22/50] batch [20/23] time 0.139 (0.214) data 0.000 (0.071) loss 1.1426 (1.1267) acc 76.5957 (69.7536) lr 1.3090e-03 eta 0:02:18
>>> alpha1: 0.675  alpha2: 0.214 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.63 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.83 <<<
epoch [23/50] batch [5/23] time 0.162 (0.434) data 0.001 (0.285) loss 0.9260 (1.1104) acc 78.3333 (69.5381) lr 1.2487e-03 eta 0:04:37
epoch [23/50] batch [10/23] time 0.149 (0.286) data 0.000 (0.143) loss 1.2039 (1.1465) acc 69.6808 (69.4054) lr 1.2487e-03 eta 0:03:01
epoch [23/50] batch [15/23] time 0.129 (0.237) data 0.000 (0.095) loss 1.2020 (1.1262) acc 67.0455 (70.5705) lr 1.2487e-03 eta 0:02:29
epoch [23/50] batch [20/23] time 0.144 (0.212) data 0.000 (0.071) loss 1.1850 (1.1022) acc 64.0000 (70.2294) lr 1.2487e-03 eta 0:02:12
>>> alpha1: 0.655  alpha2: 0.218 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.63 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.84 <<<
epoch [24/50] batch [5/23] time 0.138 (0.477) data 0.000 (0.328) loss 1.0901 (1.0658) acc 70.2128 (71.2452) lr 1.1874e-03 eta 0:04:53
epoch [24/50] batch [10/23] time 0.130 (0.379) data 0.000 (0.164) loss 1.3740 (1.0292) acc 64.4445 (72.1391) lr 1.1874e-03 eta 0:03:51
epoch [24/50] batch [15/23] time 0.135 (0.298) data 0.000 (0.110) loss 1.0126 (1.0380) acc 65.9574 (71.9411) lr 1.1874e-03 eta 0:03:00
epoch [24/50] batch [20/23] time 0.141 (0.258) data 0.000 (0.082) loss 0.9487 (1.0545) acc 75.5000 (72.0484) lr 1.1874e-03 eta 0:02:34
>>> alpha1: 0.651  alpha2: 0.220 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.63 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.84 <<<
epoch [25/50] batch [5/23] time 0.162 (0.471) data 0.000 (0.328) loss 1.1430 (0.9479) acc 71.5909 (77.2899) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [10/23] time 0.139 (0.305) data 0.000 (0.164) loss 0.8556 (1.0058) acc 70.2128 (75.1301) lr 1.1253e-03 eta 0:02:59
epoch [25/50] batch [15/23] time 0.139 (0.250) data 0.000 (0.110) loss 1.5379 (1.0988) acc 64.0625 (72.6605) lr 1.1253e-03 eta 0:02:25
epoch [25/50] batch [20/23] time 0.132 (0.223) data 0.000 (0.082) loss 1.3442 (1.1078) acc 68.8889 (72.2716) lr 1.1253e-03 eta 0:02:08
>>> alpha1: 0.659  alpha2: 0.218 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.63 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.84 <<<
epoch [26/50] batch [5/23] time 0.164 (0.456) data 0.000 (0.304) loss 1.1833 (1.1482) acc 70.5000 (72.6525) lr 1.0628e-03 eta 0:04:19
epoch [26/50] batch [10/23] time 0.149 (0.300) data 0.000 (0.152) loss 1.0620 (1.0958) acc 58.3333 (71.8623) lr 1.0628e-03 eta 0:02:49
epoch [26/50] batch [15/23] time 0.149 (0.247) data 0.000 (0.102) loss 1.2041 (1.0935) acc 71.6346 (71.9924) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [20/23] time 0.138 (0.220) data 0.000 (0.076) loss 1.3161 (1.1312) acc 68.3673 (71.4826) lr 1.0628e-03 eta 0:02:02
>>> alpha1: 0.651  alpha2: 0.231 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.61 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.83 <<<
epoch [27/50] batch [5/23] time 0.145 (0.438) data 0.001 (0.289) loss 1.5243 (1.1231) acc 53.0000 (71.4424) lr 1.0000e-03 eta 0:03:59
epoch [27/50] batch [10/23] time 0.142 (0.299) data 0.001 (0.145) loss 1.1118 (1.0797) acc 69.6808 (70.8932) lr 1.0000e-03 eta 0:02:41
epoch [27/50] batch [15/23] time 0.147 (0.246) data 0.001 (0.097) loss 1.4028 (1.1373) acc 69.5000 (69.7448) lr 1.0000e-03 eta 0:02:11
epoch [27/50] batch [20/23] time 0.143 (0.221) data 0.000 (0.073) loss 1.2820 (1.1394) acc 63.2653 (69.3795) lr 1.0000e-03 eta 0:01:57
>>> alpha1: 0.601  alpha2: 0.226 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.62 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.85 <<<
epoch [28/50] batch [5/23] time 0.159 (0.403) data 0.000 (0.252) loss 1.0259 (0.9301) acc 72.0000 (72.5790) lr 9.3721e-04 eta 0:03:31
epoch [28/50] batch [10/23] time 0.150 (0.277) data 0.000 (0.126) loss 0.9092 (0.9494) acc 73.5849 (73.5889) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [15/23] time 0.144 (0.232) data 0.000 (0.084) loss 1.0066 (1.0053) acc 69.3878 (72.7778) lr 9.3721e-04 eta 0:01:59
epoch [28/50] batch [20/23] time 0.139 (0.208) data 0.000 (0.063) loss 1.6184 (1.0708) acc 59.6939 (71.5136) lr 9.3721e-04 eta 0:01:45
>>> alpha1: 0.595  alpha2: 0.235 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.61 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.83 <<<
epoch [29/50] batch [5/23] time 0.152 (0.454) data 0.000 (0.304) loss 1.1293 (0.9586) acc 72.2826 (73.4844) lr 8.7467e-04 eta 0:03:47
epoch [29/50] batch [10/23] time 0.144 (0.299) data 0.000 (0.152) loss 1.2007 (1.0294) acc 65.3061 (71.8551) lr 8.7467e-04 eta 0:02:28
epoch [29/50] batch [15/23] time 0.143 (0.248) data 0.000 (0.102) loss 1.0362 (1.0172) acc 73.5000 (73.0136) lr 8.7467e-04 eta 0:02:01
epoch [29/50] batch [20/23] time 0.150 (0.222) data 0.001 (0.076) loss 1.4645 (1.0475) acc 60.5769 (71.8420) lr 8.7467e-04 eta 0:01:48
>>> alpha1: 0.547  alpha2: 0.216 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.81 <<<
epoch [30/50] batch [5/23] time 0.150 (0.413) data 0.000 (0.266) loss 1.1869 (1.0281) acc 61.4130 (71.5109) lr 8.1262e-04 eta 0:03:17
epoch [30/50] batch [10/23] time 0.147 (0.285) data 0.001 (0.134) loss 0.9885 (0.9900) acc 66.5000 (71.3346) lr 8.1262e-04 eta 0:02:14
epoch [30/50] batch [15/23] time 0.147 (0.238) data 0.000 (0.090) loss 0.9265 (1.0147) acc 78.4314 (70.6351) lr 8.1262e-04 eta 0:01:51
epoch [30/50] batch [20/23] time 0.134 (0.214) data 0.001 (0.067) loss 1.3401 (1.0175) acc 57.2222 (70.8426) lr 8.1262e-04 eta 0:01:39
>>> alpha1: 0.496  alpha2: 0.197 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.80 <<<
epoch [31/50] batch [5/23] time 0.146 (0.411) data 0.001 (0.260) loss 1.1530 (0.9596) acc 63.5000 (70.6052) lr 7.5131e-04 eta 0:03:07
epoch [31/50] batch [10/23] time 0.144 (0.281) data 0.000 (0.130) loss 0.9603 (0.9411) acc 73.0000 (72.6549) lr 7.5131e-04 eta 0:02:06
epoch [31/50] batch [15/23] time 0.137 (0.235) data 0.000 (0.087) loss 1.0332 (0.9308) acc 70.7447 (73.2017) lr 7.5131e-04 eta 0:01:44
epoch [31/50] batch [20/23] time 0.150 (0.211) data 0.000 (0.065) loss 0.6963 (0.9506) acc 77.3585 (72.5377) lr 7.5131e-04 eta 0:01:32
>>> alpha1: 0.495  alpha2: 0.186 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.61 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.81 <<<
epoch [32/50] batch [5/23] time 0.145 (0.413) data 0.000 (0.263) loss 1.1168 (0.9411) acc 73.9796 (73.0099) lr 6.9098e-04 eta 0:02:58
epoch [32/50] batch [10/23] time 0.153 (0.279) data 0.000 (0.132) loss 1.3132 (0.9987) acc 62.5000 (70.9658) lr 6.9098e-04 eta 0:01:59
epoch [32/50] batch [15/23] time 0.149 (0.233) data 0.000 (0.088) loss 0.8787 (1.0193) acc 78.3654 (71.5511) lr 6.9098e-04 eta 0:01:38
epoch [32/50] batch [20/23] time 0.152 (0.211) data 0.001 (0.066) loss 0.8946 (0.9878) acc 73.1132 (72.7309) lr 6.9098e-04 eta 0:01:27
>>> alpha1: 0.494  alpha2: 0.188 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.61 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.84 <<<
epoch [33/50] batch [5/23] time 0.154 (0.433) data 0.000 (0.279) loss 1.3608 (1.0074) acc 57.4468 (69.3710) lr 6.3188e-04 eta 0:02:57
epoch [33/50] batch [10/23] time 0.136 (0.290) data 0.000 (0.140) loss 1.1487 (0.9873) acc 65.4255 (71.1881) lr 6.3188e-04 eta 0:01:57
epoch [33/50] batch [15/23] time 0.155 (0.241) data 0.000 (0.093) loss 0.9796 (1.0681) acc 72.6852 (69.5119) lr 6.3188e-04 eta 0:01:36
epoch [33/50] batch [20/23] time 0.142 (0.215) data 0.000 (0.070) loss 0.8811 (1.0437) acc 68.8775 (69.7615) lr 6.3188e-04 eta 0:01:24
>>> alpha1: 0.434  alpha2: 0.162 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.78 <<<
epoch [34/50] batch [5/23] time 0.146 (0.434) data 0.000 (0.291) loss 0.9632 (0.8061) acc 77.0833 (76.0019) lr 5.7422e-04 eta 0:02:47
epoch [34/50] batch [10/23] time 0.152 (0.289) data 0.000 (0.145) loss 0.7872 (0.8694) acc 75.5319 (74.2185) lr 5.7422e-04 eta 0:01:50
epoch [34/50] batch [15/23] time 0.134 (0.239) data 0.000 (0.097) loss 1.0098 (0.8867) acc 75.0000 (74.3973) lr 5.7422e-04 eta 0:01:29
epoch [34/50] batch [20/23] time 0.133 (0.212) data 0.000 (0.073) loss 0.9390 (0.9106) acc 68.4783 (72.8717) lr 5.7422e-04 eta 0:01:18
>>> alpha1: 0.430  alpha2: 0.164 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.81 <<<
epoch [35/50] batch [5/23] time 0.141 (0.458) data 0.000 (0.309) loss 0.9885 (1.0279) acc 73.9130 (71.5913) lr 5.1825e-04 eta 0:02:46
epoch [35/50] batch [10/23] time 0.154 (0.302) data 0.000 (0.155) loss 0.8455 (0.9321) acc 68.0556 (71.8937) lr 5.1825e-04 eta 0:01:48
epoch [35/50] batch [15/23] time 0.155 (0.250) data 0.001 (0.103) loss 0.9104 (0.9227) acc 69.0909 (72.8441) lr 5.1825e-04 eta 0:01:28
epoch [35/50] batch [20/23] time 0.145 (0.222) data 0.000 (0.077) loss 1.2475 (0.9288) acc 69.2308 (73.1959) lr 5.1825e-04 eta 0:01:17
>>> alpha1: 0.420  alpha2: 0.159 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.81 <<<
epoch [36/50] batch [5/23] time 0.142 (0.484) data 0.000 (0.330) loss 0.9424 (0.8884) acc 73.4694 (74.4651) lr 4.6417e-04 eta 0:02:44
epoch [36/50] batch [10/23] time 0.135 (0.314) data 0.000 (0.165) loss 0.8529 (0.9910) acc 81.3830 (74.2879) lr 4.6417e-04 eta 0:01:45
epoch [36/50] batch [15/23] time 0.158 (0.258) data 0.000 (0.110) loss 0.8688 (0.9671) acc 80.3571 (74.3469) lr 4.6417e-04 eta 0:01:25
epoch [36/50] batch [20/23] time 0.139 (0.229) data 0.000 (0.083) loss 0.8638 (0.9483) acc 78.6458 (74.5678) lr 4.6417e-04 eta 0:01:14
>>> alpha1: 0.403  alpha2: 0.159 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.81 <<<
epoch [37/50] batch [5/23] time 0.164 (0.488) data 0.000 (0.332) loss 0.7890 (0.7487) acc 75.0000 (79.0137) lr 4.1221e-04 eta 0:02:34
epoch [37/50] batch [10/23] time 0.145 (0.321) data 0.000 (0.166) loss 0.8040 (0.8274) acc 82.1429 (77.9112) lr 4.1221e-04 eta 0:01:40
epoch [37/50] batch [15/23] time 0.142 (0.263) data 0.001 (0.111) loss 0.9174 (0.8567) acc 64.5833 (75.1606) lr 4.1221e-04 eta 0:01:20
epoch [37/50] batch [20/23] time 0.141 (0.233) data 0.000 (0.083) loss 0.8357 (0.8565) acc 76.5306 (75.3581) lr 4.1221e-04 eta 0:01:10
>>> alpha1: 0.392  alpha2: 0.155 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.82 <<<
epoch [38/50] batch [5/23] time 0.172 (0.476) data 0.000 (0.317) loss 0.8203 (0.7554) acc 76.9608 (80.6635) lr 3.6258e-04 eta 0:02:19
epoch [38/50] batch [10/23] time 0.163 (0.314) data 0.000 (0.159) loss 0.8997 (0.7917) acc 73.5849 (78.1222) lr 3.6258e-04 eta 0:01:30
epoch [38/50] batch [15/23] time 0.139 (0.257) data 0.000 (0.106) loss 0.9994 (0.8163) acc 63.2653 (75.9371) lr 3.6258e-04 eta 0:01:12
epoch [38/50] batch [20/23] time 0.144 (0.228) data 0.000 (0.079) loss 0.9534 (0.8423) acc 73.5294 (75.7222) lr 3.6258e-04 eta 0:01:03
>>> alpha1: 0.364  alpha2: 0.133 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.80 <<<
epoch [39/50] batch [5/23] time 0.182 (0.432) data 0.000 (0.277) loss 0.6812 (0.7805) acc 81.9444 (74.9938) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [10/23] time 0.139 (0.290) data 0.001 (0.139) loss 0.7257 (0.7367) acc 78.1915 (77.9003) lr 3.1545e-04 eta 0:01:17
epoch [39/50] batch [15/23] time 0.142 (0.242) data 0.000 (0.093) loss 0.7410 (0.7619) acc 78.0000 (77.6336) lr 3.1545e-04 eta 0:01:03
epoch [39/50] batch [20/23] time 0.142 (0.218) data 0.000 (0.070) loss 0.7040 (0.7572) acc 80.6122 (77.4038) lr 3.1545e-04 eta 0:00:55
>>> alpha1: 0.361  alpha2: 0.136 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.79 <<<
epoch [40/50] batch [5/23] time 0.152 (0.408) data 0.000 (0.257) loss 0.9802 (0.7876) acc 68.8679 (79.1248) lr 2.7103e-04 eta 0:01:41
epoch [40/50] batch [10/23] time 0.151 (0.279) data 0.001 (0.128) loss 0.5742 (0.7769) acc 78.3654 (77.5124) lr 2.7103e-04 eta 0:01:07
epoch [40/50] batch [15/23] time 0.139 (0.233) data 0.001 (0.086) loss 1.0233 (0.8043) acc 71.8750 (76.7063) lr 2.7103e-04 eta 0:00:55
epoch [40/50] batch [20/23] time 0.157 (0.212) data 0.000 (0.064) loss 0.5714 (0.7788) acc 87.5000 (77.9021) lr 2.7103e-04 eta 0:00:49
>>> alpha1: 0.346  alpha2: 0.131 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.79 <<<
epoch [41/50] batch [5/23] time 0.157 (0.428) data 0.000 (0.271) loss 0.7348 (0.6878) acc 75.5208 (80.3491) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [10/23] time 0.152 (0.291) data 0.000 (0.136) loss 0.6482 (0.7001) acc 83.9623 (78.3074) lr 2.2949e-04 eta 0:01:04
epoch [41/50] batch [15/23] time 0.152 (0.242) data 0.000 (0.091) loss 0.7652 (0.7217) acc 81.1321 (78.7444) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [20/23] time 0.140 (0.218) data 0.001 (0.068) loss 0.9683 (0.7624) acc 70.0000 (77.4477) lr 2.2949e-04 eta 0:00:45
>>> alpha1: 0.326  alpha2: 0.122 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.80 <<<
epoch [42/50] batch [5/23] time 0.145 (0.424) data 0.000 (0.276) loss 0.8757 (0.6895) acc 78.8889 (81.0535) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [10/23] time 0.156 (0.289) data 0.001 (0.138) loss 0.8851 (0.7331) acc 76.8519 (79.6473) lr 1.9098e-04 eta 0:00:56
epoch [42/50] batch [15/23] time 0.139 (0.240) data 0.000 (0.092) loss 0.9029 (0.7401) acc 72.3958 (78.5206) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [20/23] time 0.140 (0.216) data 0.000 (0.069) loss 0.9318 (0.7176) acc 72.4490 (78.7442) lr 1.9098e-04 eta 0:00:40
>>> alpha1: 0.309  alpha2: 0.110 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.78 <<<
epoch [43/50] batch [5/23] time 0.144 (0.438) data 0.000 (0.280) loss 0.7848 (0.7346) acc 77.0000 (77.8920) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [10/23] time 0.147 (0.293) data 0.000 (0.140) loss 0.7345 (0.7287) acc 72.9592 (77.2212) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [15/23] time 0.147 (0.242) data 0.001 (0.094) loss 0.6097 (0.7294) acc 84.3137 (77.8308) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [20/23] time 0.130 (0.217) data 0.000 (0.070) loss 0.8482 (0.7382) acc 73.8636 (77.8601) lr 1.5567e-04 eta 0:00:35
>>> alpha1: 0.292  alpha2: 0.101 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.80 <<<
epoch [44/50] batch [5/23] time 0.170 (0.409) data 0.000 (0.250) loss 0.6368 (0.6506) acc 78.3019 (82.0603) lr 1.2369e-04 eta 0:01:03
epoch [44/50] batch [10/23] time 0.142 (0.278) data 0.000 (0.125) loss 0.6211 (0.8271) acc 86.0000 (79.1726) lr 1.2369e-04 eta 0:00:42
epoch [44/50] batch [15/23] time 0.152 (0.235) data 0.000 (0.083) loss 0.8031 (0.7834) acc 75.4717 (78.8773) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [20/23] time 0.153 (0.213) data 0.000 (0.063) loss 0.7504 (0.7809) acc 80.1887 (78.3147) lr 1.2369e-04 eta 0:00:30
>>> alpha1: 0.286  alpha2: 0.099 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.81 <<<
epoch [45/50] batch [5/23] time 0.150 (0.402) data 0.000 (0.249) loss 0.8540 (0.7334) acc 72.1698 (77.0535) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [10/23] time 0.145 (0.277) data 0.000 (0.125) loss 0.7646 (0.7157) acc 78.6458 (78.7769) lr 9.5173e-05 eta 0:00:35
epoch [45/50] batch [15/23] time 0.150 (0.234) data 0.000 (0.083) loss 0.7125 (0.6913) acc 75.4808 (80.4153) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/23] time 0.144 (0.212) data 0.000 (0.062) loss 0.6150 (0.6909) acc 81.3726 (80.6106) lr 9.5173e-05 eta 0:00:25
>>> alpha1: 0.283  alpha2: 0.096 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.44 & unmatched refined noisy rate: 0.81 <<<
epoch [46/50] batch [5/23] time 0.149 (0.418) data 0.000 (0.267) loss 0.7961 (0.7388) acc 71.9388 (76.9944) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [10/23] time 0.155 (0.287) data 0.000 (0.134) loss 0.6536 (0.6876) acc 80.5556 (78.7579) lr 7.0224e-05 eta 0:00:30
epoch [46/50] batch [15/23] time 0.149 (0.240) data 0.000 (0.089) loss 0.4859 (0.6795) acc 88.4615 (80.0873) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [20/23] time 0.141 (0.218) data 0.001 (0.067) loss 0.6868 (0.6755) acc 76.5625 (80.1939) lr 7.0224e-05 eta 0:00:20
>>> alpha1: 0.283  alpha2: 0.099 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.80 <<<
epoch [47/50] batch [5/23] time 0.180 (0.437) data 0.000 (0.276) loss 0.6566 (0.7359) acc 84.6491 (80.9389) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [10/23] time 0.147 (0.294) data 0.000 (0.138) loss 0.6573 (0.6978) acc 80.8824 (80.9871) lr 4.8943e-05 eta 0:00:24
epoch [47/50] batch [15/23] time 0.135 (0.245) data 0.000 (0.092) loss 0.7655 (0.7311) acc 79.8913 (79.8098) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.152 (0.221) data 0.000 (0.069) loss 0.6674 (0.6953) acc 82.4074 (80.7486) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.283  alpha2: 0.105 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.81 <<<
epoch [48/50] batch [5/23] time 0.173 (0.421) data 0.000 (0.263) loss 0.5364 (0.6117) acc 85.2041 (85.3721) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [10/23] time 0.169 (0.290) data 0.000 (0.132) loss 0.6879 (0.6307) acc 86.3636 (83.0959) lr 3.1417e-05 eta 0:00:17
epoch [48/50] batch [15/23] time 0.141 (0.242) data 0.000 (0.088) loss 0.8450 (0.6546) acc 70.4082 (82.5365) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.142 (0.219) data 0.000 (0.066) loss 0.7734 (0.6817) acc 72.0000 (80.5889) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.283  alpha2: 0.110 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.81 <<<
epoch [49/50] batch [5/23] time 0.161 (0.409) data 0.000 (0.258) loss 0.7039 (0.6189) acc 83.1731 (81.8340) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [10/23] time 0.159 (0.282) data 0.000 (0.131) loss 0.6520 (0.6441) acc 83.5000 (81.2633) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.154 (0.238) data 0.000 (0.087) loss 0.6607 (0.6796) acc 79.5455 (79.3056) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.146 (0.213) data 0.000 (0.065) loss 0.7034 (0.6910) acc 81.2500 (78.9704) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.278  alpha2: 0.108 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.43 & unmatched refined noisy rate: 0.80 <<<
epoch [50/50] batch [5/23] time 0.152 (0.421) data 0.000 (0.267) loss 0.5270 (0.6637) acc 90.0943 (85.0463) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/23] time 0.138 (0.285) data 0.000 (0.134) loss 0.6130 (0.6473) acc 83.5106 (83.1273) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/23] time 0.154 (0.240) data 0.000 (0.089) loss 0.5830 (0.6476) acc 85.1852 (83.1478) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/23] time 0.142 (0.216) data 0.000 (0.067) loss 0.6941 (0.6791) acc 78.5714 (81.2702) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.74, 0.76, 0.78, 0.75, 0.72, 0.72, 0.7, 0.68, 0.69, 0.67, 0.66, 0.65, 0.63, 0.63, 0.63, 0.63, 0.61, 0.62, 0.61, 0.6, 0.6, 0.61, 0.61, 0.6, 0.6, 0.6, 0.6, 0.59, 0.59, 0.59, 0.59, 0.59, 0.58, 0.58, 0.59, 0.58, 0.58, 0.59, 0.58, 0.58]
* matched noise rate: [0.57, 0.55, 0.58, 0.56, 0.53, 0.51, 0.49, 0.55, 0.47, 0.45, 0.43, 0.44, 0.41, 0.41, 0.41, 0.43, 0.42, 0.43, 0.42, 0.44, 0.43, 0.42, 0.4, 0.37, 0.42, 0.44, 0.43, 0.43, 0.41, 0.44, 0.44, 0.42, 0.41, 0.42, 0.44, 0.44, 0.43, 0.43, 0.43, 0.43]
* unmatched noise rate: [0.92, 0.88, 0.92, 0.9, 0.87, 0.89, 0.88, 0.89, 0.88, 0.88, 0.89, 0.87, 0.83, 0.84, 0.84, 0.84, 0.83, 0.85, 0.83, 0.81, 0.8, 0.81, 0.84, 0.78, 0.81, 0.81, 0.81, 0.82, 0.8, 0.79, 0.79, 0.8, 0.78, 0.8, 0.81, 0.81, 0.8, 0.81, 0.81, 0.8]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:37,  2.37s/it] 12%|█▏        | 2/17 [00:02<00:15,  1.05s/it] 24%|██▎       | 4/17 [00:02<00:05,  2.31it/s] 35%|███▌      | 6/17 [00:02<00:02,  3.70it/s] 47%|████▋     | 8/17 [00:02<00:01,  5.29it/s] 59%|█████▉    | 10/17 [00:03<00:01,  6.89it/s] 71%|███████   | 12/17 [00:03<00:00,  8.41it/s] 82%|████████▏ | 14/17 [00:03<00:00,  9.78it/s] 94%|█████████▍| 16/17 [00:03<00:00, 10.93it/s]100%|██████████| 17/17 [00:04<00:00,  3.98it/s]
=> result
* total: 1,692
* correct: 652
* accuracy: 38.5%
* error: 61.5%
* macro_f1: 32.7%
=> per-class result
* class: 0 (banded)	total: 36	correct: 11	acc: 30.6%
* class: 1 (blotchy)	total: 36	correct: 0	acc: 0.0%
* class: 2 (braided)	total: 36	correct: 13	acc: 36.1%
* class: 3 (bubbly)	total: 36	correct: 24	acc: 66.7%
* class: 4 (bumpy)	total: 36	correct: 0	acc: 0.0%
* class: 5 (chequered)	total: 36	correct: 34	acc: 94.4%
* class: 6 (cobwebbed)	total: 36	correct: 11	acc: 30.6%
* class: 7 (cracked)	total: 36	correct: 24	acc: 66.7%
* class: 8 (crosshatched)	total: 36	correct: 8	acc: 22.2%
* class: 9 (crystalline)	total: 36	correct: 31	acc: 86.1%
* class: 10 (dotted)	total: 36	correct: 31	acc: 86.1%
* class: 11 (fibrous)	total: 36	correct: 0	acc: 0.0%
* class: 12 (flecked)	total: 36	correct: 0	acc: 0.0%
* class: 13 (freckled)	total: 36	correct: 26	acc: 72.2%
* class: 14 (frilly)	total: 36	correct: 12	acc: 33.3%
* class: 15 (gauzy)	total: 36	correct: 1	acc: 2.8%
* class: 16 (grid)	total: 36	correct: 15	acc: 41.7%
* class: 17 (grooved)	total: 36	correct: 0	acc: 0.0%
* class: 18 (honeycombed)	total: 36	correct: 22	acc: 61.1%
* class: 19 (interlaced)	total: 36	correct: 3	acc: 8.3%
* class: 20 (knitted)	total: 36	correct: 35	acc: 97.2%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 8	acc: 22.2%
* class: 23 (marbled)	total: 36	correct: 7	acc: 19.4%
* class: 24 (matted)	total: 36	correct: 24	acc: 66.7%
* class: 25 (meshed)	total: 36	correct: 0	acc: 0.0%
* class: 26 (paisley)	total: 36	correct: 36	acc: 100.0%
* class: 27 (perforated)	total: 36	correct: 10	acc: 27.8%
* class: 28 (pitted)	total: 36	correct: 0	acc: 0.0%
* class: 29 (pleated)	total: 36	correct: 0	acc: 0.0%
* class: 30 (polka-dotted)	total: 36	correct: 3	acc: 8.3%
* class: 31 (porous)	total: 36	correct: 0	acc: 0.0%
* class: 32 (potholed)	total: 36	correct: 20	acc: 55.6%
* class: 33 (scaly)	total: 36	correct: 16	acc: 44.4%
* class: 34 (smeared)	total: 36	correct: 13	acc: 36.1%
* class: 35 (spiralled)	total: 36	correct: 20	acc: 55.6%
* class: 36 (sprinkled)	total: 36	correct: 16	acc: 44.4%
* class: 37 (stained)	total: 36	correct: 0	acc: 0.0%
* class: 38 (stratified)	total: 36	correct: 25	acc: 69.4%
* class: 39 (striped)	total: 36	correct: 29	acc: 80.6%
* class: 40 (studded)	total: 36	correct: 27	acc: 75.0%
* class: 41 (swirly)	total: 36	correct: 0	acc: 0.0%
* class: 42 (veined)	total: 36	correct: 24	acc: 66.7%
* class: 43 (waffled)	total: 36	correct: 26	acc: 72.2%
* class: 44 (woven)	total: 36	correct: 13	acc: 36.1%
* class: 45 (wrinkled)	total: 36	correct: 23	acc: 63.9%
* class: 46 (zigzagged)	total: 36	correct: 11	acc: 30.6%
* average: 38.5%
Elapsed: 0:13:01
Run this job and save the output to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: DescribableTextures
Reading split from /data1/zhli/dpl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/dtd/split_fewshot/shots_16_symflip/fp_12-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      188
# test     1,692
---------  -------------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: DescribableTextures
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/23] time 0.148 (1.043) data 0.000 (0.330) loss 3.6596 (3.8842) acc 15.6250 (10.6250) lr 1.0000e-05 eta 0:19:54
epoch [1/50] batch [10/23] time 0.159 (0.602) data 0.000 (0.165) loss 3.6571 (3.8648) acc 9.3750 (7.5000) lr 1.0000e-05 eta 0:11:25
epoch [1/50] batch [15/23] time 0.153 (0.451) data 0.000 (0.110) loss 3.7551 (3.8455) acc 6.2500 (6.4583) lr 1.0000e-05 eta 0:08:31
epoch [1/50] batch [20/23] time 0.147 (0.375) data 0.000 (0.083) loss 3.6787 (3.8370) acc 15.6250 (6.4062) lr 1.0000e-05 eta 0:07:03
epoch [2/50] batch [5/23] time 0.147 (0.511) data 0.000 (0.329) loss 3.8351 (3.8289) acc 3.1250 (3.7500) lr 2.0000e-03 eta 0:09:33
epoch [2/50] batch [10/23] time 0.168 (0.343) data 0.000 (0.165) loss 3.5962 (3.8232) acc 12.5000 (4.6875) lr 2.0000e-03 eta 0:06:22
epoch [2/50] batch [15/23] time 0.151 (0.278) data 0.000 (0.110) loss 3.7982 (3.8110) acc 3.1250 (5.2083) lr 2.0000e-03 eta 0:05:09
epoch [2/50] batch [20/23] time 0.156 (0.247) data 0.001 (0.083) loss 3.8276 (3.8070) acc 6.2500 (5.7812) lr 2.0000e-03 eta 0:04:32
epoch [3/50] batch [5/23] time 0.177 (0.433) data 0.000 (0.240) loss 3.9169 (3.7814) acc 0.0000 (3.7500) lr 1.9980e-03 eta 0:07:55
epoch [3/50] batch [10/23] time 0.157 (0.297) data 0.000 (0.120) loss 3.6209 (3.7789) acc 18.7500 (6.8750) lr 1.9980e-03 eta 0:05:25
epoch [3/50] batch [15/23] time 0.148 (0.248) data 0.000 (0.080) loss 3.7997 (3.7702) acc 3.1250 (7.0833) lr 1.9980e-03 eta 0:04:29
epoch [3/50] batch [20/23] time 0.156 (0.223) data 0.000 (0.060) loss 3.7700 (3.7672) acc 9.3750 (7.8125) lr 1.9980e-03 eta 0:04:02
epoch [4/50] batch [5/23] time 0.161 (0.466) data 0.000 (0.279) loss 3.7523 (3.6862) acc 6.2500 (9.3750) lr 1.9921e-03 eta 0:08:21
epoch [4/50] batch [10/23] time 0.161 (0.317) data 0.000 (0.140) loss 3.7634 (3.6972) acc 9.3750 (9.0625) lr 1.9921e-03 eta 0:05:39
epoch [4/50] batch [15/23] time 0.147 (0.260) data 0.000 (0.093) loss 3.6689 (3.7429) acc 15.6250 (8.3333) lr 1.9921e-03 eta 0:04:37
epoch [4/50] batch [20/23] time 0.150 (0.232) data 0.000 (0.070) loss 3.6592 (3.7447) acc 15.6250 (8.9062) lr 1.9921e-03 eta 0:04:06
epoch [5/50] batch [5/23] time 0.185 (0.464) data 0.000 (0.283) loss 3.7477 (3.6818) acc 9.3750 (11.8750) lr 1.9823e-03 eta 0:08:08
epoch [5/50] batch [10/23] time 0.152 (0.316) data 0.000 (0.142) loss 3.6713 (3.7342) acc 15.6250 (10.3125) lr 1.9823e-03 eta 0:05:31
epoch [5/50] batch [15/23] time 0.150 (0.261) data 0.000 (0.095) loss 3.8327 (3.7355) acc 9.3750 (10.8333) lr 1.9823e-03 eta 0:04:31
epoch [5/50] batch [20/23] time 0.149 (0.233) data 0.000 (0.071) loss 3.7157 (3.7355) acc 6.2500 (9.6875) lr 1.9823e-03 eta 0:04:01
epoch [6/50] batch [5/23] time 0.168 (0.534) data 0.000 (0.348) loss 3.7164 (3.7150) acc 12.5000 (9.3750) lr 1.9686e-03 eta 0:09:10
epoch [6/50] batch [10/23] time 0.150 (0.350) data 0.000 (0.174) loss 3.7392 (3.6907) acc 12.5000 (10.3125) lr 1.9686e-03 eta 0:05:58
epoch [6/50] batch [15/23] time 0.156 (0.285) data 0.000 (0.117) loss 3.5107 (3.6994) acc 18.7500 (10.0000) lr 1.9686e-03 eta 0:04:50
epoch [6/50] batch [20/23] time 0.149 (0.251) data 0.000 (0.088) loss 3.9993 (3.6985) acc 0.0000 (10.3125) lr 1.9686e-03 eta 0:04:14
epoch [7/50] batch [5/23] time 0.148 (0.438) data 0.000 (0.247) loss 3.5161 (3.5816) acc 12.5000 (13.7500) lr 1.9511e-03 eta 0:07:21
epoch [7/50] batch [10/23] time 0.146 (0.302) data 0.000 (0.124) loss 3.5730 (3.6102) acc 15.6250 (12.1875) lr 1.9511e-03 eta 0:05:03
epoch [7/50] batch [15/23] time 0.147 (0.251) data 0.000 (0.083) loss 3.6694 (3.6533) acc 9.3750 (11.6667) lr 1.9511e-03 eta 0:04:10
epoch [7/50] batch [20/23] time 0.150 (0.225) data 0.000 (0.062) loss 3.8203 (3.6771) acc 3.1250 (11.4062) lr 1.9511e-03 eta 0:03:43
epoch [8/50] batch [5/23] time 0.149 (0.483) data 0.000 (0.300) loss 3.8024 (3.7071) acc 3.1250 (8.7500) lr 1.9298e-03 eta 0:07:55
epoch [8/50] batch [10/23] time 0.150 (0.322) data 0.000 (0.150) loss 3.6351 (3.7022) acc 12.5000 (9.6875) lr 1.9298e-03 eta 0:05:15
epoch [8/50] batch [15/23] time 0.147 (0.264) data 0.000 (0.100) loss 3.6861 (3.6634) acc 9.3750 (11.2500) lr 1.9298e-03 eta 0:04:17
epoch [8/50] batch [20/23] time 0.145 (0.234) data 0.000 (0.075) loss 3.6134 (3.6420) acc 15.6250 (12.1875) lr 1.9298e-03 eta 0:03:47
epoch [9/50] batch [5/23] time 0.150 (0.456) data 0.000 (0.265) loss 3.6434 (3.5463) acc 9.3750 (17.5000) lr 1.9048e-03 eta 0:07:18
epoch [9/50] batch [10/23] time 0.150 (0.310) data 0.000 (0.133) loss 3.6336 (3.5757) acc 15.6250 (16.8750) lr 1.9048e-03 eta 0:04:56
epoch [9/50] batch [15/23] time 0.145 (0.255) data 0.000 (0.089) loss 3.5515 (3.5883) acc 21.8750 (16.2500) lr 1.9048e-03 eta 0:04:02
epoch [9/50] batch [20/23] time 0.148 (0.228) data 0.000 (0.066) loss 3.8900 (3.6269) acc 6.2500 (14.6875) lr 1.9048e-03 eta 0:03:35
epoch [10/50] batch [5/23] time 0.160 (0.473) data 0.000 (0.289) loss 3.7651 (3.6999) acc 3.1250 (9.3750) lr 1.8763e-03 eta 0:07:23
epoch [10/50] batch [10/23] time 0.164 (0.316) data 0.000 (0.145) loss 3.7870 (3.6684) acc 0.0000 (10.0000) lr 1.8763e-03 eta 0:04:55
epoch [10/50] batch [15/23] time 0.144 (0.260) data 0.000 (0.096) loss 3.6210 (3.6387) acc 15.6250 (11.0417) lr 1.8763e-03 eta 0:04:00
epoch [10/50] batch [20/23] time 0.144 (0.231) data 0.000 (0.072) loss 3.3956 (3.6095) acc 28.1250 (12.9688) lr 1.8763e-03 eta 0:03:33
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 1.126  alpha2: 0.569 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.39 & unmatched refined noisy rate: 0.81 <<<
epoch [11/50] batch [5/23] time 0.788 (1.037) data 0.000 (0.340) loss 2.7248 (2.7243) acc 70.8333 (66.7694) lr 1.8443e-03 eta 0:15:48
epoch [11/50] batch [10/23] time 0.146 (0.722) data 0.000 (0.170) loss 2.4198 (2.7142) acc 63.0000 (65.4954) lr 1.8443e-03 eta 0:10:56
epoch [11/50] batch [15/23] time 0.723 (0.641) data 0.000 (0.114) loss 2.8129 (2.7211) acc 67.6136 (63.5636) lr 1.8443e-03 eta 0:09:40
epoch [11/50] batch [20/23] time 0.855 (0.584) data 0.001 (0.085) loss 2.2590 (2.6770) acc 75.4902 (64.6276) lr 1.8443e-03 eta 0:08:45
>>> alpha1: 0.882  alpha2: 0.527 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.58 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.72 <<<
epoch [12/50] batch [5/23] time 0.132 (0.504) data 0.000 (0.260) loss 1.8491 (2.0014) acc 62.2093 (63.6908) lr 1.8090e-03 eta 0:07:29
epoch [12/50] batch [10/23] time 0.135 (0.379) data 0.000 (0.130) loss 1.9222 (2.0076) acc 57.1429 (60.5234) lr 1.8090e-03 eta 0:05:36
epoch [12/50] batch [15/23] time 0.131 (0.371) data 0.001 (0.087) loss 1.8914 (1.9467) acc 72.1591 (62.6465) lr 1.8090e-03 eta 0:05:27
epoch [12/50] batch [20/23] time 0.127 (0.337) data 0.000 (0.065) loss 1.6250 (1.9225) acc 61.9048 (62.3586) lr 1.8090e-03 eta 0:04:55
>>> alpha1: 0.824  alpha2: 0.440 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.60 --> matched refined noisy rate: 0.42 & unmatched refined noisy rate: 0.80 <<<
epoch [13/50] batch [5/23] time 0.172 (0.419) data 0.000 (0.258) loss 1.3610 (1.5792) acc 66.0000 (64.4468) lr 1.7705e-03 eta 0:06:04
epoch [13/50] batch [10/23] time 0.152 (0.286) data 0.000 (0.129) loss 1.4591 (1.5526) acc 66.0377 (63.9313) lr 1.7705e-03 eta 0:04:06
epoch [13/50] batch [15/23] time 0.144 (0.237) data 0.000 (0.086) loss 1.4819 (1.5695) acc 76.6304 (64.1434) lr 1.7705e-03 eta 0:03:23
epoch [13/50] batch [20/23] time 0.144 (0.213) data 0.000 (0.065) loss 1.1153 (1.5445) acc 66.6667 (64.1520) lr 1.7705e-03 eta 0:03:01
>>> alpha1: 0.819  alpha2: 0.386 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.81 <<<
epoch [14/50] batch [5/23] time 0.143 (0.409) data 0.000 (0.257) loss 1.4216 (1.4099) acc 64.2157 (65.4982) lr 1.7290e-03 eta 0:05:46
epoch [14/50] batch [10/23] time 0.142 (0.279) data 0.000 (0.129) loss 1.6780 (1.4726) acc 54.0000 (65.5352) lr 1.7290e-03 eta 0:03:54
epoch [14/50] batch [15/23] time 0.143 (0.232) data 0.000 (0.086) loss 1.7499 (1.5354) acc 53.4314 (63.6661) lr 1.7290e-03 eta 0:03:13
epoch [14/50] batch [20/23] time 0.144 (0.208) data 0.000 (0.064) loss 1.4043 (1.5077) acc 68.1373 (63.1956) lr 1.7290e-03 eta 0:02:53
>>> alpha1: 0.781  alpha2: 0.347 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.57 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.80 <<<
epoch [15/50] batch [5/23] time 0.141 (0.452) data 0.001 (0.301) loss 1.3725 (1.3930) acc 61.9792 (67.0880) lr 1.6845e-03 eta 0:06:12
epoch [15/50] batch [10/23] time 0.132 (0.296) data 0.000 (0.151) loss 1.5018 (1.4053) acc 60.3261 (65.7169) lr 1.6845e-03 eta 0:04:02
epoch [15/50] batch [15/23] time 0.142 (0.243) data 0.000 (0.100) loss 1.7304 (1.4386) acc 50.0000 (62.8991) lr 1.6845e-03 eta 0:03:17
epoch [15/50] batch [20/23] time 0.138 (0.256) data 0.000 (0.075) loss 1.3510 (1.3869) acc 67.7083 (65.0217) lr 1.6845e-03 eta 0:03:26
>>> alpha1: 0.665  alpha2: 0.244 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.57 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.80 <<<
epoch [16/50] batch [5/23] time 0.140 (0.424) data 0.000 (0.274) loss 0.9779 (1.2314) acc 69.5000 (69.9117) lr 1.6374e-03 eta 0:05:39
epoch [16/50] batch [10/23] time 0.137 (0.287) data 0.000 (0.137) loss 1.3614 (1.2811) acc 61.4583 (66.3733) lr 1.6374e-03 eta 0:03:47
epoch [16/50] batch [15/23] time 0.839 (0.286) data 0.000 (0.092) loss 1.1839 (1.2549) acc 69.9074 (66.4437) lr 1.6374e-03 eta 0:03:46
epoch [16/50] batch [20/23] time 0.152 (0.288) data 0.000 (0.069) loss 1.1831 (1.2331) acc 61.7924 (67.0410) lr 1.6374e-03 eta 0:03:46
>>> alpha1: 0.566  alpha2: 0.181 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.56 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.79 <<<
epoch [17/50] batch [5/23] time 0.147 (0.444) data 0.000 (0.291) loss 1.2085 (1.1120) acc 63.0000 (69.9212) lr 1.5878e-03 eta 0:05:44
epoch [17/50] batch [10/23] time 0.151 (0.304) data 0.000 (0.146) loss 0.9205 (1.0661) acc 68.2692 (69.8482) lr 1.5878e-03 eta 0:03:54
epoch [17/50] batch [15/23] time 0.134 (0.249) data 0.001 (0.097) loss 1.1715 (1.1037) acc 75.0000 (69.2899) lr 1.5878e-03 eta 0:03:10
epoch [17/50] batch [20/23] time 0.147 (0.222) data 0.000 (0.073) loss 0.9216 (1.1065) acc 72.1154 (68.5041) lr 1.5878e-03 eta 0:02:49
>>> alpha1: 0.530  alpha2: 0.160 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.55 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.76 <<<
epoch [18/50] batch [5/23] time 0.151 (0.477) data 0.000 (0.319) loss 1.0932 (1.3410) acc 77.5510 (65.2530) lr 1.5358e-03 eta 0:05:59
epoch [18/50] batch [10/23] time 0.135 (0.310) data 0.000 (0.160) loss 0.8926 (1.1499) acc 80.9783 (67.9308) lr 1.5358e-03 eta 0:03:52
epoch [18/50] batch [15/23] time 0.154 (0.257) data 0.000 (0.106) loss 0.9222 (1.0987) acc 69.4444 (70.0222) lr 1.5358e-03 eta 0:03:10
epoch [18/50] batch [20/23] time 0.140 (0.228) data 0.000 (0.080) loss 1.1052 (1.1114) acc 72.9167 (69.2482) lr 1.5358e-03 eta 0:02:48
>>> alpha1: 0.397  alpha2: 0.110 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.35 & unmatched refined noisy rate: 0.72 <<<
epoch [19/50] batch [5/23] time 0.212 (0.735) data 0.000 (0.560) loss 0.8508 (0.8480) acc 83.5000 (79.9481) lr 1.4818e-03 eta 0:08:57
epoch [19/50] batch [10/23] time 0.169 (0.451) data 0.000 (0.280) loss 0.7073 (0.8289) acc 80.8511 (78.7338) lr 1.4818e-03 eta 0:05:27
epoch [19/50] batch [15/23] time 0.164 (0.355) data 0.001 (0.187) loss 0.9137 (0.8416) acc 72.0588 (78.1517) lr 1.4818e-03 eta 0:04:15
epoch [19/50] batch [20/23] time 0.150 (0.304) data 0.000 (0.140) loss 1.0400 (0.9058) acc 64.6739 (75.6804) lr 1.4818e-03 eta 0:03:37
>>> alpha1: 0.353  alpha2: 0.089 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.36 & unmatched refined noisy rate: 0.70 <<<
epoch [20/50] batch [5/23] time 0.166 (0.413) data 0.000 (0.257) loss 0.6836 (0.7726) acc 81.2500 (78.8673) lr 1.4258e-03 eta 0:04:52
epoch [20/50] batch [10/23] time 0.141 (0.279) data 0.000 (0.128) loss 0.8495 (0.8188) acc 77.0833 (77.1578) lr 1.4258e-03 eta 0:03:16
epoch [20/50] batch [15/23] time 0.145 (0.236) data 0.000 (0.086) loss 0.8167 (0.8354) acc 66.5000 (75.9696) lr 1.4258e-03 eta 0:02:44
epoch [20/50] batch [20/23] time 0.145 (0.212) data 0.000 (0.064) loss 0.7219 (0.8773) acc 80.8824 (74.8509) lr 1.4258e-03 eta 0:02:26
>>> alpha1: 0.330  alpha2: 0.077 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.33 & unmatched refined noisy rate: 0.71 <<<
epoch [21/50] batch [5/23] time 0.153 (0.416) data 0.000 (0.268) loss 0.6837 (0.7721) acc 80.5000 (78.0145) lr 1.3681e-03 eta 0:04:45
epoch [21/50] batch [10/23] time 0.160 (0.281) data 0.000 (0.134) loss 0.9092 (0.7510) acc 76.5306 (78.3271) lr 1.3681e-03 eta 0:03:11
epoch [21/50] batch [15/23] time 0.130 (0.233) data 0.000 (0.090) loss 0.9100 (0.7733) acc 66.8605 (77.9018) lr 1.3681e-03 eta 0:02:37
epoch [21/50] batch [20/23] time 0.128 (0.208) data 0.000 (0.067) loss 0.8262 (0.8211) acc 78.9773 (75.6900) lr 1.3681e-03 eta 0:02:19
>>> alpha1: 0.308  alpha2: 0.065 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.54 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.79 <<<
epoch [22/50] batch [5/23] time 0.155 (0.484) data 0.000 (0.329) loss 0.7814 (0.7272) acc 76.3889 (81.7721) lr 1.3090e-03 eta 0:05:20
epoch [22/50] batch [10/23] time 0.146 (0.323) data 0.000 (0.165) loss 1.0581 (0.7544) acc 63.4615 (80.8534) lr 1.3090e-03 eta 0:03:32
epoch [22/50] batch [15/23] time 0.154 (0.264) data 0.000 (0.110) loss 0.8040 (0.7836) acc 82.4074 (79.3355) lr 1.3090e-03 eta 0:02:52
epoch [22/50] batch [20/23] time 0.138 (0.235) data 0.000 (0.083) loss 0.8041 (0.7861) acc 77.1277 (78.6315) lr 1.3090e-03 eta 0:02:32
>>> alpha1: 0.298  alpha2: 0.068 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.32 & unmatched refined noisy rate: 0.70 <<<
epoch [23/50] batch [5/23] time 0.167 (0.438) data 0.000 (0.289) loss 0.6145 (0.7206) acc 80.3191 (80.3200) lr 1.2487e-03 eta 0:04:39
epoch [23/50] batch [10/23] time 0.146 (0.291) data 0.001 (0.145) loss 0.8831 (0.7596) acc 78.5714 (79.2450) lr 1.2487e-03 eta 0:03:04
epoch [23/50] batch [15/23] time 0.132 (0.240) data 0.000 (0.097) loss 0.5263 (0.7432) acc 80.4348 (79.4405) lr 1.2487e-03 eta 0:02:31
epoch [23/50] batch [20/23] time 0.130 (0.215) data 0.000 (0.072) loss 1.1430 (0.7903) acc 69.4445 (77.8487) lr 1.2487e-03 eta 0:02:14
>>> alpha1: 0.284  alpha2: 0.067 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.78 <<<
epoch [24/50] batch [5/23] time 0.173 (0.721) data 0.002 (0.536) loss 0.9232 (0.7457) acc 71.2963 (78.2137) lr 1.1874e-03 eta 0:07:24
epoch [24/50] batch [10/23] time 0.186 (0.444) data 0.000 (0.268) loss 0.6162 (0.7290) acc 84.2593 (78.3327) lr 1.1874e-03 eta 0:04:31
epoch [24/50] batch [15/23] time 0.150 (0.346) data 0.000 (0.179) loss 0.8881 (0.7515) acc 72.9592 (77.3656) lr 1.1874e-03 eta 0:03:29
epoch [24/50] batch [20/23] time 0.170 (0.301) data 0.000 (0.134) loss 0.8350 (0.7305) acc 82.5893 (77.7880) lr 1.1874e-03 eta 0:03:00
>>> alpha1: 0.271  alpha2: 0.059 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.78 <<<
epoch [25/50] batch [5/23] time 0.172 (0.482) data 0.000 (0.328) loss 1.0033 (0.8493) acc 66.0714 (75.3011) lr 1.1253e-03 eta 0:04:45
epoch [25/50] batch [10/23] time 0.157 (0.321) data 0.000 (0.164) loss 0.9535 (0.8241) acc 75.9091 (76.4614) lr 1.1253e-03 eta 0:03:08
epoch [25/50] batch [15/23] time 0.154 (0.265) data 0.000 (0.110) loss 0.7208 (0.7813) acc 69.5455 (76.8705) lr 1.1253e-03 eta 0:02:34
epoch [25/50] batch [20/23] time 0.153 (0.235) data 0.000 (0.082) loss 0.7963 (0.7615) acc 68.8679 (77.0806) lr 1.1253e-03 eta 0:02:16
>>> alpha1: 0.251  alpha2: 0.056 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.76 <<<
epoch [26/50] batch [5/23] time 0.159 (0.438) data 0.000 (0.283) loss 0.8033 (0.7614) acc 84.5455 (78.5163) lr 1.0628e-03 eta 0:04:09
epoch [26/50] batch [10/23] time 0.152 (0.298) data 0.001 (0.142) loss 0.7547 (0.7331) acc 79.7170 (78.9070) lr 1.0628e-03 eta 0:02:48
epoch [26/50] batch [15/23] time 0.141 (0.247) data 0.000 (0.095) loss 0.7161 (0.7082) acc 78.5714 (79.4107) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [20/23] time 0.153 (0.260) data 0.000 (0.071) loss 0.6922 (0.6924) acc 84.9057 (80.3980) lr 1.0628e-03 eta 0:02:24
>>> alpha1: 0.242  alpha2: 0.050 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.76 <<<
epoch [27/50] batch [5/23] time 0.148 (0.466) data 0.000 (0.309) loss 0.9438 (0.7321) acc 71.5000 (79.4495) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [10/23] time 0.148 (0.311) data 0.000 (0.155) loss 0.6514 (0.7405) acc 89.2157 (78.6377) lr 1.0000e-03 eta 0:02:48
epoch [27/50] batch [15/23] time 0.150 (0.258) data 0.000 (0.103) loss 0.5842 (0.7155) acc 77.8302 (78.5641) lr 1.0000e-03 eta 0:02:18
epoch [27/50] batch [20/23] time 0.159 (0.231) data 0.001 (0.078) loss 0.4881 (0.6924) acc 89.2857 (79.8329) lr 1.0000e-03 eta 0:02:02
>>> alpha1: 0.230  alpha2: 0.036 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.51 --> matched refined noisy rate: 0.35 & unmatched refined noisy rate: 0.71 <<<
epoch [28/50] batch [5/23] time 0.169 (0.423) data 0.000 (0.265) loss 2.4650 (0.9890) acc 53.3654 (74.6244) lr 9.3721e-04 eta 0:03:41
epoch [28/50] batch [10/23] time 0.153 (0.285) data 0.000 (0.133) loss 0.6810 (0.9565) acc 83.0189 (76.5311) lr 9.3721e-04 eta 0:02:27
epoch [28/50] batch [15/23] time 0.143 (0.239) data 0.000 (0.089) loss 0.5115 (0.8561) acc 95.5000 (79.3577) lr 9.3721e-04 eta 0:02:02
epoch [28/50] batch [20/23] time 0.144 (0.214) data 0.000 (0.067) loss 0.7840 (0.7975) acc 77.5000 (79.7149) lr 9.3721e-04 eta 0:01:48
>>> alpha1: 0.224  alpha2: 0.035 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.75 <<<
epoch [29/50] batch [5/23] time 0.165 (0.482) data 0.000 (0.297) loss 0.5555 (0.5908) acc 84.6154 (85.0952) lr 8.7467e-04 eta 0:04:01
epoch [29/50] batch [10/23] time 0.146 (0.319) data 0.001 (0.149) loss 0.4688 (0.5756) acc 88.2353 (84.1084) lr 8.7467e-04 eta 0:02:38
epoch [29/50] batch [15/23] time 0.140 (0.261) data 0.000 (0.099) loss 0.6644 (0.6263) acc 76.5306 (82.1401) lr 8.7467e-04 eta 0:02:08
epoch [29/50] batch [20/23] time 0.136 (0.232) data 0.000 (0.075) loss 0.5686 (0.6262) acc 83.5106 (81.7763) lr 8.7467e-04 eta 0:01:52
>>> alpha1: 0.215  alpha2: 0.031 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.76 <<<
epoch [30/50] batch [5/23] time 0.147 (0.435) data 0.000 (0.284) loss 0.5471 (0.6357) acc 81.2500 (82.1805) lr 8.1262e-04 eta 0:03:28
epoch [30/50] batch [10/23] time 0.170 (0.300) data 0.000 (0.144) loss 0.4858 (0.6270) acc 79.8077 (80.0625) lr 8.1262e-04 eta 0:02:21
epoch [30/50] batch [15/23] time 0.142 (0.249) data 0.000 (0.096) loss 0.7723 (0.6212) acc 72.4490 (79.9547) lr 8.1262e-04 eta 0:01:56
epoch [30/50] batch [20/23] time 0.137 (0.222) data 0.000 (0.072) loss 0.8021 (0.6569) acc 84.5745 (79.6026) lr 8.1262e-04 eta 0:01:42
>>> alpha1: 0.213  alpha2: 0.034 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.39 & unmatched refined noisy rate: 0.75 <<<
epoch [31/50] batch [5/23] time 0.165 (0.449) data 0.000 (0.288) loss 0.4651 (0.5521) acc 87.9808 (82.9460) lr 7.5131e-04 eta 0:03:24
epoch [31/50] batch [10/23] time 0.153 (0.300) data 0.000 (0.144) loss 0.8300 (0.6339) acc 76.4423 (82.0764) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [15/23] time 0.139 (0.249) data 0.001 (0.096) loss 0.6403 (0.6140) acc 82.2917 (82.3093) lr 7.5131e-04 eta 0:01:50
epoch [31/50] batch [20/23] time 0.140 (0.224) data 0.001 (0.072) loss 0.6585 (0.6090) acc 84.8958 (83.1973) lr 7.5131e-04 eta 0:01:38
>>> alpha1: 0.207  alpha2: 0.038 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.75 <<<
epoch [32/50] batch [5/23] time 0.151 (0.482) data 0.000 (0.315) loss 0.9878 (0.6807) acc 73.0000 (81.9188) lr 6.9098e-04 eta 0:03:28
epoch [32/50] batch [10/23] time 0.145 (0.317) data 0.000 (0.158) loss 0.4374 (0.6040) acc 86.2745 (82.9535) lr 6.9098e-04 eta 0:02:15
epoch [32/50] batch [15/23] time 0.145 (0.262) data 0.000 (0.105) loss 0.4689 (0.5903) acc 76.5625 (82.8152) lr 6.9098e-04 eta 0:01:50
epoch [32/50] batch [20/23] time 0.139 (0.232) data 0.000 (0.079) loss 0.6653 (0.5914) acc 80.3191 (82.5192) lr 6.9098e-04 eta 0:01:36
>>> alpha1: 0.204  alpha2: 0.046 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.36 & unmatched refined noisy rate: 0.73 <<<
epoch [33/50] batch [5/23] time 0.162 (0.465) data 0.000 (0.300) loss 0.6408 (0.5348) acc 74.5098 (84.1575) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [10/23] time 0.159 (0.307) data 0.000 (0.150) loss 0.6060 (0.5933) acc 79.4118 (82.6273) lr 6.3188e-04 eta 0:02:04
epoch [33/50] batch [15/23] time 0.148 (0.252) data 0.000 (0.100) loss 0.6237 (0.6088) acc 85.0962 (83.0636) lr 6.3188e-04 eta 0:01:40
epoch [33/50] batch [20/23] time 0.154 (0.227) data 0.000 (0.075) loss 0.5052 (0.5898) acc 80.4545 (83.2971) lr 6.3188e-04 eta 0:01:29
>>> alpha1: 0.199  alpha2: 0.043 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.73 <<<
epoch [34/50] batch [5/23] time 0.211 (0.428) data 0.000 (0.264) loss 0.3151 (0.5424) acc 87.7358 (85.4012) lr 5.7422e-04 eta 0:02:45
epoch [34/50] batch [10/23] time 0.149 (0.296) data 0.000 (0.132) loss 0.6043 (0.5060) acc 79.2553 (85.1596) lr 5.7422e-04 eta 0:01:52
epoch [34/50] batch [15/23] time 0.132 (0.248) data 0.000 (0.088) loss 0.5631 (0.5518) acc 78.8043 (84.0743) lr 5.7422e-04 eta 0:01:33
epoch [34/50] batch [20/23] time 0.138 (0.221) data 0.000 (0.066) loss 0.7184 (0.5479) acc 81.2500 (84.2106) lr 5.7422e-04 eta 0:01:22
>>> alpha1: 0.196  alpha2: 0.045 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.74 <<<
epoch [35/50] batch [5/23] time 0.153 (0.433) data 0.001 (0.278) loss 0.7727 (0.6069) acc 77.0000 (83.5000) lr 5.1825e-04 eta 0:02:37
epoch [35/50] batch [10/23] time 0.148 (0.293) data 0.000 (0.139) loss 0.4801 (0.5791) acc 87.2549 (83.3635) lr 5.1825e-04 eta 0:01:45
epoch [35/50] batch [15/23] time 0.148 (0.243) data 0.000 (0.093) loss 0.5075 (0.5891) acc 84.6154 (83.1910) lr 5.1825e-04 eta 0:01:25
epoch [35/50] batch [20/23] time 0.144 (0.218) data 0.000 (0.070) loss 0.3497 (0.5730) acc 82.5000 (83.0822) lr 5.1825e-04 eta 0:01:15
>>> alpha1: 0.195  alpha2: 0.042 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.73 <<<
epoch [36/50] batch [5/23] time 0.153 (0.460) data 0.000 (0.307) loss 0.5763 (0.5164) acc 88.8298 (83.3132) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [10/23] time 0.156 (0.307) data 0.000 (0.154) loss 0.5651 (0.5370) acc 90.7407 (84.4134) lr 4.6417e-04 eta 0:01:42
epoch [36/50] batch [15/23] time 0.143 (0.253) data 0.000 (0.103) loss 0.7480 (0.6547) acc 84.5000 (83.6637) lr 4.6417e-04 eta 0:01:23
epoch [36/50] batch [20/23] time 0.136 (0.226) data 0.001 (0.077) loss 0.5107 (0.6381) acc 88.8889 (83.5107) lr 4.6417e-04 eta 0:01:13
>>> alpha1: 0.193  alpha2: 0.039 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.69 <<<
epoch [37/50] batch [5/23] time 0.142 (0.459) data 0.000 (0.309) loss 0.6133 (0.5092) acc 75.5208 (84.2330) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [10/23] time 0.141 (0.300) data 0.000 (0.155) loss 0.5208 (0.5374) acc 86.2245 (83.5924) lr 4.1221e-04 eta 0:01:33
epoch [37/50] batch [15/23] time 0.154 (0.248) data 0.000 (0.103) loss 0.5311 (0.5338) acc 86.1111 (83.6075) lr 4.1221e-04 eta 0:01:16
epoch [37/50] batch [20/23] time 0.141 (0.221) data 0.000 (0.077) loss 0.6272 (0.5414) acc 80.6122 (83.5186) lr 4.1221e-04 eta 0:01:06
>>> alpha1: 0.190  alpha2: 0.034 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.71 <<<
epoch [38/50] batch [5/23] time 0.143 (0.457) data 0.000 (0.309) loss 0.4318 (0.5300) acc 89.2857 (87.5557) lr 3.6258e-04 eta 0:02:14
epoch [38/50] batch [10/23] time 0.151 (0.386) data 0.000 (0.155) loss 0.5135 (0.5162) acc 90.7407 (87.1596) lr 3.6258e-04 eta 0:01:51
epoch [38/50] batch [15/23] time 0.136 (0.304) data 0.001 (0.103) loss 0.5472 (0.5341) acc 86.9792 (87.4729) lr 3.6258e-04 eta 0:01:26
epoch [38/50] batch [20/23] time 0.140 (0.264) data 0.000 (0.078) loss 0.4326 (0.5181) acc 84.5000 (86.8121) lr 3.6258e-04 eta 0:01:13
>>> alpha1: 0.187  alpha2: 0.037 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.74 <<<
epoch [39/50] batch [5/23] time 0.162 (0.447) data 0.001 (0.292) loss 0.4463 (0.4495) acc 87.7451 (89.9798) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [10/23] time 0.150 (0.303) data 0.001 (0.148) loss 0.6352 (0.5218) acc 82.5000 (86.7210) lr 3.1545e-04 eta 0:01:20
epoch [39/50] batch [15/23] time 0.144 (0.250) data 0.001 (0.099) loss 0.7021 (0.5282) acc 83.1633 (86.3481) lr 3.1545e-04 eta 0:01:05
epoch [39/50] batch [20/23] time 0.152 (0.224) data 0.000 (0.074) loss 0.6134 (0.5440) acc 79.1667 (84.7593) lr 3.1545e-04 eta 0:00:57
>>> alpha1: 0.185  alpha2: 0.039 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.36 & unmatched refined noisy rate: 0.72 <<<
epoch [40/50] batch [5/23] time 0.159 (0.452) data 0.000 (0.299) loss 0.4974 (0.5850) acc 88.2353 (83.9592) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [10/23] time 0.142 (0.299) data 0.000 (0.150) loss 0.4352 (0.5621) acc 92.5532 (85.3871) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [15/23] time 0.152 (0.248) data 0.000 (0.100) loss 0.3596 (0.5265) acc 89.1509 (86.0098) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [20/23] time 0.141 (0.221) data 0.000 (0.075) loss 0.6303 (0.5231) acc 87.2549 (85.3082) lr 2.7103e-04 eta 0:00:51
>>> alpha1: 0.179  alpha2: 0.039 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.37 & unmatched refined noisy rate: 0.71 <<<
epoch [41/50] batch [5/23] time 0.161 (0.421) data 0.000 (0.270) loss 0.7040 (0.5105) acc 74.0000 (84.8700) lr 2.2949e-04 eta 0:01:34
epoch [41/50] batch [10/23] time 0.140 (0.287) data 0.000 (0.135) loss 0.6043 (0.5086) acc 78.5000 (84.0973) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [15/23] time 0.141 (0.237) data 0.001 (0.090) loss 0.4448 (0.6108) acc 81.5000 (83.3325) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [20/23] time 0.139 (0.213) data 0.000 (0.068) loss 0.6565 (0.5860) acc 83.3333 (84.1305) lr 2.2949e-04 eta 0:00:44
>>> alpha1: 0.176  alpha2: 0.042 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.36 & unmatched refined noisy rate: 0.71 <<<
epoch [42/50] batch [5/23] time 0.148 (0.427) data 0.000 (0.273) loss 0.5223 (0.4804) acc 91.6667 (88.8306) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [10/23] time 0.144 (0.288) data 0.000 (0.137) loss 0.4099 (0.5033) acc 94.3878 (87.1303) lr 1.9098e-04 eta 0:00:56
epoch [42/50] batch [15/23] time 0.146 (0.239) data 0.000 (0.091) loss 0.4618 (0.4934) acc 85.5769 (87.2377) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [20/23] time 0.144 (0.215) data 0.000 (0.069) loss 0.3735 (0.4927) acc 81.5000 (86.7703) lr 1.9098e-04 eta 0:00:40
>>> alpha1: 0.175  alpha2: 0.040 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.75 <<<
epoch [43/50] batch [5/23] time 0.152 (0.444) data 0.000 (0.286) loss 0.4563 (0.5681) acc 79.4118 (82.3393) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [10/23] time 0.146 (0.299) data 0.000 (0.143) loss 0.4164 (0.5344) acc 92.3077 (85.0370) lr 1.5567e-04 eta 0:00:51
epoch [43/50] batch [15/23] time 0.153 (0.249) data 0.000 (0.096) loss 0.6467 (0.5231) acc 82.8704 (85.4935) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [20/23] time 0.147 (0.225) data 0.000 (0.072) loss 0.4678 (0.4988) acc 87.9808 (86.4586) lr 1.5567e-04 eta 0:00:36
>>> alpha1: 0.175  alpha2: 0.042 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.76 <<<
epoch [44/50] batch [5/23] time 0.145 (0.589) data 0.000 (0.271) loss 0.6684 (0.4721) acc 76.5000 (83.9568) lr 1.2369e-04 eta 0:01:31
epoch [44/50] batch [10/23] time 0.163 (0.373) data 0.000 (0.136) loss 0.3799 (0.5606) acc 89.2241 (84.4429) lr 1.2369e-04 eta 0:00:56
epoch [44/50] batch [15/23] time 0.159 (0.300) data 0.000 (0.091) loss 0.3328 (0.5184) acc 94.0909 (85.2433) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [20/23] time 0.159 (0.263) data 0.000 (0.068) loss 0.5140 (0.5284) acc 87.9464 (85.0676) lr 1.2369e-04 eta 0:00:37
>>> alpha1: 0.174  alpha2: 0.042 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.76 <<<
epoch [45/50] batch [5/23] time 0.175 (0.468) data 0.000 (0.312) loss 0.6103 (0.5360) acc 85.8491 (84.7402) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [10/23] time 0.161 (0.311) data 0.000 (0.156) loss 0.5411 (0.5141) acc 88.1818 (85.5057) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [15/23] time 0.159 (0.259) data 0.001 (0.104) loss 0.5091 (0.5052) acc 87.7273 (86.1848) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [20/23] time 0.155 (0.233) data 0.000 (0.078) loss 0.3525 (0.5019) acc 94.9074 (86.3253) lr 9.5173e-05 eta 0:00:27
>>> alpha1: 0.174  alpha2: 0.045 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.77 <<<
epoch [46/50] batch [5/23] time 0.148 (0.468) data 0.000 (0.302) loss 0.4064 (0.4644) acc 87.9808 (85.2069) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [10/23] time 0.143 (0.316) data 0.000 (0.153) loss 0.6611 (0.4756) acc 79.9020 (86.5785) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [15/23] time 0.155 (0.261) data 0.000 (0.102) loss 0.6024 (0.4879) acc 85.1852 (86.7989) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [20/23] time 0.153 (0.234) data 0.000 (0.077) loss 0.6021 (0.4921) acc 83.4906 (85.9431) lr 7.0224e-05 eta 0:00:22
>>> alpha1: 0.174  alpha2: 0.044 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.76 <<<
epoch [47/50] batch [5/23] time 0.160 (0.423) data 0.001 (0.258) loss 0.5705 (0.4939) acc 80.5556 (85.8629) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [10/23] time 0.156 (0.290) data 0.000 (0.129) loss 0.5282 (0.5184) acc 85.8491 (85.8376) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/23] time 0.157 (0.242) data 0.000 (0.086) loss 0.3812 (0.5254) acc 83.0189 (85.4672) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/23] time 0.149 (0.220) data 0.000 (0.065) loss 0.4722 (0.5191) acc 86.5385 (86.0731) lr 4.8943e-05 eta 0:00:15
>>> alpha1: 0.173  alpha2: 0.048 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.76 <<<
epoch [48/50] batch [5/23] time 0.172 (0.410) data 0.001 (0.246) loss 0.4797 (0.4355) acc 90.2778 (89.4477) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [10/23] time 0.169 (0.286) data 0.000 (0.123) loss 0.4696 (0.4272) acc 90.2778 (90.6275) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [15/23] time 0.156 (0.242) data 0.001 (0.082) loss 0.5915 (0.4517) acc 78.1818 (88.6855) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/23] time 0.156 (0.221) data 0.000 (0.062) loss 0.6389 (0.4814) acc 81.4815 (87.3345) lr 3.1417e-05 eta 0:00:10
>>> alpha1: 0.172  alpha2: 0.044 <<<
>>> noisy rate: 0.74 --> refined noisy rate: 0.52 --> matched refined noisy rate: 0.41 & unmatched refined noisy rate: 0.76 <<<
epoch [49/50] batch [5/23] time 0.175 (0.432) data 0.001 (0.262) loss 0.4339 (0.4716) acc 83.6364 (85.6052) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [10/23] time 0.152 (0.294) data 0.000 (0.131) loss 0.3514 (0.4782) acc 88.4259 (86.6154) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/23] time 0.165 (0.246) data 0.000 (0.088) loss 0.6053 (0.4883) acc 83.7719 (86.2327) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/23] time 0.157 (0.223) data 0.000 (0.066) loss 0.6587 (0.4893) acc 78.7037 (85.8278) lr 1.7713e-05 eta 0:00:05
>>> alpha1: 0.173  alpha2: 0.043 <<<
>>> noisy rate: 0.73 --> refined noisy rate: 0.51 --> matched refined noisy rate: 0.40 & unmatched refined noisy rate: 0.74 <<<
epoch [50/50] batch [5/23] time 0.171 (0.511) data 0.001 (0.347) loss 0.3586 (0.4046) acc 87.5000 (87.7698) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [10/23] time 0.149 (0.335) data 0.000 (0.174) loss 0.6951 (0.4790) acc 73.0769 (85.8788) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [15/23] time 0.161 (0.275) data 0.001 (0.116) loss 0.5623 (0.4379) acc 87.9464 (87.4250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/23] time 0.155 (0.244) data 0.000 (0.087) loss 0.4684 (0.4559) acc 87.0536 (86.7787) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/dtd_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.59, 0.58, 0.6, 0.59, 0.57, 0.57, 0.56, 0.55, 0.53, 0.52, 0.52, 0.54, 0.52, 0.52, 0.53, 0.52, 0.52, 0.51, 0.53, 0.53, 0.53, 0.52, 0.52, 0.53, 0.52, 0.53, 0.52, 0.52, 0.53, 0.52, 0.53, 0.52, 0.52, 0.52, 0.52, 0.53, 0.53, 0.52, 0.52, 0.51]
* matched noise rate: [0.39, 0.28, 0.42, 0.4, 0.38, 0.41, 0.38, 0.38, 0.35, 0.36, 0.33, 0.38, 0.32, 0.38, 0.38, 0.38, 0.38, 0.35, 0.37, 0.37, 0.39, 0.38, 0.36, 0.37, 0.37, 0.37, 0.37, 0.38, 0.38, 0.36, 0.37, 0.36, 0.4, 0.4, 0.4, 0.4, 0.41, 0.41, 0.41, 0.4]
* unmatched noise rate: [0.81, 0.72, 0.8, 0.81, 0.8, 0.8, 0.79, 0.76, 0.72, 0.7, 0.71, 0.79, 0.7, 0.78, 0.78, 0.76, 0.76, 0.71, 0.75, 0.76, 0.75, 0.75, 0.73, 0.73, 0.74, 0.73, 0.69, 0.71, 0.74, 0.72, 0.71, 0.71, 0.75, 0.76, 0.76, 0.77, 0.76, 0.76, 0.76, 0.74]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:02<00:41,  2.62s/it] 18%|█▊        | 3/17 [00:02<00:10,  1.37it/s] 29%|██▉       | 5/17 [00:02<00:04,  2.56it/s] 41%|████      | 7/17 [00:03<00:02,  3.70it/s] 53%|█████▎    | 9/17 [00:03<00:01,  5.14it/s] 65%|██████▍   | 11/17 [00:03<00:00,  6.65it/s] 76%|███████▋  | 13/17 [00:03<00:00,  8.11it/s] 88%|████████▊ | 15/17 [00:03<00:00,  9.45it/s]100%|██████████| 17/17 [00:04<00:00,  5.14it/s]100%|██████████| 17/17 [00:04<00:00,  3.66it/s]
=> result
* total: 1,692
* correct: 665
* accuracy: 39.3%
* error: 60.7%
* macro_f1: 34.4%
=> per-class result
* class: 0 (banded)	total: 36	correct: 3	acc: 8.3%
* class: 1 (blotchy)	total: 36	correct: 0	acc: 0.0%
* class: 2 (braided)	total: 36	correct: 7	acc: 19.4%
* class: 3 (bubbly)	total: 36	correct: 32	acc: 88.9%
* class: 4 (bumpy)	total: 36	correct: 0	acc: 0.0%
* class: 5 (chequered)	total: 36	correct: 33	acc: 91.7%
* class: 6 (cobwebbed)	total: 36	correct: 28	acc: 77.8%
* class: 7 (cracked)	total: 36	correct: 23	acc: 63.9%
* class: 8 (crosshatched)	total: 36	correct: 10	acc: 27.8%
* class: 9 (crystalline)	total: 36	correct: 27	acc: 75.0%
* class: 10 (dotted)	total: 36	correct: 11	acc: 30.6%
* class: 11 (fibrous)	total: 36	correct: 8	acc: 22.2%
* class: 12 (flecked)	total: 36	correct: 0	acc: 0.0%
* class: 13 (freckled)	total: 36	correct: 24	acc: 66.7%
* class: 14 (frilly)	total: 36	correct: 22	acc: 61.1%
* class: 15 (gauzy)	total: 36	correct: 9	acc: 25.0%
* class: 16 (grid)	total: 36	correct: 1	acc: 2.8%
* class: 17 (grooved)	total: 36	correct: 10	acc: 27.8%
* class: 18 (honeycombed)	total: 36	correct: 11	acc: 30.6%
* class: 19 (interlaced)	total: 36	correct: 0	acc: 0.0%
* class: 20 (knitted)	total: 36	correct: 33	acc: 91.7%
* class: 21 (lacelike)	total: 36	correct: 0	acc: 0.0%
* class: 22 (lined)	total: 36	correct: 0	acc: 0.0%
* class: 23 (marbled)	total: 36	correct: 11	acc: 30.6%
* class: 24 (matted)	total: 36	correct: 0	acc: 0.0%
* class: 25 (meshed)	total: 36	correct: 28	acc: 77.8%
* class: 26 (paisley)	total: 36	correct: 36	acc: 100.0%
* class: 27 (perforated)	total: 36	correct: 3	acc: 8.3%
* class: 28 (pitted)	total: 36	correct: 0	acc: 0.0%
* class: 29 (pleated)	total: 36	correct: 12	acc: 33.3%
* class: 30 (polka-dotted)	total: 36	correct: 23	acc: 63.9%
* class: 31 (porous)	total: 36	correct: 6	acc: 16.7%
* class: 32 (potholed)	total: 36	correct: 22	acc: 61.1%
* class: 33 (scaly)	total: 36	correct: 10	acc: 27.8%
* class: 34 (smeared)	total: 36	correct: 10	acc: 27.8%
* class: 35 (spiralled)	total: 36	correct: 18	acc: 50.0%
* class: 36 (sprinkled)	total: 36	correct: 17	acc: 47.2%
* class: 37 (stained)	total: 36	correct: 26	acc: 72.2%
* class: 38 (stratified)	total: 36	correct: 25	acc: 69.4%
* class: 39 (striped)	total: 36	correct: 34	acc: 94.4%
* class: 40 (studded)	total: 36	correct: 28	acc: 77.8%
* class: 41 (swirly)	total: 36	correct: 3	acc: 8.3%
* class: 42 (veined)	total: 36	correct: 15	acc: 41.7%
* class: 43 (waffled)	total: 36	correct: 23	acc: 63.9%
* class: 44 (woven)	total: 36	correct: 5	acc: 13.9%
* class: 45 (wrinkled)	total: 36	correct: 18	acc: 50.0%
* class: 46 (zigzagged)	total: 36	correct: 0	acc: 0.0%
* average: 39.3%
Elapsed: 0:13:14
scripts/parse.sh: line 10: output/ablation/dtd_MixDivideWarmupAugmentationBLIP/parsing_results.log: No such file or directory
