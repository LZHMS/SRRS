nohup: ignoring input
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_0-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.674) data 0.000 (0.254) loss 1.0703 (1.0852) acc 10.1562 (17.0312) lr 2.0000e-03 eta 0:05:33
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [41/160] noisy rate: 0.00 --> 0.12 --> 0.00 <<<
epoch [2/100] batch [5/5] time 0.196 (0.350) data 0.000 (0.176) loss 1.9186 (2.0790) acc 16.6667 (23.7500) lr 1.9995e-03 eta 0:02:51
epoch [3/100] batch [5/5] time 0.042 (0.342) data 0.000 (0.162) loss 1.8732 (1.8852) acc 38.8889 (34.5556) lr 1.9980e-03 eta 0:02:45
epoch [4/100] batch [5/5] time 0.223 (0.255) data 0.000 (0.169) loss 1.9096 (1.8066) acc 29.1667 (32.6111) lr 1.9956e-03 eta 0:02:02
epoch [5/100] batch [5/5] time 0.036 (0.272) data 0.000 (0.174) loss 1.8800 (1.7091) acc 28.5714 (46.8178) lr 1.9921e-03 eta 0:02:09
epoch [6/100] batch [5/5] time 0.354 (0.322) data 0.000 (0.183) loss 1.5251 (1.8101) acc 67.8571 (39.2262) lr 1.9877e-03 eta 0:02:31
>>> samples [53/160] noisy rate: 0.00 --> 0.36 --> 0.04 <<<
epoch [7/100] batch [5/5] time 0.038 (0.280) data 0.000 (0.189) loss 1.6066 (1.6891) acc 56.8182 (48.2828) lr 1.9823e-03 eta 0:02:10
epoch [8/100] batch [5/5] time 0.311 (0.277) data 0.000 (0.180) loss 1.7252 (1.5565) acc 29.1667 (49.8999) lr 1.9759e-03 eta 0:02:07
epoch [9/100] batch [5/5] time 0.039 (0.229) data 0.000 (0.181) loss 1.5478 (1.4831) acc 50.0000 (57.4658) lr 1.9686e-03 eta 0:01:44
epoch [10/100] batch [5/5] time 0.032 (0.199) data 0.000 (0.159) loss 1.3900 (1.3796) acc 58.3333 (59.5772) lr 1.9603e-03 eta 0:01:29
epoch [11/100] batch [5/5] time 0.042 (0.207) data 0.000 (0.159) loss 1.2616 (1.3889) acc 65.9091 (64.5000) lr 1.9511e-03 eta 0:01:32
>>> samples [53/160] noisy rate: 0.00 --> 0.26 --> 0.04 <<<
epoch [12/100] batch [5/5] time 0.045 (0.201) data 0.000 (0.151) loss 1.2383 (1.3072) acc 72.9167 (68.5768) lr 1.9409e-03 eta 0:01:28
epoch [13/100] batch [5/5] time 0.039 (0.201) data 0.000 (0.152) loss 1.3595 (1.2749) acc 75.0000 (67.8771) lr 1.9298e-03 eta 0:01:27
epoch [14/100] batch [5/5] time 0.035 (0.222) data 0.000 (0.175) loss 1.0956 (1.2115) acc 69.4444 (70.8269) lr 1.9178e-03 eta 0:01:35
epoch [15/100] batch [5/5] time 0.040 (0.207) data 0.000 (0.162) loss 1.0617 (1.2042) acc 80.0000 (74.8889) lr 1.9048e-03 eta 0:01:28
epoch [16/100] batch [5/5] time 0.039 (0.246) data 0.000 (0.207) loss 1.0847 (1.1480) acc 75.0000 (75.4654) lr 1.8910e-03 eta 0:01:43
>>> samples [55/160] noisy rate: 0.00 --> 0.31 --> 0.07 <<<
epoch [17/100] batch [5/5] time 0.038 (0.301) data 0.000 (0.188) loss 1.2323 (1.1315) acc 69.4444 (76.6932) lr 1.8763e-03 eta 0:02:04
epoch [18/100] batch [5/5] time 0.043 (0.251) data 0.000 (0.198) loss 1.1148 (1.1151) acc 81.2500 (77.7045) lr 1.8607e-03 eta 0:01:42
epoch [19/100] batch [5/5] time 0.039 (0.213) data 0.000 (0.161) loss 1.0248 (1.0243) acc 84.3750 (80.8045) lr 1.8443e-03 eta 0:01:26
epoch [20/100] batch [5/5] time 0.040 (0.219) data 0.001 (0.173) loss 1.1507 (1.0604) acc 75.0000 (80.1667) lr 1.8271e-03 eta 0:01:27
epoch [21/100] batch [5/5] time 0.040 (0.209) data 0.000 (0.161) loss 1.1140 (1.0902) acc 72.9167 (76.0417) lr 1.8090e-03 eta 0:01:22
>>> samples [55/160] noisy rate: 0.00 --> 0.29 --> 0.07 <<<
epoch [22/100] batch [5/5] time 0.043 (0.218) data 0.000 (0.169) loss 1.3179 (1.0076) acc 73.0769 (80.7265) lr 1.7902e-03 eta 0:01:24
epoch [23/100] batch [5/5] time 0.045 (0.219) data 0.000 (0.169) loss 1.1772 (1.0460) acc 77.0833 (84.3236) lr 1.7705e-03 eta 0:01:24
epoch [24/100] batch [5/5] time 0.052 (0.225) data 0.000 (0.174) loss 1.0384 (1.0568) acc 78.3333 (82.1187) lr 1.7501e-03 eta 0:01:25
epoch [25/100] batch [5/5] time 0.035 (0.208) data 0.001 (0.161) loss 0.7749 (1.0096) acc 86.1111 (80.5556) lr 1.7290e-03 eta 0:01:17
epoch [26/100] batch [5/5] time 0.035 (0.190) data 0.000 (0.144) loss 1.0790 (1.0240) acc 80.0000 (81.8712) lr 1.7071e-03 eta 0:01:10
>>> samples [66/160] noisy rate: 0.00 --> 0.18 --> 0.11 <<<
epoch [27/100] batch [5/5] time 0.037 (0.274) data 0.000 (0.154) loss 0.6961 (0.9575) acc 88.6364 (84.0424) lr 1.6845e-03 eta 0:01:39
epoch [28/100] batch [5/5] time 0.048 (0.224) data 0.000 (0.172) loss 0.8059 (0.9287) acc 88.3333 (86.0064) lr 1.6613e-03 eta 0:01:20
epoch [29/100] batch [5/5] time 0.050 (0.212) data 0.000 (0.160) loss 0.8188 (0.9248) acc 93.7500 (86.3799) lr 1.6374e-03 eta 0:01:15
epoch [30/100] batch [5/5] time 0.048 (0.184) data 0.000 (0.135) loss 1.2524 (0.9518) acc 66.6667 (80.4167) lr 1.6129e-03 eta 0:01:04
epoch [31/100] batch [5/5] time 0.047 (0.207) data 0.000 (0.155) loss 0.8637 (0.9425) acc 91.0714 (81.1288) lr 1.5878e-03 eta 0:01:11
>>> samples [68/160] noisy rate: 0.00 --> 0.20 --> 0.10 <<<
epoch [32/100] batch [5/5] time 0.394 (0.287) data 0.000 (0.161) loss 0.8498 (0.9614) acc 88.1579 (86.5027) lr 1.5621e-03 eta 0:01:37
epoch [33/100] batch [5/5] time 0.059 (0.202) data 0.000 (0.148) loss 0.8998 (0.9100) acc 83.8235 (84.4809) lr 1.5358e-03 eta 0:01:07
epoch [34/100] batch [5/5] time 0.052 (0.268) data 0.000 (0.212) loss 0.9300 (0.8778) acc 87.5000 (86.1111) lr 1.5090e-03 eta 0:01:28
epoch [35/100] batch [5/5] time 0.043 (0.232) data 0.000 (0.177) loss 0.8916 (0.8784) acc 96.4286 (87.1502) lr 1.4818e-03 eta 0:01:15
epoch [36/100] batch [5/5] time 0.041 (0.282) data 0.000 (0.163) loss 1.1355 (0.8842) acc 76.9231 (85.6449) lr 1.4540e-03 eta 0:01:30
>>> samples [70/160] noisy rate: 0.00 --> 0.21 --> 0.10 <<<
epoch [37/100] batch [5/5] time 0.047 (0.248) data 0.000 (0.197) loss 0.8716 (0.8913) acc 75.0000 (86.3445) lr 1.4258e-03 eta 0:01:18
epoch [38/100] batch [5/5] time 0.049 (0.201) data 0.000 (0.142) loss 0.9189 (0.9001) acc 89.2857 (87.0858) lr 1.3971e-03 eta 0:01:02
epoch [39/100] batch [5/5] time 0.051 (0.220) data 0.000 (0.165) loss 1.0729 (0.9049) acc 76.7857 (83.8390) lr 1.3681e-03 eta 0:01:07
epoch [40/100] batch [5/5] time 0.057 (0.221) data 0.000 (0.161) loss 0.7781 (0.9224) acc 86.7647 (82.3163) lr 1.3387e-03 eta 0:01:06
epoch [41/100] batch [5/5] time 0.040 (0.281) data 0.000 (0.155) loss 0.8613 (0.9051) acc 95.8333 (86.8571) lr 1.3090e-03 eta 0:01:22
>>> samples [71/160] noisy rate: 0.00 --> 0.21 --> 0.10 <<<
epoch [42/100] batch [5/5] time 0.052 (0.204) data 0.000 (0.151) loss 0.8950 (0.8844) acc 84.6154 (85.2214) lr 1.2790e-03 eta 0:00:59
epoch [43/100] batch [5/5] time 0.049 (0.211) data 0.000 (0.153) loss 0.8045 (0.8751) acc 85.7143 (86.4194) lr 1.2487e-03 eta 0:01:00
epoch [44/100] batch [5/5] time 0.045 (0.209) data 0.000 (0.152) loss 1.0648 (0.8710) acc 65.9091 (85.8303) lr 1.2181e-03 eta 0:00:58
epoch [45/100] batch [5/5] time 0.047 (0.251) data 0.000 (0.195) loss 0.8647 (0.8394) acc 84.6154 (90.0897) lr 1.1874e-03 eta 0:01:08
epoch [46/100] batch [5/5] time 0.042 (0.209) data 0.000 (0.154) loss 0.6169 (0.8464) acc 86.3636 (86.2678) lr 1.1564e-03 eta 0:00:56
>>> samples [76/160] noisy rate: 0.00 --> 0.22 --> 0.11 <<<
epoch [47/100] batch [5/5] time 0.058 (0.252) data 0.001 (0.192) loss 0.9514 (0.8973) acc 91.1765 (84.0875) lr 1.1253e-03 eta 0:01:06
epoch [48/100] batch [5/5] time 0.048 (0.203) data 0.000 (0.143) loss 0.8596 (0.8516) acc 88.3333 (85.7619) lr 1.0941e-03 eta 0:00:52
epoch [49/100] batch [5/5] time 0.061 (0.206) data 0.000 (0.152) loss 0.8945 (0.9130) acc 81.5789 (85.3741) lr 1.0628e-03 eta 0:00:52
epoch [50/100] batch [5/5] time 0.049 (0.207) data 0.000 (0.153) loss 0.9224 (0.8435) acc 85.9375 (87.2302) lr 1.0314e-03 eta 0:00:51
epoch [51/100] batch [5/5] time 0.039 (0.214) data 0.000 (0.158) loss 0.7827 (0.8092) acc 83.3333 (91.6869) lr 1.0000e-03 eta 0:00:52
>>> samples [78/160] noisy rate: 0.00 --> 0.22 --> 0.10 <<<
epoch [52/100] batch [5/5] time 0.051 (0.221) data 0.000 (0.166) loss 0.7291 (0.8098) acc 95.0000 (90.4476) lr 9.6859e-04 eta 0:00:53
epoch [53/100] batch [5/5] time 0.055 (0.235) data 0.000 (0.179) loss 0.8603 (0.7866) acc 85.5263 (89.7777) lr 9.3721e-04 eta 0:00:55
epoch [54/100] batch [5/5] time 0.038 (0.222) data 0.001 (0.165) loss 1.0219 (0.8389) acc 84.0909 (86.7706) lr 9.0589e-04 eta 0:00:51
epoch [55/100] batch [5/5] time 0.048 (0.218) data 0.000 (0.161) loss 0.7634 (0.7942) acc 89.0625 (90.5689) lr 8.7467e-04 eta 0:00:49
epoch [56/100] batch [5/5] time 0.046 (0.205) data 0.000 (0.148) loss 0.8114 (0.8394) acc 93.3333 (88.8347) lr 8.4357e-04 eta 0:00:45
>>> samples [78/160] noisy rate: 0.00 --> 0.23 --> 0.10 <<<
epoch [57/100] batch [5/5] time 0.049 (0.235) data 0.000 (0.177) loss 0.7223 (0.7882) acc 85.7143 (89.0179) lr 8.1262e-04 eta 0:00:50
epoch [58/100] batch [5/5] time 0.064 (0.231) data 0.000 (0.174) loss 0.7874 (0.8077) acc 94.7368 (90.0157) lr 7.8186e-04 eta 0:00:48
epoch [59/100] batch [5/5] time 0.055 (0.252) data 0.000 (0.191) loss 0.9957 (0.8072) acc 81.2500 (89.8613) lr 7.5131e-04 eta 0:00:51
epoch [60/100] batch [5/5] time 0.054 (0.222) data 0.000 (0.164) loss 0.6669 (0.8076) acc 88.3333 (90.1256) lr 7.2101e-04 eta 0:00:44
epoch [61/100] batch [5/5] time 0.057 (0.216) data 0.000 (0.154) loss 0.7741 (0.7643) acc 95.5882 (91.3654) lr 6.9098e-04 eta 0:00:42
>>> samples [78/160] noisy rate: 0.00 --> 0.23 --> 0.10 <<<
epoch [62/100] batch [5/5] time 0.037 (0.192) data 0.000 (0.136) loss 0.9384 (0.7921) acc 100.0000 (93.6603) lr 6.6126e-04 eta 0:00:36
epoch [63/100] batch [5/5] time 0.058 (0.228) data 0.000 (0.168) loss 0.7303 (0.8093) acc 93.4211 (88.6595) lr 6.3188e-04 eta 0:00:42
epoch [64/100] batch [5/5] time 0.057 (0.239) data 0.000 (0.186) loss 0.8340 (0.7907) acc 89.7059 (90.1798) lr 6.0285e-04 eta 0:00:42
epoch [65/100] batch [5/5] time 0.042 (0.298) data 0.000 (0.164) loss 0.7144 (0.7427) acc 98.0769 (91.7438) lr 5.7422e-04 eta 0:00:52
epoch [66/100] batch [5/5] time 0.049 (0.255) data 0.000 (0.200) loss 0.7061 (0.7648) acc 94.6429 (92.7325) lr 5.4601e-04 eta 0:00:43
>>> samples [78/160] noisy rate: 0.00 --> 0.25 --> 0.10 <<<
epoch [67/100] batch [5/5] time 0.051 (0.228) data 0.000 (0.172) loss 0.8282 (0.7894) acc 94.1176 (88.8714) lr 5.1825e-04 eta 0:00:37
epoch [68/100] batch [5/5] time 0.047 (0.234) data 0.000 (0.178) loss 0.9046 (0.7980) acc 88.3333 (93.0588) lr 4.9096e-04 eta 0:00:37
epoch [69/100] batch [5/5] time 0.047 (0.241) data 0.000 (0.186) loss 0.8403 (0.7603) acc 89.2857 (92.6419) lr 4.6417e-04 eta 0:00:37
epoch [70/100] batch [5/5] time 0.466 (0.320) data 0.000 (0.188) loss 0.8293 (0.8046) acc 92.0455 (91.5549) lr 4.3792e-04 eta 0:00:48
epoch [71/100] batch [5/5] time 0.068 (0.235) data 0.000 (0.178) loss 0.8729 (0.7599) acc 92.8571 (90.1270) lr 4.1221e-04 eta 0:00:34
>>> samples [78/160] noisy rate: 0.00 --> 0.23 --> 0.10 <<<
epoch [72/100] batch [5/5] time 0.060 (0.227) data 0.000 (0.171) loss 0.8143 (0.7675) acc 92.1053 (92.9568) lr 3.8709e-04 eta 0:00:31
epoch [73/100] batch [5/5] time 0.040 (0.230) data 0.000 (0.174) loss 0.5954 (0.7904) acc 92.5000 (90.2222) lr 3.6258e-04 eta 0:00:31
epoch [74/100] batch [5/5] time 0.052 (0.236) data 0.000 (0.182) loss 0.8759 (0.7704) acc 89.2857 (91.4332) lr 3.3869e-04 eta 0:00:30
epoch [75/100] batch [5/5] time 0.048 (0.216) data 0.000 (0.160) loss 0.9769 (0.7851) acc 85.4167 (90.0050) lr 3.1545e-04 eta 0:00:26
epoch [76/100] batch [5/5] time 0.063 (0.254) data 0.000 (0.192) loss 0.7331 (0.7946) acc 93.4211 (90.9744) lr 2.9289e-04 eta 0:00:30
>>> samples [78/160] noisy rate: 0.00 --> 0.24 --> 0.10 <<<
epoch [77/100] batch [5/5] time 0.054 (0.222) data 0.000 (0.162) loss 0.7165 (0.7975) acc 93.7500 (90.3571) lr 2.7103e-04 eta 0:00:25
epoch [78/100] batch [5/5] time 0.052 (0.211) data 0.001 (0.150) loss 0.7895 (0.7894) acc 88.3333 (89.2927) lr 2.4989e-04 eta 0:00:23
epoch [79/100] batch [5/5] time 0.044 (0.242) data 0.001 (0.187) loss 0.7970 (0.7579) acc 91.0714 (92.2318) lr 2.2949e-04 eta 0:00:25
epoch [80/100] batch [5/5] time 0.049 (0.255) data 0.000 (0.204) loss 0.8321 (0.7843) acc 92.1875 (91.9277) lr 2.0984e-04 eta 0:00:25
epoch [81/100] batch [5/5] time 0.052 (0.235) data 0.001 (0.181) loss 0.7503 (0.7762) acc 89.7059 (90.7983) lr 1.9098e-04 eta 0:00:22
>>> samples [82/160] noisy rate: 0.00 --> 0.22 --> 0.12 <<<
epoch [82/100] batch [5/5] time 0.058 (0.238) data 0.000 (0.181) loss 0.6814 (0.8251) acc 91.2500 (89.0522) lr 1.7292e-04 eta 0:00:21
epoch [83/100] batch [5/5] time 0.049 (0.233) data 0.000 (0.178) loss 0.7154 (0.7957) acc 95.3125 (92.5779) lr 1.5567e-04 eta 0:00:19
epoch [84/100] batch [5/5] time 0.049 (0.228) data 0.000 (0.172) loss 0.7800 (0.7963) acc 89.0625 (91.7411) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.054 (0.252) data 0.000 (0.193) loss 1.0086 (0.8357) acc 92.1875 (89.4856) lr 1.2369e-04 eta 0:00:18
epoch [86/100] batch [5/5] time 0.048 (0.262) data 0.001 (0.208) loss 0.8044 (0.7725) acc 100.0000 (91.0000) lr 1.0899e-04 eta 0:00:18
>>> samples [82/160] noisy rate: 0.00 --> 0.22 --> 0.12 <<<
epoch [87/100] batch [5/5] time 0.049 (0.233) data 0.000 (0.175) loss 1.0601 (0.8271) acc 78.1250 (87.3123) lr 9.5173e-05 eta 0:00:15
epoch [88/100] batch [5/5] time 0.065 (0.297) data 0.000 (0.239) loss 0.7157 (0.7757) acc 83.7500 (90.6009) lr 8.2245e-05 eta 0:00:17
epoch [89/100] batch [5/5] time 0.059 (0.286) data 0.000 (0.227) loss 0.8983 (0.7904) acc 91.6667 (90.5778) lr 7.0224e-05 eta 0:00:15
epoch [90/100] batch [5/5] time 0.063 (0.250) data 0.000 (0.192) loss 0.6834 (0.7774) acc 93.7500 (90.9048) lr 5.9119e-05 eta 0:00:12
epoch [91/100] batch [5/5] time 0.051 (0.245) data 0.000 (0.187) loss 0.6122 (0.7644) acc 94.6429 (91.6811) lr 4.8943e-05 eta 0:00:11
>>> samples [82/160] noisy rate: 0.00 --> 0.21 --> 0.12 <<<
epoch [92/100] batch [5/5] time 0.051 (0.263) data 0.001 (0.209) loss 0.6529 (0.7780) acc 93.3333 (92.9454) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.054 (0.206) data 0.000 (0.148) loss 0.9037 (0.7627) acc 95.3125 (93.1300) lr 3.1417e-05 eta 0:00:07
epoch [94/100] batch [5/5] time 0.061 (0.253) data 0.000 (0.194) loss 0.7771 (0.7757) acc 93.4211 (91.7856) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.046 (0.240) data 0.000 (0.183) loss 0.8167 (0.7883) acc 91.0714 (88.3861) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.057 (0.249) data 0.000 (0.191) loss 0.7230 (0.7796) acc 89.7059 (91.3459) lr 1.2312e-05 eta 0:00:04
>>> samples [82/160] noisy rate: 0.00 --> 0.22 --> 0.12 <<<
epoch [97/100] batch [5/5] time 0.058 (0.224) data 0.000 (0.158) loss 1.0262 (0.8158) acc 86.7647 (87.6147) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.047 (0.229) data 0.000 (0.174) loss 0.8730 (0.7862) acc 91.0714 (92.3757) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.052 (0.220) data 0.000 (0.159) loss 0.8272 (0.7877) acc 91.1765 (91.3392) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.050 (0.272) data 0.000 (0.211) loss 0.6291 (0.7368) acc 96.6667 (92.5979) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.12, 0.36, 0.26, 0.31, 0.29, 0.18, 0.2, 0.21, 0.21, 0.22, 0.22, 0.23, 0.23, 0.25, 0.23, 0.24, 0.22, 0.22, 0.21, 0.22]
* learned noise rate: [0.0, 0.04, 0.04, 0.07, 0.07, 0.11, 0.1, 0.1, 0.1, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.12, 0.12, 0.12, 0.12]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:34,  1.94s/it]  4%|▎         | 3/81 [00:02<00:42,  1.82it/s]  6%|▌         | 5/81 [00:02<00:22,  3.34it/s]  9%|▊         | 7/81 [00:02<00:14,  4.97it/s] 11%|█         | 9/81 [00:02<00:10,  6.71it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.41it/s] 16%|█▌        | 13/81 [00:02<00:06,  9.98it/s] 19%|█▊        | 15/81 [00:02<00:06, 10.94it/s] 21%|██        | 17/81 [00:02<00:05, 12.14it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.88it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.68it/s] 28%|██▊       | 23/81 [00:03<00:04, 14.30it/s] 31%|███       | 25/81 [00:03<00:03, 14.28it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.59it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.97it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.24it/s] 41%|████      | 33/81 [00:04<00:03, 15.07it/s] 43%|████▎     | 35/81 [00:04<00:03, 15.31it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.44it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.55it/s] 51%|█████     | 41/81 [00:04<00:02, 15.65it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.29it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.47it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.37it/s] 60%|██████    | 49/81 [00:05<00:02, 15.01it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.27it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.02it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.96it/s] 70%|███████   | 57/81 [00:05<00:01, 15.08it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.23it/s] 75%|███████▌  | 61/81 [00:05<00:01, 14.62it/s] 78%|███████▊  | 63/81 [00:06<00:01, 14.54it/s] 80%|████████  | 65/81 [00:06<00:01, 14.70it/s] 83%|████████▎ | 67/81 [00:06<00:00, 14.67it/s] 85%|████████▌ | 69/81 [00:06<00:00, 14.96it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.22it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.44it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.59it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.71it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.79it/s]100%|██████████| 81/81 [00:07<00:00, 15.84it/s]100%|██████████| 81/81 [00:07<00:00, 11.08it/s]
=> result
* total: 8,100
* correct: 4,644
* accuracy: 57.3%
* error: 42.7%
* macro_f1: 53.2%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 116	acc: 12.9%
* class: 1 (Forest)	total: 900	correct: 854	acc: 94.9%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 12	acc: 1.3%
* class: 3 (Highway or Road)	total: 750	correct: 541	acc: 72.1%
* class: 4 (Industrial Buildings)	total: 750	correct: 724	acc: 96.5%
* class: 5 (Pasture Land)	total: 600	correct: 416	acc: 69.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 660	acc: 88.0%
* class: 7 (Residential Buildings)	total: 900	correct: 744	acc: 82.7%
* class: 8 (River)	total: 750	correct: 311	acc: 41.5%
* class: 9 (Sea or Lake)	total: 900	correct: 266	acc: 29.6%
* average: 58.9%
Elapsed: 0:04:03
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_0-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.762) data 0.000 (0.248) loss 1.0986 (1.0610) acc 16.4062 (19.0625) lr 2.0000e-03 eta 0:06:17
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [57/160] noisy rate: 0.00 --> 0.18 --> 0.05 <<<
epoch [2/100] batch [5/5] time 0.309 (0.398) data 0.000 (0.189) loss 1.9108 (2.1513) acc 38.6364 (17.8860) lr 1.9995e-03 eta 0:03:15
epoch [3/100] batch [5/5] time 0.377 (0.513) data 0.000 (0.194) loss 2.1367 (2.1054) acc 23.3333 (29.5476) lr 1.9980e-03 eta 0:04:08
epoch [4/100] batch [5/5] time 0.037 (0.256) data 0.000 (0.207) loss 2.1515 (2.1085) acc 27.5000 (28.8333) lr 1.9956e-03 eta 0:02:02
epoch [5/100] batch [5/5] time 0.038 (0.311) data 0.000 (0.266) loss 2.1119 (2.1212) acc 22.5000 (34.1538) lr 1.9921e-03 eta 0:02:27
epoch [6/100] batch [5/5] time 0.041 (0.260) data 0.000 (0.210) loss 2.1919 (2.0884) acc 25.0000 (26.5641) lr 1.9877e-03 eta 0:02:02
>>> samples [58/160] noisy rate: 0.00 --> 0.17 --> 0.05 <<<
epoch [7/100] batch [5/5] time 0.042 (0.242) data 0.000 (0.200) loss 2.0737 (1.9755) acc 34.6154 (36.7383) lr 1.9823e-03 eta 0:01:52
epoch [8/100] batch [5/5] time 0.051 (0.277) data 0.000 (0.222) loss 1.7766 (1.9672) acc 46.6667 (36.9365) lr 1.9759e-03 eta 0:02:07
epoch [9/100] batch [5/5] time 0.039 (0.344) data 0.000 (0.230) loss 1.7322 (1.8657) acc 50.0000 (42.8125) lr 1.9686e-03 eta 0:02:36
epoch [10/100] batch [5/5] time 0.035 (0.252) data 0.000 (0.201) loss 1.7152 (1.8140) acc 65.0000 (51.0280) lr 1.9603e-03 eta 0:01:53
epoch [11/100] batch [5/5] time 0.040 (0.243) data 0.000 (0.191) loss 1.7113 (1.7704) acc 56.2500 (58.1882) lr 1.9511e-03 eta 0:01:48
>>> samples [58/160] noisy rate: 0.00 --> 0.10 --> 0.05 <<<
epoch [12/100] batch [5/5] time 0.034 (0.338) data 0.000 (0.291) loss 1.6039 (1.7019) acc 62.5000 (57.8851) lr 1.9409e-03 eta 0:02:28
epoch [13/100] batch [5/5] time 0.035 (0.287) data 0.000 (0.244) loss 1.7325 (1.7588) acc 42.5000 (53.5794) lr 1.9298e-03 eta 0:02:05
epoch [14/100] batch [5/5] time 0.054 (0.354) data 0.000 (0.264) loss 1.7847 (1.7109) acc 44.6429 (60.3036) lr 1.9178e-03 eta 0:02:32
epoch [15/100] batch [5/5] time 0.032 (0.355) data 0.000 (0.234) loss 1.7795 (1.6882) acc 50.0000 (57.7003) lr 1.9048e-03 eta 0:02:30
epoch [16/100] batch [5/5] time 0.052 (0.267) data 0.000 (0.213) loss 1.7475 (1.7378) acc 50.0000 (60.7500) lr 1.8910e-03 eta 0:01:52
>>> samples [59/160] noisy rate: 0.00 --> 0.13 --> 0.05 <<<
epoch [17/100] batch [5/5] time 0.031 (0.255) data 0.000 (0.208) loss 1.8323 (1.6440) acc 44.4444 (63.3636) lr 1.8763e-03 eta 0:01:45
epoch [18/100] batch [5/5] time 0.041 (0.235) data 0.000 (0.182) loss 1.5337 (1.6072) acc 65.9091 (67.3810) lr 1.8607e-03 eta 0:01:36
epoch [19/100] batch [5/5] time 0.040 (0.249) data 0.000 (0.194) loss 1.4670 (1.5719) acc 72.9167 (70.1748) lr 1.8443e-03 eta 0:01:40
epoch [20/100] batch [5/5] time 0.044 (0.256) data 0.000 (0.201) loss 1.6703 (1.5624) acc 62.5000 (70.5342) lr 1.8271e-03 eta 0:01:42
epoch [21/100] batch [5/5] time 0.041 (0.253) data 0.000 (0.202) loss 1.4356 (1.5890) acc 68.1818 (67.0530) lr 1.8090e-03 eta 0:01:40
>>> samples [60/160] noisy rate: 0.00 --> 0.22 --> 0.05 <<<
epoch [22/100] batch [5/5] time 0.225 (0.305) data 0.000 (0.214) loss 1.5828 (1.5556) acc 60.0000 (64.0577) lr 1.7902e-03 eta 0:01:58
epoch [23/100] batch [5/5] time 0.032 (0.270) data 0.000 (0.219) loss 1.6836 (1.5460) acc 77.7778 (72.7146) lr 1.7705e-03 eta 0:01:44
epoch [24/100] batch [5/5] time 0.052 (0.311) data 0.000 (0.263) loss 1.4951 (1.5401) acc 70.3125 (73.9615) lr 1.7501e-03 eta 0:01:58
epoch [25/100] batch [5/5] time 0.045 (0.287) data 0.000 (0.239) loss 1.4589 (1.5499) acc 69.6429 (69.1733) lr 1.7290e-03 eta 0:01:47
epoch [26/100] batch [5/5] time 0.037 (0.309) data 0.000 (0.256) loss 1.4481 (1.4697) acc 65.9091 (72.8389) lr 1.7071e-03 eta 0:01:54
>>> samples [60/160] noisy rate: 0.00 --> 0.21 --> 0.05 <<<
epoch [27/100] batch [5/5] time 0.050 (0.286) data 0.000 (0.239) loss 1.3801 (1.4811) acc 80.0000 (76.8312) lr 1.6845e-03 eta 0:01:44
epoch [28/100] batch [5/5] time 0.049 (0.318) data 0.000 (0.272) loss 1.5165 (1.4775) acc 91.0714 (79.0357) lr 1.6613e-03 eta 0:01:54
epoch [29/100] batch [5/5] time 0.038 (0.283) data 0.000 (0.225) loss 1.6079 (1.4965) acc 72.5000 (73.6911) lr 1.6374e-03 eta 0:01:40
epoch [30/100] batch [5/5] time 0.048 (0.274) data 0.000 (0.218) loss 1.4308 (1.4430) acc 67.3077 (76.6358) lr 1.6129e-03 eta 0:01:35
epoch [31/100] batch [5/5] time 0.046 (0.349) data 0.000 (0.221) loss 1.6471 (1.4525) acc 79.1667 (81.4861) lr 1.5878e-03 eta 0:02:00
>>> samples [62/160] noisy rate: 0.00 --> 0.19 --> 0.05 <<<
epoch [32/100] batch [5/5] time 0.043 (0.316) data 0.000 (0.261) loss 1.4636 (1.4195) acc 70.4545 (78.0512) lr 1.5621e-03 eta 0:01:47
epoch [33/100] batch [5/5] time 0.047 (0.296) data 0.000 (0.238) loss 1.2822 (1.4505) acc 79.1667 (75.1742) lr 1.5358e-03 eta 0:01:39
epoch [34/100] batch [5/5] time 0.042 (0.316) data 0.000 (0.267) loss 1.2929 (1.3950) acc 68.1818 (77.3485) lr 1.5090e-03 eta 0:01:44
epoch [35/100] batch [5/5] time 0.049 (0.299) data 0.000 (0.245) loss 1.5144 (1.4364) acc 76.9231 (79.8050) lr 1.4818e-03 eta 0:01:37
epoch [36/100] batch [5/5] time 0.036 (0.300) data 0.000 (0.249) loss 1.7098 (1.4206) acc 71.8750 (81.5479) lr 1.4540e-03 eta 0:01:35
>>> samples [63/160] noisy rate: 0.00 --> 0.22 --> 0.06 <<<
epoch [37/100] batch [5/5] time 0.050 (0.266) data 0.000 (0.212) loss 1.4612 (1.4278) acc 71.4286 (77.1255) lr 1.4258e-03 eta 0:01:23
epoch [38/100] batch [5/5] time 0.036 (0.257) data 0.000 (0.203) loss 1.6339 (1.4474) acc 75.0000 (79.8087) lr 1.3971e-03 eta 0:01:19
epoch [39/100] batch [5/5] time 0.058 (0.268) data 0.001 (0.216) loss 1.4914 (1.4225) acc 76.4706 (79.9078) lr 1.3681e-03 eta 0:01:21
epoch [40/100] batch [5/5] time 0.422 (0.356) data 0.000 (0.231) loss 1.5200 (1.4064) acc 69.7368 (80.5397) lr 1.3387e-03 eta 0:01:46
epoch [41/100] batch [5/5] time 0.040 (0.290) data 0.000 (0.239) loss 1.5243 (1.3767) acc 79.1667 (80.6923) lr 1.3090e-03 eta 0:01:25
>>> samples [63/160] noisy rate: 0.00 --> 0.21 --> 0.06 <<<
epoch [42/100] batch [5/5] time 0.035 (0.299) data 0.000 (0.244) loss 1.4277 (1.3925) acc 87.5000 (81.4826) lr 1.2790e-03 eta 0:01:26
epoch [43/100] batch [5/5] time 0.044 (0.295) data 0.001 (0.241) loss 1.4175 (1.3845) acc 60.7143 (79.5281) lr 1.2487e-03 eta 0:01:23
epoch [44/100] batch [5/5] time 0.043 (0.286) data 0.000 (0.237) loss 1.3496 (1.3982) acc 79.1667 (78.6061) lr 1.2181e-03 eta 0:01:20
epoch [45/100] batch [5/5] time 0.030 (0.278) data 0.000 (0.226) loss 1.2176 (1.3542) acc 93.7500 (86.9643) lr 1.1874e-03 eta 0:01:16
epoch [46/100] batch [5/5] time 0.036 (0.279) data 0.000 (0.226) loss 1.5866 (1.3865) acc 72.2222 (82.2192) lr 1.1564e-03 eta 0:01:15
>>> samples [63/160] noisy rate: 0.00 --> 0.22 --> 0.06 <<<
epoch [47/100] batch [5/5] time 0.036 (0.259) data 0.000 (0.206) loss 1.2931 (1.4102) acc 84.0909 (80.1378) lr 1.1253e-03 eta 0:01:08
epoch [48/100] batch [5/5] time 0.039 (0.264) data 0.000 (0.213) loss 1.4841 (1.3771) acc 83.3333 (82.8380) lr 1.0941e-03 eta 0:01:08
epoch [49/100] batch [5/5] time 0.042 (0.291) data 0.000 (0.240) loss 1.3001 (1.3793) acc 77.2727 (80.9994) lr 1.0628e-03 eta 0:01:14
epoch [50/100] batch [5/5] time 0.044 (0.269) data 0.000 (0.214) loss 1.3183 (1.3752) acc 87.5000 (84.3681) lr 1.0314e-03 eta 0:01:07
epoch [51/100] batch [5/5] time 0.030 (0.265) data 0.000 (0.216) loss 1.1578 (1.3219) acc 93.7500 (86.7262) lr 1.0000e-03 eta 0:01:04
>>> samples [65/160] noisy rate: 0.00 --> 0.19 --> 0.09 <<<
epoch [52/100] batch [5/5] time 0.042 (0.275) data 0.000 (0.225) loss 1.4895 (1.3882) acc 79.5455 (84.3929) lr 9.6859e-04 eta 0:01:06
epoch [53/100] batch [5/5] time 0.055 (0.260) data 0.000 (0.205) loss 1.4043 (1.3257) acc 85.7143 (88.0790) lr 9.3721e-04 eta 0:01:01
epoch [54/100] batch [5/5] time 0.053 (0.311) data 0.000 (0.251) loss 1.6813 (1.3454) acc 73.3333 (83.1731) lr 9.0589e-04 eta 0:01:11
epoch [55/100] batch [5/5] time 0.048 (0.314) data 0.000 (0.260) loss 1.3032 (1.3456) acc 84.6154 (84.2262) lr 8.7467e-04 eta 0:01:10
epoch [56/100] batch [5/5] time 0.043 (0.248) data 0.000 (0.192) loss 1.0003 (1.3614) acc 95.8333 (83.3969) lr 8.4357e-04 eta 0:00:54
>>> samples [65/160] noisy rate: 0.00 --> 0.19 --> 0.09 <<<
epoch [57/100] batch [5/5] time 0.039 (0.251) data 0.001 (0.200) loss 1.3154 (1.3072) acc 77.5000 (83.9020) lr 8.1262e-04 eta 0:00:54
epoch [58/100] batch [5/5] time 0.050 (0.257) data 0.000 (0.205) loss 1.3096 (1.3075) acc 79.6875 (85.6017) lr 7.8186e-04 eta 0:00:53
epoch [59/100] batch [5/5] time 0.047 (0.258) data 0.000 (0.201) loss 1.4545 (1.3349) acc 76.7857 (83.0262) lr 7.5131e-04 eta 0:00:52
epoch [60/100] batch [5/5] time 0.032 (0.261) data 0.000 (0.211) loss 1.2955 (1.2896) acc 94.4444 (87.6068) lr 7.2101e-04 eta 0:00:52
epoch [61/100] batch [5/5] time 0.440 (0.395) data 0.000 (0.267) loss 1.2900 (1.3179) acc 77.5000 (84.1410) lr 6.9098e-04 eta 0:01:17
>>> samples [66/160] noisy rate: 0.00 --> 0.21 --> 0.11 <<<
epoch [62/100] batch [5/5] time 0.055 (0.244) data 0.000 (0.191) loss 1.0784 (1.3349) acc 88.2353 (88.4387) lr 6.6126e-04 eta 0:00:46
epoch [63/100] batch [5/5] time 0.039 (0.262) data 0.000 (0.207) loss 1.1563 (1.3091) acc 89.5833 (86.3278) lr 6.3188e-04 eta 0:00:48
epoch [64/100] batch [5/5] time 0.040 (0.265) data 0.000 (0.211) loss 1.1670 (1.2871) acc 81.8182 (88.2166) lr 6.0285e-04 eta 0:00:47
epoch [65/100] batch [5/5] time 0.051 (0.246) data 0.000 (0.194) loss 1.4054 (1.3176) acc 78.3333 (87.0804) lr 5.7422e-04 eta 0:00:43
epoch [66/100] batch [5/5] time 0.044 (0.265) data 0.000 (0.211) loss 1.3298 (1.2928) acc 82.1429 (87.9167) lr 5.4601e-04 eta 0:00:44
>>> samples [66/160] noisy rate: 0.00 --> 0.19 --> 0.11 <<<
epoch [67/100] batch [5/5] time 0.046 (0.282) data 0.000 (0.230) loss 1.3813 (1.3215) acc 83.9286 (84.5055) lr 5.1825e-04 eta 0:00:46
epoch [68/100] batch [5/5] time 0.041 (0.235) data 0.000 (0.181) loss 1.0092 (1.2933) acc 97.7273 (86.0752) lr 4.9096e-04 eta 0:00:37
epoch [69/100] batch [5/5] time 0.046 (0.266) data 0.000 (0.215) loss 1.4972 (1.3345) acc 78.8462 (81.3071) lr 4.6417e-04 eta 0:00:41
epoch [70/100] batch [5/5] time 0.048 (0.257) data 0.000 (0.206) loss 1.4670 (1.3005) acc 78.1250 (80.9035) lr 4.3792e-04 eta 0:00:38
epoch [71/100] batch [5/5] time 0.043 (0.266) data 0.000 (0.215) loss 1.2677 (1.2942) acc 100.0000 (87.7381) lr 4.1221e-04 eta 0:00:38
>>> samples [66/160] noisy rate: 0.00 --> 0.18 --> 0.11 <<<
epoch [72/100] batch [5/5] time 0.031 (0.274) data 0.000 (0.221) loss 0.9461 (1.2459) acc 100.0000 (88.5952) lr 3.8709e-04 eta 0:00:38
epoch [73/100] batch [5/5] time 0.049 (0.294) data 0.000 (0.248) loss 1.0384 (1.2924) acc 85.7143 (84.8942) lr 3.6258e-04 eta 0:00:39
epoch [74/100] batch [5/5] time 0.048 (0.289) data 0.000 (0.237) loss 1.2628 (1.2618) acc 87.5000 (85.7049) lr 3.3869e-04 eta 0:00:37
epoch [75/100] batch [5/5] time 0.057 (0.279) data 0.000 (0.224) loss 1.1753 (1.3276) acc 88.1579 (87.3279) lr 3.1545e-04 eta 0:00:34
epoch [76/100] batch [5/5] time 0.038 (0.259) data 0.001 (0.207) loss 1.4002 (1.3006) acc 77.2727 (86.3351) lr 2.9289e-04 eta 0:00:31
>>> samples [66/160] noisy rate: 0.00 --> 0.21 --> 0.11 <<<
epoch [77/100] batch [5/5] time 0.046 (0.245) data 0.000 (0.193) loss 1.3894 (1.2951) acc 80.0000 (86.4145) lr 2.7103e-04 eta 0:00:28
epoch [78/100] batch [5/5] time 0.038 (0.252) data 0.000 (0.197) loss 1.3753 (1.2732) acc 79.5455 (88.1246) lr 2.4989e-04 eta 0:00:27
epoch [79/100] batch [5/5] time 0.042 (0.230) data 0.000 (0.177) loss 1.1780 (1.3034) acc 82.6923 (84.4649) lr 2.2949e-04 eta 0:00:24
epoch [80/100] batch [5/5] time 0.045 (0.278) data 0.000 (0.223) loss 1.1947 (1.2662) acc 94.6429 (88.4041) lr 2.0984e-04 eta 0:00:27
epoch [81/100] batch [5/5] time 0.052 (0.274) data 0.000 (0.217) loss 1.3489 (1.2803) acc 80.8824 (85.9232) lr 1.9098e-04 eta 0:00:26
>>> samples [66/160] noisy rate: 0.00 --> 0.21 --> 0.11 <<<
epoch [82/100] batch [5/5] time 0.041 (0.271) data 0.000 (0.218) loss 1.4139 (1.2922) acc 94.2308 (87.9740) lr 1.7292e-04 eta 0:00:24
epoch [83/100] batch [5/5] time 0.048 (0.255) data 0.000 (0.203) loss 1.3791 (1.2835) acc 83.9286 (86.7565) lr 1.5567e-04 eta 0:00:21
epoch [84/100] batch [5/5] time 0.053 (0.269) data 0.000 (0.215) loss 1.1953 (1.3053) acc 86.7647 (85.5929) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.044 (0.254) data 0.000 (0.198) loss 1.3374 (1.2726) acc 80.3571 (89.1068) lr 1.2369e-04 eta 0:00:19
epoch [86/100] batch [5/5] time 0.049 (0.275) data 0.000 (0.223) loss 1.2768 (1.3039) acc 81.6667 (85.0292) lr 1.0899e-04 eta 0:00:19
>>> samples [66/160] noisy rate: 0.00 --> 0.21 --> 0.11 <<<
epoch [87/100] batch [5/5] time 0.054 (0.251) data 0.000 (0.198) loss 1.0287 (1.3063) acc 92.1875 (87.0760) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.051 (0.248) data 0.000 (0.192) loss 1.4094 (1.2946) acc 83.3333 (87.4775) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.047 (0.270) data 0.000 (0.220) loss 1.1481 (1.2799) acc 84.6154 (88.2749) lr 7.0224e-05 eta 0:00:14
epoch [90/100] batch [5/5] time 0.039 (0.264) data 0.000 (0.208) loss 1.2832 (1.2981) acc 85.4167 (85.5846) lr 5.9119e-05 eta 0:00:13
epoch [91/100] batch [5/5] time 0.044 (0.250) data 0.000 (0.196) loss 1.1611 (1.3398) acc 85.7143 (83.2110) lr 4.8943e-05 eta 0:00:11
>>> samples [66/160] noisy rate: 0.00 --> 0.18 --> 0.11 <<<
epoch [92/100] batch [5/5] time 0.051 (0.300) data 0.000 (0.239) loss 1.1017 (1.2800) acc 87.5000 (88.0556) lr 3.9706e-05 eta 0:00:12
epoch [93/100] batch [5/5] time 0.034 (0.271) data 0.000 (0.219) loss 1.1345 (1.2812) acc 93.7500 (89.5746) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.041 (0.317) data 0.000 (0.261) loss 1.3552 (1.3064) acc 84.0909 (85.5722) lr 2.4083e-05 eta 0:00:09
epoch [95/100] batch [5/5] time 0.044 (0.304) data 0.000 (0.251) loss 1.5074 (1.2728) acc 87.5000 (87.5455) lr 1.7713e-05 eta 0:00:07
epoch [96/100] batch [5/5] time 0.049 (0.327) data 0.000 (0.273) loss 1.3103 (1.2965) acc 92.8571 (85.7413) lr 1.2312e-05 eta 0:00:06
>>> samples [66/160] noisy rate: 0.00 --> 0.20 --> 0.11 <<<
epoch [97/100] batch [5/5] time 0.033 (0.241) data 0.000 (0.188) loss 0.9913 (1.2414) acc 93.7500 (89.9167) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.044 (0.235) data 0.000 (0.183) loss 1.3553 (1.2320) acc 80.3571 (88.9256) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.043 (0.289) data 0.000 (0.243) loss 1.1210 (1.3055) acc 85.7143 (86.8493) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.049 (0.282) data 0.000 (0.232) loss 1.2628 (1.3156) acc 93.7500 (83.7958) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.18, 0.17, 0.1, 0.13, 0.22, 0.21, 0.19, 0.22, 0.21, 0.22, 0.19, 0.19, 0.21, 0.19, 0.18, 0.21, 0.21, 0.21, 0.18, 0.2]
* learned noise rate: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.06, 0.06, 0.09, 0.09, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<03:09,  2.36s/it]  4%|▎         | 3/81 [00:02<00:51,  1.51it/s]  6%|▌         | 5/81 [00:02<00:26,  2.82it/s]  9%|▊         | 7/81 [00:02<00:17,  4.22it/s] 11%|█         | 9/81 [00:02<00:12,  5.80it/s] 14%|█▎        | 11/81 [00:03<00:09,  7.46it/s] 16%|█▌        | 13/81 [00:03<00:07,  9.06it/s] 19%|█▊        | 15/81 [00:03<00:06, 10.13it/s] 21%|██        | 17/81 [00:03<00:05, 11.44it/s] 23%|██▎       | 19/81 [00:03<00:05, 12.37it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.23it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.81it/s] 31%|███       | 25/81 [00:03<00:04, 13.66it/s] 33%|███▎      | 27/81 [00:04<00:03, 13.88it/s] 36%|███▌      | 29/81 [00:04<00:03, 13.97it/s] 38%|███▊      | 31/81 [00:04<00:03, 14.51it/s] 41%|████      | 33/81 [00:04<00:03, 14.78it/s] 43%|████▎     | 35/81 [00:04<00:03, 14.82it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.14it/s] 48%|████▊     | 39/81 [00:04<00:02, 14.43it/s] 51%|█████     | 41/81 [00:05<00:02, 14.83it/s] 53%|█████▎    | 43/81 [00:05<00:02, 15.08it/s] 56%|█████▌    | 45/81 [00:05<00:02, 15.32it/s] 58%|█████▊    | 47/81 [00:05<00:02, 14.71it/s] 60%|██████    | 49/81 [00:05<00:02, 14.96it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.22it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.29it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.42it/s] 70%|███████   | 57/81 [00:06<00:01, 15.47it/s] 73%|███████▎  | 59/81 [00:06<00:01, 15.60it/s] 75%|███████▌  | 61/81 [00:06<00:01, 15.68it/s] 78%|███████▊  | 63/81 [00:06<00:01, 15.65it/s] 80%|████████  | 65/81 [00:06<00:01, 15.24it/s] 83%|████████▎ | 67/81 [00:06<00:00, 14.96it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.21it/s] 88%|████████▊ | 71/81 [00:07<00:00, 15.41it/s] 90%|█████████ | 73/81 [00:07<00:00, 15.50it/s] 93%|█████████▎| 75/81 [00:07<00:00, 15.63it/s] 95%|█████████▌| 77/81 [00:07<00:00, 15.74it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.83it/s]100%|██████████| 81/81 [00:07<00:00, 15.89it/s]100%|██████████| 81/81 [00:07<00:00, 10.27it/s]
=> result
* total: 8,100
* correct: 5,005
* accuracy: 61.8%
* error: 38.2%
* macro_f1: 59.1%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 494	acc: 54.9%
* class: 1 (Forest)	total: 900	correct: 860	acc: 95.6%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 44	acc: 4.9%
* class: 3 (Highway or Road)	total: 750	correct: 452	acc: 60.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 637	acc: 84.9%
* class: 5 (Pasture Land)	total: 600	correct: 403	acc: 67.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 561	acc: 74.8%
* class: 7 (Residential Buildings)	total: 900	correct: 732	acc: 81.3%
* class: 8 (River)	total: 750	correct: 287	acc: 38.3%
* class: 9 (Sea or Lake)	total: 900	correct: 535	acc: 59.4%
* average: 62.2%
Elapsed: 0:04:44
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_0-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.746) data 0.000 (0.221) loss 1.0308 (1.0651) acc 25.7812 (19.2188) lr 2.0000e-03 eta 0:06:09
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [47/160] noisy rate: 0.00 --> 0.22 --> 0.00 <<<
epoch [2/100] batch [5/5] time 0.036 (0.424) data 0.001 (0.231) loss 2.0575 (2.1557) acc 21.8750 (15.5972) lr 1.9995e-03 eta 0:03:27
epoch [3/100] batch [5/5] time 0.051 (0.354) data 0.000 (0.217) loss 1.8509 (2.0784) acc 45.8333 (25.3568) lr 1.9980e-03 eta 0:02:51
epoch [4/100] batch [5/5] time 0.205 (0.334) data 0.000 (0.185) loss 1.8528 (1.9253) acc 50.0000 (34.4583) lr 1.9956e-03 eta 0:02:40
epoch [5/100] batch [5/5] time 0.208 (0.334) data 0.000 (0.197) loss 1.9188 (1.8189) acc 29.1667 (44.4571) lr 1.9921e-03 eta 0:02:38
epoch [6/100] batch [5/5] time 0.042 (0.234) data 0.000 (0.190) loss 1.6513 (1.6459) acc 57.6923 (58.4968) lr 1.9877e-03 eta 0:01:50
>>> samples [51/160] noisy rate: 0.00 --> 0.36 --> 0.06 <<<
epoch [7/100] batch [5/5] time 0.033 (0.211) data 0.000 (0.161) loss 1.4599 (1.6468) acc 65.0000 (61.2525) lr 1.9823e-03 eta 0:01:37
epoch [8/100] batch [5/5] time 0.041 (0.308) data 0.000 (0.163) loss 1.7126 (1.5698) acc 60.0000 (65.3153) lr 1.9759e-03 eta 0:02:21
epoch [9/100] batch [5/5] time 0.041 (0.252) data 0.000 (0.206) loss 1.5354 (1.4743) acc 65.9091 (69.0344) lr 1.9686e-03 eta 0:01:54
epoch [10/100] batch [5/5] time 0.035 (0.204) data 0.000 (0.165) loss 1.2679 (1.4377) acc 75.0000 (70.9135) lr 1.9603e-03 eta 0:01:31
epoch [11/100] batch [5/5] time 0.030 (0.212) data 0.000 (0.172) loss 1.3594 (1.3283) acc 71.8750 (78.9370) lr 1.9511e-03 eta 0:01:34
>>> samples [54/160] noisy rate: 0.00 --> 0.24 --> 0.07 <<<
epoch [12/100] batch [5/5] time 0.049 (0.211) data 0.000 (0.160) loss 1.2756 (1.4686) acc 81.6667 (74.7564) lr 1.9409e-03 eta 0:01:32
epoch [13/100] batch [5/5] time 0.043 (0.206) data 0.000 (0.156) loss 1.1107 (1.2904) acc 81.8182 (82.7098) lr 1.9298e-03 eta 0:01:29
epoch [14/100] batch [5/5] time 0.045 (0.203) data 0.000 (0.159) loss 1.3705 (1.2990) acc 76.9231 (80.7255) lr 1.9178e-03 eta 0:01:27
epoch [15/100] batch [5/5] time 0.052 (0.208) data 0.000 (0.155) loss 1.1048 (1.2626) acc 78.5714 (85.0072) lr 1.9048e-03 eta 0:01:28
epoch [16/100] batch [5/5] time 0.039 (0.268) data 0.000 (0.157) loss 1.2289 (1.2301) acc 80.5555 (83.5069) lr 1.8910e-03 eta 0:01:52
>>> samples [56/160] noisy rate: 0.00 --> 0.24 --> 0.07 <<<
epoch [17/100] batch [5/5] time 0.034 (0.247) data 0.000 (0.199) loss 0.9682 (1.2165) acc 82.5000 (80.2576) lr 1.8763e-03 eta 0:01:42
epoch [18/100] batch [5/5] time 0.042 (0.223) data 0.000 (0.181) loss 1.4056 (1.2714) acc 84.0909 (83.7626) lr 1.8607e-03 eta 0:01:31
epoch [19/100] batch [5/5] time 0.054 (0.234) data 0.000 (0.186) loss 1.2389 (1.2470) acc 70.3125 (80.9931) lr 1.8443e-03 eta 0:01:34
epoch [20/100] batch [5/5] time 0.053 (0.211) data 0.000 (0.165) loss 1.2120 (1.2552) acc 79.6875 (81.1298) lr 1.8271e-03 eta 0:01:24
epoch [21/100] batch [5/5] time 0.037 (0.224) data 0.000 (0.178) loss 1.0841 (1.1998) acc 84.0909 (85.7348) lr 1.8090e-03 eta 0:01:28
>>> samples [58/160] noisy rate: 0.00 --> 0.24 --> 0.07 <<<
epoch [22/100] batch [5/5] time 0.042 (0.231) data 0.000 (0.186) loss 1.2206 (1.2323) acc 86.3636 (80.5844) lr 1.7902e-03 eta 0:01:30
epoch [23/100] batch [5/5] time 0.035 (0.211) data 0.001 (0.166) loss 1.0191 (1.2334) acc 90.0000 (81.6026) lr 1.7705e-03 eta 0:01:21
epoch [24/100] batch [5/5] time 0.031 (0.218) data 0.000 (0.174) loss 1.0395 (1.1608) acc 83.3333 (85.4780) lr 1.7501e-03 eta 0:01:22
epoch [25/100] batch [5/5] time 0.038 (0.218) data 0.000 (0.173) loss 1.1424 (1.1291) acc 83.3333 (86.5216) lr 1.7290e-03 eta 0:01:21
epoch [26/100] batch [5/5] time 0.034 (0.278) data 0.000 (0.230) loss 1.0042 (1.1337) acc 86.1111 (82.8813) lr 1.7071e-03 eta 0:01:42
>>> samples [58/160] noisy rate: 0.00 --> 0.27 --> 0.07 <<<
epoch [27/100] batch [5/5] time 0.039 (0.220) data 0.000 (0.170) loss 1.1219 (1.1839) acc 83.3333 (85.9659) lr 1.6845e-03 eta 0:01:20
epoch [28/100] batch [5/5] time 0.036 (0.222) data 0.000 (0.167) loss 0.7205 (1.1246) acc 92.8571 (83.5190) lr 1.6613e-03 eta 0:01:19
epoch [29/100] batch [5/5] time 0.036 (0.331) data 0.000 (0.205) loss 0.8126 (1.0441) acc 85.0000 (89.6144) lr 1.6374e-03 eta 0:01:57
epoch [30/100] batch [5/5] time 0.040 (0.256) data 0.000 (0.204) loss 1.1231 (1.1191) acc 80.5555 (87.3763) lr 1.6129e-03 eta 0:01:29
epoch [31/100] batch [5/5] time 0.050 (0.209) data 0.000 (0.159) loss 1.0763 (1.1791) acc 92.3077 (86.4808) lr 1.5878e-03 eta 0:01:12
>>> samples [59/160] noisy rate: 0.00 --> 0.23 --> 0.08 <<<
epoch [32/100] batch [5/5] time 0.056 (0.249) data 0.000 (0.199) loss 1.0125 (1.0784) acc 96.8750 (92.2810) lr 1.5621e-03 eta 0:01:24
epoch [33/100] batch [5/5] time 0.049 (0.222) data 0.000 (0.167) loss 1.1407 (1.0525) acc 80.7692 (90.2473) lr 1.5358e-03 eta 0:01:14
epoch [34/100] batch [5/5] time 0.049 (0.217) data 0.000 (0.164) loss 1.2749 (1.0619) acc 80.7692 (88.0096) lr 1.5090e-03 eta 0:01:11
epoch [35/100] batch [5/5] time 0.052 (0.221) data 0.000 (0.166) loss 1.2009 (1.0747) acc 89.2857 (89.9784) lr 1.4818e-03 eta 0:01:11
epoch [36/100] batch [5/5] time 0.043 (0.253) data 0.000 (0.206) loss 1.1329 (1.0692) acc 87.5000 (90.8782) lr 1.4540e-03 eta 0:01:21
>>> samples [66/160] noisy rate: 0.00 --> 0.21 --> 0.12 <<<
epoch [37/100] batch [5/5] time 0.048 (0.298) data 0.000 (0.175) loss 1.1042 (1.0606) acc 93.7500 (90.1356) lr 1.4258e-03 eta 0:01:33
epoch [38/100] batch [5/5] time 0.436 (0.300) data 0.000 (0.171) loss 1.0131 (1.0318) acc 82.9545 (89.4996) lr 1.3971e-03 eta 0:01:33
epoch [39/100] batch [5/5] time 0.035 (0.272) data 0.000 (0.221) loss 0.9896 (1.0158) acc 93.7500 (91.1387) lr 1.3681e-03 eta 0:01:22
epoch [40/100] batch [5/5] time 0.042 (0.233) data 0.000 (0.175) loss 0.7993 (1.0346) acc 95.4545 (89.3450) lr 1.3387e-03 eta 0:01:09
epoch [41/100] batch [5/5] time 0.052 (0.221) data 0.000 (0.163) loss 0.9119 (1.0252) acc 97.0588 (89.9193) lr 1.3090e-03 eta 0:01:05
>>> samples [66/160] noisy rate: 0.00 --> 0.22 --> 0.12 <<<
epoch [42/100] batch [5/5] time 0.056 (0.270) data 0.001 (0.221) loss 0.8496 (1.0255) acc 93.3333 (92.0260) lr 1.2790e-03 eta 0:01:18
epoch [43/100] batch [5/5] time 0.042 (0.254) data 0.000 (0.204) loss 1.1489 (1.0186) acc 92.3077 (91.6490) lr 1.2487e-03 eta 0:01:12
epoch [44/100] batch [5/5] time 0.044 (0.313) data 0.000 (0.181) loss 1.2074 (1.0229) acc 90.9091 (91.7627) lr 1.2181e-03 eta 0:01:27
epoch [45/100] batch [5/5] time 0.061 (0.232) data 0.000 (0.175) loss 0.9625 (1.0241) acc 94.1176 (89.7851) lr 1.1874e-03 eta 0:01:03
epoch [46/100] batch [5/5] time 0.039 (0.241) data 0.001 (0.189) loss 0.9964 (0.9934) acc 89.5833 (91.7949) lr 1.1564e-03 eta 0:01:05
>>> samples [68/160] noisy rate: 0.00 --> 0.30 --> 0.15 <<<
epoch [47/100] batch [5/5] time 0.051 (0.269) data 0.000 (0.217) loss 0.9714 (1.0494) acc 95.0000 (86.9242) lr 1.1253e-03 eta 0:01:11
epoch [48/100] batch [5/5] time 0.052 (0.263) data 0.000 (0.201) loss 0.6574 (1.0689) acc 96.6667 (89.0417) lr 1.0941e-03 eta 0:01:08
epoch [49/100] batch [5/5] time 0.043 (0.236) data 0.000 (0.182) loss 0.7987 (1.0388) acc 98.0769 (89.7949) lr 1.0628e-03 eta 0:01:00
epoch [50/100] batch [5/5] time 0.059 (0.247) data 0.000 (0.193) loss 1.1732 (1.0031) acc 88.7500 (89.7721) lr 1.0314e-03 eta 0:01:01
epoch [51/100] batch [5/5] time 0.049 (0.258) data 0.000 (0.209) loss 0.6413 (0.9876) acc 91.0714 (90.4787) lr 1.0000e-03 eta 0:01:03
>>> samples [68/160] noisy rate: 0.00 --> 0.28 --> 0.15 <<<
epoch [52/100] batch [5/5] time 0.048 (0.225) data 0.000 (0.170) loss 0.8748 (1.0376) acc 85.0000 (88.2153) lr 9.6859e-04 eta 0:00:53
epoch [53/100] batch [5/5] time 0.056 (0.269) data 0.001 (0.210) loss 1.2375 (1.0140) acc 82.8125 (87.3807) lr 9.3721e-04 eta 0:01:03
epoch [54/100] batch [5/5] time 0.053 (0.266) data 0.000 (0.209) loss 1.1367 (0.9976) acc 89.2857 (91.0496) lr 9.0589e-04 eta 0:01:01
epoch [55/100] batch [5/5] time 0.053 (0.252) data 0.000 (0.203) loss 0.9280 (1.0278) acc 92.1875 (89.5042) lr 8.7467e-04 eta 0:00:56
epoch [56/100] batch [5/5] time 0.039 (0.228) data 0.000 (0.174) loss 1.0062 (1.0100) acc 93.7500 (89.2546) lr 8.4357e-04 eta 0:00:50
>>> samples [69/160] noisy rate: 0.00 --> 0.29 --> 0.14 <<<
epoch [57/100] batch [5/5] time 0.048 (0.259) data 0.000 (0.202) loss 0.8915 (1.0043) acc 89.2857 (89.5945) lr 8.1262e-04 eta 0:00:55
epoch [58/100] batch [5/5] time 0.046 (0.211) data 0.000 (0.161) loss 1.1610 (0.9938) acc 86.5385 (92.1600) lr 7.8186e-04 eta 0:00:44
epoch [59/100] batch [5/5] time 0.044 (0.265) data 0.000 (0.209) loss 1.1400 (1.0046) acc 86.5385 (88.3205) lr 7.5131e-04 eta 0:00:54
epoch [60/100] batch [5/5] time 0.055 (0.231) data 0.000 (0.175) loss 0.7450 (0.9868) acc 87.5000 (88.7959) lr 7.2101e-04 eta 0:00:46
epoch [61/100] batch [5/5] time 0.044 (0.218) data 0.000 (0.160) loss 1.1595 (0.9895) acc 89.5833 (90.1636) lr 6.9098e-04 eta 0:00:42
>>> samples [69/160] noisy rate: 0.00 --> 0.29 --> 0.14 <<<
epoch [62/100] batch [5/5] time 0.042 (0.237) data 0.000 (0.183) loss 0.8270 (0.9409) acc 95.4545 (92.1154) lr 6.6126e-04 eta 0:00:44
epoch [63/100] batch [5/5] time 0.043 (0.225) data 0.000 (0.176) loss 0.7991 (1.0105) acc 91.6667 (88.8270) lr 6.3188e-04 eta 0:00:41
epoch [64/100] batch [5/5] time 0.049 (0.206) data 0.000 (0.150) loss 1.0014 (0.9909) acc 90.6250 (91.2214) lr 6.0285e-04 eta 0:00:37
epoch [65/100] batch [5/5] time 0.045 (0.252) data 0.000 (0.197) loss 0.9874 (0.9828) acc 96.4286 (93.3290) lr 5.7422e-04 eta 0:00:44
epoch [66/100] batch [5/5] time 0.045 (0.232) data 0.000 (0.173) loss 0.7742 (0.9503) acc 96.4286 (93.8262) lr 5.4601e-04 eta 0:00:39
>>> samples [75/160] noisy rate: 0.00 --> 0.31 --> 0.16 <<<
epoch [67/100] batch [5/5] time 0.045 (0.303) data 0.000 (0.184) loss 1.1491 (0.9844) acc 85.0000 (89.4056) lr 5.1825e-04 eta 0:00:50
epoch [68/100] batch [5/5] time 0.049 (0.208) data 0.000 (0.154) loss 0.9236 (0.9966) acc 91.0714 (88.0910) lr 4.9096e-04 eta 0:00:33
epoch [69/100] batch [5/5] time 0.042 (0.215) data 0.000 (0.165) loss 1.1466 (0.9997) acc 89.5833 (87.5382) lr 4.6417e-04 eta 0:00:33
epoch [70/100] batch [5/5] time 0.056 (0.206) data 0.000 (0.151) loss 0.9406 (0.9696) acc 92.1875 (90.9931) lr 4.3792e-04 eta 0:00:30
epoch [71/100] batch [5/5] time 0.045 (0.252) data 0.000 (0.201) loss 1.2017 (1.0028) acc 80.3571 (88.0639) lr 4.1221e-04 eta 0:00:36
>>> samples [79/160] noisy rate: 0.00 --> 0.29 --> 0.18 <<<
epoch [72/100] batch [5/5] time 0.057 (0.228) data 0.000 (0.173) loss 1.1895 (0.9833) acc 77.9412 (87.3301) lr 3.8709e-04 eta 0:00:31
epoch [73/100] batch [5/5] time 0.054 (0.217) data 0.000 (0.163) loss 0.7505 (0.9785) acc 95.8333 (90.7108) lr 3.6258e-04 eta 0:00:29
epoch [74/100] batch [5/5] time 0.050 (0.222) data 0.001 (0.166) loss 1.0022 (0.9669) acc 87.5000 (90.2773) lr 3.3869e-04 eta 0:00:28
epoch [75/100] batch [5/5] time 0.046 (0.236) data 0.000 (0.179) loss 0.9950 (0.9837) acc 88.4615 (90.2191) lr 3.1545e-04 eta 0:00:29
epoch [76/100] batch [5/5] time 0.044 (0.325) data 0.000 (0.180) loss 0.8730 (0.9733) acc 89.5833 (89.5185) lr 2.9289e-04 eta 0:00:39
>>> samples [79/160] noisy rate: 0.00 --> 0.28 --> 0.18 <<<
epoch [77/100] batch [5/5] time 0.056 (0.269) data 0.000 (0.206) loss 0.9494 (0.9897) acc 93.7500 (91.7421) lr 2.7103e-04 eta 0:00:30
epoch [78/100] batch [5/5] time 0.049 (0.266) data 0.000 (0.207) loss 0.8314 (0.9638) acc 89.0625 (90.3234) lr 2.4989e-04 eta 0:00:29
epoch [79/100] batch [5/5] time 0.054 (0.222) data 0.000 (0.164) loss 0.9614 (1.0264) acc 90.6250 (89.4381) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.062 (0.253) data 0.001 (0.194) loss 0.9265 (0.9944) acc 86.1111 (87.9060) lr 2.0984e-04 eta 0:00:25
epoch [81/100] batch [5/5] time 0.044 (0.215) data 0.000 (0.158) loss 0.7637 (0.9843) acc 80.3571 (88.1730) lr 1.9098e-04 eta 0:00:20
>>> samples [81/160] noisy rate: 0.00 --> 0.29 --> 0.19 <<<
epoch [82/100] batch [5/5] time 0.056 (0.274) data 0.000 (0.217) loss 0.8013 (0.9677) acc 91.1765 (89.6795) lr 1.7292e-04 eta 0:00:24
epoch [83/100] batch [5/5] time 0.058 (0.360) data 0.001 (0.224) loss 1.0980 (0.9717) acc 87.5000 (89.4345) lr 1.5567e-04 eta 0:00:30
epoch [84/100] batch [5/5] time 0.056 (0.270) data 0.000 (0.207) loss 1.0573 (1.0153) acc 85.9375 (87.1597) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.060 (0.220) data 0.000 (0.158) loss 1.0365 (0.9767) acc 89.7059 (87.9071) lr 1.2369e-04 eta 0:00:16
epoch [86/100] batch [5/5] time 0.064 (0.247) data 0.000 (0.188) loss 1.0059 (0.9705) acc 88.1579 (86.2679) lr 1.0899e-04 eta 0:00:17
>>> samples [81/160] noisy rate: 0.00 --> 0.28 --> 0.19 <<<
epoch [87/100] batch [5/5] time 0.052 (0.244) data 0.000 (0.185) loss 0.9887 (0.9791) acc 87.5000 (89.3267) lr 9.5173e-05 eta 0:00:15
epoch [88/100] batch [5/5] time 0.056 (0.256) data 0.000 (0.200) loss 0.9525 (0.9784) acc 84.3750 (90.3750) lr 8.2245e-05 eta 0:00:15
epoch [89/100] batch [5/5] time 0.046 (0.202) data 0.000 (0.146) loss 1.1706 (0.9856) acc 91.0714 (90.9618) lr 7.0224e-05 eta 0:00:11
epoch [90/100] batch [5/5] time 0.048 (0.217) data 0.001 (0.154) loss 0.7588 (0.9653) acc 89.0625 (90.9302) lr 5.9119e-05 eta 0:00:10
epoch [91/100] batch [5/5] time 0.034 (0.226) data 0.000 (0.167) loss 1.2986 (0.9949) acc 82.5000 (89.5193) lr 4.8943e-05 eta 0:00:10
>>> samples [81/160] noisy rate: 0.00 --> 0.29 --> 0.19 <<<
epoch [92/100] batch [5/5] time 0.061 (0.230) data 0.000 (0.168) loss 0.7905 (0.9721) acc 88.2353 (87.3307) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.053 (0.210) data 0.000 (0.149) loss 0.8377 (0.9587) acc 85.7143 (89.2030) lr 3.1417e-05 eta 0:00:07
epoch [94/100] batch [5/5] time 0.051 (0.231) data 0.000 (0.169) loss 1.0571 (1.0014) acc 82.1429 (88.0801) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.056 (0.222) data 0.000 (0.164) loss 0.9898 (0.9838) acc 96.8750 (89.3374) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.051 (0.253) data 0.000 (0.195) loss 1.0011 (0.9594) acc 86.6667 (89.0136) lr 1.2312e-05 eta 0:00:05
>>> samples [82/160] noisy rate: 0.00 --> 0.32 --> 0.18 <<<
epoch [97/100] batch [5/5] time 0.051 (0.219) data 0.000 (0.159) loss 0.9835 (0.9432) acc 91.0714 (91.4087) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.051 (0.252) data 0.000 (0.188) loss 0.9303 (0.9872) acc 76.7857 (89.0667) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.045 (0.219) data 0.000 (0.161) loss 1.0400 (0.9903) acc 85.4167 (86.1338) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.063 (0.234) data 0.000 (0.177) loss 0.8507 (0.9762) acc 88.7500 (91.8125) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_0FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.22, 0.36, 0.24, 0.24, 0.24, 0.27, 0.23, 0.21, 0.22, 0.3, 0.28, 0.29, 0.29, 0.31, 0.29, 0.28, 0.29, 0.28, 0.29, 0.32]
* learned noise rate: [0.0, 0.06, 0.07, 0.07, 0.07, 0.07, 0.08, 0.12, 0.12, 0.15, 0.15, 0.14, 0.14, 0.16, 0.18, 0.18, 0.19, 0.19, 0.19, 0.18]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:34,  1.93s/it]  4%|▎         | 3/81 [00:02<00:42,  1.82it/s]  6%|▌         | 5/81 [00:02<00:22,  3.34it/s]  9%|▊         | 7/81 [00:02<00:14,  5.00it/s] 11%|█         | 9/81 [00:02<00:10,  6.72it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.39it/s] 16%|█▌        | 13/81 [00:02<00:06,  9.87it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.14it/s] 21%|██        | 17/81 [00:02<00:05, 12.14it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.85it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.60it/s] 28%|██▊       | 23/81 [00:03<00:04, 14.16it/s] 31%|███       | 25/81 [00:03<00:03, 14.57it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.45it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.77it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.89it/s] 41%|████      | 33/81 [00:04<00:03, 15.08it/s] 43%|████▎     | 35/81 [00:04<00:03, 14.89it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.09it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.22it/s] 51%|█████     | 41/81 [00:04<00:02, 15.30it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.23it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.94it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.04it/s] 60%|██████    | 49/81 [00:05<00:02, 15.22it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.10it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.25it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.34it/s] 70%|███████   | 57/81 [00:05<00:01, 15.41it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.99it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.18it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.32it/s] 80%|████████  | 65/81 [00:06<00:01, 15.40it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.43it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.46it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.53it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.57it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.61it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.63it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.64it/s]100%|██████████| 81/81 [00:07<00:00, 15.64it/s]100%|██████████| 81/81 [00:07<00:00, 11.11it/s]
=> result
* total: 8,100
* correct: 4,577
* accuracy: 56.5%
* error: 43.5%
* macro_f1: 51.3%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 335	acc: 37.2%
* class: 1 (Forest)	total: 900	correct: 831	acc: 92.3%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 21	acc: 2.3%
* class: 3 (Highway or Road)	total: 750	correct: 524	acc: 69.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 635	acc: 84.7%
* class: 5 (Pasture Land)	total: 600	correct: 351	acc: 58.5%
* class: 6 (Permanent Crop Land)	total: 750	correct: 565	acc: 75.3%
* class: 7 (Residential Buildings)	total: 900	correct: 829	acc: 92.1%
* class: 8 (River)	total: 750	correct: 480	acc: 64.0%
* class: 9 (Sea or Lake)	total: 900	correct: 6	acc: 0.7%
* average: 57.7%
Elapsed: 0:04:10
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_2-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.084 (0.743) data 0.000 (0.234) loss 1.0822 (1.0797) acc 10.9375 (17.0312) lr 2.0000e-03 eta 0:06:07
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [34/160] noisy rate: 0.12 --> 0.22 --> 0.03 <<<
epoch [2/100] batch [5/5] time 0.181 (0.410) data 0.000 (0.205) loss 1.8988 (2.0821) acc 41.6667 (28.1667) lr 1.9995e-03 eta 0:03:20
epoch [3/100] batch [5/5] time 0.282 (0.310) data 0.000 (0.215) loss 2.0262 (1.9310) acc 30.0000 (33.7917) lr 1.9980e-03 eta 0:02:30
epoch [4/100] batch [5/5] time 0.241 (0.302) data 0.000 (0.217) loss 2.1629 (1.9115) acc 17.8571 (39.5298) lr 1.9956e-03 eta 0:02:24
epoch [5/100] batch [5/5] time 0.031 (0.293) data 0.000 (0.255) loss 2.2320 (1.8067) acc 4.1667 (42.7619) lr 1.9921e-03 eta 0:02:18
epoch [6/100] batch [5/5] time 0.331 (0.408) data 0.001 (0.220) loss 1.6133 (1.8262) acc 47.9167 (41.1111) lr 1.9877e-03 eta 0:03:11
>>> samples [48/160] noisy rate: 0.12 --> 0.11 --> 0.02 <<<
epoch [7/100] batch [5/5] time 0.046 (0.356) data 0.000 (0.239) loss 1.5812 (1.6041) acc 50.0000 (47.3761) lr 1.9823e-03 eta 0:02:45
epoch [8/100] batch [5/5] time 0.044 (0.319) data 0.000 (0.269) loss 1.5850 (1.4780) acc 35.4167 (55.0595) lr 1.9759e-03 eta 0:02:26
epoch [9/100] batch [5/5] time 0.044 (0.301) data 0.000 (0.257) loss 1.4311 (1.4134) acc 70.8333 (64.7778) lr 1.9686e-03 eta 0:02:16
epoch [10/100] batch [5/5] time 0.038 (0.360) data 0.000 (0.259) loss 1.3627 (1.3340) acc 50.0000 (61.1086) lr 1.9603e-03 eta 0:02:41
epoch [11/100] batch [5/5] time 0.043 (0.286) data 0.000 (0.235) loss 1.0969 (1.2571) acc 79.1667 (71.5734) lr 1.9511e-03 eta 0:02:07
>>> samples [50/160] noisy rate: 0.12 --> 0.15 --> 0.02 <<<
epoch [12/100] batch [5/5] time 0.047 (0.323) data 0.000 (0.275) loss 1.4426 (1.2168) acc 77.2727 (74.3732) lr 1.9409e-03 eta 0:02:22
epoch [13/100] batch [5/5] time 0.036 (0.288) data 0.001 (0.239) loss 1.1923 (1.1424) acc 67.5000 (72.3961) lr 1.9298e-03 eta 0:02:05
epoch [14/100] batch [5/5] time 0.034 (0.312) data 0.000 (0.264) loss 1.0422 (1.1327) acc 75.0000 (76.4583) lr 1.9178e-03 eta 0:02:13
epoch [15/100] batch [5/5] time 0.040 (0.289) data 0.000 (0.240) loss 1.1640 (1.1532) acc 70.8333 (74.3182) lr 1.9048e-03 eta 0:02:02
epoch [16/100] batch [5/5] time 0.044 (0.324) data 0.000 (0.278) loss 0.9974 (1.0894) acc 72.9167 (77.2601) lr 1.8910e-03 eta 0:02:15
>>> samples [51/160] noisy rate: 0.12 --> 0.17 --> 0.02 <<<
epoch [17/100] batch [5/5] time 0.037 (0.313) data 0.000 (0.264) loss 1.1055 (1.0774) acc 71.8750 (78.9331) lr 1.8763e-03 eta 0:02:09
epoch [18/100] batch [5/5] time 0.042 (0.285) data 0.000 (0.236) loss 1.0419 (1.0673) acc 63.6364 (79.0657) lr 1.8607e-03 eta 0:01:56
epoch [19/100] batch [5/5] time 0.033 (0.294) data 0.000 (0.242) loss 0.8854 (1.0143) acc 87.5000 (83.2652) lr 1.8443e-03 eta 0:01:59
epoch [20/100] batch [5/5] time 0.038 (0.277) data 0.000 (0.228) loss 0.8627 (1.0234) acc 90.0000 (78.5909) lr 1.8271e-03 eta 0:01:50
epoch [21/100] batch [5/5] time 0.035 (0.274) data 0.000 (0.233) loss 1.1746 (1.0621) acc 78.1250 (79.3523) lr 1.8090e-03 eta 0:01:48
>>> samples [51/160] noisy rate: 0.12 --> 0.28 --> 0.02 <<<
epoch [22/100] batch [5/5] time 0.043 (0.268) data 0.000 (0.218) loss 1.2749 (1.0088) acc 66.6667 (81.6667) lr 1.7902e-03 eta 0:01:44
epoch [23/100] batch [5/5] time 0.038 (0.293) data 0.000 (0.246) loss 1.2753 (1.0289) acc 62.5000 (82.5610) lr 1.7705e-03 eta 0:01:52
epoch [24/100] batch [5/5] time 0.043 (0.268) data 0.000 (0.222) loss 1.0375 (1.0329) acc 87.5000 (80.6984) lr 1.7501e-03 eta 0:01:41
epoch [25/100] batch [5/5] time 0.037 (0.309) data 0.001 (0.261) loss 1.2448 (1.0449) acc 75.0000 (80.6061) lr 1.7290e-03 eta 0:01:55
epoch [26/100] batch [5/5] time 0.041 (0.313) data 0.000 (0.268) loss 1.1904 (0.9053) acc 75.0000 (89.0114) lr 1.7071e-03 eta 0:01:55
>>> samples [56/160] noisy rate: 0.12 --> 0.30 --> 0.07 <<<
epoch [27/100] batch [5/5] time 0.036 (0.297) data 0.000 (0.249) loss 0.7228 (0.9576) acc 80.0000 (80.5940) lr 1.6845e-03 eta 0:01:48
epoch [28/100] batch [5/5] time 0.037 (0.290) data 0.000 (0.236) loss 0.7201 (0.8846) acc 93.1818 (85.5274) lr 1.6613e-03 eta 0:01:44
epoch [29/100] batch [5/5] time 0.042 (0.256) data 0.000 (0.203) loss 0.7329 (0.9234) acc 92.3077 (87.2819) lr 1.6374e-03 eta 0:01:30
epoch [30/100] batch [5/5] time 0.039 (0.269) data 0.000 (0.215) loss 1.0488 (0.8791) acc 85.4167 (88.7745) lr 1.6129e-03 eta 0:01:34
epoch [31/100] batch [5/5] time 0.046 (0.280) data 0.000 (0.231) loss 0.9254 (0.9029) acc 80.7692 (86.7264) lr 1.5878e-03 eta 0:01:36
>>> samples [56/160] noisy rate: 0.12 --> 0.36 --> 0.07 <<<
epoch [32/100] batch [5/5] time 0.369 (0.316) data 0.000 (0.201) loss 0.9488 (0.8800) acc 91.6667 (88.2921) lr 1.5621e-03 eta 0:01:47
epoch [33/100] batch [5/5] time 0.046 (0.258) data 0.000 (0.203) loss 0.8160 (0.8434) acc 91.6667 (89.1131) lr 1.5358e-03 eta 0:01:26
epoch [34/100] batch [5/5] time 0.035 (0.271) data 0.000 (0.223) loss 0.7392 (0.8246) acc 97.5000 (90.8619) lr 1.5090e-03 eta 0:01:29
epoch [35/100] batch [5/5] time 0.039 (0.253) data 0.000 (0.202) loss 0.7896 (0.8899) acc 84.0909 (86.7955) lr 1.4818e-03 eta 0:01:22
epoch [36/100] batch [5/5] time 0.031 (0.269) data 0.000 (0.225) loss 0.7751 (0.8790) acc 90.6250 (88.5038) lr 1.4540e-03 eta 0:01:26
>>> samples [64/160] noisy rate: 0.12 --> 0.36 --> 0.16 <<<
epoch [37/100] batch [5/5] time 0.038 (0.244) data 0.000 (0.190) loss 0.6902 (0.8388) acc 90.0000 (88.6136) lr 1.4258e-03 eta 0:01:16
epoch [38/100] batch [5/5] time 0.044 (0.247) data 0.000 (0.197) loss 0.8490 (0.8612) acc 87.5000 (87.2698) lr 1.3971e-03 eta 0:01:16
epoch [39/100] batch [5/5] time 0.045 (0.233) data 0.000 (0.184) loss 0.8039 (0.8353) acc 93.7500 (90.4258) lr 1.3681e-03 eta 0:01:11
epoch [40/100] batch [5/5] time 0.043 (0.353) data 0.000 (0.226) loss 0.6975 (0.8909) acc 94.2308 (86.2285) lr 1.3387e-03 eta 0:01:45
epoch [41/100] batch [5/5] time 0.035 (0.306) data 0.000 (0.191) loss 0.6455 (0.8492) acc 90.0000 (86.3814) lr 1.3090e-03 eta 0:01:30
>>> samples [65/160] noisy rate: 0.12 --> 0.37 --> 0.17 <<<
epoch [42/100] batch [5/5] time 0.042 (0.226) data 0.000 (0.174) loss 0.7406 (0.8533) acc 96.1538 (88.1612) lr 1.2790e-03 eta 0:01:05
epoch [43/100] batch [5/5] time 0.049 (0.264) data 0.000 (0.211) loss 0.6725 (0.8348) acc 92.3077 (88.9819) lr 1.2487e-03 eta 0:01:15
epoch [44/100] batch [5/5] time 0.041 (0.262) data 0.000 (0.212) loss 0.9516 (0.8401) acc 81.8182 (89.1017) lr 1.2181e-03 eta 0:01:13
epoch [45/100] batch [5/5] time 0.048 (0.254) data 0.000 (0.206) loss 0.8588 (0.8522) acc 90.3846 (88.4162) lr 1.1874e-03 eta 0:01:09
epoch [46/100] batch [5/5] time 0.044 (0.239) data 0.000 (0.185) loss 0.5984 (0.8707) acc 93.1818 (84.8809) lr 1.1564e-03 eta 0:01:04
>>> samples [66/160] noisy rate: 0.12 --> 0.36 --> 0.18 <<<
epoch [47/100] batch [5/5] time 0.050 (0.288) data 0.000 (0.236) loss 0.9984 (0.8484) acc 88.3333 (90.5188) lr 1.1253e-03 eta 0:01:16
epoch [48/100] batch [5/5] time 0.045 (0.283) data 0.000 (0.236) loss 0.9823 (0.8475) acc 89.2857 (87.7225) lr 1.0941e-03 eta 0:01:13
epoch [49/100] batch [5/5] time 0.052 (0.284) data 0.000 (0.232) loss 0.8216 (0.8545) acc 88.3333 (88.0671) lr 1.0628e-03 eta 0:01:12
epoch [50/100] batch [5/5] time 0.051 (0.280) data 0.000 (0.226) loss 0.8668 (0.8042) acc 96.6667 (92.2251) lr 1.0314e-03 eta 0:01:09
epoch [51/100] batch [5/5] time 0.037 (0.295) data 0.000 (0.246) loss 0.5075 (0.7572) acc 94.4444 (91.5470) lr 1.0000e-03 eta 0:01:12
>>> samples [67/160] noisy rate: 0.12 --> 0.36 --> 0.19 <<<
epoch [52/100] batch [5/5] time 0.045 (0.321) data 0.000 (0.261) loss 0.6867 (0.8349) acc 91.0714 (88.4821) lr 9.6859e-04 eta 0:01:17
epoch [53/100] batch [5/5] time 0.048 (0.315) data 0.000 (0.260) loss 0.7921 (0.8517) acc 93.7500 (85.7348) lr 9.3721e-04 eta 0:01:14
epoch [54/100] batch [5/5] time 0.037 (0.359) data 0.000 (0.234) loss 0.8659 (0.8798) acc 88.8889 (87.1074) lr 9.0589e-04 eta 0:01:22
epoch [55/100] batch [5/5] time 0.044 (0.278) data 0.000 (0.226) loss 0.6853 (0.8436) acc 94.6429 (88.4432) lr 8.7467e-04 eta 0:01:02
epoch [56/100] batch [5/5] time 0.043 (0.261) data 0.000 (0.212) loss 1.0850 (0.8779) acc 89.2857 (88.3333) lr 8.4357e-04 eta 0:00:57
>>> samples [67/160] noisy rate: 0.12 --> 0.36 --> 0.19 <<<
epoch [57/100] batch [5/5] time 0.046 (0.259) data 0.000 (0.203) loss 0.7137 (0.7889) acc 88.4615 (90.8388) lr 8.1262e-04 eta 0:00:55
epoch [58/100] batch [5/5] time 0.051 (0.326) data 0.000 (0.265) loss 0.6679 (0.8719) acc 91.0714 (86.4394) lr 7.8186e-04 eta 0:01:08
epoch [59/100] batch [5/5] time 0.051 (0.284) data 0.000 (0.228) loss 0.9888 (0.8673) acc 80.0000 (88.2589) lr 7.5131e-04 eta 0:00:58
epoch [60/100] batch [5/5] time 0.038 (0.283) data 0.000 (0.228) loss 0.9825 (0.8407) acc 86.3636 (88.8770) lr 7.2101e-04 eta 0:00:56
epoch [61/100] batch [5/5] time 0.415 (0.314) data 0.000 (0.185) loss 0.9240 (0.7842) acc 92.5000 (91.3485) lr 6.9098e-04 eta 0:01:01
>>> samples [67/160] noisy rate: 0.12 --> 0.37 --> 0.19 <<<
epoch [62/100] batch [5/5] time 0.037 (0.258) data 0.000 (0.209) loss 0.9854 (0.8454) acc 96.4286 (90.7002) lr 6.6126e-04 eta 0:00:49
epoch [63/100] batch [5/5] time 0.062 (0.299) data 0.000 (0.248) loss 0.7766 (0.8569) acc 90.0000 (89.7902) lr 6.3188e-04 eta 0:00:55
epoch [64/100] batch [5/5] time 0.051 (0.260) data 0.000 (0.205) loss 0.8356 (0.8226) acc 92.6471 (90.7217) lr 6.0285e-04 eta 0:00:46
epoch [65/100] batch [5/5] time 0.034 (0.353) data 0.000 (0.227) loss 0.7396 (0.7747) acc 100.0000 (92.3392) lr 5.7422e-04 eta 0:01:01
epoch [66/100] batch [5/5] time 0.034 (0.270) data 0.000 (0.216) loss 0.8365 (0.7836) acc 85.0000 (91.3371) lr 5.4601e-04 eta 0:00:45
>>> samples [67/160] noisy rate: 0.12 --> 0.37 --> 0.19 <<<
epoch [67/100] batch [5/5] time 0.045 (0.298) data 0.000 (0.248) loss 0.7641 (0.8279) acc 92.3077 (90.5092) lr 5.1825e-04 eta 0:00:49
epoch [68/100] batch [5/5] time 0.048 (0.265) data 0.000 (0.213) loss 0.8611 (0.8459) acc 92.8571 (92.7251) lr 4.9096e-04 eta 0:00:42
epoch [69/100] batch [5/5] time 0.042 (0.275) data 0.000 (0.221) loss 0.9935 (0.8008) acc 94.2308 (91.8462) lr 4.6417e-04 eta 0:00:42
epoch [70/100] batch [5/5] time 0.454 (0.322) data 0.000 (0.183) loss 1.0160 (0.8475) acc 94.0476 (87.3544) lr 4.3792e-04 eta 0:00:48
epoch [71/100] batch [5/5] time 0.047 (0.315) data 0.000 (0.262) loss 1.0215 (0.8162) acc 89.2857 (90.5184) lr 4.1221e-04 eta 0:00:45
>>> samples [67/160] noisy rate: 0.12 --> 0.38 --> 0.19 <<<
epoch [72/100] batch [5/5] time 0.057 (0.293) data 0.000 (0.240) loss 0.8514 (0.8312) acc 90.6250 (87.7557) lr 3.8709e-04 eta 0:00:41
epoch [73/100] batch [5/5] time 0.051 (0.289) data 0.000 (0.230) loss 0.8419 (0.8515) acc 87.5000 (86.9142) lr 3.6258e-04 eta 0:00:38
epoch [74/100] batch [5/5] time 0.042 (0.275) data 0.000 (0.222) loss 0.8673 (0.8071) acc 90.3846 (89.1613) lr 3.3869e-04 eta 0:00:35
epoch [75/100] batch [5/5] time 0.040 (0.315) data 0.000 (0.262) loss 0.9971 (0.8347) acc 85.4167 (90.2411) lr 3.1545e-04 eta 0:00:39
epoch [76/100] batch [5/5] time 0.049 (0.287) data 0.000 (0.234) loss 1.0325 (0.8525) acc 82.8125 (88.7461) lr 2.9289e-04 eta 0:00:34
>>> samples [67/160] noisy rate: 0.12 --> 0.39 --> 0.19 <<<
epoch [77/100] batch [5/5] time 0.053 (0.260) data 0.000 (0.210) loss 0.6498 (0.8644) acc 93.7500 (86.6921) lr 2.7103e-04 eta 0:00:29
epoch [78/100] batch [5/5] time 0.043 (0.290) data 0.001 (0.238) loss 0.5187 (0.8373) acc 95.4545 (86.5909) lr 2.4989e-04 eta 0:00:31
epoch [79/100] batch [5/5] time 0.044 (0.269) data 0.000 (0.210) loss 0.7424 (0.7969) acc 91.6667 (87.7324) lr 2.2949e-04 eta 0:00:28
epoch [80/100] batch [5/5] time 0.045 (0.269) data 0.000 (0.218) loss 0.7691 (0.8329) acc 97.9167 (88.4306) lr 2.0984e-04 eta 0:00:26
epoch [81/100] batch [5/5] time 0.042 (0.280) data 0.000 (0.220) loss 0.7533 (0.7879) acc 95.4545 (89.8386) lr 1.9098e-04 eta 0:00:26
>>> samples [69/160] noisy rate: 0.12 --> 0.36 --> 0.20 <<<
epoch [82/100] batch [5/5] time 0.052 (0.309) data 0.000 (0.254) loss 0.9520 (0.8232) acc 95.5882 (91.0792) lr 1.7292e-04 eta 0:00:27
epoch [83/100] batch [5/5] time 0.042 (0.258) data 0.000 (0.202) loss 0.8904 (0.8341) acc 94.2308 (93.4829) lr 1.5567e-04 eta 0:00:21
epoch [84/100] batch [5/5] time 0.041 (0.283) data 0.000 (0.233) loss 0.7950 (0.8552) acc 92.3077 (87.1992) lr 1.3926e-04 eta 0:00:22
epoch [85/100] batch [5/5] time 0.049 (0.274) data 0.000 (0.221) loss 1.0491 (0.8696) acc 86.6667 (87.0887) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.040 (0.309) data 0.000 (0.255) loss 0.7654 (0.8119) acc 85.4167 (89.2150) lr 1.0899e-04 eta 0:00:21
>>> samples [70/160] noisy rate: 0.12 --> 0.34 --> 0.21 <<<
epoch [87/100] batch [5/5] time 0.040 (0.297) data 0.000 (0.248) loss 1.0014 (0.8624) acc 89.5833 (90.6310) lr 9.5173e-05 eta 0:00:19
epoch [88/100] batch [5/5] time 0.057 (0.322) data 0.000 (0.265) loss 0.8657 (0.7885) acc 91.6667 (89.5735) lr 8.2245e-05 eta 0:00:19
epoch [89/100] batch [5/5] time 0.055 (0.309) data 0.000 (0.254) loss 0.9355 (0.8438) acc 89.7059 (88.8459) lr 7.0224e-05 eta 0:00:16
epoch [90/100] batch [5/5] time 0.057 (0.327) data 0.000 (0.278) loss 0.6586 (0.8304) acc 95.5882 (91.0154) lr 5.9119e-05 eta 0:00:16
epoch [91/100] batch [5/5] time 0.050 (0.310) data 0.000 (0.262) loss 0.7584 (0.8022) acc 86.6667 (92.7281) lr 4.8943e-05 eta 0:00:13
>>> samples [70/160] noisy rate: 0.12 --> 0.33 --> 0.21 <<<
epoch [92/100] batch [5/5] time 0.048 (0.324) data 0.000 (0.270) loss 0.7480 (0.8368) acc 89.2857 (90.1564) lr 3.9706e-05 eta 0:00:12
epoch [93/100] batch [5/5] time 0.054 (0.234) data 0.000 (0.186) loss 0.9640 (0.7773) acc 91.6667 (91.5985) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.043 (0.268) data 0.000 (0.214) loss 0.8005 (0.8246) acc 94.6429 (90.9817) lr 2.4083e-05 eta 0:00:08
epoch [95/100] batch [5/5] time 0.049 (0.269) data 0.000 (0.214) loss 0.9786 (0.8305) acc 90.3846 (90.7854) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.048 (0.279) data 0.000 (0.227) loss 0.8447 (0.8754) acc 90.6250 (90.5552) lr 1.2312e-05 eta 0:00:05
>>> samples [70/160] noisy rate: 0.12 --> 0.34 --> 0.21 <<<
epoch [97/100] batch [5/5] time 0.052 (0.329) data 0.000 (0.273) loss 0.8346 (0.8434) acc 89.7059 (90.0516) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.048 (0.268) data 0.000 (0.212) loss 0.8817 (0.8272) acc 85.7143 (90.2760) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.051 (0.265) data 0.000 (0.207) loss 0.8814 (0.8291) acc 91.6667 (93.8163) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.039 (0.341) data 0.000 (0.292) loss 0.8411 (0.8058) acc 89.5833 (91.0805) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.22, 0.11, 0.15, 0.17, 0.28, 0.3, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.37, 0.38, 0.39, 0.36, 0.34, 0.33, 0.34]
* learned noise rate: [0.03, 0.02, 0.02, 0.02, 0.02, 0.07, 0.07, 0.16, 0.17, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.2, 0.21, 0.21, 0.21]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<02:53,  2.17s/it]  4%|▎         | 3/81 [00:02<00:47,  1.64it/s]  6%|▌         | 5/81 [00:02<00:25,  3.04it/s]  9%|▊         | 7/81 [00:02<00:16,  4.55it/s] 11%|█         | 9/81 [00:02<00:11,  6.10it/s] 14%|█▎        | 11/81 [00:02<00:09,  7.70it/s] 16%|█▌        | 13/81 [00:02<00:07,  9.27it/s] 19%|█▊        | 15/81 [00:03<00:06, 10.58it/s] 21%|██        | 17/81 [00:03<00:05, 11.78it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.77it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.57it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.83it/s] 31%|███       | 25/81 [00:03<00:04, 13.89it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.45it/s] 36%|███▌      | 29/81 [00:04<00:03, 14.77it/s] 38%|███▊      | 31/81 [00:04<00:03, 14.67it/s] 41%|████      | 33/81 [00:04<00:03, 14.48it/s] 43%|████▎     | 35/81 [00:04<00:03, 14.51it/s] 46%|████▌     | 37/81 [00:04<00:02, 14.90it/s] 48%|████▊     | 39/81 [00:04<00:02, 14.93it/s] 51%|█████     | 41/81 [00:04<00:02, 15.19it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.33it/s] 56%|█████▌    | 45/81 [00:05<00:02, 15.29it/s] 58%|█████▊    | 47/81 [00:05<00:02, 15.45it/s] 60%|██████    | 49/81 [00:05<00:02, 15.60it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.66it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.72it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.12it/s] 70%|███████   | 57/81 [00:05<00:01, 15.32it/s] 73%|███████▎  | 59/81 [00:06<00:01, 14.72it/s] 75%|███████▌  | 61/81 [00:06<00:01, 15.04it/s] 78%|███████▊  | 63/81 [00:06<00:01, 15.30it/s] 80%|████████  | 65/81 [00:06<00:01, 15.01it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.11it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.36it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.57it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.72it/s] 93%|█████████▎| 75/81 [00:07<00:00, 15.82it/s] 95%|█████████▌| 77/81 [00:07<00:00, 15.88it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.94it/s]100%|██████████| 81/81 [00:07<00:00, 15.97it/s]100%|██████████| 81/81 [00:07<00:00, 10.71it/s]
=> result
* total: 8,100
* correct: 4,224
* accuracy: 52.1%
* error: 47.9%
* macro_f1: 48.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 29	acc: 3.2%
* class: 1 (Forest)	total: 900	correct: 862	acc: 95.8%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 1	acc: 0.1%
* class: 3 (Highway or Road)	total: 750	correct: 466	acc: 62.1%
* class: 4 (Industrial Buildings)	total: 750	correct: 735	acc: 98.0%
* class: 5 (Pasture Land)	total: 600	correct: 291	acc: 48.5%
* class: 6 (Permanent Crop Land)	total: 750	correct: 641	acc: 85.5%
* class: 7 (Residential Buildings)	total: 900	correct: 593	acc: 65.9%
* class: 8 (River)	total: 750	correct: 309	acc: 41.2%
* class: 9 (Sea or Lake)	total: 900	correct: 297	acc: 33.0%
* average: 53.3%
Elapsed: 0:04:49
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_2-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.087 (0.618) data 0.000 (0.219) loss 1.1291 (1.0794) acc 16.4062 (18.7500) lr 2.0000e-03 eta 0:05:06
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [48/160] noisy rate: 0.12 --> 0.28 --> 0.06 <<<
epoch [2/100] batch [5/5] time 0.312 (0.455) data 0.001 (0.206) loss 2.0381 (2.1747) acc 31.8182 (20.1713) lr 1.9995e-03 eta 0:03:42
epoch [3/100] batch [5/5] time 0.323 (0.264) data 0.000 (0.158) loss 2.0175 (2.0954) acc 41.6667 (32.0281) lr 1.9980e-03 eta 0:02:07
epoch [4/100] batch [5/5] time 0.043 (0.320) data 0.000 (0.181) loss 2.0519 (2.0689) acc 33.3333 (30.1535) lr 1.9956e-03 eta 0:02:33
epoch [5/100] batch [5/5] time 0.036 (0.237) data 0.000 (0.190) loss 2.1802 (2.0635) acc 35.0000 (32.6048) lr 1.9921e-03 eta 0:01:52
epoch [6/100] batch [5/5] time 0.036 (0.208) data 0.000 (0.153) loss 1.9694 (1.9826) acc 40.6250 (34.7917) lr 1.9877e-03 eta 0:01:37
>>> samples [49/160] noisy rate: 0.12 --> 0.30 --> 0.06 <<<
epoch [7/100] batch [5/5] time 0.052 (0.242) data 0.000 (0.192) loss 2.1059 (1.9449) acc 30.7692 (37.0440) lr 1.9823e-03 eta 0:01:52
epoch [8/100] batch [5/5] time 0.048 (0.210) data 0.000 (0.160) loss 1.6633 (1.9657) acc 59.6154 (41.8590) lr 1.9759e-03 eta 0:01:36
epoch [9/100] batch [5/5] time 0.041 (0.265) data 0.000 (0.152) loss 1.7731 (1.8583) acc 70.0000 (48.7321) lr 1.9686e-03 eta 0:02:00
epoch [10/100] batch [5/5] time 0.037 (0.247) data 0.000 (0.198) loss 1.6113 (1.8067) acc 65.6250 (54.3876) lr 1.9603e-03 eta 0:01:51
epoch [11/100] batch [5/5] time 0.030 (0.226) data 0.000 (0.176) loss 1.6678 (1.7975) acc 59.3750 (56.7614) lr 1.9511e-03 eta 0:01:40
>>> samples [50/160] noisy rate: 0.12 --> 0.26 --> 0.06 <<<
epoch [12/100] batch [5/5] time 0.202 (0.241) data 0.000 (0.156) loss 1.6574 (1.7638) acc 55.0000 (61.1591) lr 1.9409e-03 eta 0:01:46
epoch [13/100] batch [5/5] time 0.032 (0.196) data 0.000 (0.154) loss 1.9450 (1.8127) acc 47.2222 (51.1468) lr 1.9298e-03 eta 0:01:25
epoch [14/100] batch [5/5] time 0.058 (0.320) data 0.001 (0.243) loss 1.7818 (1.7088) acc 55.7692 (64.7729) lr 1.9178e-03 eta 0:02:17
epoch [15/100] batch [5/5] time 0.032 (0.330) data 0.000 (0.228) loss 1.6333 (1.6822) acc 61.1111 (63.2778) lr 1.9048e-03 eta 0:02:20
epoch [16/100] batch [5/5] time 0.380 (0.327) data 0.000 (0.205) loss 1.7743 (1.7088) acc 57.8125 (66.0069) lr 1.8910e-03 eta 0:02:17
>>> samples [52/160] noisy rate: 0.12 --> 0.12 --> 0.06 <<<
epoch [17/100] batch [5/5] time 0.036 (0.211) data 0.000 (0.163) loss 1.7504 (1.6094) acc 61.1111 (70.1485) lr 1.8763e-03 eta 0:01:27
epoch [18/100] batch [5/5] time 0.036 (0.231) data 0.000 (0.184) loss 1.5424 (1.5993) acc 75.0000 (69.2917) lr 1.8607e-03 eta 0:01:34
epoch [19/100] batch [5/5] time 0.034 (0.213) data 0.001 (0.161) loss 1.4161 (1.5425) acc 77.7778 (77.3207) lr 1.8443e-03 eta 0:01:26
epoch [20/100] batch [5/5] time 0.037 (0.220) data 0.000 (0.175) loss 1.6243 (1.5923) acc 77.5000 (69.2500) lr 1.8271e-03 eta 0:01:27
epoch [21/100] batch [5/5] time 0.040 (0.267) data 0.001 (0.221) loss 1.4560 (1.6098) acc 82.5000 (73.9924) lr 1.8090e-03 eta 0:01:45
>>> samples [53/160] noisy rate: 0.12 --> 0.11 --> 0.06 <<<
epoch [22/100] batch [5/5] time 0.035 (0.268) data 0.000 (0.218) loss 1.3713 (1.5059) acc 75.0000 (69.9936) lr 1.7902e-03 eta 0:01:44
epoch [23/100] batch [5/5] time 0.032 (0.212) data 0.000 (0.168) loss 1.4527 (1.5127) acc 82.1429 (75.2316) lr 1.7705e-03 eta 0:01:21
epoch [24/100] batch [5/5] time 0.056 (0.225) data 0.000 (0.176) loss 1.4346 (1.5018) acc 76.5625 (84.4738) lr 1.7501e-03 eta 0:01:25
epoch [25/100] batch [5/5] time 0.050 (0.277) data 0.000 (0.234) loss 1.3132 (1.4856) acc 81.2500 (75.5884) lr 1.7290e-03 eta 0:01:43
epoch [26/100] batch [5/5] time 0.034 (0.251) data 0.000 (0.194) loss 1.4351 (1.4287) acc 78.1250 (83.5577) lr 1.7071e-03 eta 0:01:32
>>> samples [54/160] noisy rate: 0.12 --> 0.20 --> 0.06 <<<
epoch [27/100] batch [5/5] time 0.046 (0.224) data 0.000 (0.175) loss 1.3492 (1.4530) acc 80.0000 (83.0000) lr 1.6845e-03 eta 0:01:21
epoch [28/100] batch [5/5] time 0.039 (0.265) data 0.000 (0.224) loss 1.2879 (1.4501) acc 87.5000 (79.2308) lr 1.6613e-03 eta 0:01:35
epoch [29/100] batch [5/5] time 0.032 (0.234) data 0.001 (0.185) loss 1.5722 (1.4376) acc 66.6667 (81.4087) lr 1.6374e-03 eta 0:01:22
epoch [30/100] batch [5/5] time 0.037 (0.206) data 0.000 (0.157) loss 1.2949 (1.3910) acc 80.0000 (84.4834) lr 1.6129e-03 eta 0:01:11
epoch [31/100] batch [5/5] time 0.049 (0.342) data 0.000 (0.225) loss 1.7510 (1.4199) acc 70.4545 (82.7020) lr 1.5878e-03 eta 0:01:58
>>> samples [55/160] noisy rate: 0.12 --> 0.21 --> 0.05 <<<
epoch [32/100] batch [5/5] time 0.035 (0.253) data 0.000 (0.203) loss 1.1496 (1.3704) acc 81.2500 (81.6261) lr 1.5621e-03 eta 0:01:25
epoch [33/100] batch [5/5] time 0.037 (0.220) data 0.000 (0.169) loss 1.1502 (1.3845) acc 88.8889 (81.1111) lr 1.5358e-03 eta 0:01:13
epoch [34/100] batch [5/5] time 0.036 (0.215) data 0.000 (0.168) loss 1.1247 (1.3262) acc 80.5555 (89.5527) lr 1.5090e-03 eta 0:01:10
epoch [35/100] batch [5/5] time 0.044 (0.264) data 0.000 (0.213) loss 1.3989 (1.3828) acc 87.5000 (85.0641) lr 1.4818e-03 eta 0:01:25
epoch [36/100] batch [5/5] time 0.035 (0.221) data 0.000 (0.170) loss 1.6861 (1.3777) acc 81.2500 (86.5530) lr 1.4540e-03 eta 0:01:10
>>> samples [58/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [37/100] batch [5/5] time 0.050 (0.239) data 0.000 (0.188) loss 1.3126 (1.3509) acc 80.3571 (81.5179) lr 1.4258e-03 eta 0:01:15
epoch [38/100] batch [5/5] time 0.042 (0.214) data 0.000 (0.164) loss 1.4423 (1.3592) acc 86.5385 (86.6886) lr 1.3971e-03 eta 0:01:06
epoch [39/100] batch [5/5] time 0.047 (0.214) data 0.000 (0.168) loss 1.5009 (1.3633) acc 90.3846 (82.8212) lr 1.3681e-03 eta 0:01:05
epoch [40/100] batch [5/5] time 0.047 (0.227) data 0.000 (0.173) loss 1.1396 (1.3952) acc 85.0000 (81.8504) lr 1.3387e-03 eta 0:01:08
epoch [41/100] batch [5/5] time 0.034 (0.234) data 0.000 (0.187) loss 1.4165 (1.3332) acc 92.5000 (87.2143) lr 1.3090e-03 eta 0:01:09
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [42/100] batch [5/5] time 0.043 (0.269) data 0.000 (0.219) loss 1.5213 (1.3284) acc 84.0909 (86.5278) lr 1.2790e-03 eta 0:01:18
epoch [43/100] batch [5/5] time 0.039 (0.233) data 0.000 (0.178) loss 1.2320 (1.3224) acc 79.5455 (85.0639) lr 1.2487e-03 eta 0:01:06
epoch [44/100] batch [5/5] time 0.043 (0.224) data 0.001 (0.177) loss 1.4792 (1.3259) acc 84.6154 (87.0620) lr 1.2181e-03 eta 0:01:02
epoch [45/100] batch [5/5] time 0.032 (0.213) data 0.000 (0.161) loss 1.3232 (1.3083) acc 83.3333 (85.4354) lr 1.1874e-03 eta 0:00:58
epoch [46/100] batch [5/5] time 0.038 (0.262) data 0.000 (0.213) loss 1.4640 (1.3302) acc 88.8889 (86.4481) lr 1.1564e-03 eta 0:01:10
>>> samples [59/160] noisy rate: 0.12 --> 0.21 --> 0.07 <<<
epoch [47/100] batch [5/5] time 0.041 (0.225) data 0.000 (0.178) loss 1.3356 (1.3201) acc 88.6364 (87.1270) lr 1.1253e-03 eta 0:00:59
epoch [48/100] batch [5/5] time 0.034 (0.219) data 0.000 (0.172) loss 1.4092 (1.3257) acc 84.3750 (88.2242) lr 1.0941e-03 eta 0:00:56
epoch [49/100] batch [5/5] time 0.035 (0.241) data 0.001 (0.196) loss 1.2199 (1.3208) acc 90.6250 (84.3039) lr 1.0628e-03 eta 0:01:01
epoch [50/100] batch [5/5] time 0.046 (0.218) data 0.000 (0.165) loss 1.1925 (1.3297) acc 83.9286 (86.1120) lr 1.0314e-03 eta 0:00:54
epoch [51/100] batch [5/5] time 0.027 (0.203) data 0.000 (0.156) loss 1.2837 (1.2716) acc 85.7143 (90.4697) lr 1.0000e-03 eta 0:00:49
>>> samples [59/160] noisy rate: 0.12 --> 0.21 --> 0.07 <<<
epoch [52/100] batch [5/5] time 0.038 (0.251) data 0.000 (0.198) loss 1.4789 (1.3196) acc 77.2727 (84.5747) lr 9.6859e-04 eta 0:01:00
epoch [53/100] batch [5/5] time 0.046 (0.222) data 0.000 (0.173) loss 1.1275 (1.2739) acc 88.4615 (89.8584) lr 9.3721e-04 eta 0:00:52
epoch [54/100] batch [5/5] time 0.049 (0.233) data 0.000 (0.183) loss 1.6218 (1.2586) acc 83.9286 (90.3052) lr 9.0589e-04 eta 0:00:53
epoch [55/100] batch [5/5] time 0.045 (0.289) data 0.000 (0.158) loss 1.1957 (1.3092) acc 93.1818 (86.1230) lr 8.7467e-04 eta 0:01:05
epoch [56/100] batch [5/5] time 0.047 (0.234) data 0.000 (0.178) loss 1.0645 (1.3182) acc 89.5833 (84.9470) lr 8.4357e-04 eta 0:00:51
>>> samples [59/160] noisy rate: 0.12 --> 0.21 --> 0.07 <<<
epoch [57/100] batch [5/5] time 0.041 (0.231) data 0.000 (0.172) loss 1.4347 (1.2724) acc 85.0000 (89.0942) lr 8.1262e-04 eta 0:00:49
epoch [58/100] batch [5/5] time 0.052 (0.234) data 0.000 (0.184) loss 1.1803 (1.2877) acc 86.6667 (88.5509) lr 7.8186e-04 eta 0:00:49
epoch [59/100] batch [5/5] time 0.048 (0.216) data 0.000 (0.166) loss 1.3390 (1.2560) acc 86.5385 (89.8970) lr 7.5131e-04 eta 0:00:44
epoch [60/100] batch [5/5] time 0.035 (0.223) data 0.000 (0.177) loss 1.2812 (1.2351) acc 87.5000 (88.4499) lr 7.2101e-04 eta 0:00:44
epoch [61/100] batch [5/5] time 0.059 (0.214) data 0.000 (0.164) loss 1.2705 (1.3342) acc 80.5555 (85.4907) lr 6.9098e-04 eta 0:00:41
>>> samples [59/160] noisy rate: 0.12 --> 0.18 --> 0.07 <<<
epoch [62/100] batch [5/5] time 0.047 (0.278) data 0.000 (0.232) loss 1.0222 (1.2183) acc 91.6667 (94.1458) lr 6.6126e-04 eta 0:00:52
epoch [63/100] batch [5/5] time 0.034 (0.255) data 0.000 (0.204) loss 0.9241 (1.2801) acc 92.5000 (87.9487) lr 6.3188e-04 eta 0:00:47
epoch [64/100] batch [5/5] time 0.043 (0.214) data 0.000 (0.161) loss 1.0759 (1.2304) acc 88.6364 (87.8093) lr 6.0285e-04 eta 0:00:38
epoch [65/100] batch [5/5] time 0.047 (0.226) data 0.000 (0.172) loss 1.2802 (1.2518) acc 86.3636 (88.6985) lr 5.7422e-04 eta 0:00:39
epoch [66/100] batch [5/5] time 0.050 (0.229) data 0.000 (0.178) loss 1.2053 (1.2715) acc 82.1429 (90.0839) lr 5.4601e-04 eta 0:00:38
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [67/100] batch [5/5] time 0.052 (0.245) data 0.000 (0.186) loss 1.4521 (1.2707) acc 91.6667 (90.4517) lr 5.1825e-04 eta 0:00:40
epoch [68/100] batch [5/5] time 0.048 (0.253) data 0.000 (0.197) loss 1.0249 (1.2628) acc 94.2308 (91.3739) lr 4.9096e-04 eta 0:00:40
epoch [69/100] batch [5/5] time 0.051 (0.247) data 0.000 (0.195) loss 1.2689 (1.2758) acc 82.6923 (85.5320) lr 4.6417e-04 eta 0:00:38
epoch [70/100] batch [5/5] time 0.045 (0.253) data 0.000 (0.205) loss 1.4164 (1.2797) acc 86.3636 (86.2791) lr 4.3792e-04 eta 0:00:37
epoch [71/100] batch [5/5] time 0.044 (0.214) data 0.000 (0.163) loss 1.0619 (1.2852) acc 97.7273 (90.3206) lr 4.1221e-04 eta 0:00:30
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [72/100] batch [5/5] time 0.040 (0.243) data 0.001 (0.192) loss 1.2074 (1.2624) acc 97.5000 (89.2987) lr 3.8709e-04 eta 0:00:34
epoch [73/100] batch [5/5] time 0.047 (0.220) data 0.000 (0.174) loss 1.0790 (1.2426) acc 98.0769 (91.0261) lr 3.6258e-04 eta 0:00:29
epoch [74/100] batch [5/5] time 0.030 (0.265) data 0.000 (0.219) loss 1.0970 (1.2028) acc 90.6250 (91.6026) lr 3.3869e-04 eta 0:00:34
epoch [75/100] batch [5/5] time 0.058 (0.276) data 0.000 (0.229) loss 1.0491 (1.2568) acc 94.1176 (90.7571) lr 3.1545e-04 eta 0:00:34
epoch [76/100] batch [5/5] time 0.046 (0.245) data 0.001 (0.191) loss 1.4700 (1.2795) acc 87.5000 (86.4167) lr 2.9289e-04 eta 0:00:29
>>> samples [59/160] noisy rate: 0.12 --> 0.17 --> 0.07 <<<
epoch [77/100] batch [5/5] time 0.045 (0.220) data 0.000 (0.172) loss 1.4169 (1.2123) acc 83.9286 (86.1595) lr 2.7103e-04 eta 0:00:25
epoch [78/100] batch [5/5] time 0.029 (0.225) data 0.000 (0.173) loss 1.0731 (1.2185) acc 89.2857 (93.1269) lr 2.4989e-04 eta 0:00:24
epoch [79/100] batch [5/5] time 0.039 (0.239) data 0.000 (0.188) loss 1.1507 (1.2549) acc 93.1818 (90.9280) lr 2.2949e-04 eta 0:00:25
epoch [80/100] batch [5/5] time 0.042 (0.246) data 0.000 (0.196) loss 1.1508 (1.2492) acc 96.1538 (88.9103) lr 2.0984e-04 eta 0:00:24
epoch [81/100] batch [5/5] time 0.056 (0.263) data 0.000 (0.217) loss 1.2306 (1.2156) acc 90.6250 (91.0873) lr 1.9098e-04 eta 0:00:24
>>> samples [59/160] noisy rate: 0.12 --> 0.17 --> 0.07 <<<
epoch [82/100] batch [5/5] time 0.047 (0.208) data 0.000 (0.156) loss 1.5075 (1.2188) acc 94.2308 (87.1623) lr 1.7292e-04 eta 0:00:18
epoch [83/100] batch [5/5] time 0.049 (0.224) data 0.000 (0.173) loss 1.4871 (1.2289) acc 89.2857 (91.0673) lr 1.5567e-04 eta 0:00:19
epoch [84/100] batch [5/5] time 0.044 (0.239) data 0.000 (0.193) loss 1.1061 (1.2415) acc 92.8571 (88.4341) lr 1.3926e-04 eta 0:00:19
epoch [85/100] batch [5/5] time 0.043 (0.294) data 0.000 (0.251) loss 1.1823 (1.2366) acc 95.8333 (93.1944) lr 1.2369e-04 eta 0:00:22
epoch [86/100] batch [5/5] time 0.049 (0.239) data 0.000 (0.187) loss 1.1053 (1.2346) acc 87.5000 (88.2273) lr 1.0899e-04 eta 0:00:16
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [87/100] batch [5/5] time 0.047 (0.217) data 0.000 (0.169) loss 1.0147 (1.2238) acc 96.1538 (90.0000) lr 9.5173e-05 eta 0:00:14
epoch [88/100] batch [5/5] time 0.038 (0.242) data 0.000 (0.197) loss 1.2516 (1.2506) acc 88.6364 (90.9034) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.041 (0.224) data 0.000 (0.179) loss 1.1645 (1.2173) acc 93.7500 (91.9615) lr 7.0224e-05 eta 0:00:12
epoch [90/100] batch [5/5] time 0.038 (0.219) data 0.000 (0.169) loss 1.1161 (1.2704) acc 86.3636 (88.7661) lr 5.9119e-05 eta 0:00:10
epoch [91/100] batch [5/5] time 0.044 (0.237) data 0.000 (0.195) loss 1.0396 (1.2918) acc 90.3846 (85.6538) lr 4.8943e-05 eta 0:00:10
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [92/100] batch [5/5] time 0.044 (0.238) data 0.000 (0.188) loss 0.9391 (1.2129) acc 92.8571 (89.5310) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.034 (0.214) data 0.000 (0.164) loss 0.9944 (1.2482) acc 87.5000 (85.0833) lr 3.1417e-05 eta 0:00:07
epoch [94/100] batch [5/5] time 0.034 (0.218) data 0.000 (0.168) loss 1.2272 (1.2334) acc 85.0000 (88.0385) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.045 (0.251) data 0.000 (0.203) loss 1.5589 (1.1914) acc 85.7143 (91.0173) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.042 (0.221) data 0.000 (0.174) loss 1.1734 (1.2363) acc 96.1538 (90.0343) lr 1.2312e-05 eta 0:00:04
>>> samples [59/160] noisy rate: 0.12 --> 0.19 --> 0.07 <<<
epoch [97/100] batch [5/5] time 0.042 (0.218) data 0.000 (0.167) loss 1.0900 (1.1969) acc 90.0000 (90.7797) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.040 (0.206) data 0.000 (0.165) loss 1.2780 (1.1928) acc 95.8333 (92.5422) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.039 (0.216) data 0.001 (0.168) loss 0.7763 (1.2457) acc 95.0000 (88.5340) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.047 (0.245) data 0.000 (0.198) loss 1.1609 (1.2475) acc 96.6667 (86.3712) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.28, 0.3, 0.26, 0.12, 0.11, 0.2, 0.21, 0.19, 0.19, 0.21, 0.21, 0.21, 0.18, 0.19, 0.19, 0.17, 0.17, 0.19, 0.19, 0.19]
* learned noise rate: [0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.05, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:36,  1.96s/it]  4%|▎         | 3/81 [00:02<00:43,  1.80it/s]  6%|▌         | 5/81 [00:02<00:22,  3.30it/s]  9%|▊         | 7/81 [00:02<00:14,  4.96it/s] 11%|█         | 9/81 [00:02<00:11,  6.49it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.15it/s] 16%|█▌        | 13/81 [00:02<00:07,  9.69it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.04it/s] 21%|██        | 17/81 [00:03<00:05, 12.14it/s] 23%|██▎       | 19/81 [00:03<00:04, 13.04it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.14it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.81it/s] 31%|███       | 25/81 [00:03<00:03, 14.31it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.52it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.82it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.73it/s] 41%|████      | 33/81 [00:04<00:03, 14.82it/s] 43%|████▎     | 35/81 [00:04<00:03, 15.03it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.19it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.15it/s] 51%|█████     | 41/81 [00:04<00:02, 14.60it/s] 53%|█████▎    | 43/81 [00:04<00:02, 14.54it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.84it/s] 58%|█████▊    | 47/81 [00:05<00:02, 14.96it/s] 60%|██████    | 49/81 [00:05<00:02, 14.40it/s] 63%|██████▎   | 51/81 [00:05<00:02, 14.30it/s] 65%|██████▌   | 53/81 [00:05<00:01, 14.30it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.64it/s] 70%|███████   | 57/81 [00:05<00:01, 14.46it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.32it/s] 75%|███████▌  | 61/81 [00:05<00:01, 14.51it/s] 78%|███████▊  | 63/81 [00:06<00:01, 14.81it/s] 80%|████████  | 65/81 [00:06<00:01, 15.04it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.20it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.31it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.40it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.46it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.52it/s] 95%|█████████▌| 77/81 [00:07<00:00, 15.55it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.55it/s]100%|██████████| 81/81 [00:07<00:00, 15.57it/s]100%|██████████| 81/81 [00:07<00:00, 10.94it/s]
=> result
* total: 8,100
* correct: 5,382
* accuracy: 66.4%
* error: 33.6%
* macro_f1: 62.9%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 522	acc: 58.0%
* class: 1 (Forest)	total: 900	correct: 868	acc: 96.4%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 6	acc: 0.7%
* class: 3 (Highway or Road)	total: 750	correct: 497	acc: 66.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 685	acc: 91.3%
* class: 5 (Pasture Land)	total: 600	correct: 395	acc: 65.8%
* class: 6 (Permanent Crop Land)	total: 750	correct: 623	acc: 83.1%
* class: 7 (Residential Buildings)	total: 900	correct: 718	acc: 79.8%
* class: 8 (River)	total: 750	correct: 290	acc: 38.7%
* class: 9 (Sea or Lake)	total: 900	correct: 778	acc: 86.4%
* average: 66.7%
Elapsed: 0:04:06
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_2-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.084 (0.617) data 0.000 (0.218) loss 1.0562 (1.0898) acc 25.0000 (17.5000) lr 2.0000e-03 eta 0:05:05
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [41/160] noisy rate: 0.12 --> 0.30 --> 0.02 <<<
epoch [2/100] batch [5/5] time 0.231 (0.372) data 0.000 (0.202) loss 2.0774 (2.1692) acc 33.3333 (21.1310) lr 1.9995e-03 eta 0:03:02
epoch [3/100] batch [5/5] time 0.287 (0.423) data 0.000 (0.232) loss 1.9541 (2.1425) acc 30.5556 (20.3419) lr 1.9980e-03 eta 0:03:25
epoch [4/100] batch [5/5] time 0.039 (0.445) data 0.000 (0.244) loss 1.7168 (1.9451) acc 81.2500 (46.4571) lr 1.9956e-03 eta 0:03:33
epoch [5/100] batch [5/5] time 0.033 (0.271) data 0.000 (0.218) loss 1.7985 (1.8499) acc 50.0000 (44.3056) lr 1.9921e-03 eta 0:02:08
epoch [6/100] batch [5/5] time 0.044 (0.247) data 0.000 (0.198) loss 1.6688 (1.7513) acc 59.0909 (47.8182) lr 1.9877e-03 eta 0:01:56
>>> samples [51/160] noisy rate: 0.12 --> 0.34 --> 0.06 <<<
epoch [7/100] batch [5/5] time 0.042 (0.250) data 0.000 (0.200) loss 1.6387 (1.7363) acc 58.3333 (53.7356) lr 1.9823e-03 eta 0:01:56
epoch [8/100] batch [5/5] time 0.037 (0.275) data 0.000 (0.168) loss 1.5494 (1.6052) acc 72.5000 (60.2857) lr 1.9759e-03 eta 0:02:06
epoch [9/100] batch [5/5] time 0.042 (0.255) data 0.001 (0.204) loss 1.6656 (1.5282) acc 47.7273 (64.1656) lr 1.9686e-03 eta 0:01:55
epoch [10/100] batch [5/5] time 0.038 (0.254) data 0.000 (0.201) loss 1.3030 (1.4372) acc 69.4444 (71.8492) lr 1.9603e-03 eta 0:01:54
epoch [11/100] batch [5/5] time 0.031 (0.254) data 0.000 (0.214) loss 1.4248 (1.4086) acc 65.6250 (73.9642) lr 1.9511e-03 eta 0:01:53
>>> samples [54/160] noisy rate: 0.12 --> 0.33 --> 0.06 <<<
epoch [12/100] batch [5/5] time 0.374 (0.291) data 0.000 (0.176) loss 1.2942 (1.4134) acc 80.0000 (71.1212) lr 1.9409e-03 eta 0:02:08
epoch [13/100] batch [5/5] time 0.039 (0.242) data 0.000 (0.195) loss 1.2339 (1.4059) acc 87.5000 (73.8636) lr 1.9298e-03 eta 0:01:45
epoch [14/100] batch [5/5] time 0.041 (0.245) data 0.000 (0.199) loss 1.3383 (1.3532) acc 66.6667 (77.1667) lr 1.9178e-03 eta 0:01:45
epoch [15/100] batch [5/5] time 0.046 (0.223) data 0.000 (0.170) loss 1.3383 (1.3255) acc 70.0000 (79.9116) lr 1.9048e-03 eta 0:01:34
epoch [16/100] batch [5/5] time 0.033 (0.314) data 0.000 (0.197) loss 1.2911 (1.3072) acc 80.5555 (78.1332) lr 1.8910e-03 eta 0:02:12
>>> samples [57/160] noisy rate: 0.12 --> 0.29 --> 0.07 <<<
epoch [17/100] batch [5/5] time 0.042 (0.247) data 0.000 (0.197) loss 1.2730 (1.2952) acc 82.6923 (80.5385) lr 1.8763e-03 eta 0:01:42
epoch [18/100] batch [5/5] time 0.043 (0.247) data 0.000 (0.199) loss 1.3114 (1.3050) acc 81.8182 (80.0468) lr 1.8607e-03 eta 0:01:41
epoch [19/100] batch [5/5] time 0.390 (0.396) data 0.000 (0.213) loss 1.2605 (1.2844) acc 65.6250 (84.6429) lr 1.8443e-03 eta 0:02:40
epoch [20/100] batch [5/5] time 0.043 (0.241) data 0.000 (0.189) loss 1.2000 (1.3270) acc 76.7857 (75.6525) lr 1.8271e-03 eta 0:01:36
epoch [21/100] batch [5/5] time 0.044 (0.237) data 0.000 (0.188) loss 1.2244 (1.3310) acc 76.7857 (74.9594) lr 1.8090e-03 eta 0:01:33
>>> samples [60/160] noisy rate: 0.12 --> 0.32 --> 0.08 <<<
epoch [22/100] batch [5/5] time 0.043 (0.306) data 0.000 (0.260) loss 1.3199 (1.2915) acc 75.0000 (80.6177) lr 1.7902e-03 eta 0:01:59
epoch [23/100] batch [5/5] time 0.044 (0.261) data 0.000 (0.208) loss 1.1296 (1.2933) acc 81.2500 (78.3654) lr 1.7705e-03 eta 0:01:40
epoch [24/100] batch [5/5] time 0.036 (0.268) data 0.000 (0.219) loss 1.2541 (1.2506) acc 82.5000 (83.4231) lr 1.7501e-03 eta 0:01:41
epoch [25/100] batch [5/5] time 0.038 (0.281) data 0.000 (0.238) loss 1.3262 (1.2421) acc 77.2727 (85.1558) lr 1.7290e-03 eta 0:01:45
epoch [26/100] batch [5/5] time 0.030 (0.274) data 0.000 (0.230) loss 1.2343 (1.2365) acc 81.2500 (83.3838) lr 1.7071e-03 eta 0:01:41
>>> samples [63/160] noisy rate: 0.12 --> 0.31 --> 0.11 <<<
epoch [27/100] batch [5/5] time 0.046 (0.336) data 0.001 (0.206) loss 0.9972 (1.2291) acc 85.4167 (83.8611) lr 1.6845e-03 eta 0:02:02
epoch [28/100] batch [5/5] time 0.026 (0.285) data 0.000 (0.237) loss 0.9368 (1.1679) acc 85.0000 (86.8611) lr 1.6613e-03 eta 0:01:42
epoch [29/100] batch [5/5] time 0.031 (0.251) data 0.000 (0.203) loss 0.9497 (1.1492) acc 89.2857 (88.7703) lr 1.6374e-03 eta 0:01:29
epoch [30/100] batch [5/5] time 0.031 (0.256) data 0.000 (0.207) loss 1.0889 (1.1769) acc 88.8889 (87.0885) lr 1.6129e-03 eta 0:01:29
epoch [31/100] batch [5/5] time 0.038 (0.262) data 0.000 (0.209) loss 1.1227 (1.2183) acc 85.0000 (85.5165) lr 1.5878e-03 eta 0:01:30
>>> samples [63/160] noisy rate: 0.12 --> 0.31 --> 0.11 <<<
epoch [32/100] batch [5/5] time 0.057 (0.261) data 0.000 (0.210) loss 1.2368 (1.1725) acc 91.1765 (89.4358) lr 1.5621e-03 eta 0:01:28
epoch [33/100] batch [5/5] time 0.038 (0.284) data 0.000 (0.239) loss 1.1672 (1.1887) acc 87.5000 (86.1538) lr 1.5358e-03 eta 0:01:35
epoch [34/100] batch [5/5] time 0.043 (0.284) data 0.000 (0.234) loss 1.4339 (1.1641) acc 87.5000 (87.2799) lr 1.5090e-03 eta 0:01:33
epoch [35/100] batch [5/5] time 0.045 (0.264) data 0.000 (0.215) loss 1.3243 (1.1921) acc 98.3333 (87.8371) lr 1.4818e-03 eta 0:01:25
epoch [36/100] batch [5/5] time 0.041 (0.261) data 0.000 (0.209) loss 1.2741 (1.1460) acc 90.3846 (91.7853) lr 1.4540e-03 eta 0:01:23
>>> samples [67/160] noisy rate: 0.12 --> 0.27 --> 0.15 <<<
epoch [37/100] batch [5/5] time 0.054 (0.272) data 0.000 (0.226) loss 1.2202 (1.2135) acc 90.6250 (87.2440) lr 1.4258e-03 eta 0:01:25
epoch [38/100] batch [5/5] time 0.475 (0.371) data 0.000 (0.232) loss 1.1502 (1.1450) acc 90.4762 (88.8244) lr 1.3971e-03 eta 0:01:54
epoch [39/100] batch [5/5] time 0.034 (0.328) data 0.000 (0.199) loss 1.1246 (1.1624) acc 90.6250 (85.2293) lr 1.3681e-03 eta 0:01:39
epoch [40/100] batch [5/5] time 0.033 (0.250) data 0.000 (0.199) loss 1.2189 (1.1602) acc 77.5000 (83.6190) lr 1.3387e-03 eta 0:01:15
epoch [41/100] batch [5/5] time 0.051 (0.304) data 0.000 (0.250) loss 1.0793 (1.1286) acc 85.0000 (87.7669) lr 1.3090e-03 eta 0:01:29
>>> samples [68/160] noisy rate: 0.12 --> 0.29 --> 0.16 <<<
epoch [42/100] batch [5/5] time 0.050 (0.242) data 0.000 (0.191) loss 1.1668 (1.1849) acc 83.3333 (86.4712) lr 1.2790e-03 eta 0:01:10
epoch [43/100] batch [5/5] time 0.045 (0.278) data 0.000 (0.224) loss 1.2638 (1.1868) acc 81.6667 (84.6488) lr 1.2487e-03 eta 0:01:19
epoch [44/100] batch [5/5] time 0.045 (0.259) data 0.000 (0.204) loss 1.5274 (1.1886) acc 72.9167 (86.8333) lr 1.2181e-03 eta 0:01:12
epoch [45/100] batch [5/5] time 0.056 (0.268) data 0.000 (0.214) loss 1.0767 (1.1617) acc 95.5882 (89.2138) lr 1.1874e-03 eta 0:01:13
epoch [46/100] batch [5/5] time 0.044 (0.264) data 0.000 (0.213) loss 1.0046 (1.1559) acc 89.5833 (88.5310) lr 1.1564e-03 eta 0:01:11
>>> samples [73/160] noisy rate: 0.12 --> 0.26 --> 0.16 <<<
epoch [47/100] batch [5/5] time 0.050 (0.252) data 0.000 (0.200) loss 1.2325 (1.1590) acc 88.3333 (86.2104) lr 1.1253e-03 eta 0:01:06
epoch [48/100] batch [5/5] time 0.052 (0.230) data 0.000 (0.178) loss 0.9037 (1.2021) acc 88.2353 (83.6181) lr 1.0941e-03 eta 0:00:59
epoch [49/100] batch [5/5] time 0.041 (0.250) data 0.000 (0.198) loss 0.8488 (1.1516) acc 98.0769 (85.8866) lr 1.0628e-03 eta 0:01:03
epoch [50/100] batch [5/5] time 0.476 (0.314) data 0.000 (0.177) loss 1.1553 (1.1570) acc 87.5000 (84.6731) lr 1.0314e-03 eta 0:01:18
epoch [51/100] batch [5/5] time 0.048 (0.254) data 0.000 (0.200) loss 0.9464 (1.1164) acc 81.2500 (87.6815) lr 1.0000e-03 eta 0:01:02
>>> samples [76/160] noisy rate: 0.12 --> 0.26 --> 0.18 <<<
epoch [52/100] batch [5/5] time 0.049 (0.250) data 0.000 (0.200) loss 1.0197 (1.1410) acc 85.0000 (85.3727) lr 9.6859e-04 eta 0:01:00
epoch [53/100] batch [5/5] time 0.056 (0.264) data 0.000 (0.209) loss 1.1619 (1.1383) acc 89.7059 (85.0534) lr 9.3721e-04 eta 0:01:02
epoch [54/100] batch [5/5] time 0.048 (0.282) data 0.000 (0.227) loss 1.4737 (1.1419) acc 76.7857 (85.7869) lr 9.0589e-04 eta 0:01:04
epoch [55/100] batch [5/5] time 0.059 (0.258) data 0.000 (0.203) loss 1.1235 (1.1563) acc 83.8235 (83.3957) lr 8.7467e-04 eta 0:00:58
epoch [56/100] batch [5/5] time 0.045 (0.261) data 0.000 (0.210) loss 1.0765 (1.1264) acc 96.6667 (89.4940) lr 8.4357e-04 eta 0:00:57
>>> samples [76/160] noisy rate: 0.12 --> 0.27 --> 0.18 <<<
epoch [57/100] batch [5/5] time 0.058 (0.287) data 0.000 (0.227) loss 1.2747 (1.1041) acc 84.7222 (89.2323) lr 8.1262e-04 eta 0:01:01
epoch [58/100] batch [5/5] time 0.046 (0.248) data 0.000 (0.193) loss 1.4731 (1.1085) acc 83.3333 (88.5351) lr 7.8186e-04 eta 0:00:52
epoch [59/100] batch [5/5] time 0.050 (0.260) data 0.000 (0.203) loss 1.2055 (1.1238) acc 86.6667 (88.5208) lr 7.5131e-04 eta 0:00:53
epoch [60/100] batch [5/5] time 0.055 (0.291) data 0.000 (0.238) loss 1.0274 (1.1071) acc 82.3529 (86.9706) lr 7.2101e-04 eta 0:00:58
epoch [61/100] batch [5/5] time 0.050 (0.273) data 0.000 (0.217) loss 0.9139 (1.0918) acc 92.8571 (91.2088) lr 6.9098e-04 eta 0:00:53
>>> samples [77/160] noisy rate: 0.12 --> 0.26 --> 0.19 <<<
epoch [62/100] batch [5/5] time 0.047 (0.250) data 0.000 (0.195) loss 1.2910 (1.1083) acc 81.2500 (87.9223) lr 6.6126e-04 eta 0:00:47
epoch [63/100] batch [5/5] time 0.048 (0.261) data 0.000 (0.204) loss 0.8215 (1.1129) acc 98.0769 (86.3535) lr 6.3188e-04 eta 0:00:48
epoch [64/100] batch [5/5] time 0.051 (0.264) data 0.000 (0.209) loss 0.9977 (1.1125) acc 92.6471 (88.0235) lr 6.0285e-04 eta 0:00:47
epoch [65/100] batch [5/5] time 0.051 (0.282) data 0.000 (0.226) loss 1.3226 (1.0882) acc 82.8125 (90.7390) lr 5.7422e-04 eta 0:00:49
epoch [66/100] batch [5/5] time 0.044 (0.260) data 0.000 (0.204) loss 0.8658 (1.0924) acc 94.6429 (88.5060) lr 5.4601e-04 eta 0:00:44
>>> samples [78/160] noisy rate: 0.12 --> 0.26 --> 0.19 <<<
epoch [67/100] batch [5/5] time 0.050 (0.255) data 0.000 (0.199) loss 1.1173 (1.0835) acc 95.0000 (89.1838) lr 5.1825e-04 eta 0:00:42
epoch [68/100] batch [5/5] time 0.051 (0.260) data 0.000 (0.206) loss 1.0488 (1.1010) acc 91.1765 (89.1196) lr 4.9096e-04 eta 0:00:41
epoch [69/100] batch [5/5] time 0.045 (0.257) data 0.000 (0.200) loss 1.0861 (1.1077) acc 87.5000 (84.7104) lr 4.6417e-04 eta 0:00:39
epoch [70/100] batch [5/5] time 0.052 (0.227) data 0.000 (0.170) loss 0.9916 (1.0908) acc 83.8235 (86.3219) lr 4.3792e-04 eta 0:00:34
epoch [71/100] batch [5/5] time 0.049 (0.221) data 0.000 (0.167) loss 1.4065 (1.1300) acc 85.7143 (87.3870) lr 4.1221e-04 eta 0:00:32
>>> samples [82/160] noisy rate: 0.12 --> 0.26 --> 0.20 <<<
epoch [72/100] batch [5/5] time 0.058 (0.286) data 0.000 (0.227) loss 1.1950 (1.1122) acc 80.5555 (86.1137) lr 3.8709e-04 eta 0:00:40
epoch [73/100] batch [5/5] time 0.057 (0.293) data 0.000 (0.238) loss 1.0175 (1.0918) acc 94.1176 (87.2988) lr 3.6258e-04 eta 0:00:39
epoch [74/100] batch [5/5] time 0.058 (0.265) data 0.000 (0.205) loss 1.1653 (1.0822) acc 86.1111 (89.8926) lr 3.3869e-04 eta 0:00:34
epoch [75/100] batch [5/5] time 0.038 (0.259) data 0.000 (0.202) loss 0.9295 (1.0941) acc 87.5000 (87.5931) lr 3.1545e-04 eta 0:00:32
epoch [76/100] batch [5/5] time 0.050 (0.360) data 0.000 (0.218) loss 1.1313 (1.1180) acc 76.6667 (83.3016) lr 2.9289e-04 eta 0:00:43
>>> samples [82/160] noisy rate: 0.12 --> 0.26 --> 0.20 <<<
epoch [77/100] batch [5/5] time 0.051 (0.265) data 0.000 (0.210) loss 1.0903 (1.1129) acc 89.7059 (83.8051) lr 2.7103e-04 eta 0:00:30
epoch [78/100] batch [5/5] time 0.059 (0.271) data 0.000 (0.209) loss 0.8168 (1.1032) acc 95.8333 (86.0574) lr 2.4989e-04 eta 0:00:29
epoch [79/100] batch [5/5] time 0.055 (0.260) data 0.000 (0.202) loss 1.1140 (1.1389) acc 81.5789 (87.7565) lr 2.2949e-04 eta 0:00:27
epoch [80/100] batch [5/5] time 0.061 (0.267) data 0.000 (0.211) loss 1.0328 (1.1161) acc 90.7895 (83.0073) lr 2.0984e-04 eta 0:00:26
epoch [81/100] batch [5/5] time 0.049 (0.270) data 0.000 (0.215) loss 1.0410 (1.1054) acc 82.8125 (85.2054) lr 1.9098e-04 eta 0:00:25
>>> samples [83/160] noisy rate: 0.12 --> 0.24 --> 0.19 <<<
epoch [82/100] batch [5/5] time 0.060 (0.257) data 0.000 (0.198) loss 0.9810 (1.0834) acc 91.6667 (88.0155) lr 1.7292e-04 eta 0:00:23
epoch [83/100] batch [5/5] time 0.059 (0.275) data 0.000 (0.218) loss 1.0255 (1.0410) acc 84.7222 (87.4265) lr 1.5567e-04 eta 0:00:23
epoch [84/100] batch [5/5] time 0.045 (0.268) data 0.000 (0.212) loss 1.1142 (1.1185) acc 94.6429 (86.4755) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.051 (0.263) data 0.000 (0.206) loss 0.9502 (1.0808) acc 91.1765 (88.7276) lr 1.2369e-04 eta 0:00:19
epoch [86/100] batch [5/5] time 0.053 (0.265) data 0.000 (0.208) loss 1.1271 (1.0797) acc 90.2778 (86.6121) lr 1.0899e-04 eta 0:00:18
>>> samples [83/160] noisy rate: 0.12 --> 0.25 --> 0.19 <<<
epoch [87/100] batch [5/5] time 0.051 (0.249) data 0.000 (0.191) loss 1.0545 (1.0779) acc 81.6667 (85.6926) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.049 (0.289) data 0.000 (0.231) loss 1.0288 (1.1012) acc 87.5000 (86.4415) lr 8.2245e-05 eta 0:00:17
epoch [89/100] batch [5/5] time 0.043 (0.263) data 0.000 (0.206) loss 1.4636 (1.1392) acc 85.4167 (84.0935) lr 7.0224e-05 eta 0:00:14
epoch [90/100] batch [5/5] time 0.049 (0.291) data 0.000 (0.236) loss 0.9699 (1.0883) acc 87.5000 (87.4178) lr 5.9119e-05 eta 0:00:14
epoch [91/100] batch [5/5] time 0.048 (0.293) data 0.000 (0.239) loss 1.1343 (1.0699) acc 92.8571 (89.5461) lr 4.8943e-05 eta 0:00:13
>>> samples [83/160] noisy rate: 0.12 --> 0.28 --> 0.19 <<<
epoch [92/100] batch [5/5] time 0.052 (0.236) data 0.000 (0.179) loss 0.9971 (1.1161) acc 92.6471 (86.5424) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.057 (0.257) data 0.000 (0.198) loss 0.8293 (1.0842) acc 90.2778 (86.0516) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.045 (0.213) data 0.000 (0.157) loss 1.0561 (1.1316) acc 88.3333 (85.8183) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.053 (0.278) data 0.000 (0.222) loss 1.1443 (1.0856) acc 90.2778 (88.5833) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.051 (0.269) data 0.000 (0.215) loss 1.2394 (1.0805) acc 80.8824 (87.8607) lr 1.2312e-05 eta 0:00:05
>>> samples [83/160] noisy rate: 0.12 --> 0.26 --> 0.19 <<<
epoch [97/100] batch [5/5] time 0.050 (0.224) data 0.000 (0.167) loss 1.0265 (1.0518) acc 85.0000 (86.1497) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.049 (0.231) data 0.000 (0.173) loss 1.0305 (1.1042) acc 78.1250 (85.8720) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.040 (0.222) data 0.000 (0.166) loss 0.9593 (1.0927) acc 91.6667 (84.7903) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.062 (0.222) data 0.000 (0.163) loss 1.1666 (1.0984) acc 76.2500 (85.3878) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_2FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.3, 0.34, 0.33, 0.29, 0.32, 0.31, 0.31, 0.27, 0.29, 0.26, 0.26, 0.27, 0.26, 0.26, 0.26, 0.26, 0.24, 0.25, 0.28, 0.26]
* learned noise rate: [0.02, 0.06, 0.06, 0.07, 0.08, 0.11, 0.11, 0.15, 0.16, 0.16, 0.18, 0.18, 0.19, 0.19, 0.2, 0.2, 0.19, 0.19, 0.19, 0.19]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:16,  1.71s/it]  4%|▎         | 3/81 [00:01<00:38,  2.04it/s]  6%|▌         | 5/81 [00:01<00:20,  3.70it/s]  9%|▊         | 7/81 [00:02<00:13,  5.42it/s] 11%|█         | 9/81 [00:02<00:09,  7.21it/s] 14%|█▎        | 11/81 [00:02<00:07,  8.91it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.39it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.61it/s] 21%|██        | 17/81 [00:02<00:05, 12.67it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.51it/s] 26%|██▌       | 21/81 [00:02<00:04, 14.16it/s] 28%|██▊       | 23/81 [00:03<00:03, 14.56it/s] 31%|███       | 25/81 [00:03<00:03, 14.93it/s] 33%|███▎      | 27/81 [00:03<00:03, 15.19it/s] 36%|███▌      | 29/81 [00:03<00:03, 15.37it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.19it/s] 41%|████      | 33/81 [00:03<00:03, 15.08it/s] 43%|████▎     | 35/81 [00:03<00:03, 15.31it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.41it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.46it/s] 51%|█████     | 41/81 [00:04<00:02, 15.40it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.51it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.33it/s] 58%|█████▊    | 47/81 [00:04<00:02, 14.97it/s] 60%|██████    | 49/81 [00:04<00:02, 15.22it/s] 63%|██████▎   | 51/81 [00:04<00:01, 15.40it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.46it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.47it/s] 70%|███████   | 57/81 [00:05<00:01, 15.53it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.62it/s] 75%|███████▌  | 61/81 [00:05<00:01, 14.88it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.16it/s] 80%|████████  | 65/81 [00:05<00:01, 15.16it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.37it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.53it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.66it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.77it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.84it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.89it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.93it/s]100%|██████████| 81/81 [00:06<00:00, 15.95it/s]100%|██████████| 81/81 [00:06<00:00, 11.61it/s]
=> result
* total: 8,100
* correct: 4,801
* accuracy: 59.3%
* error: 40.7%
* macro_f1: 53.8%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 527	acc: 58.6%
* class: 1 (Forest)	total: 900	correct: 836	acc: 92.9%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 6	acc: 0.7%
* class: 3 (Highway or Road)	total: 750	correct: 501	acc: 66.8%
* class: 4 (Industrial Buildings)	total: 750	correct: 581	acc: 77.5%
* class: 5 (Pasture Land)	total: 600	correct: 451	acc: 75.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 587	acc: 78.3%
* class: 7 (Residential Buildings)	total: 900	correct: 849	acc: 94.3%
* class: 8 (River)	total: 750	correct: 463	acc: 61.7%
* class: 9 (Sea or Lake)	total: 900	correct: 0	acc: 0.0%
* average: 60.6%
Elapsed: 0:04:29
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_4-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.090 (0.604) data 0.000 (0.213) loss 1.0950 (1.1162) acc 10.9375 (15.4688) lr 2.0000e-03 eta 0:04:58
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [54/160] noisy rate: 0.25 --> 0.26 --> 0.07 <<<
epoch [2/100] batch [5/5] time 0.269 (0.434) data 0.000 (0.196) loss 2.0217 (2.0141) acc 25.0000 (24.7646) lr 1.9995e-03 eta 0:03:32
epoch [3/100] batch [5/5] time 0.323 (0.400) data 0.001 (0.182) loss 1.9599 (1.8685) acc 27.0833 (38.4722) lr 1.9980e-03 eta 0:03:13
epoch [4/100] batch [5/5] time 0.043 (0.303) data 0.000 (0.199) loss 1.9022 (1.7524) acc 37.5000 (46.7570) lr 1.9956e-03 eta 0:02:25
epoch [5/100] batch [5/5] time 0.043 (0.288) data 0.000 (0.179) loss 1.8393 (1.7483) acc 33.3333 (45.0498) lr 1.9921e-03 eta 0:02:16
epoch [6/100] batch [5/5] time 0.050 (0.296) data 0.000 (0.206) loss 1.6088 (1.7442) acc 48.3333 (47.7125) lr 1.9877e-03 eta 0:02:18
>>> samples [64/160] noisy rate: 0.25 --> 0.29 --> 0.17 <<<
epoch [7/100] batch [5/5] time 0.045 (0.280) data 0.000 (0.164) loss 1.6539 (1.6830) acc 55.7692 (53.0705) lr 1.9823e-03 eta 0:02:09
epoch [8/100] batch [5/5] time 0.038 (0.246) data 0.000 (0.201) loss 1.5919 (1.6415) acc 35.4167 (54.7551) lr 1.9759e-03 eta 0:01:53
epoch [9/100] batch [5/5] time 0.039 (0.229) data 0.000 (0.187) loss 1.5293 (1.6267) acc 69.2308 (56.8082) lr 1.9686e-03 eta 0:01:44
epoch [10/100] batch [5/5] time 0.035 (0.232) data 0.000 (0.185) loss 1.3703 (1.4578) acc 65.6250 (64.8846) lr 1.9603e-03 eta 0:01:44
epoch [11/100] batch [5/5] time 0.044 (0.221) data 0.000 (0.171) loss 1.4145 (1.4948) acc 68.3333 (59.6131) lr 1.9511e-03 eta 0:01:38
>>> samples [64/160] noisy rate: 0.25 --> 0.22 --> 0.17 <<<
epoch [12/100] batch [5/5] time 0.039 (0.294) data 0.000 (0.244) loss 1.3483 (1.5026) acc 70.8333 (65.4982) lr 1.9409e-03 eta 0:02:09
epoch [13/100] batch [5/5] time 0.047 (0.263) data 0.000 (0.212) loss 1.4552 (1.4150) acc 66.0714 (70.5433) lr 1.9298e-03 eta 0:01:54
epoch [14/100] batch [5/5] time 0.043 (0.239) data 0.000 (0.187) loss 1.3038 (1.3949) acc 62.5000 (68.4827) lr 1.9178e-03 eta 0:01:42
epoch [15/100] batch [5/5] time 0.050 (0.243) data 0.000 (0.190) loss 1.4407 (1.3519) acc 67.1875 (72.6193) lr 1.9048e-03 eta 0:01:43
epoch [16/100] batch [5/5] time 0.039 (0.248) data 0.000 (0.197) loss 1.3829 (1.3436) acc 70.8333 (74.9634) lr 1.8910e-03 eta 0:01:44
>>> samples [65/160] noisy rate: 0.25 --> 0.20 --> 0.17 <<<
epoch [17/100] batch [5/5] time 0.039 (0.283) data 0.000 (0.232) loss 1.5549 (1.3408) acc 55.0000 (70.8720) lr 1.8763e-03 eta 0:01:57
epoch [18/100] batch [5/5] time 0.034 (0.254) data 0.000 (0.201) loss 1.1289 (1.3062) acc 77.5000 (75.5476) lr 1.8607e-03 eta 0:01:44
epoch [19/100] batch [5/5] time 0.038 (0.269) data 0.000 (0.225) loss 1.5005 (1.2746) acc 77.5000 (80.3482) lr 1.8443e-03 eta 0:01:49
epoch [20/100] batch [5/5] time 0.046 (0.244) data 0.000 (0.193) loss 1.3412 (1.2464) acc 76.6667 (80.0521) lr 1.8271e-03 eta 0:01:37
epoch [21/100] batch [5/5] time 0.040 (0.277) data 0.000 (0.226) loss 1.2773 (1.2528) acc 68.7500 (79.5119) lr 1.8090e-03 eta 0:01:49
>>> samples [67/160] noisy rate: 0.25 --> 0.20 --> 0.16 <<<
epoch [22/100] batch [5/5] time 0.047 (0.282) data 0.000 (0.235) loss 1.2936 (1.2055) acc 83.3333 (80.1374) lr 1.7902e-03 eta 0:01:50
epoch [23/100] batch [5/5] time 0.044 (0.263) data 0.000 (0.214) loss 1.0949 (1.2067) acc 89.2857 (79.5446) lr 1.7705e-03 eta 0:01:41
epoch [24/100] batch [5/5] time 0.049 (0.247) data 0.000 (0.194) loss 1.1313 (1.1999) acc 85.9375 (84.1069) lr 1.7501e-03 eta 0:01:34
epoch [25/100] batch [5/5] time 0.052 (0.256) data 0.000 (0.207) loss 1.3907 (1.1881) acc 82.8125 (81.5000) lr 1.7290e-03 eta 0:01:36
epoch [26/100] batch [5/5] time 0.045 (0.254) data 0.000 (0.204) loss 1.3533 (1.1758) acc 80.0000 (83.6845) lr 1.7071e-03 eta 0:01:33
>>> samples [69/160] noisy rate: 0.25 --> 0.19 --> 0.16 <<<
epoch [27/100] batch [5/5] time 0.044 (0.294) data 0.000 (0.242) loss 1.0737 (1.2035) acc 93.1818 (78.3422) lr 1.6845e-03 eta 0:01:47
epoch [28/100] batch [5/5] time 0.045 (0.250) data 0.000 (0.197) loss 1.1688 (1.1266) acc 86.5385 (84.6881) lr 1.6613e-03 eta 0:01:29
epoch [29/100] batch [5/5] time 0.049 (0.271) data 0.000 (0.217) loss 0.9366 (1.1520) acc 89.0625 (84.0823) lr 1.6374e-03 eta 0:01:36
epoch [30/100] batch [5/5] time 0.051 (0.264) data 0.000 (0.216) loss 1.3432 (1.1872) acc 80.0000 (82.2244) lr 1.6129e-03 eta 0:01:32
epoch [31/100] batch [5/5] time 0.045 (0.275) data 0.000 (0.224) loss 1.2715 (1.1615) acc 78.3333 (82.8720) lr 1.5878e-03 eta 0:01:34
>>> samples [71/160] noisy rate: 0.25 --> 0.23 --> 0.15 <<<
epoch [32/100] batch [5/5] time 0.419 (0.346) data 0.000 (0.219) loss 1.2581 (1.0951) acc 81.9444 (82.6426) lr 1.5621e-03 eta 0:01:57
epoch [33/100] batch [5/5] time 0.051 (0.261) data 0.000 (0.209) loss 1.0897 (1.0965) acc 83.8235 (85.3659) lr 1.5358e-03 eta 0:01:27
epoch [34/100] batch [5/5] time 0.043 (0.254) data 0.000 (0.199) loss 1.0268 (1.0875) acc 82.1429 (80.7043) lr 1.5090e-03 eta 0:01:23
epoch [35/100] batch [5/5] time 0.046 (0.283) data 0.000 (0.230) loss 1.3066 (1.1455) acc 86.5385 (81.8693) lr 1.4818e-03 eta 0:01:32
epoch [36/100] batch [5/5] time 0.053 (0.260) data 0.000 (0.207) loss 1.2229 (1.1707) acc 78.1250 (81.1830) lr 1.4540e-03 eta 0:01:23
>>> samples [76/160] noisy rate: 0.25 --> 0.25 --> 0.17 <<<
epoch [37/100] batch [5/5] time 0.053 (0.323) data 0.000 (0.196) loss 1.0143 (1.1005) acc 71.1538 (80.8405) lr 1.4258e-03 eta 0:01:41
epoch [38/100] batch [5/5] time 0.039 (0.249) data 0.000 (0.194) loss 1.1043 (1.1332) acc 95.8333 (84.0641) lr 1.3971e-03 eta 0:01:17
epoch [39/100] batch [5/5] time 0.045 (0.339) data 0.000 (0.210) loss 1.1883 (1.1206) acc 71.6667 (83.1429) lr 1.3681e-03 eta 0:01:43
epoch [40/100] batch [5/5] time 0.048 (0.336) data 0.000 (0.202) loss 0.7279 (1.0996) acc 94.6429 (81.6558) lr 1.3387e-03 eta 0:01:40
epoch [41/100] batch [5/5] time 0.036 (0.273) data 0.000 (0.219) loss 1.2331 (1.1179) acc 81.8182 (82.7739) lr 1.3090e-03 eta 0:01:20
>>> samples [77/160] noisy rate: 0.25 --> 0.26 --> 0.18 <<<
epoch [42/100] batch [5/5] time 0.048 (0.267) data 0.000 (0.212) loss 1.1639 (1.0870) acc 92.8571 (83.5875) lr 1.2790e-03 eta 0:01:17
epoch [43/100] batch [5/5] time 0.050 (0.251) data 0.000 (0.197) loss 1.1679 (1.1099) acc 84.3750 (80.9668) lr 1.2487e-03 eta 0:01:11
epoch [44/100] batch [5/5] time 0.037 (0.263) data 0.000 (0.208) loss 1.0962 (1.0844) acc 79.5455 (85.9231) lr 1.2181e-03 eta 0:01:13
epoch [45/100] batch [5/5] time 0.046 (0.274) data 0.000 (0.218) loss 0.9992 (1.0712) acc 86.6667 (83.7650) lr 1.1874e-03 eta 0:01:15
epoch [46/100] batch [5/5] time 0.043 (0.275) data 0.000 (0.225) loss 1.0438 (1.1151) acc 78.5714 (80.0133) lr 1.1564e-03 eta 0:01:14
>>> samples [78/160] noisy rate: 0.25 --> 0.28 --> 0.19 <<<
epoch [47/100] batch [5/5] time 0.053 (0.279) data 0.000 (0.227) loss 1.2174 (1.1174) acc 90.6250 (84.0150) lr 1.1253e-03 eta 0:01:13
epoch [48/100] batch [5/5] time 0.048 (0.246) data 0.000 (0.189) loss 1.4148 (1.0971) acc 66.0714 (84.7204) lr 1.0941e-03 eta 0:01:04
epoch [49/100] batch [5/5] time 0.061 (0.272) data 0.000 (0.218) loss 1.1017 (1.1067) acc 80.2632 (80.2220) lr 1.0628e-03 eta 0:01:09
epoch [50/100] batch [5/5] time 0.046 (0.248) data 0.000 (0.192) loss 1.0154 (1.0521) acc 90.3846 (84.7107) lr 1.0314e-03 eta 0:01:01
epoch [51/100] batch [5/5] time 0.056 (0.270) data 0.000 (0.212) loss 1.1714 (1.0609) acc 94.1176 (87.1505) lr 1.0000e-03 eta 0:01:06
>>> samples [78/160] noisy rate: 0.25 --> 0.27 --> 0.19 <<<
epoch [52/100] batch [5/5] time 0.049 (0.262) data 0.000 (0.204) loss 1.1516 (1.0802) acc 83.3333 (84.9221) lr 9.6859e-04 eta 0:01:02
epoch [53/100] batch [5/5] time 0.050 (0.252) data 0.000 (0.194) loss 0.9546 (1.0654) acc 92.1875 (86.1518) lr 9.3721e-04 eta 0:00:59
epoch [54/100] batch [5/5] time 0.041 (0.239) data 0.000 (0.181) loss 1.2011 (1.1066) acc 86.3636 (86.7176) lr 9.0589e-04 eta 0:00:54
epoch [55/100] batch [5/5] time 0.047 (0.260) data 0.000 (0.206) loss 0.9541 (1.0668) acc 85.7143 (84.1429) lr 8.7467e-04 eta 0:00:58
epoch [56/100] batch [5/5] time 0.043 (0.256) data 0.001 (0.200) loss 1.1951 (1.1041) acc 82.1429 (81.8852) lr 8.4357e-04 eta 0:00:56
>>> samples [78/160] noisy rate: 0.25 --> 0.26 --> 0.19 <<<
epoch [57/100] batch [5/5] time 0.050 (0.261) data 0.000 (0.209) loss 1.2034 (1.0690) acc 66.6667 (81.9357) lr 8.1262e-04 eta 0:00:56
epoch [58/100] batch [5/5] time 0.044 (0.267) data 0.000 (0.210) loss 0.9050 (1.0628) acc 88.4615 (85.0256) lr 7.8186e-04 eta 0:00:56
epoch [59/100] batch [5/5] time 0.056 (0.260) data 0.000 (0.205) loss 1.2573 (1.0623) acc 72.3684 (83.1126) lr 7.5131e-04 eta 0:00:53
epoch [60/100] batch [5/5] time 0.038 (0.244) data 0.000 (0.189) loss 1.0962 (1.0710) acc 72.7273 (83.8907) lr 7.2101e-04 eta 0:00:48
epoch [61/100] batch [5/5] time 0.058 (0.257) data 0.000 (0.203) loss 0.9684 (1.0544) acc 90.2778 (87.2222) lr 6.9098e-04 eta 0:00:50
>>> samples [78/160] noisy rate: 0.25 --> 0.27 --> 0.19 <<<
epoch [62/100] batch [5/5] time 0.035 (0.251) data 0.000 (0.196) loss 1.3791 (1.0864) acc 90.6250 (86.1234) lr 6.6126e-04 eta 0:00:47
epoch [63/100] batch [5/5] time 0.067 (0.268) data 0.000 (0.208) loss 1.1166 (1.0634) acc 83.3333 (85.8056) lr 6.3188e-04 eta 0:00:49
epoch [64/100] batch [5/5] time 0.048 (0.252) data 0.000 (0.195) loss 1.0301 (1.0618) acc 85.9375 (86.7722) lr 6.0285e-04 eta 0:00:45
epoch [65/100] batch [5/5] time 0.040 (0.250) data 0.000 (0.196) loss 1.0224 (1.0621) acc 91.6667 (84.2840) lr 5.7422e-04 eta 0:00:43
epoch [66/100] batch [5/5] time 0.042 (0.276) data 0.000 (0.224) loss 0.9824 (1.0396) acc 86.5385 (84.8260) lr 5.4601e-04 eta 0:00:46
>>> samples [79/160] noisy rate: 0.25 --> 0.26 --> 0.20 <<<
epoch [67/100] batch [5/5] time 0.056 (0.242) data 0.000 (0.184) loss 1.1024 (1.0543) acc 85.2941 (84.1663) lr 5.1825e-04 eta 0:00:39
epoch [68/100] batch [5/5] time 0.039 (0.251) data 0.000 (0.195) loss 1.1865 (1.0815) acc 85.4167 (86.3970) lr 4.9096e-04 eta 0:00:40
epoch [69/100] batch [5/5] time 0.052 (0.257) data 0.000 (0.202) loss 1.2275 (1.0332) acc 84.3750 (85.6822) lr 4.6417e-04 eta 0:00:39
epoch [70/100] batch [5/5] time 0.476 (0.361) data 0.000 (0.220) loss 1.1687 (1.0866) acc 92.0455 (85.8377) lr 4.3792e-04 eta 0:00:54
epoch [71/100] batch [5/5] time 0.054 (0.222) data 0.000 (0.170) loss 1.0100 (1.0583) acc 96.8750 (84.7779) lr 4.1221e-04 eta 0:00:32
>>> samples [79/160] noisy rate: 0.25 --> 0.23 --> 0.20 <<<
epoch [72/100] batch [5/5] time 0.050 (0.239) data 0.000 (0.178) loss 0.8195 (1.0566) acc 95.0000 (84.7613) lr 3.8709e-04 eta 0:00:33
epoch [73/100] batch [5/5] time 0.044 (0.219) data 0.000 (0.158) loss 1.0576 (1.0690) acc 79.5455 (84.4124) lr 3.6258e-04 eta 0:00:29
epoch [74/100] batch [5/5] time 0.050 (0.273) data 0.000 (0.214) loss 1.1678 (1.0289) acc 85.7143 (86.9382) lr 3.3869e-04 eta 0:00:35
epoch [75/100] batch [5/5] time 0.051 (0.262) data 0.000 (0.205) loss 1.3468 (1.0905) acc 78.5714 (84.6796) lr 3.1545e-04 eta 0:00:32
epoch [76/100] batch [5/5] time 0.063 (0.277) data 0.000 (0.214) loss 1.0782 (1.0714) acc 75.0000 (82.4621) lr 2.9289e-04 eta 0:00:33
>>> samples [82/160] noisy rate: 0.25 --> 0.24 --> 0.23 <<<
epoch [77/100] batch [5/5] time 0.057 (0.248) data 0.000 (0.193) loss 0.8804 (1.0597) acc 89.7059 (83.9203) lr 2.7103e-04 eta 0:00:28
epoch [78/100] batch [5/5] time 0.052 (0.279) data 0.000 (0.221) loss 0.9935 (1.0869) acc 86.7647 (86.0113) lr 2.4989e-04 eta 0:00:30
epoch [79/100] batch [5/5] time 0.059 (0.249) data 0.000 (0.192) loss 1.0236 (1.0501) acc 91.1765 (87.0628) lr 2.2949e-04 eta 0:00:26
epoch [80/100] batch [5/5] time 0.060 (0.248) data 0.000 (0.188) loss 1.0086 (1.0493) acc 91.6667 (88.0000) lr 2.0984e-04 eta 0:00:24
epoch [81/100] batch [5/5] time 0.056 (0.209) data 0.000 (0.148) loss 1.0495 (1.0705) acc 92.6471 (87.3158) lr 1.9098e-04 eta 0:00:19
>>> samples [82/160] noisy rate: 0.25 --> 0.24 --> 0.23 <<<
epoch [82/100] batch [5/5] time 0.062 (0.257) data 0.001 (0.198) loss 1.0838 (1.0627) acc 85.7143 (84.2875) lr 1.7292e-04 eta 0:00:23
epoch [83/100] batch [5/5] time 0.048 (0.227) data 0.000 (0.167) loss 0.9265 (1.0792) acc 90.3846 (84.6074) lr 1.5567e-04 eta 0:00:19
epoch [84/100] batch [5/5] time 0.049 (0.228) data 0.000 (0.169) loss 1.0437 (1.0606) acc 81.2500 (83.8988) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.049 (0.274) data 0.000 (0.215) loss 1.1020 (1.0796) acc 87.5000 (86.8444) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.046 (0.271) data 0.000 (0.215) loss 1.1921 (1.0550) acc 91.0714 (88.7045) lr 1.0899e-04 eta 0:00:18
>>> samples [83/160] noisy rate: 0.25 --> 0.24 --> 0.24 <<<
epoch [87/100] batch [5/5] time 0.049 (0.263) data 0.000 (0.202) loss 1.2849 (1.0650) acc 78.8462 (83.8645) lr 9.5173e-05 eta 0:00:17
epoch [88/100] batch [5/5] time 0.072 (0.228) data 0.000 (0.164) loss 0.9133 (1.0510) acc 92.0455 (89.3941) lr 8.2245e-05 eta 0:00:13
epoch [89/100] batch [5/5] time 0.058 (0.236) data 0.000 (0.176) loss 1.1147 (1.0328) acc 86.1111 (87.1119) lr 7.0224e-05 eta 0:00:12
epoch [90/100] batch [5/5] time 0.062 (0.221) data 0.000 (0.161) loss 0.9428 (1.0678) acc 89.4737 (85.0619) lr 5.9119e-05 eta 0:00:11
epoch [91/100] batch [5/5] time 0.056 (0.258) data 0.000 (0.202) loss 0.9485 (1.0469) acc 92.1875 (86.4522) lr 4.8943e-05 eta 0:00:11
>>> samples [83/160] noisy rate: 0.25 --> 0.24 --> 0.24 <<<
epoch [92/100] batch [5/5] time 0.056 (0.266) data 0.000 (0.210) loss 0.9665 (1.0613) acc 85.2941 (85.2914) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.062 (0.252) data 0.000 (0.191) loss 1.1930 (1.0262) acc 77.5000 (84.1254) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.054 (0.238) data 0.000 (0.182) loss 1.1267 (1.0514) acc 84.3750 (87.7994) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.058 (0.225) data 0.000 (0.163) loss 1.2119 (1.0473) acc 90.0000 (86.9744) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.053 (0.279) data 0.000 (0.222) loss 1.1605 (1.0688) acc 87.5000 (84.3713) lr 1.2312e-05 eta 0:00:05
>>> samples [83/160] noisy rate: 0.25 --> 0.23 --> 0.24 <<<
epoch [97/100] batch [5/5] time 0.053 (0.290) data 0.000 (0.230) loss 1.0931 (1.0757) acc 80.8824 (81.8704) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.055 (0.241) data 0.000 (0.181) loss 1.0549 (1.0608) acc 84.3750 (86.5253) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.054 (0.246) data 0.000 (0.187) loss 1.0954 (1.0501) acc 86.1111 (87.0407) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.043 (0.336) data 0.000 (0.195) loss 1.1182 (1.0443) acc 82.6923 (85.9677) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.26, 0.29, 0.22, 0.2, 0.2, 0.19, 0.23, 0.25, 0.26, 0.28, 0.27, 0.26, 0.27, 0.26, 0.23, 0.24, 0.24, 0.24, 0.24, 0.23]
* learned noise rate: [0.07, 0.17, 0.17, 0.17, 0.16, 0.16, 0.15, 0.17, 0.18, 0.19, 0.19, 0.19, 0.19, 0.2, 0.2, 0.23, 0.23, 0.24, 0.24, 0.24]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:29,  1.87s/it]  4%|▎         | 3/81 [00:02<00:41,  1.86it/s]  6%|▌         | 5/81 [00:02<00:22,  3.40it/s]  9%|▊         | 7/81 [00:02<00:14,  5.04it/s] 11%|█         | 9/81 [00:02<00:10,  6.62it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.17it/s] 16%|█▌        | 13/81 [00:02<00:07,  9.70it/s] 19%|█▊        | 15/81 [00:02<00:06, 10.83it/s] 21%|██        | 17/81 [00:02<00:05, 11.93it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.85it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.60it/s] 28%|██▊       | 23/81 [00:03<00:04, 14.04it/s] 31%|███       | 25/81 [00:03<00:03, 14.55it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.55it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.55it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.92it/s] 41%|████      | 33/81 [00:04<00:03, 14.89it/s] 43%|████▎     | 35/81 [00:04<00:03, 15.07it/s] 46%|████▌     | 37/81 [00:04<00:02, 15.21it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.39it/s] 51%|█████     | 41/81 [00:04<00:02, 14.08it/s] 53%|█████▎    | 43/81 [00:04<00:02, 14.18it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.18it/s] 58%|█████▊    | 47/81 [00:04<00:02, 14.65it/s] 60%|██████    | 49/81 [00:05<00:02, 14.71it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.02it/s] 65%|██████▌   | 53/81 [00:05<00:01, 14.98it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.20it/s] 70%|███████   | 57/81 [00:05<00:01, 15.32it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.12it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.33it/s] 78%|███████▊  | 63/81 [00:06<00:01, 15.16it/s] 80%|████████  | 65/81 [00:06<00:01, 15.37it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.52it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.63it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.74it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.82it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.87it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.90it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.91it/s]100%|██████████| 81/81 [00:07<00:00, 15.94it/s]100%|██████████| 81/81 [00:07<00:00, 11.10it/s]
=> result
* total: 8,100
* correct: 4,323
* accuracy: 53.4%
* error: 46.6%
* macro_f1: 51.5%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 194	acc: 21.6%
* class: 1 (Forest)	total: 900	correct: 860	acc: 95.6%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 93	acc: 10.3%
* class: 3 (Highway or Road)	total: 750	correct: 441	acc: 58.8%
* class: 4 (Industrial Buildings)	total: 750	correct: 726	acc: 96.8%
* class: 5 (Pasture Land)	total: 600	correct: 415	acc: 69.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 629	acc: 83.9%
* class: 7 (Residential Buildings)	total: 900	correct: 362	acc: 40.2%
* class: 8 (River)	total: 750	correct: 306	acc: 40.8%
* class: 9 (Sea or Lake)	total: 900	correct: 297	acc: 33.0%
* average: 55.0%
Elapsed: 0:04:26
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_4-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.083 (0.583) data 0.000 (0.189) loss 1.1672 (1.1033) acc 14.8438 (17.3438) lr 2.0000e-03 eta 0:04:48
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [66/160] noisy rate: 0.25 --> 0.32 --> 0.17 <<<
epoch [2/100] batch [5/5] time 0.051 (0.444) data 0.000 (0.154) loss 2.1825 (2.1774) acc 16.6667 (21.8788) lr 1.9995e-03 eta 0:03:37
epoch [3/100] batch [5/5] time 0.393 (0.402) data 0.000 (0.169) loss 2.0733 (2.1476) acc 32.3529 (26.7490) lr 1.9980e-03 eta 0:03:14
epoch [4/100] batch [5/5] time 0.044 (0.220) data 0.000 (0.171) loss 2.1108 (2.0582) acc 14.2857 (26.9002) lr 1.9956e-03 eta 0:01:45
epoch [5/100] batch [5/5] time 0.046 (0.218) data 0.000 (0.165) loss 2.2351 (2.0888) acc 19.2308 (37.7198) lr 1.9921e-03 eta 0:01:43
epoch [6/100] batch [5/5] time 0.050 (0.206) data 0.000 (0.156) loss 2.1793 (2.0776) acc 33.3333 (34.5996) lr 1.9877e-03 eta 0:01:36
>>> samples [71/160] noisy rate: 0.25 --> 0.30 --> 0.15 <<<
epoch [7/100] batch [5/5] time 0.050 (0.230) data 0.000 (0.177) loss 2.0850 (2.0236) acc 32.1429 (34.0952) lr 1.9823e-03 eta 0:01:46
epoch [8/100] batch [5/5] time 0.053 (0.283) data 0.000 (0.163) loss 1.7068 (1.9951) acc 53.1250 (40.7177) lr 1.9759e-03 eta 0:02:10
epoch [9/100] batch [5/5] time 0.050 (0.325) data 0.001 (0.203) loss 1.8519 (1.9206) acc 39.2857 (42.5145) lr 1.9686e-03 eta 0:02:27
epoch [10/100] batch [5/5] time 0.046 (0.243) data 0.000 (0.187) loss 1.8547 (1.8925) acc 66.6667 (51.1955) lr 1.9603e-03 eta 0:01:49
epoch [11/100] batch [5/5] time 0.042 (0.306) data 0.000 (0.179) loss 1.8420 (1.8806) acc 40.3846 (49.7831) lr 1.9511e-03 eta 0:02:15
>>> samples [72/160] noisy rate: 0.25 --> 0.32 --> 0.15 <<<
epoch [12/100] batch [5/5] time 0.044 (0.237) data 0.000 (0.181) loss 1.7810 (1.8425) acc 60.4167 (54.2672) lr 1.9409e-03 eta 0:01:44
epoch [13/100] batch [5/5] time 0.045 (0.313) data 0.000 (0.177) loss 2.0927 (1.9054) acc 38.4615 (50.5641) lr 1.9298e-03 eta 0:02:16
epoch [14/100] batch [5/5] time 0.043 (0.231) data 0.000 (0.175) loss 1.6645 (1.8203) acc 61.5385 (58.2741) lr 1.9178e-03 eta 0:01:39
epoch [15/100] batch [5/5] time 0.050 (0.216) data 0.000 (0.162) loss 1.7909 (1.8182) acc 51.6667 (61.8158) lr 1.9048e-03 eta 0:01:31
epoch [16/100] batch [5/5] time 0.057 (0.226) data 0.000 (0.176) loss 1.8407 (1.8654) acc 51.2500 (58.9038) lr 1.8910e-03 eta 0:01:34
>>> samples [74/160] noisy rate: 0.25 --> 0.36 --> 0.16 <<<
epoch [17/100] batch [5/5] time 0.044 (0.227) data 0.000 (0.172) loss 1.6968 (1.7900) acc 75.0000 (63.9167) lr 1.8763e-03 eta 0:01:34
epoch [18/100] batch [5/5] time 0.061 (0.234) data 0.001 (0.178) loss 1.8344 (1.7772) acc 59.7222 (64.6468) lr 1.8607e-03 eta 0:01:36
epoch [19/100] batch [5/5] time 0.050 (0.224) data 0.000 (0.167) loss 1.7015 (1.7795) acc 71.6667 (62.7917) lr 1.8443e-03 eta 0:01:30
epoch [20/100] batch [5/5] time 0.052 (0.207) data 0.000 (0.153) loss 1.7795 (1.7494) acc 64.7059 (65.1268) lr 1.8271e-03 eta 0:01:22
epoch [21/100] batch [5/5] time 0.042 (0.200) data 0.000 (0.145) loss 1.5180 (1.7567) acc 78.8462 (65.4176) lr 1.8090e-03 eta 0:01:19
>>> samples [74/160] noisy rate: 0.25 --> 0.27 --> 0.16 <<<
epoch [22/100] batch [5/5] time 0.321 (0.278) data 0.000 (0.164) loss 1.8993 (1.7602) acc 52.7778 (65.6627) lr 1.7902e-03 eta 0:01:48
epoch [23/100] batch [5/5] time 0.053 (0.237) data 0.000 (0.183) loss 1.8061 (1.7278) acc 73.6111 (70.5240) lr 1.7705e-03 eta 0:01:31
epoch [24/100] batch [5/5] time 0.048 (0.232) data 0.000 (0.177) loss 1.5520 (1.7353) acc 84.3750 (70.9162) lr 1.7501e-03 eta 0:01:28
epoch [25/100] batch [5/5] time 0.042 (0.226) data 0.000 (0.170) loss 1.4680 (1.7093) acc 61.5385 (68.0444) lr 1.7290e-03 eta 0:01:24
epoch [26/100] batch [5/5] time 0.039 (0.227) data 0.000 (0.171) loss 1.7495 (1.6881) acc 55.0000 (69.8272) lr 1.7071e-03 eta 0:01:24
>>> samples [74/160] noisy rate: 0.25 --> 0.26 --> 0.16 <<<
epoch [27/100] batch [5/5] time 0.471 (0.335) data 0.000 (0.198) loss 1.5906 (1.7207) acc 75.0000 (64.5585) lr 1.6845e-03 eta 0:02:02
epoch [28/100] batch [5/5] time 0.056 (0.244) data 0.000 (0.189) loss 1.6583 (1.7225) acc 79.6875 (68.4908) lr 1.6613e-03 eta 0:01:27
epoch [29/100] batch [5/5] time 0.040 (0.230) data 0.000 (0.177) loss 1.7587 (1.6957) acc 68.7500 (71.9683) lr 1.6374e-03 eta 0:01:21
epoch [30/100] batch [5/5] time 0.046 (0.235) data 0.000 (0.183) loss 1.3741 (1.6399) acc 86.5385 (73.5520) lr 1.6129e-03 eta 0:01:22
epoch [31/100] batch [5/5] time 0.039 (0.221) data 0.000 (0.167) loss 1.8669 (1.7059) acc 75.0000 (70.5325) lr 1.5878e-03 eta 0:01:16
>>> samples [74/160] noisy rate: 0.25 --> 0.23 --> 0.16 <<<
epoch [32/100] batch [5/5] time 0.042 (0.233) data 0.000 (0.179) loss 1.5896 (1.6425) acc 71.1538 (75.1131) lr 1.5621e-03 eta 0:01:19
epoch [33/100] batch [5/5] time 0.047 (0.231) data 0.000 (0.176) loss 1.5827 (1.6719) acc 85.7143 (70.5686) lr 1.5358e-03 eta 0:01:17
epoch [34/100] batch [5/5] time 0.055 (0.235) data 0.000 (0.182) loss 1.5839 (1.6254) acc 76.5625 (77.6938) lr 1.5090e-03 eta 0:01:17
epoch [35/100] batch [5/5] time 0.053 (0.224) data 0.000 (0.171) loss 1.7846 (1.6776) acc 75.0000 (74.5635) lr 1.4818e-03 eta 0:01:12
epoch [36/100] batch [5/5] time 0.044 (0.225) data 0.000 (0.171) loss 1.9463 (1.6347) acc 58.9286 (73.6607) lr 1.4540e-03 eta 0:01:12
>>> samples [76/160] noisy rate: 0.25 --> 0.24 --> 0.17 <<<
epoch [37/100] batch [5/5] time 0.051 (0.244) data 0.000 (0.189) loss 1.5217 (1.6482) acc 58.3333 (74.3922) lr 1.4258e-03 eta 0:01:16
epoch [38/100] batch [5/5] time 0.053 (0.214) data 0.000 (0.159) loss 1.6954 (1.6698) acc 75.0000 (69.9699) lr 1.3971e-03 eta 0:01:06
epoch [39/100] batch [5/5] time 0.047 (0.202) data 0.001 (0.146) loss 1.4728 (1.6249) acc 80.7692 (75.4991) lr 1.3681e-03 eta 0:01:01
epoch [40/100] batch [5/5] time 0.066 (0.225) data 0.000 (0.166) loss 1.5263 (1.6525) acc 70.2381 (73.9507) lr 1.3387e-03 eta 0:01:07
epoch [41/100] batch [5/5] time 0.040 (0.240) data 0.000 (0.184) loss 1.6006 (1.6269) acc 80.0000 (76.1634) lr 1.3090e-03 eta 0:01:10
>>> samples [76/160] noisy rate: 0.25 --> 0.23 --> 0.17 <<<
epoch [42/100] batch [5/5] time 0.047 (0.232) data 0.000 (0.173) loss 1.6360 (1.6198) acc 82.1429 (74.8206) lr 1.2790e-03 eta 0:01:07
epoch [43/100] batch [5/5] time 0.058 (0.232) data 0.000 (0.173) loss 1.6760 (1.6374) acc 52.9412 (75.2549) lr 1.2487e-03 eta 0:01:06
epoch [44/100] batch [5/5] time 0.045 (0.226) data 0.000 (0.172) loss 1.8783 (1.6679) acc 60.7143 (72.3663) lr 1.2181e-03 eta 0:01:03
epoch [45/100] batch [5/5] time 0.049 (0.214) data 0.000 (0.159) loss 1.6693 (1.5901) acc 76.5625 (79.1880) lr 1.1874e-03 eta 0:00:58
epoch [46/100] batch [5/5] time 0.046 (0.220) data 0.000 (0.166) loss 1.8157 (1.6164) acc 71.1538 (75.3548) lr 1.1564e-03 eta 0:00:59
>>> samples [77/160] noisy rate: 0.25 --> 0.21 --> 0.17 <<<
epoch [47/100] batch [5/5] time 0.057 (0.219) data 0.000 (0.166) loss 1.8664 (1.6146) acc 58.8235 (75.9925) lr 1.1253e-03 eta 0:00:58
epoch [48/100] batch [5/5] time 0.045 (0.219) data 0.000 (0.166) loss 1.6579 (1.6038) acc 87.5000 (78.1075) lr 1.0941e-03 eta 0:00:56
epoch [49/100] batch [5/5] time 0.047 (0.223) data 0.000 (0.167) loss 1.7189 (1.6158) acc 63.4615 (76.7315) lr 1.0628e-03 eta 0:00:56
epoch [50/100] batch [5/5] time 0.048 (0.208) data 0.000 (0.153) loss 1.6801 (1.5977) acc 68.7500 (80.3542) lr 1.0314e-03 eta 0:00:51
epoch [51/100] batch [5/5] time 0.030 (0.219) data 0.000 (0.169) loss 1.8790 (1.6082) acc 78.1250 (76.7014) lr 1.0000e-03 eta 0:00:53
>>> samples [78/160] noisy rate: 0.25 --> 0.22 --> 0.18 <<<
epoch [52/100] batch [5/5] time 0.047 (0.213) data 0.000 (0.155) loss 1.7270 (1.6089) acc 73.0769 (76.2551) lr 9.6859e-04 eta 0:00:51
epoch [53/100] batch [5/5] time 0.045 (0.219) data 0.000 (0.166) loss 1.6745 (1.5886) acc 70.0000 (78.6458) lr 9.3721e-04 eta 0:00:51
epoch [54/100] batch [5/5] time 0.057 (0.226) data 0.000 (0.169) loss 1.8291 (1.6037) acc 76.3889 (76.2085) lr 9.0589e-04 eta 0:00:51
epoch [55/100] batch [5/5] time 0.050 (0.230) data 0.000 (0.175) loss 1.4232 (1.5910) acc 73.3333 (77.7456) lr 8.7467e-04 eta 0:00:51
epoch [56/100] batch [5/5] time 0.052 (0.222) data 0.001 (0.165) loss 1.5241 (1.6441) acc 76.4706 (79.2971) lr 8.4357e-04 eta 0:00:48
>>> samples [79/160] noisy rate: 0.25 --> 0.22 --> 0.19 <<<
epoch [57/100] batch [5/5] time 0.049 (0.237) data 0.000 (0.178) loss 1.7085 (1.5859) acc 75.0000 (79.3814) lr 8.1262e-04 eta 0:00:50
epoch [58/100] batch [5/5] time 0.061 (0.218) data 0.000 (0.165) loss 1.5708 (1.6080) acc 81.5789 (75.8158) lr 7.8186e-04 eta 0:00:45
epoch [59/100] batch [5/5] time 0.062 (0.216) data 0.000 (0.156) loss 1.6941 (1.5757) acc 77.5000 (78.8750) lr 7.5131e-04 eta 0:00:44
epoch [60/100] batch [5/5] time 0.050 (0.197) data 0.000 (0.142) loss 1.7608 (1.5680) acc 75.0000 (80.6250) lr 7.2101e-04 eta 0:00:39
epoch [61/100] batch [5/5] time 0.065 (0.234) data 0.000 (0.176) loss 1.5058 (1.5712) acc 71.2500 (81.4327) lr 6.9098e-04 eta 0:00:45
>>> samples [79/160] noisy rate: 0.25 --> 0.20 --> 0.19 <<<
epoch [62/100] batch [5/5] time 0.057 (0.249) data 0.000 (0.194) loss 1.4578 (1.5760) acc 76.2500 (80.3920) lr 6.6126e-04 eta 0:00:47
epoch [63/100] batch [5/5] time 0.051 (0.243) data 0.000 (0.190) loss 1.3193 (1.5704) acc 76.6667 (78.9449) lr 6.3188e-04 eta 0:00:44
epoch [64/100] batch [5/5] time 0.054 (0.231) data 0.001 (0.176) loss 1.8624 (1.5688) acc 62.5000 (79.9148) lr 6.0285e-04 eta 0:00:41
epoch [65/100] batch [5/5] time 0.055 (0.233) data 0.000 (0.174) loss 1.5257 (1.5751) acc 85.2941 (79.3023) lr 5.7422e-04 eta 0:00:40
epoch [66/100] batch [5/5] time 0.054 (0.243) data 0.000 (0.186) loss 1.4023 (1.5656) acc 79.4118 (79.3382) lr 5.4601e-04 eta 0:00:41
>>> samples [79/160] noisy rate: 0.25 --> 0.21 --> 0.19 <<<
epoch [67/100] batch [5/5] time 0.052 (0.228) data 0.000 (0.169) loss 1.7434 (1.5744) acc 68.7500 (77.5635) lr 5.1825e-04 eta 0:00:37
epoch [68/100] batch [5/5] time 0.055 (0.233) data 0.000 (0.177) loss 1.4355 (1.5630) acc 80.2632 (81.9885) lr 4.9096e-04 eta 0:00:37
epoch [69/100] batch [5/5] time 0.054 (0.219) data 0.000 (0.161) loss 1.6562 (1.5736) acc 68.0555 (76.7743) lr 4.6417e-04 eta 0:00:33
epoch [70/100] batch [5/5] time 0.055 (0.220) data 0.000 (0.165) loss 1.7268 (1.5605) acc 76.4706 (77.4767) lr 4.3792e-04 eta 0:00:32
epoch [71/100] batch [5/5] time 0.044 (0.232) data 0.000 (0.177) loss 1.2972 (1.5669) acc 80.3571 (78.5658) lr 4.1221e-04 eta 0:00:33
>>> samples [79/160] noisy rate: 0.25 --> 0.21 --> 0.19 <<<
epoch [72/100] batch [5/5] time 0.043 (0.230) data 0.000 (0.174) loss 1.6970 (1.5855) acc 68.7500 (77.5286) lr 3.8709e-04 eta 0:00:32
epoch [73/100] batch [5/5] time 0.054 (0.226) data 0.000 (0.170) loss 1.6657 (1.5732) acc 62.5000 (79.8207) lr 3.6258e-04 eta 0:00:30
epoch [74/100] batch [5/5] time 0.062 (0.232) data 0.000 (0.177) loss 1.5756 (1.5228) acc 78.7500 (80.5714) lr 3.3869e-04 eta 0:00:30
epoch [75/100] batch [5/5] time 0.067 (0.233) data 0.000 (0.176) loss 1.4098 (1.5743) acc 79.7619 (81.2485) lr 3.1545e-04 eta 0:00:29
epoch [76/100] batch [5/5] time 0.047 (0.282) data 0.000 (0.150) loss 1.7456 (1.5851) acc 78.8462 (80.6758) lr 2.9289e-04 eta 0:00:33
>>> samples [79/160] noisy rate: 0.25 --> 0.20 --> 0.19 <<<
epoch [77/100] batch [5/5] time 0.060 (0.235) data 0.000 (0.176) loss 1.6892 (1.5529) acc 76.4706 (80.2465) lr 2.7103e-04 eta 0:00:27
epoch [78/100] batch [5/5] time 0.047 (0.221) data 0.000 (0.163) loss 1.5943 (1.5517) acc 75.0000 (78.1244) lr 2.4989e-04 eta 0:00:24
epoch [79/100] batch [5/5] time 0.057 (0.228) data 0.001 (0.170) loss 1.4196 (1.5677) acc 89.7059 (82.2745) lr 2.2949e-04 eta 0:00:23
epoch [80/100] batch [5/5] time 0.055 (0.231) data 0.000 (0.177) loss 1.5290 (1.5361) acc 93.0555 (83.0481) lr 2.0984e-04 eta 0:00:23
epoch [81/100] batch [5/5] time 0.054 (0.222) data 0.000 (0.161) loss 1.6402 (1.5601) acc 84.7222 (78.7062) lr 1.9098e-04 eta 0:00:21
>>> samples [79/160] noisy rate: 0.25 --> 0.22 --> 0.19 <<<
epoch [82/100] batch [5/5] time 0.056 (0.231) data 0.000 (0.174) loss 1.7671 (1.5349) acc 76.3158 (79.7171) lr 1.7292e-04 eta 0:00:20
epoch [83/100] batch [5/5] time 0.050 (0.222) data 0.000 (0.164) loss 1.6298 (1.5563) acc 83.9286 (80.0886) lr 1.5567e-04 eta 0:00:18
epoch [84/100] batch [5/5] time 0.046 (0.205) data 0.000 (0.151) loss 1.3067 (1.5514) acc 83.3333 (79.2299) lr 1.3926e-04 eta 0:00:16
epoch [85/100] batch [5/5] time 0.046 (0.239) data 0.000 (0.184) loss 1.4209 (1.5338) acc 80.7692 (81.2997) lr 1.2369e-04 eta 0:00:17
epoch [86/100] batch [5/5] time 0.053 (0.238) data 0.000 (0.183) loss 1.4610 (1.5595) acc 80.8824 (79.4994) lr 1.0899e-04 eta 0:00:16
>>> samples [79/160] noisy rate: 0.25 --> 0.19 --> 0.19 <<<
epoch [87/100] batch [5/5] time 0.056 (0.209) data 0.000 (0.148) loss 1.5566 (1.5706) acc 83.8235 (79.2250) lr 9.5173e-05 eta 0:00:13
epoch [88/100] batch [5/5] time 0.049 (0.200) data 0.000 (0.147) loss 1.5564 (1.5593) acc 78.1250 (81.3362) lr 8.2245e-05 eta 0:00:12
epoch [89/100] batch [5/5] time 0.059 (0.204) data 0.000 (0.146) loss 1.6513 (1.5461) acc 73.6111 (82.1224) lr 7.0224e-05 eta 0:00:11
epoch [90/100] batch [5/5] time 0.043 (0.202) data 0.000 (0.144) loss 1.4739 (1.5414) acc 82.6923 (82.3894) lr 5.9119e-05 eta 0:00:10
epoch [91/100] batch [5/5] time 0.057 (0.212) data 0.000 (0.157) loss 1.5082 (1.5715) acc 81.2500 (81.1470) lr 4.8943e-05 eta 0:00:09
>>> samples [81/160] noisy rate: 0.25 --> 0.19 --> 0.19 <<<
epoch [92/100] batch [5/5] time 0.062 (0.254) data 0.000 (0.197) loss 1.4493 (1.5044) acc 71.2500 (81.7775) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.046 (0.241) data 0.000 (0.185) loss 1.3458 (1.5384) acc 75.0000 (78.1803) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.045 (0.211) data 0.001 (0.155) loss 1.3721 (1.4938) acc 83.9286 (82.7002) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.053 (0.202) data 0.000 (0.142) loss 1.6875 (1.5190) acc 85.9375 (82.0903) lr 1.7713e-05 eta 0:00:05
epoch [96/100] batch [5/5] time 0.049 (0.295) data 0.000 (0.153) loss 1.3812 (1.5495) acc 87.5000 (80.0676) lr 1.2312e-05 eta 0:00:05
>>> samples [81/160] noisy rate: 0.25 --> 0.17 --> 0.19 <<<
epoch [97/100] batch [5/5] time 0.038 (0.216) data 0.000 (0.160) loss 1.4140 (1.4980) acc 85.0000 (82.8333) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.049 (0.335) data 0.000 (0.195) loss 1.4605 (1.5252) acc 83.9286 (82.8005) lr 4.4380e-06 eta 0:00:03
epoch [99/100] batch [5/5] time 0.054 (0.220) data 0.000 (0.161) loss 1.3482 (1.5475) acc 85.9375 (80.8207) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.062 (0.230) data 0.000 (0.175) loss 1.4291 (1.5651) acc 92.8571 (77.7936) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.32, 0.3, 0.32, 0.36, 0.27, 0.26, 0.23, 0.24, 0.23, 0.21, 0.22, 0.22, 0.2, 0.21, 0.21, 0.2, 0.22, 0.19, 0.19, 0.17]
* learned noise rate: [0.17, 0.15, 0.15, 0.16, 0.16, 0.16, 0.16, 0.17, 0.17, 0.17, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:35,  1.94s/it]  4%|▎         | 3/81 [00:02<00:42,  1.82it/s]  6%|▌         | 5/81 [00:02<00:22,  3.34it/s]  9%|▊         | 7/81 [00:02<00:14,  5.01it/s] 11%|█         | 9/81 [00:02<00:10,  6.76it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.36it/s] 16%|█▌        | 13/81 [00:02<00:06,  9.72it/s] 19%|█▊        | 15/81 [00:02<00:06, 10.99it/s] 21%|██        | 17/81 [00:02<00:05, 12.12it/s] 23%|██▎       | 19/81 [00:03<00:04, 13.06it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.49it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.90it/s] 31%|███       | 25/81 [00:03<00:03, 14.08it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.29it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.41it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.72it/s] 41%|████      | 33/81 [00:04<00:03, 14.88it/s] 43%|████▎     | 35/81 [00:04<00:03, 15.15it/s] 46%|████▌     | 37/81 [00:04<00:03, 14.38it/s] 48%|████▊     | 39/81 [00:04<00:02, 14.61it/s] 51%|█████     | 41/81 [00:04<00:02, 14.94it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.19it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.35it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.30it/s] 60%|██████    | 49/81 [00:05<00:02, 15.03it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.27it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.44it/s] 68%|██████▊   | 55/81 [00:05<00:01, 15.57it/s] 70%|███████   | 57/81 [00:05<00:01, 15.57it/s] 73%|███████▎  | 59/81 [00:05<00:01, 15.59it/s] 75%|███████▌  | 61/81 [00:05<00:01, 15.67it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.75it/s] 80%|████████  | 65/81 [00:06<00:01, 15.75it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.80it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.84it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.89it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.91it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.94it/s] 95%|█████████▌| 77/81 [00:06<00:00, 15.97it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.99it/s]100%|██████████| 81/81 [00:07<00:00, 16.01it/s]100%|██████████| 81/81 [00:07<00:00, 11.08it/s]
=> result
* total: 8,100
* correct: 5,050
* accuracy: 62.3%
* error: 37.7%
* macro_f1: 60.9%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 475	acc: 52.8%
* class: 1 (Forest)	total: 900	correct: 870	acc: 96.7%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 78	acc: 8.7%
* class: 3 (Highway or Road)	total: 750	correct: 491	acc: 65.5%
* class: 4 (Industrial Buildings)	total: 750	correct: 654	acc: 87.2%
* class: 5 (Pasture Land)	total: 600	correct: 440	acc: 73.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 551	acc: 73.5%
* class: 7 (Residential Buildings)	total: 900	correct: 557	acc: 61.9%
* class: 8 (River)	total: 750	correct: 453	acc: 60.4%
* class: 9 (Sea or Lake)	total: 900	correct: 481	acc: 53.4%
* average: 63.3%
Elapsed: 0:04:01
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_4-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.591) data 0.000 (0.200) loss 1.1128 (1.1103) acc 21.0938 (16.2500) lr 2.0000e-03 eta 0:04:52
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [38/160] noisy rate: 0.25 --> 0.38 --> 0.05 <<<
epoch [2/100] batch [5/5] time 0.036 (0.312) data 0.000 (0.147) loss 2.0781 (2.1261) acc 31.2500 (20.8532) lr 1.9995e-03 eta 0:02:32
epoch [3/100] batch [5/5] time 0.292 (0.389) data 0.000 (0.162) loss 1.8512 (2.1236) acc 47.5000 (25.5714) lr 1.9980e-03 eta 0:03:08
epoch [4/100] batch [5/5] time 0.042 (0.253) data 0.000 (0.210) loss 1.8999 (1.9431) acc 50.0000 (40.7083) lr 1.9956e-03 eta 0:02:01
epoch [5/100] batch [5/5] time 0.181 (0.233) data 0.001 (0.160) loss 1.8448 (1.8502) acc 50.0000 (44.4286) lr 1.9921e-03 eta 0:01:50
epoch [6/100] batch [5/5] time 0.046 (0.221) data 0.000 (0.171) loss 1.7466 (1.8381) acc 45.8333 (47.3611) lr 1.9877e-03 eta 0:01:43
>>> samples [49/160] noisy rate: 0.25 --> 0.44 --> 0.10 <<<
epoch [7/100] batch [5/5] time 0.046 (0.293) data 0.000 (0.188) loss 1.6981 (1.7721) acc 61.3636 (57.4513) lr 1.9823e-03 eta 0:02:16
epoch [8/100] batch [5/5] time 0.042 (0.252) data 0.001 (0.207) loss 1.6195 (1.6648) acc 72.7273 (58.8750) lr 1.9759e-03 eta 0:01:55
epoch [9/100] batch [5/5] time 0.044 (0.259) data 0.000 (0.204) loss 1.7713 (1.6161) acc 50.0000 (60.9821) lr 1.9686e-03 eta 0:01:57
epoch [10/100] batch [5/5] time 0.045 (0.276) data 0.000 (0.157) loss 1.2456 (1.5030) acc 71.8750 (69.2192) lr 1.9603e-03 eta 0:02:04
epoch [11/100] batch [5/5] time 0.035 (0.268) data 0.000 (0.158) loss 1.6094 (1.5293) acc 60.7143 (67.7662) lr 1.9511e-03 eta 0:01:59
>>> samples [52/160] noisy rate: 0.25 --> 0.38 --> 0.12 <<<
epoch [12/100] batch [5/5] time 0.344 (0.247) data 0.000 (0.138) loss 1.4088 (1.5189) acc 78.8462 (71.0030) lr 1.9409e-03 eta 0:01:48
epoch [13/100] batch [5/5] time 0.039 (0.212) data 0.000 (0.167) loss 1.4171 (1.4692) acc 72.5000 (71.8995) lr 1.9298e-03 eta 0:01:32
epoch [14/100] batch [5/5] time 0.038 (0.201) data 0.000 (0.156) loss 1.5076 (1.4636) acc 59.0909 (70.3278) lr 1.9178e-03 eta 0:01:26
epoch [15/100] batch [5/5] time 0.049 (0.195) data 0.000 (0.154) loss 1.3812 (1.4153) acc 64.2857 (73.6667) lr 1.9048e-03 eta 0:01:22
epoch [16/100] batch [5/5] time 0.030 (0.263) data 0.000 (0.152) loss 1.4560 (1.3868) acc 78.1250 (75.6384) lr 1.8910e-03 eta 0:01:50
>>> samples [54/160] noisy rate: 0.25 --> 0.38 --> 0.11 <<<
epoch [17/100] batch [5/5] time 0.040 (0.231) data 0.000 (0.187) loss 1.1830 (1.3497) acc 90.0000 (78.6742) lr 1.8763e-03 eta 0:01:35
epoch [18/100] batch [5/5] time 0.039 (0.271) data 0.000 (0.154) loss 1.3294 (1.3884) acc 77.5000 (74.0000) lr 1.8607e-03 eta 0:01:50
epoch [19/100] batch [5/5] time 0.044 (0.206) data 0.000 (0.158) loss 1.4395 (1.3778) acc 57.1429 (80.5952) lr 1.8443e-03 eta 0:01:23
epoch [20/100] batch [5/5] time 0.045 (0.205) data 0.000 (0.153) loss 1.2170 (1.3840) acc 76.7857 (74.6429) lr 1.8271e-03 eta 0:01:21
epoch [21/100] batch [5/5] time 0.043 (0.213) data 0.000 (0.164) loss 1.4294 (1.4185) acc 75.0000 (73.4154) lr 1.8090e-03 eta 0:01:24
>>> samples [56/160] noisy rate: 0.25 --> 0.37 --> 0.12 <<<
epoch [22/100] batch [5/5] time 0.044 (0.206) data 0.000 (0.156) loss 1.3748 (1.3835) acc 72.9167 (73.6667) lr 1.7902e-03 eta 0:01:20
epoch [23/100] batch [5/5] time 0.042 (0.210) data 0.001 (0.160) loss 1.2330 (1.3612) acc 85.4167 (78.0972) lr 1.7705e-03 eta 0:01:20
epoch [24/100] batch [5/5] time 0.040 (0.230) data 0.000 (0.187) loss 1.6239 (1.3598) acc 65.0000 (76.3566) lr 1.7501e-03 eta 0:01:27
epoch [25/100] batch [5/5] time 0.045 (0.275) data 0.000 (0.231) loss 1.6217 (1.3374) acc 69.2308 (83.7830) lr 1.7290e-03 eta 0:01:42
epoch [26/100] batch [5/5] time 0.040 (0.341) data 0.000 (0.219) loss 1.2658 (1.3286) acc 77.7778 (82.5758) lr 1.7071e-03 eta 0:02:06
>>> samples [68/160] noisy rate: 0.25 --> 0.36 --> 0.19 <<<
epoch [27/100] batch [5/5] time 0.044 (0.308) data 0.000 (0.181) loss 1.2087 (1.3450) acc 87.5000 (77.4920) lr 1.6845e-03 eta 0:01:52
epoch [28/100] batch [5/5] time 0.039 (0.238) data 0.000 (0.181) loss 1.4935 (1.3691) acc 80.0000 (80.7842) lr 1.6613e-03 eta 0:01:25
epoch [29/100] batch [5/5] time 0.039 (0.224) data 0.000 (0.173) loss 1.3321 (1.3187) acc 90.0000 (83.1786) lr 1.6374e-03 eta 0:01:19
epoch [30/100] batch [5/5] time 0.035 (0.231) data 0.000 (0.179) loss 1.0414 (1.2663) acc 84.3750 (82.4186) lr 1.6129e-03 eta 0:01:20
epoch [31/100] batch [5/5] time 0.044 (0.230) data 0.000 (0.175) loss 1.4202 (1.3332) acc 80.7692 (78.6035) lr 1.5878e-03 eta 0:01:19
>>> samples [74/160] noisy rate: 0.25 --> 0.41 --> 0.22 <<<
epoch [32/100] batch [5/5] time 0.058 (0.216) data 0.000 (0.160) loss 1.2760 (1.3254) acc 88.2353 (77.0079) lr 1.5621e-03 eta 0:01:13
epoch [33/100] batch [5/5] time 0.049 (0.208) data 0.001 (0.156) loss 1.4079 (1.3290) acc 71.8750 (82.1875) lr 1.5358e-03 eta 0:01:09
epoch [34/100] batch [5/5] time 0.051 (0.214) data 0.000 (0.158) loss 1.4436 (1.3380) acc 80.0000 (78.0000) lr 1.5090e-03 eta 0:01:10
epoch [35/100] batch [5/5] time 0.061 (0.284) data 0.000 (0.153) loss 1.4668 (1.3473) acc 88.1579 (75.5482) lr 1.4818e-03 eta 0:01:32
epoch [36/100] batch [5/5] time 0.050 (0.235) data 0.000 (0.179) loss 1.3825 (1.2934) acc 73.2143 (80.8912) lr 1.4540e-03 eta 0:01:15
>>> samples [75/160] noisy rate: 0.25 --> 0.35 --> 0.23 <<<
epoch [37/100] batch [5/5] time 0.051 (0.212) data 0.000 (0.159) loss 1.3348 (1.3533) acc 81.6667 (78.2338) lr 1.4258e-03 eta 0:01:06
epoch [38/100] batch [5/5] time 0.471 (0.333) data 0.000 (0.192) loss 1.2966 (1.3195) acc 90.2174 (79.8629) lr 1.3971e-03 eta 0:01:43
epoch [39/100] batch [5/5] time 0.039 (0.285) data 0.000 (0.158) loss 1.2546 (1.3230) acc 80.0000 (77.6050) lr 1.3681e-03 eta 0:01:26
epoch [40/100] batch [5/5] time 0.039 (0.226) data 0.000 (0.173) loss 1.3032 (1.3237) acc 83.3333 (79.2557) lr 1.3387e-03 eta 0:01:07
epoch [41/100] batch [5/5] time 0.056 (0.222) data 0.000 (0.166) loss 1.3504 (1.2977) acc 73.6111 (78.2891) lr 1.3090e-03 eta 0:01:05
>>> samples [75/160] noisy rate: 0.25 --> 0.31 --> 0.23 <<<
epoch [42/100] batch [5/5] time 0.058 (0.255) data 0.001 (0.195) loss 1.3428 (1.3164) acc 80.5555 (78.7546) lr 1.2790e-03 eta 0:01:13
epoch [43/100] batch [5/5] time 0.057 (0.229) data 0.000 (0.175) loss 1.4457 (1.3233) acc 70.8333 (79.9586) lr 1.2487e-03 eta 0:01:05
epoch [44/100] batch [5/5] time 0.050 (0.257) data 0.000 (0.201) loss 1.5695 (1.3315) acc 68.3333 (81.7083) lr 1.2181e-03 eta 0:01:11
epoch [45/100] batch [5/5] time 0.062 (0.228) data 0.000 (0.169) loss 1.3494 (1.2952) acc 84.2105 (79.6596) lr 1.1874e-03 eta 0:01:02
epoch [46/100] batch [5/5] time 0.047 (0.232) data 0.000 (0.180) loss 1.1964 (1.3299) acc 90.3846 (80.9728) lr 1.1564e-03 eta 0:01:02
>>> samples [76/160] noisy rate: 0.25 --> 0.34 --> 0.22 <<<
epoch [47/100] batch [5/5] time 0.055 (0.215) data 0.000 (0.161) loss 1.2180 (1.2993) acc 89.0625 (79.4803) lr 1.1253e-03 eta 0:00:56
epoch [48/100] batch [5/5] time 0.049 (0.230) data 0.000 (0.173) loss 1.0534 (1.3281) acc 89.0625 (76.1213) lr 1.0941e-03 eta 0:00:59
epoch [49/100] batch [5/5] time 0.039 (0.209) data 0.000 (0.152) loss 0.9683 (1.2898) acc 93.1818 (80.4750) lr 1.0628e-03 eta 0:00:53
epoch [50/100] batch [5/5] time 0.469 (0.338) data 0.000 (0.203) loss 1.2298 (1.2978) acc 87.5000 (78.3718) lr 1.0314e-03 eta 0:01:24
epoch [51/100] batch [5/5] time 0.056 (0.249) data 0.000 (0.195) loss 1.1910 (1.2610) acc 80.8824 (83.8989) lr 1.0000e-03 eta 0:01:01
>>> samples [78/160] noisy rate: 0.25 --> 0.33 --> 0.24 <<<
epoch [52/100] batch [5/5] time 0.045 (0.224) data 0.000 (0.170) loss 1.1991 (1.3051) acc 83.3333 (80.6954) lr 9.6859e-04 eta 0:00:53
epoch [53/100] batch [5/5] time 0.056 (0.224) data 0.000 (0.169) loss 1.2655 (1.2985) acc 80.8824 (75.8868) lr 9.3721e-04 eta 0:00:52
epoch [54/100] batch [5/5] time 0.046 (0.215) data 0.000 (0.161) loss 1.5532 (1.2769) acc 68.3333 (81.2071) lr 9.0589e-04 eta 0:00:49
epoch [55/100] batch [5/5] time 0.057 (0.212) data 0.000 (0.157) loss 1.2271 (1.3082) acc 86.1111 (78.2027) lr 8.7467e-04 eta 0:00:47
epoch [56/100] batch [5/5] time 0.050 (0.219) data 0.000 (0.162) loss 1.2783 (1.2843) acc 83.3333 (83.8210) lr 8.4357e-04 eta 0:00:48
>>> samples [80/160] noisy rate: 0.25 --> 0.27 --> 0.25 <<<
epoch [57/100] batch [5/5] time 0.052 (0.220) data 0.000 (0.161) loss 1.4090 (1.2747) acc 75.0000 (81.4062) lr 8.1262e-04 eta 0:00:47
epoch [58/100] batch [5/5] time 0.052 (0.211) data 0.000 (0.155) loss 1.6264 (1.2632) acc 78.1250 (83.4512) lr 7.8186e-04 eta 0:00:44
epoch [59/100] batch [5/5] time 0.045 (0.205) data 0.000 (0.148) loss 1.2339 (1.2989) acc 82.1429 (82.8215) lr 7.5131e-04 eta 0:00:42
epoch [60/100] batch [5/5] time 0.056 (0.198) data 0.000 (0.144) loss 1.1310 (1.2602) acc 80.2632 (82.8327) lr 7.2101e-04 eta 0:00:39
epoch [61/100] batch [5/5] time 0.052 (0.198) data 0.000 (0.140) loss 1.2330 (1.2852) acc 76.7857 (79.2546) lr 6.9098e-04 eta 0:00:38
>>> samples [81/160] noisy rate: 0.25 --> 0.27 --> 0.25 <<<
epoch [62/100] batch [5/5] time 0.052 (0.232) data 0.000 (0.172) loss 1.3388 (1.2677) acc 75.0000 (81.6082) lr 6.6126e-04 eta 0:00:44
epoch [63/100] batch [5/5] time 0.050 (0.238) data 0.000 (0.179) loss 1.0059 (1.2685) acc 88.4615 (83.0938) lr 6.3188e-04 eta 0:00:44
epoch [64/100] batch [5/5] time 0.060 (0.237) data 0.000 (0.182) loss 1.2664 (1.2549) acc 79.1667 (80.7490) lr 6.0285e-04 eta 0:00:42
epoch [65/100] batch [5/5] time 0.053 (0.245) data 0.000 (0.183) loss 1.3693 (1.2687) acc 78.5714 (81.6668) lr 5.7422e-04 eta 0:00:42
epoch [66/100] batch [5/5] time 0.055 (0.208) data 0.000 (0.147) loss 1.0965 (1.2554) acc 82.3529 (83.5168) lr 5.4601e-04 eta 0:00:35
>>> samples [82/160] noisy rate: 0.25 --> 0.28 --> 0.26 <<<
epoch [67/100] batch [5/5] time 0.063 (0.242) data 0.000 (0.184) loss 1.2484 (1.2140) acc 78.9474 (84.1325) lr 5.1825e-04 eta 0:00:39
epoch [68/100] batch [5/5] time 0.055 (0.216) data 0.000 (0.159) loss 1.1164 (1.2224) acc 87.5000 (84.4911) lr 4.9096e-04 eta 0:00:34
epoch [69/100] batch [5/5] time 0.042 (0.209) data 0.000 (0.155) loss 1.5004 (1.2713) acc 75.0000 (81.9048) lr 4.6417e-04 eta 0:00:32
epoch [70/100] batch [5/5] time 0.060 (0.206) data 0.000 (0.148) loss 1.0983 (1.2273) acc 84.7222 (85.2453) lr 4.3792e-04 eta 0:00:30
epoch [71/100] batch [5/5] time 0.059 (0.231) data 0.000 (0.173) loss 1.5052 (1.2583) acc 72.0588 (80.7398) lr 4.1221e-04 eta 0:00:33
>>> samples [82/160] noisy rate: 0.25 --> 0.28 --> 0.26 <<<
epoch [72/100] batch [5/5] time 0.056 (0.206) data 0.000 (0.144) loss 1.4859 (1.2464) acc 82.8125 (84.0134) lr 3.8709e-04 eta 0:00:28
epoch [73/100] batch [5/5] time 0.065 (0.189) data 0.000 (0.128) loss 1.3111 (1.2401) acc 83.7500 (83.7143) lr 3.6258e-04 eta 0:00:25
epoch [74/100] batch [5/5] time 0.051 (0.210) data 0.001 (0.153) loss 1.1341 (1.2585) acc 85.9375 (85.5747) lr 3.3869e-04 eta 0:00:27
epoch [75/100] batch [5/5] time 0.039 (0.235) data 0.000 (0.178) loss 1.1811 (1.2473) acc 87.5000 (84.8056) lr 3.1545e-04 eta 0:00:29
epoch [76/100] batch [5/5] time 0.059 (0.310) data 0.000 (0.163) loss 1.3168 (1.2760) acc 76.4706 (80.0025) lr 2.9289e-04 eta 0:00:37
>>> samples [82/160] noisy rate: 0.25 --> 0.28 --> 0.26 <<<
epoch [77/100] batch [5/5] time 0.056 (0.260) data 0.000 (0.200) loss 1.2169 (1.2545) acc 86.1111 (81.1401) lr 2.7103e-04 eta 0:00:29
epoch [78/100] batch [5/5] time 0.063 (0.226) data 0.000 (0.162) loss 0.9302 (1.2556) acc 97.3684 (81.8366) lr 2.4989e-04 eta 0:00:24
epoch [79/100] batch [5/5] time 0.050 (0.229) data 0.000 (0.172) loss 1.3296 (1.2954) acc 75.0000 (80.9399) lr 2.2949e-04 eta 0:00:24
epoch [80/100] batch [5/5] time 0.069 (0.217) data 0.000 (0.158) loss 1.2598 (1.2348) acc 83.3333 (85.1512) lr 2.0984e-04 eta 0:00:21
epoch [81/100] batch [5/5] time 0.049 (0.257) data 0.000 (0.200) loss 1.1137 (1.2416) acc 78.5714 (84.2907) lr 1.9098e-04 eta 0:00:24
>>> samples [85/160] noisy rate: 0.25 --> 0.28 --> 0.26 <<<
epoch [82/100] batch [5/5] time 0.056 (0.252) data 0.000 (0.193) loss 1.2577 (1.2152) acc 80.5555 (84.4993) lr 1.7292e-04 eta 0:00:22
epoch [83/100] batch [5/5] time 0.057 (0.268) data 0.000 (0.211) loss 1.1485 (1.1990) acc 89.7059 (83.5294) lr 1.5567e-04 eta 0:00:22
epoch [84/100] batch [5/5] time 0.049 (0.231) data 0.000 (0.171) loss 1.0227 (1.2321) acc 92.8571 (83.4329) lr 1.3926e-04 eta 0:00:18
epoch [85/100] batch [5/5] time 0.059 (0.246) data 0.000 (0.184) loss 1.1198 (1.2240) acc 83.3333 (83.7990) lr 1.2369e-04 eta 0:00:18
epoch [86/100] batch [5/5] time 0.065 (0.240) data 0.000 (0.176) loss 1.3104 (1.2298) acc 82.5000 (82.7408) lr 1.0899e-04 eta 0:00:16
>>> samples [85/160] noisy rate: 0.25 --> 0.29 --> 0.26 <<<
epoch [87/100] batch [5/5] time 0.052 (0.252) data 0.000 (0.189) loss 1.2529 (1.2362) acc 82.1429 (84.3866) lr 9.5173e-05 eta 0:00:16
epoch [88/100] batch [5/5] time 0.060 (0.234) data 0.000 (0.168) loss 1.0745 (1.2132) acc 86.7647 (85.0734) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.052 (0.239) data 0.000 (0.176) loss 1.3857 (1.2260) acc 83.3333 (82.0533) lr 7.0224e-05 eta 0:00:13
epoch [90/100] batch [5/5] time 0.056 (0.227) data 0.000 (0.162) loss 1.0874 (1.2001) acc 82.8125 (83.8514) lr 5.9119e-05 eta 0:00:11
epoch [91/100] batch [5/5] time 0.051 (0.240) data 0.000 (0.172) loss 1.4400 (1.2046) acc 75.0000 (85.3862) lr 4.8943e-05 eta 0:00:10
>>> samples [85/160] noisy rate: 0.25 --> 0.28 --> 0.26 <<<
epoch [92/100] batch [5/5] time 0.057 (0.222) data 0.000 (0.165) loss 1.0579 (1.2365) acc 82.3529 (82.3354) lr 3.9706e-05 eta 0:00:08
epoch [93/100] batch [5/5] time 0.056 (0.260) data 0.000 (0.200) loss 0.9676 (1.2231) acc 85.9375 (82.0167) lr 3.1417e-05 eta 0:00:09
epoch [94/100] batch [5/5] time 0.045 (0.215) data 0.000 (0.154) loss 1.0428 (1.2256) acc 91.0714 (84.6161) lr 2.4083e-05 eta 0:00:06
epoch [95/100] batch [5/5] time 0.057 (0.244) data 0.000 (0.185) loss 1.3416 (1.2109) acc 86.7647 (85.1307) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.051 (0.246) data 0.000 (0.182) loss 1.4472 (1.2273) acc 78.5714 (81.2723) lr 1.2312e-05 eta 0:00:04
>>> samples [85/160] noisy rate: 0.25 --> 0.30 --> 0.26 <<<
epoch [97/100] batch [5/5] time 0.047 (0.246) data 0.000 (0.192) loss 1.2300 (1.1973) acc 83.3333 (86.4301) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.044 (0.208) data 0.000 (0.150) loss 1.0026 (1.2289) acc 83.9286 (83.3413) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.044 (0.215) data 0.000 (0.159) loss 1.0916 (1.2184) acc 94.6429 (83.7827) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.052 (0.256) data 0.000 (0.201) loss 1.4103 (1.2371) acc 69.1176 (82.5140) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_4FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.38, 0.44, 0.38, 0.38, 0.37, 0.36, 0.41, 0.35, 0.31, 0.34, 0.33, 0.27, 0.27, 0.28, 0.28, 0.28, 0.28, 0.29, 0.28, 0.3]
* learned noise rate: [0.05, 0.1, 0.12, 0.11, 0.12, 0.19, 0.22, 0.23, 0.23, 0.22, 0.24, 0.25, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:00<01:15,  1.05it/s]  4%|▎         | 3/81 [00:01<00:23,  3.26it/s]  6%|▌         | 5/81 [00:01<00:14,  5.31it/s]  9%|▊         | 7/81 [00:01<00:10,  6.87it/s] 11%|█         | 9/81 [00:01<00:08,  8.32it/s] 14%|█▎        | 11/81 [00:01<00:07,  9.94it/s] 16%|█▌        | 13/81 [00:01<00:06, 11.10it/s] 19%|█▊        | 15/81 [00:01<00:05, 12.29it/s] 21%|██        | 17/81 [00:02<00:04, 13.17it/s] 23%|██▎       | 19/81 [00:02<00:04, 13.88it/s] 26%|██▌       | 21/81 [00:02<00:04, 14.07it/s] 28%|██▊       | 23/81 [00:02<00:03, 14.57it/s] 31%|███       | 25/81 [00:02<00:03, 14.94it/s] 33%|███▎      | 27/81 [00:02<00:03, 14.88it/s] 36%|███▌      | 29/81 [00:02<00:03, 15.16it/s] 38%|███▊      | 31/81 [00:03<00:03, 15.36it/s] 41%|████      | 33/81 [00:03<00:03, 15.47it/s] 43%|████▎     | 35/81 [00:03<00:02, 15.56it/s] 46%|████▌     | 37/81 [00:03<00:02, 15.06it/s] 48%|████▊     | 39/81 [00:03<00:02, 15.31it/s] 51%|█████     | 41/81 [00:03<00:02, 15.47it/s] 53%|█████▎    | 43/81 [00:03<00:02, 15.28it/s] 56%|█████▌    | 45/81 [00:03<00:02, 15.44it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.58it/s] 60%|██████    | 49/81 [00:04<00:02, 15.36it/s] 63%|██████▎   | 51/81 [00:04<00:01, 15.33it/s] 65%|██████▌   | 53/81 [00:04<00:01, 15.49it/s] 68%|██████▊   | 55/81 [00:04<00:01, 15.29it/s] 70%|███████   | 57/81 [00:04<00:01, 15.31it/s] 73%|███████▎  | 59/81 [00:04<00:01, 15.46it/s] 75%|███████▌  | 61/81 [00:04<00:01, 15.58it/s] 78%|███████▊  | 63/81 [00:05<00:01, 15.58it/s] 80%|████████  | 65/81 [00:05<00:01, 15.65it/s] 83%|████████▎ | 67/81 [00:05<00:00, 15.35it/s] 85%|████████▌ | 69/81 [00:05<00:00, 15.52it/s] 88%|████████▊ | 71/81 [00:05<00:00, 15.65it/s] 90%|█████████ | 73/81 [00:05<00:00, 15.74it/s] 93%|█████████▎| 75/81 [00:05<00:00, 15.81it/s] 95%|█████████▌| 77/81 [00:05<00:00, 15.86it/s] 98%|█████████▊| 79/81 [00:06<00:00, 15.90it/s]100%|██████████| 81/81 [00:06<00:00, 15.94it/s]100%|██████████| 81/81 [00:06<00:00, 12.72it/s]
=> result
* total: 8,100
* correct: 4,708
* accuracy: 58.1%
* error: 41.9%
* macro_f1: 53.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 457	acc: 50.8%
* class: 1 (Forest)	total: 900	correct: 857	acc: 95.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 37	acc: 4.1%
* class: 3 (Highway or Road)	total: 750	correct: 527	acc: 70.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 629	acc: 83.9%
* class: 5 (Pasture Land)	total: 600	correct: 470	acc: 78.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 474	acc: 63.2%
* class: 7 (Residential Buildings)	total: 900	correct: 850	acc: 94.4%
* class: 8 (River)	total: 750	correct: 383	acc: 51.1%
* class: 9 (Sea or Lake)	total: 900	correct: 24	acc: 2.7%
* average: 59.4%
Elapsed: 0:04:02
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_6-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.091 (0.709) data 0.000 (0.239) loss 1.1257 (1.1397) acc 9.3750 (14.5312) lr 2.0000e-03 eta 0:05:51
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [47/160] noisy rate: 0.38 --> 0.39 --> 0.13 <<<
epoch [2/100] batch [5/5] time 0.039 (0.444) data 0.000 (0.205) loss 2.1517 (2.2140) acc 12.5000 (11.7302) lr 1.9995e-03 eta 0:03:37
epoch [3/100] batch [5/5] time 0.040 (0.258) data 0.000 (0.212) loss 2.0463 (2.0476) acc 22.5000 (24.8472) lr 1.9980e-03 eta 0:02:05
epoch [4/100] batch [5/5] time 0.030 (0.292) data 0.000 (0.194) loss 1.8218 (1.9020) acc 37.5000 (33.4040) lr 1.9956e-03 eta 0:02:20
epoch [5/100] batch [5/5] time 0.038 (0.262) data 0.000 (0.212) loss 1.6668 (1.7720) acc 47.7273 (48.1705) lr 1.9921e-03 eta 0:02:04
epoch [6/100] batch [5/5] time 0.050 (0.368) data 0.000 (0.212) loss 1.5764 (1.7903) acc 48.2143 (42.6429) lr 1.9877e-03 eta 0:02:52
>>> samples [55/160] noisy rate: 0.38 --> 0.34 --> 0.11 <<<
epoch [7/100] batch [5/5] time 0.042 (0.298) data 0.000 (0.191) loss 1.8166 (1.8301) acc 45.4545 (39.5076) lr 1.9823e-03 eta 0:02:18
epoch [8/100] batch [5/5] time 0.043 (0.307) data 0.000 (0.199) loss 1.5829 (1.7254) acc 31.8182 (46.7940) lr 1.9759e-03 eta 0:02:21
epoch [9/100] batch [5/5] time 0.048 (0.246) data 0.000 (0.200) loss 1.5745 (1.7631) acc 57.1429 (47.6142) lr 1.9686e-03 eta 0:01:52
epoch [10/100] batch [5/5] time 0.186 (0.253) data 0.000 (0.179) loss 1.5449 (1.6197) acc 58.3333 (52.5992) lr 1.9603e-03 eta 0:01:53
epoch [11/100] batch [5/5] time 0.040 (0.288) data 0.000 (0.181) loss 1.4595 (1.5920) acc 50.0000 (48.6496) lr 1.9511e-03 eta 0:02:08
>>> samples [57/160] noisy rate: 0.38 --> 0.33 --> 0.14 <<<
epoch [12/100] batch [5/5] time 0.033 (0.356) data 0.001 (0.241) loss 1.6195 (1.6490) acc 60.7143 (54.6258) lr 1.9409e-03 eta 0:02:36
epoch [13/100] batch [5/5] time 0.043 (0.273) data 0.000 (0.228) loss 1.4794 (1.4520) acc 60.4167 (67.4721) lr 1.9298e-03 eta 0:01:58
epoch [14/100] batch [5/5] time 0.046 (0.268) data 0.000 (0.219) loss 1.4080 (1.5047) acc 66.6667 (59.5833) lr 1.9178e-03 eta 0:01:55
epoch [15/100] batch [5/5] time 0.041 (0.269) data 0.001 (0.219) loss 1.3845 (1.4719) acc 69.2308 (63.8352) lr 1.9048e-03 eta 0:01:54
epoch [16/100] batch [5/5] time 0.048 (0.270) data 0.000 (0.217) loss 1.4962 (1.4364) acc 76.7857 (68.5261) lr 1.8910e-03 eta 0:01:53
>>> samples [65/160] noisy rate: 0.38 --> 0.33 --> 0.18 <<<
epoch [17/100] batch [5/5] time 0.038 (0.322) data 0.000 (0.197) loss 1.3650 (1.4703) acc 62.5000 (67.1499) lr 1.8763e-03 eta 0:02:13
epoch [18/100] batch [5/5] time 0.046 (0.343) data 0.000 (0.215) loss 1.3681 (1.4381) acc 77.2727 (69.6037) lr 1.8607e-03 eta 0:02:20
epoch [19/100] batch [5/5] time 0.042 (0.241) data 0.000 (0.191) loss 1.5881 (1.4073) acc 68.1818 (71.2534) lr 1.8443e-03 eta 0:01:37
epoch [20/100] batch [5/5] time 0.042 (0.249) data 0.000 (0.191) loss 1.5262 (1.3387) acc 69.2308 (73.9476) lr 1.8271e-03 eta 0:01:39
epoch [21/100] batch [5/5] time 0.034 (0.225) data 0.000 (0.174) loss 1.4046 (1.3839) acc 65.0000 (73.1374) lr 1.8090e-03 eta 0:01:28
>>> samples [67/160] noisy rate: 0.38 --> 0.31 --> 0.18 <<<
epoch [22/100] batch [5/5] time 0.059 (0.269) data 0.000 (0.214) loss 1.6777 (1.3311) acc 70.8333 (74.0851) lr 1.7902e-03 eta 0:01:45
epoch [23/100] batch [5/5] time 0.053 (0.268) data 0.000 (0.220) loss 1.2850 (1.3430) acc 76.4706 (72.1400) lr 1.7705e-03 eta 0:01:43
epoch [24/100] batch [5/5] time 0.045 (0.255) data 0.000 (0.201) loss 1.0686 (1.3506) acc 89.2857 (74.9496) lr 1.7501e-03 eta 0:01:36
epoch [25/100] batch [5/5] time 0.048 (0.242) data 0.000 (0.189) loss 1.2693 (1.3764) acc 83.9286 (75.7381) lr 1.7290e-03 eta 0:01:30
epoch [26/100] batch [5/5] time 0.044 (0.262) data 0.001 (0.214) loss 1.3587 (1.3411) acc 64.2857 (73.6296) lr 1.7071e-03 eta 0:01:36
>>> samples [70/160] noisy rate: 0.38 --> 0.28 --> 0.20 <<<
epoch [27/100] batch [5/5] time 0.042 (0.280) data 0.000 (0.228) loss 1.3226 (1.3529) acc 78.8462 (71.3575) lr 1.6845e-03 eta 0:01:42
epoch [28/100] batch [5/5] time 0.041 (0.210) data 0.000 (0.162) loss 1.4485 (1.3283) acc 75.0000 (77.1325) lr 1.6613e-03 eta 0:01:15
epoch [29/100] batch [5/5] time 0.048 (0.283) data 0.000 (0.229) loss 1.1991 (1.3268) acc 91.0714 (74.7605) lr 1.6374e-03 eta 0:01:40
epoch [30/100] batch [5/5] time 0.051 (0.267) data 0.000 (0.215) loss 1.5786 (1.3157) acc 71.6667 (78.6190) lr 1.6129e-03 eta 0:01:33
epoch [31/100] batch [5/5] time 0.046 (0.284) data 0.000 (0.237) loss 1.4220 (1.2879) acc 73.0769 (75.6527) lr 1.5878e-03 eta 0:01:38
>>> samples [76/160] noisy rate: 0.38 --> 0.26 --> 0.21 <<<
epoch [32/100] batch [5/5] time 0.424 (0.300) data 0.000 (0.173) loss 1.3911 (1.3285) acc 70.0000 (76.4293) lr 1.5621e-03 eta 0:01:42
epoch [33/100] batch [5/5] time 0.050 (0.274) data 0.000 (0.220) loss 1.0525 (1.2610) acc 85.9375 (78.8899) lr 1.5358e-03 eta 0:01:31
epoch [34/100] batch [5/5] time 0.048 (0.259) data 0.000 (0.202) loss 1.2019 (1.3166) acc 73.4375 (74.3638) lr 1.5090e-03 eta 0:01:25
epoch [35/100] batch [5/5] time 0.048 (0.283) data 0.000 (0.227) loss 1.1828 (1.2856) acc 87.5000 (77.6509) lr 1.4818e-03 eta 0:01:32
epoch [36/100] batch [5/5] time 0.054 (0.253) data 0.000 (0.195) loss 1.3248 (1.2770) acc 77.7778 (78.0311) lr 1.4540e-03 eta 0:01:20
>>> samples [81/160] noisy rate: 0.38 --> 0.31 --> 0.23 <<<
epoch [37/100] batch [5/5] time 0.045 (0.257) data 0.000 (0.198) loss 1.1311 (1.2801) acc 75.0000 (75.9080) lr 1.4258e-03 eta 0:01:20
epoch [38/100] batch [5/5] time 0.054 (0.246) data 0.000 (0.188) loss 1.4130 (1.2925) acc 75.0000 (77.6968) lr 1.3971e-03 eta 0:01:16
epoch [39/100] batch [5/5] time 0.051 (0.244) data 0.000 (0.185) loss 1.4217 (1.2661) acc 64.7059 (78.0503) lr 1.3681e-03 eta 0:01:14
epoch [40/100] batch [5/5] time 0.046 (0.259) data 0.000 (0.196) loss 1.1409 (1.3188) acc 78.3333 (72.7579) lr 1.3387e-03 eta 0:01:17
epoch [41/100] batch [5/5] time 0.042 (0.297) data 0.000 (0.241) loss 1.2011 (1.2712) acc 76.9231 (77.1424) lr 1.3090e-03 eta 0:01:27
>>> samples [81/160] noisy rate: 0.38 --> 0.26 --> 0.23 <<<
epoch [42/100] batch [5/5] time 0.052 (0.261) data 0.000 (0.202) loss 1.3563 (1.2833) acc 76.4706 (78.1622) lr 1.2790e-03 eta 0:01:15
epoch [43/100] batch [5/5] time 0.053 (0.271) data 0.000 (0.215) loss 1.3503 (1.2963) acc 73.4375 (79.9736) lr 1.2487e-03 eta 0:01:17
epoch [44/100] batch [5/5] time 0.049 (0.275) data 0.000 (0.216) loss 1.5052 (1.2566) acc 71.6667 (78.9013) lr 1.2181e-03 eta 0:01:17
epoch [45/100] batch [5/5] time 0.042 (0.259) data 0.000 (0.200) loss 1.3843 (1.2751) acc 69.2308 (77.0855) lr 1.1874e-03 eta 0:01:11
epoch [46/100] batch [5/5] time 0.045 (0.254) data 0.000 (0.198) loss 1.2345 (1.2537) acc 73.0769 (77.8205) lr 1.1564e-03 eta 0:01:08
>>> samples [81/160] noisy rate: 0.38 --> 0.26 --> 0.23 <<<
epoch [47/100] batch [5/5] time 0.060 (0.267) data 0.000 (0.207) loss 1.4267 (1.2636) acc 76.3889 (78.8846) lr 1.1253e-03 eta 0:01:10
epoch [48/100] batch [5/5] time 0.045 (0.260) data 0.000 (0.206) loss 1.2908 (1.2804) acc 71.1538 (77.4190) lr 1.0941e-03 eta 0:01:07
epoch [49/100] batch [5/5] time 0.059 (0.253) data 0.000 (0.191) loss 1.2733 (1.2672) acc 81.9444 (79.9841) lr 1.0628e-03 eta 0:01:04
epoch [50/100] batch [5/5] time 0.058 (0.257) data 0.000 (0.201) loss 1.3824 (1.2234) acc 72.5000 (79.2500) lr 1.0314e-03 eta 0:01:04
epoch [51/100] batch [5/5] time 0.048 (0.242) data 0.000 (0.186) loss 1.1832 (1.2282) acc 84.3750 (79.7161) lr 1.0000e-03 eta 0:00:59
>>> samples [85/160] noisy rate: 0.38 --> 0.29 --> 0.26 <<<
epoch [52/100] batch [5/5] time 0.060 (0.226) data 0.000 (0.167) loss 1.4628 (1.2407) acc 80.2632 (79.1551) lr 9.6859e-04 eta 0:00:54
epoch [53/100] batch [5/5] time 0.464 (0.340) data 0.000 (0.199) loss 1.2876 (1.2490) acc 75.0000 (77.5740) lr 9.3721e-04 eta 0:01:20
epoch [54/100] batch [5/5] time 0.042 (0.327) data 0.000 (0.195) loss 1.3695 (1.2888) acc 78.8462 (78.5767) lr 9.0589e-04 eta 0:01:15
epoch [55/100] batch [5/5] time 0.049 (0.248) data 0.000 (0.189) loss 1.1319 (1.2262) acc 81.2500 (80.8252) lr 8.7467e-04 eta 0:00:55
epoch [56/100] batch [5/5] time 0.053 (0.273) data 0.000 (0.214) loss 1.3593 (1.2495) acc 76.3889 (79.0613) lr 8.4357e-04 eta 0:01:00
>>> samples [85/160] noisy rate: 0.38 --> 0.31 --> 0.26 <<<
epoch [57/100] batch [5/5] time 0.051 (0.267) data 0.000 (0.207) loss 1.3302 (1.2303) acc 76.4706 (78.5410) lr 8.1262e-04 eta 0:00:57
epoch [58/100] batch [5/5] time 0.059 (0.268) data 0.000 (0.205) loss 1.2661 (1.2221) acc 79.1667 (78.4234) lr 7.8186e-04 eta 0:00:56
epoch [59/100] batch [5/5] time 0.053 (0.262) data 0.000 (0.203) loss 1.5160 (1.2192) acc 77.7778 (82.6958) lr 7.5131e-04 eta 0:00:53
epoch [60/100] batch [5/5] time 0.053 (0.292) data 0.001 (0.232) loss 1.2498 (1.2201) acc 75.0000 (80.3715) lr 7.2101e-04 eta 0:00:58
epoch [61/100] batch [5/5] time 0.068 (0.273) data 0.001 (0.214) loss 1.1216 (1.2431) acc 84.5238 (81.4073) lr 6.9098e-04 eta 0:00:53
>>> samples [85/160] noisy rate: 0.38 --> 0.35 --> 0.26 <<<
epoch [62/100] batch [5/5] time 0.046 (0.267) data 0.000 (0.209) loss 1.4390 (1.2224) acc 78.8462 (79.0209) lr 6.6126e-04 eta 0:00:50
epoch [63/100] batch [5/5] time 0.067 (0.269) data 0.000 (0.210) loss 1.3489 (1.2060) acc 70.2381 (81.4035) lr 6.3188e-04 eta 0:00:49
epoch [64/100] batch [5/5] time 0.056 (0.276) data 0.000 (0.217) loss 1.0283 (1.2138) acc 89.0625 (82.5982) lr 6.0285e-04 eta 0:00:49
epoch [65/100] batch [5/5] time 0.053 (0.265) data 0.000 (0.206) loss 1.3209 (1.2285) acc 71.8750 (80.6455) lr 5.7422e-04 eta 0:00:46
epoch [66/100] batch [5/5] time 0.049 (0.214) data 0.000 (0.154) loss 1.1065 (1.2133) acc 82.1429 (81.6891) lr 5.4601e-04 eta 0:00:36
>>> samples [86/160] noisy rate: 0.38 --> 0.34 --> 0.27 <<<
epoch [67/100] batch [5/5] time 0.485 (0.342) data 0.000 (0.195) loss 1.3217 (1.1853) acc 82.2917 (82.0139) lr 5.1825e-04 eta 0:00:56
epoch [68/100] batch [5/5] time 0.055 (0.328) data 0.000 (0.180) loss 1.2376 (1.2157) acc 85.0000 (80.4857) lr 4.9096e-04 eta 0:00:52
epoch [69/100] batch [5/5] time 0.050 (0.240) data 0.000 (0.176) loss 1.1117 (1.1830) acc 85.7143 (81.9806) lr 4.6417e-04 eta 0:00:37
epoch [70/100] batch [5/5] time 0.062 (0.345) data 0.000 (0.199) loss 1.3401 (1.2078) acc 83.7500 (81.6810) lr 4.3792e-04 eta 0:00:51
epoch [71/100] batch [5/5] time 0.051 (0.266) data 0.000 (0.208) loss 1.2784 (1.2047) acc 73.5294 (81.9804) lr 4.1221e-04 eta 0:00:38
>>> samples [86/160] noisy rate: 0.38 --> 0.34 --> 0.27 <<<
epoch [72/100] batch [5/5] time 0.056 (0.245) data 0.000 (0.188) loss 1.2116 (1.2083) acc 77.6316 (81.4330) lr 3.8709e-04 eta 0:00:34
epoch [73/100] batch [5/5] time 0.048 (0.244) data 0.000 (0.183) loss 1.3012 (1.2120) acc 67.8571 (81.6224) lr 3.6258e-04 eta 0:00:32
epoch [74/100] batch [5/5] time 0.047 (0.256) data 0.000 (0.198) loss 1.3394 (1.1690) acc 87.5000 (86.3750) lr 3.3869e-04 eta 0:00:33
epoch [75/100] batch [5/5] time 0.058 (0.257) data 0.000 (0.194) loss 1.3506 (1.2115) acc 76.3889 (83.8922) lr 3.1545e-04 eta 0:00:32
epoch [76/100] batch [5/5] time 0.055 (0.262) data 0.000 (0.199) loss 1.1635 (1.2098) acc 80.2632 (80.6057) lr 2.9289e-04 eta 0:00:31
>>> samples [88/160] noisy rate: 0.38 --> 0.36 --> 0.27 <<<
epoch [77/100] batch [5/5] time 0.060 (0.292) data 0.000 (0.233) loss 1.0943 (1.1891) acc 84.2105 (79.5218) lr 2.7103e-04 eta 0:00:33
epoch [78/100] batch [5/5] time 0.056 (0.277) data 0.000 (0.213) loss 1.1604 (1.2205) acc 80.2632 (77.4301) lr 2.4989e-04 eta 0:00:30
epoch [79/100] batch [5/5] time 0.062 (0.278) data 0.000 (0.215) loss 1.0743 (1.1848) acc 95.5882 (83.4510) lr 2.2949e-04 eta 0:00:29
epoch [80/100] batch [5/5] time 0.062 (0.282) data 0.001 (0.223) loss 1.2187 (1.1914) acc 80.9524 (81.4879) lr 2.0984e-04 eta 0:00:28
epoch [81/100] batch [5/5] time 0.055 (0.296) data 0.000 (0.240) loss 1.2652 (1.2018) acc 91.1765 (83.4809) lr 1.9098e-04 eta 0:00:28
>>> samples [89/160] noisy rate: 0.38 --> 0.34 --> 0.27 <<<
epoch [82/100] batch [5/5] time 0.060 (0.250) data 0.000 (0.189) loss 1.1480 (1.2291) acc 84.2105 (78.3855) lr 1.7292e-04 eta 0:00:22
epoch [83/100] batch [5/5] time 0.050 (0.252) data 0.000 (0.190) loss 0.9877 (1.2295) acc 90.0000 (84.5833) lr 1.5567e-04 eta 0:00:21
epoch [84/100] batch [5/5] time 0.057 (0.264) data 0.000 (0.203) loss 1.1127 (1.2318) acc 86.1111 (80.0997) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.056 (0.275) data 0.000 (0.216) loss 1.4015 (1.2316) acc 76.4706 (81.8536) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.054 (0.268) data 0.000 (0.208) loss 1.3037 (1.1742) acc 80.8824 (83.4704) lr 1.0899e-04 eta 0:00:18
>>> samples [89/160] noisy rate: 0.38 --> 0.34 --> 0.27 <<<
epoch [87/100] batch [5/5] time 0.060 (0.271) data 0.000 (0.209) loss 1.5234 (1.1894) acc 76.3158 (82.5427) lr 9.5173e-05 eta 0:00:17
epoch [88/100] batch [5/5] time 0.063 (0.273) data 0.000 (0.212) loss 1.1101 (1.2305) acc 90.4762 (82.3764) lr 8.2245e-05 eta 0:00:16
epoch [89/100] batch [5/5] time 0.063 (0.275) data 0.000 (0.216) loss 1.2583 (1.1871) acc 79.5455 (81.8615) lr 7.0224e-05 eta 0:00:15
epoch [90/100] batch [5/5] time 0.068 (0.232) data 0.000 (0.172) loss 1.0711 (1.2087) acc 84.5238 (81.0159) lr 5.9119e-05 eta 0:00:11
epoch [91/100] batch [5/5] time 0.056 (0.242) data 0.001 (0.180) loss 1.1795 (1.1857) acc 81.2500 (81.6681) lr 4.8943e-05 eta 0:00:10
>>> samples [89/160] noisy rate: 0.38 --> 0.33 --> 0.27 <<<
epoch [92/100] batch [5/5] time 0.059 (0.268) data 0.000 (0.207) loss 1.1119 (1.2278) acc 91.6667 (82.0479) lr 3.9706e-05 eta 0:00:10
epoch [93/100] batch [5/5] time 0.057 (0.242) data 0.000 (0.182) loss 1.3834 (1.1924) acc 71.2500 (82.5529) lr 3.1417e-05 eta 0:00:08
epoch [94/100] batch [5/5] time 0.048 (0.259) data 0.000 (0.201) loss 1.1657 (1.1942) acc 85.7143 (80.1347) lr 2.4083e-05 eta 0:00:07
epoch [95/100] batch [5/5] time 0.051 (0.277) data 0.000 (0.215) loss 1.3916 (1.2152) acc 78.1250 (78.9671) lr 1.7713e-05 eta 0:00:06
epoch [96/100] batch [5/5] time 0.060 (0.228) data 0.000 (0.166) loss 1.3316 (1.1983) acc 84.2105 (82.4210) lr 1.2312e-05 eta 0:00:04
>>> samples [89/160] noisy rate: 0.38 --> 0.35 --> 0.27 <<<
epoch [97/100] batch [5/5] time 0.062 (0.224) data 0.000 (0.160) loss 1.2856 (1.2061) acc 77.5000 (82.5347) lr 7.8853e-06 eta 0:00:03
epoch [98/100] batch [5/5] time 0.059 (0.232) data 0.000 (0.166) loss 1.4401 (1.2134) acc 76.3889 (83.5866) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.064 (0.243) data 0.000 (0.181) loss 1.0967 (1.1834) acc 85.0000 (81.7265) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.060 (0.254) data 0.000 (0.193) loss 1.2553 (1.1886) acc 76.3158 (82.0509) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.39, 0.34, 0.33, 0.33, 0.31, 0.28, 0.26, 0.31, 0.26, 0.26, 0.29, 0.31, 0.35, 0.34, 0.34, 0.36, 0.34, 0.34, 0.33, 0.35]
* learned noise rate: [0.13, 0.11, 0.14, 0.18, 0.18, 0.2, 0.21, 0.23, 0.23, 0.23, 0.26, 0.26, 0.26, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:00<01:00,  1.32it/s]  2%|▏         | 2/81 [00:00<00:34,  2.27it/s]  5%|▍         | 4/81 [00:01<00:16,  4.60it/s]  7%|▋         | 6/81 [00:01<00:11,  6.47it/s] 10%|▉         | 8/81 [00:01<00:09,  7.99it/s] 12%|█▏        | 10/81 [00:01<00:07,  9.27it/s] 15%|█▍        | 12/81 [00:01<00:06, 10.53it/s] 17%|█▋        | 14/81 [00:01<00:05, 11.84it/s] 20%|█▉        | 16/81 [00:02<00:05, 12.90it/s] 22%|██▏       | 18/81 [00:02<00:04, 13.72it/s] 25%|██▍       | 20/81 [00:02<00:04, 14.11it/s] 27%|██▋       | 22/81 [00:02<00:04, 14.52it/s] 30%|██▉       | 24/81 [00:02<00:03, 14.93it/s] 32%|███▏      | 26/81 [00:02<00:03, 15.23it/s] 35%|███▍      | 28/81 [00:02<00:03, 15.06it/s] 37%|███▋      | 30/81 [00:02<00:03, 15.29it/s] 40%|███▉      | 32/81 [00:03<00:03, 15.37it/s] 42%|████▏     | 34/81 [00:03<00:03, 15.54it/s] 44%|████▍     | 36/81 [00:03<00:02, 15.34it/s] 47%|████▋     | 38/81 [00:03<00:02, 15.50it/s] 49%|████▉     | 40/81 [00:03<00:02, 15.62it/s] 52%|█████▏    | 42/81 [00:03<00:02, 15.46it/s] 54%|█████▍    | 44/81 [00:03<00:02, 15.49it/s] 57%|█████▋    | 46/81 [00:03<00:02, 15.27it/s] 59%|█████▉    | 48/81 [00:04<00:02, 15.42it/s] 62%|██████▏   | 50/81 [00:04<00:02, 15.40it/s] 64%|██████▍   | 52/81 [00:04<00:01, 15.18it/s] 67%|██████▋   | 54/81 [00:04<00:01, 15.40it/s] 69%|██████▉   | 56/81 [00:04<00:01, 15.47it/s] 72%|███████▏  | 58/81 [00:04<00:01, 14.98it/s] 74%|███████▍  | 60/81 [00:04<00:01, 14.79it/s] 77%|███████▋  | 62/81 [00:05<00:01, 15.11it/s] 79%|███████▉  | 64/81 [00:05<00:01, 15.35it/s] 81%|████████▏ | 66/81 [00:05<00:00, 15.52it/s] 84%|████████▍ | 68/81 [00:05<00:00, 15.66it/s] 86%|████████▋ | 70/81 [00:05<00:00, 15.78it/s] 89%|████████▉ | 72/81 [00:05<00:00, 15.87it/s] 91%|█████████▏| 74/81 [00:05<00:00, 15.94it/s] 94%|█████████▍| 76/81 [00:05<00:00, 15.99it/s] 96%|█████████▋| 78/81 [00:06<00:00, 16.02it/s] 99%|█████████▉| 80/81 [00:06<00:00, 16.05it/s]100%|██████████| 81/81 [00:06<00:00, 12.70it/s]
=> result
* total: 8,100
* correct: 4,464
* accuracy: 55.1%
* error: 44.9%
* macro_f1: 52.3%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 221	acc: 24.6%
* class: 1 (Forest)	total: 900	correct: 875	acc: 97.2%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 50	acc: 5.6%
* class: 3 (Highway or Road)	total: 750	correct: 402	acc: 53.6%
* class: 4 (Industrial Buildings)	total: 750	correct: 729	acc: 97.2%
* class: 5 (Pasture Land)	total: 600	correct: 409	acc: 68.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 628	acc: 83.7%
* class: 7 (Residential Buildings)	total: 900	correct: 493	acc: 54.8%
* class: 8 (River)	total: 750	correct: 444	acc: 59.2%
* class: 9 (Sea or Lake)	total: 900	correct: 213	acc: 23.7%
* average: 56.8%
Elapsed: 0:04:28
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_6-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/100] batch [5/5] time 0.089 (0.713) data 0.000 (0.214) loss 1.1719 (1.1333) acc 13.2812 (15.3125) lr 2.0000e-03 eta 0:05:52
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [68/160] noisy rate: 0.38 --> 0.45 --> 0.25 <<<
epoch [2/100] batch [5/5] time 0.364 (0.565) data 0.000 (0.215) loss 2.0696 (2.1616) acc 21.6667 (27.6631) lr 1.9995e-03 eta 0:04:36
epoch [3/100] batch [5/5] time 0.062 (0.304) data 0.000 (0.201) loss 2.1059 (2.1614) acc 35.2941 (29.4050) lr 1.9980e-03 eta 0:02:27
epoch [4/100] batch [5/5] time 0.050 (0.334) data 0.000 (0.217) loss 2.1675 (2.0884) acc 25.0000 (31.0417) lr 1.9956e-03 eta 0:02:40
epoch [5/100] batch [5/5] time 0.046 (0.315) data 0.001 (0.199) loss 2.2993 (2.0873) acc 15.3846 (36.8407) lr 1.9921e-03 eta 0:02:29
epoch [6/100] batch [5/5] time 0.050 (0.344) data 0.000 (0.221) loss 2.1553 (2.0570) acc 31.6667 (37.4594) lr 1.9877e-03 eta 0:02:41
>>> samples [69/160] noisy rate: 0.38 --> 0.40 --> 0.23 <<<
epoch [7/100] batch [5/5] time 0.043 (0.261) data 0.000 (0.213) loss 2.1596 (2.0069) acc 55.3571 (42.6824) lr 1.9823e-03 eta 0:02:01
epoch [8/100] batch [5/5] time 0.059 (0.320) data 0.000 (0.224) loss 1.7148 (1.9735) acc 47.2222 (47.6944) lr 1.9759e-03 eta 0:02:27
epoch [9/100] batch [5/5] time 0.041 (0.336) data 0.000 (0.210) loss 1.8441 (1.9010) acc 63.6364 (48.2529) lr 1.9686e-03 eta 0:02:32
epoch [10/100] batch [5/5] time 0.050 (0.251) data 0.000 (0.199) loss 1.9392 (1.9110) acc 45.0000 (52.6691) lr 1.9603e-03 eta 0:01:52
epoch [11/100] batch [5/5] time 0.046 (0.252) data 0.000 (0.198) loss 1.9341 (1.9127) acc 38.4615 (50.3791) lr 1.9511e-03 eta 0:01:52
>>> samples [73/160] noisy rate: 0.38 --> 0.39 --> 0.22 <<<
epoch [12/100] batch [5/5] time 0.043 (0.348) data 0.000 (0.222) loss 1.7320 (1.8568) acc 67.3077 (54.4488) lr 1.9409e-03 eta 0:02:33
epoch [13/100] batch [5/5] time 0.045 (0.299) data 0.000 (0.249) loss 2.0401 (1.8755) acc 40.0000 (50.0854) lr 1.9298e-03 eta 0:02:10
epoch [14/100] batch [5/5] time 0.048 (0.246) data 0.000 (0.193) loss 1.7402 (1.8570) acc 62.5000 (55.4293) lr 1.9178e-03 eta 0:01:45
epoch [15/100] batch [5/5] time 0.044 (0.266) data 0.000 (0.215) loss 1.6763 (1.7975) acc 67.8571 (63.1767) lr 1.9048e-03 eta 0:01:53
epoch [16/100] batch [5/5] time 0.061 (0.271) data 0.000 (0.218) loss 1.7993 (1.8219) acc 56.2500 (65.4902) lr 1.8910e-03 eta 0:01:53
>>> samples [73/160] noisy rate: 0.38 --> 0.38 --> 0.22 <<<
epoch [17/100] batch [5/5] time 0.032 (0.278) data 0.000 (0.230) loss 1.7683 (1.8028) acc 63.8889 (67.5547) lr 1.8763e-03 eta 0:01:55
epoch [18/100] batch [5/5] time 0.051 (0.250) data 0.000 (0.194) loss 1.8247 (1.8024) acc 58.8235 (61.7250) lr 1.8607e-03 eta 0:01:42
epoch [19/100] batch [5/5] time 0.046 (0.265) data 0.000 (0.210) loss 1.6834 (1.7877) acc 76.6667 (65.1859) lr 1.8443e-03 eta 0:01:47
epoch [20/100] batch [5/5] time 0.048 (0.244) data 0.000 (0.188) loss 1.8339 (1.7501) acc 65.6250 (64.7779) lr 1.8271e-03 eta 0:01:37
epoch [21/100] batch [5/5] time 0.049 (0.261) data 0.000 (0.207) loss 1.7007 (1.7768) acc 60.9375 (61.8549) lr 1.8090e-03 eta 0:01:43
>>> samples [73/160] noisy rate: 0.38 --> 0.32 --> 0.22 <<<
epoch [22/100] batch [5/5] time 0.041 (0.251) data 0.000 (0.202) loss 1.7946 (1.7292) acc 71.1538 (68.9839) lr 1.7902e-03 eta 0:01:38
epoch [23/100] batch [5/5] time 0.053 (0.271) data 0.000 (0.214) loss 1.6751 (1.7307) acc 79.6875 (73.0208) lr 1.7705e-03 eta 0:01:44
epoch [24/100] batch [5/5] time 0.045 (0.263) data 0.000 (0.212) loss 1.7509 (1.7195) acc 71.6667 (65.4167) lr 1.7501e-03 eta 0:01:40
epoch [25/100] batch [5/5] time 0.049 (0.267) data 0.000 (0.218) loss 1.5394 (1.7295) acc 83.9286 (71.9438) lr 1.7290e-03 eta 0:01:39
epoch [26/100] batch [5/5] time 0.035 (0.263) data 0.001 (0.212) loss 1.7724 (1.7128) acc 65.0000 (70.6891) lr 1.7071e-03 eta 0:01:37
>>> samples [73/160] noisy rate: 0.38 --> 0.31 --> 0.22 <<<
epoch [27/100] batch [5/5] time 0.052 (0.261) data 0.000 (0.211) loss 1.5984 (1.7296) acc 73.5294 (68.2942) lr 1.6845e-03 eta 0:01:35
epoch [28/100] batch [5/5] time 0.051 (0.233) data 0.000 (0.178) loss 1.7070 (1.7001) acc 80.8824 (71.2332) lr 1.6613e-03 eta 0:01:24
epoch [29/100] batch [5/5] time 0.044 (0.250) data 0.000 (0.194) loss 1.6330 (1.7005) acc 73.2143 (69.8634) lr 1.6374e-03 eta 0:01:28
epoch [30/100] batch [5/5] time 0.054 (0.261) data 0.000 (0.207) loss 1.3215 (1.6544) acc 85.9375 (74.5599) lr 1.6129e-03 eta 0:01:31
epoch [31/100] batch [5/5] time 0.033 (0.259) data 0.000 (0.202) loss 1.7798 (1.7159) acc 75.0000 (70.9742) lr 1.5878e-03 eta 0:01:29
>>> samples [73/160] noisy rate: 0.38 --> 0.32 --> 0.22 <<<
epoch [32/100] batch [5/5] time 0.053 (0.257) data 0.000 (0.203) loss 1.7555 (1.6720) acc 68.7500 (75.9787) lr 1.5621e-03 eta 0:01:27
epoch [33/100] batch [5/5] time 0.052 (0.251) data 0.000 (0.196) loss 1.5809 (1.6863) acc 79.6875 (72.5327) lr 1.5358e-03 eta 0:01:24
epoch [34/100] batch [5/5] time 0.039 (0.243) data 0.000 (0.187) loss 1.6998 (1.6565) acc 87.5000 (79.7108) lr 1.5090e-03 eta 0:01:20
epoch [35/100] batch [5/5] time 0.051 (0.240) data 0.000 (0.188) loss 1.7190 (1.6876) acc 76.4706 (75.0635) lr 1.4818e-03 eta 0:01:18
epoch [36/100] batch [5/5] time 0.051 (0.253) data 0.000 (0.198) loss 1.9550 (1.6639) acc 58.8235 (75.0683) lr 1.4540e-03 eta 0:01:20
>>> samples [77/160] noisy rate: 0.38 --> 0.33 --> 0.22 <<<
epoch [37/100] batch [5/5] time 0.051 (0.275) data 0.001 (0.219) loss 1.4573 (1.6557) acc 71.6667 (72.2239) lr 1.4258e-03 eta 0:01:26
epoch [38/100] batch [5/5] time 0.060 (0.257) data 0.000 (0.200) loss 1.6823 (1.6538) acc 82.8947 (76.1636) lr 1.3971e-03 eta 0:01:19
epoch [39/100] batch [5/5] time 0.038 (0.278) data 0.001 (0.224) loss 1.4182 (1.6561) acc 86.3636 (77.2380) lr 1.3681e-03 eta 0:01:24
epoch [40/100] batch [5/5] time 0.057 (0.251) data 0.000 (0.196) loss 1.5204 (1.6524) acc 81.2500 (75.5563) lr 1.3387e-03 eta 0:01:15
epoch [41/100] batch [5/5] time 0.289 (0.293) data 0.000 (0.185) loss 1.6770 (1.6560) acc 64.2857 (75.0794) lr 1.3090e-03 eta 0:01:26
>>> samples [77/160] noisy rate: 0.38 --> 0.32 --> 0.22 <<<
epoch [42/100] batch [5/5] time 0.057 (0.274) data 0.000 (0.217) loss 1.8198 (1.6392) acc 72.0588 (75.5496) lr 1.2790e-03 eta 0:01:19
epoch [43/100] batch [5/5] time 0.046 (0.262) data 0.000 (0.207) loss 1.5864 (1.6370) acc 81.6667 (79.1967) lr 1.2487e-03 eta 0:01:14
epoch [44/100] batch [5/5] time 0.054 (0.244) data 0.000 (0.185) loss 1.9008 (1.6758) acc 59.3750 (74.0625) lr 1.2181e-03 eta 0:01:08
epoch [45/100] batch [5/5] time 0.046 (0.236) data 0.001 (0.182) loss 1.6926 (1.6002) acc 83.3333 (80.7003) lr 1.1874e-03 eta 0:01:04
epoch [46/100] batch [5/5] time 0.039 (0.241) data 0.000 (0.191) loss 1.7317 (1.6477) acc 75.0000 (73.7778) lr 1.1564e-03 eta 0:01:05
>>> samples [78/160] noisy rate: 0.38 --> 0.30 --> 0.23 <<<
epoch [47/100] batch [5/5] time 0.045 (0.251) data 0.000 (0.197) loss 1.8200 (1.6441) acc 70.0000 (76.8235) lr 1.1253e-03 eta 0:01:06
epoch [48/100] batch [5/5] time 0.041 (0.272) data 0.000 (0.219) loss 1.6927 (1.6333) acc 86.5385 (77.3959) lr 1.0941e-03 eta 0:01:10
epoch [49/100] batch [5/5] time 0.042 (0.247) data 0.000 (0.192) loss 1.6315 (1.6574) acc 63.4615 (73.0812) lr 1.0628e-03 eta 0:01:03
epoch [50/100] batch [5/5] time 0.053 (0.256) data 0.000 (0.200) loss 1.5393 (1.6409) acc 60.9375 (75.0978) lr 1.0314e-03 eta 0:01:04
epoch [51/100] batch [5/5] time 0.041 (0.359) data 0.000 (0.221) loss 1.7672 (1.6295) acc 75.0000 (76.8750) lr 1.0000e-03 eta 0:01:27
>>> samples [82/160] noisy rate: 0.38 --> 0.30 --> 0.24 <<<
epoch [52/100] batch [5/5] time 0.059 (0.282) data 0.000 (0.223) loss 1.5168 (1.6346) acc 75.0000 (74.2955) lr 9.6859e-04 eta 0:01:07
epoch [53/100] batch [5/5] time 0.046 (0.255) data 0.001 (0.197) loss 1.5295 (1.6131) acc 71.6667 (77.2958) lr 9.3721e-04 eta 0:00:59
epoch [54/100] batch [5/5] time 0.044 (0.344) data 0.000 (0.206) loss 1.8166 (1.6382) acc 67.8571 (75.4261) lr 9.0589e-04 eta 0:01:19
epoch [55/100] batch [5/5] time 0.048 (0.282) data 0.000 (0.225) loss 1.5632 (1.6182) acc 82.8125 (75.7468) lr 8.7467e-04 eta 0:01:03
epoch [56/100] batch [5/5] time 0.046 (0.285) data 0.000 (0.233) loss 1.5857 (1.6021) acc 85.0000 (80.3500) lr 8.4357e-04 eta 0:01:02
>>> samples [82/160] noisy rate: 0.38 --> 0.30 --> 0.24 <<<
epoch [57/100] batch [5/5] time 0.046 (0.249) data 0.000 (0.193) loss 1.6320 (1.5911) acc 75.0000 (78.4960) lr 8.1262e-04 eta 0:00:53
epoch [58/100] batch [5/5] time 0.070 (0.257) data 0.000 (0.195) loss 1.5351 (1.6104) acc 84.5238 (78.4653) lr 7.8186e-04 eta 0:00:53
epoch [59/100] batch [5/5] time 0.061 (0.264) data 0.000 (0.208) loss 1.7828 (1.5988) acc 69.7368 (76.8138) lr 7.5131e-04 eta 0:00:54
epoch [60/100] batch [5/5] time 0.048 (0.260) data 0.000 (0.202) loss 1.7353 (1.5946) acc 82.8125 (78.8401) lr 7.2101e-04 eta 0:00:51
epoch [61/100] batch [5/5] time 0.058 (0.246) data 0.000 (0.187) loss 1.4560 (1.5868) acc 66.6667 (80.2053) lr 6.9098e-04 eta 0:00:47
>>> samples [82/160] noisy rate: 0.38 --> 0.30 --> 0.24 <<<
epoch [62/100] batch [5/5] time 0.060 (0.281) data 0.000 (0.228) loss 1.4958 (1.5926) acc 86.8421 (77.1006) lr 6.6126e-04 eta 0:00:53
epoch [63/100] batch [5/5] time 0.048 (0.280) data 0.000 (0.223) loss 1.3792 (1.5978) acc 73.2143 (75.8459) lr 6.3188e-04 eta 0:00:51
epoch [64/100] batch [5/5] time 0.053 (0.276) data 0.000 (0.220) loss 1.8013 (1.5926) acc 68.0555 (79.0565) lr 6.0285e-04 eta 0:00:49
epoch [65/100] batch [5/5] time 0.058 (0.286) data 0.000 (0.229) loss 1.6138 (1.6006) acc 76.2500 (78.3736) lr 5.7422e-04 eta 0:00:50
epoch [66/100] batch [5/5] time 0.053 (0.269) data 0.001 (0.211) loss 1.4507 (1.5834) acc 85.9375 (77.9326) lr 5.4601e-04 eta 0:00:45
>>> samples [82/160] noisy rate: 0.38 --> 0.28 --> 0.24 <<<
epoch [67/100] batch [5/5] time 0.045 (0.244) data 0.000 (0.189) loss 1.8578 (1.5906) acc 70.0000 (79.6391) lr 5.1825e-04 eta 0:00:40
epoch [68/100] batch [5/5] time 0.488 (0.345) data 0.000 (0.205) loss 1.5073 (1.5725) acc 80.4348 (81.3075) lr 4.9096e-04 eta 0:00:55
epoch [69/100] batch [5/5] time 0.051 (0.257) data 0.000 (0.198) loss 1.7313 (1.5849) acc 69.1176 (76.2635) lr 4.6417e-04 eta 0:00:39
epoch [70/100] batch [5/5] time 0.058 (0.281) data 0.000 (0.226) loss 1.5779 (1.5938) acc 77.7778 (74.4124) lr 4.3792e-04 eta 0:00:42
epoch [71/100] batch [5/5] time 0.053 (0.224) data 0.000 (0.168) loss 1.5140 (1.6049) acc 70.3125 (77.3976) lr 4.1221e-04 eta 0:00:32
>>> samples [82/160] noisy rate: 0.38 --> 0.29 --> 0.24 <<<
epoch [72/100] batch [5/5] time 0.046 (0.260) data 0.000 (0.205) loss 1.8022 (1.6031) acc 71.1538 (78.8288) lr 3.8709e-04 eta 0:00:36
epoch [73/100] batch [5/5] time 0.057 (0.261) data 0.000 (0.204) loss 1.5465 (1.5673) acc 77.6316 (81.3319) lr 3.6258e-04 eta 0:00:35
epoch [74/100] batch [5/5] time 0.065 (0.263) data 0.000 (0.208) loss 1.4808 (1.5640) acc 83.3333 (81.7708) lr 3.3869e-04 eta 0:00:34
epoch [75/100] batch [5/5] time 0.057 (0.285) data 0.000 (0.232) loss 1.4116 (1.5928) acc 81.9444 (80.9834) lr 3.1545e-04 eta 0:00:35
epoch [76/100] batch [5/5] time 0.050 (0.362) data 0.000 (0.221) loss 1.9414 (1.6123) acc 73.3333 (79.9583) lr 2.9289e-04 eta 0:00:43
>>> samples [84/160] noisy rate: 0.38 --> 0.30 --> 0.24 <<<
epoch [77/100] batch [5/5] time 0.053 (0.280) data 0.000 (0.223) loss 1.6022 (1.5669) acc 73.6111 (76.8145) lr 2.7103e-04 eta 0:00:32
epoch [78/100] batch [5/5] time 0.044 (0.278) data 0.000 (0.220) loss 1.8413 (1.5740) acc 77.0833 (79.6042) lr 2.4989e-04 eta 0:00:30
epoch [79/100] batch [5/5] time 0.043 (0.286) data 0.000 (0.232) loss 1.4499 (1.5649) acc 83.9286 (80.2619) lr 2.2949e-04 eta 0:00:30
epoch [80/100] batch [5/5] time 0.062 (0.270) data 0.000 (0.209) loss 1.5014 (1.5680) acc 79.7619 (79.5434) lr 2.0984e-04 eta 0:00:27
epoch [81/100] batch [5/5] time 0.055 (0.261) data 0.000 (0.202) loss 1.4911 (1.5800) acc 76.4706 (74.8824) lr 1.9098e-04 eta 0:00:24
>>> samples [84/160] noisy rate: 0.38 --> 0.31 --> 0.24 <<<
epoch [82/100] batch [5/5] time 0.063 (0.261) data 0.000 (0.196) loss 1.7760 (1.5672) acc 84.2105 (79.6090) lr 1.7292e-04 eta 0:00:23
epoch [83/100] batch [5/5] time 0.047 (0.244) data 0.000 (0.181) loss 1.6363 (1.5867) acc 92.3077 (84.5516) lr 1.5567e-04 eta 0:00:20
epoch [84/100] batch [5/5] time 0.054 (0.270) data 0.000 (0.214) loss 1.4431 (1.5884) acc 71.8750 (78.7550) lr 1.3926e-04 eta 0:00:21
epoch [85/100] batch [5/5] time 0.049 (0.272) data 0.000 (0.212) loss 1.3850 (1.5556) acc 78.5714 (79.5069) lr 1.2369e-04 eta 0:00:20
epoch [86/100] batch [5/5] time 0.063 (0.247) data 0.000 (0.186) loss 1.4414 (1.5931) acc 83.3333 (77.2222) lr 1.0899e-04 eta 0:00:17
>>> samples [84/160] noisy rate: 0.38 --> 0.28 --> 0.24 <<<
epoch [87/100] batch [5/5] time 0.064 (0.262) data 0.000 (0.205) loss 1.5765 (1.5552) acc 81.2500 (79.7341) lr 9.5173e-05 eta 0:00:17
epoch [88/100] batch [5/5] time 0.056 (0.243) data 0.000 (0.182) loss 1.6288 (1.5756) acc 77.9412 (79.0247) lr 8.2245e-05 eta 0:00:14
epoch [89/100] batch [5/5] time 0.059 (0.251) data 0.000 (0.190) loss 1.7962 (1.5573) acc 63.8889 (83.2701) lr 7.0224e-05 eta 0:00:13
epoch [90/100] batch [5/5] time 0.049 (0.269) data 0.000 (0.211) loss 1.4661 (1.5715) acc 82.1429 (79.5743) lr 5.9119e-05 eta 0:00:13
epoch [91/100] batch [5/5] time 0.052 (0.233) data 0.000 (0.174) loss 1.5562 (1.5928) acc 82.3529 (78.4559) lr 4.8943e-05 eta 0:00:10
>>> samples [85/160] noisy rate: 0.38 --> 0.31 --> 0.24 <<<
epoch [92/100] batch [5/5] time 0.068 (0.241) data 0.000 (0.179) loss 1.5199 (1.5350) acc 75.0000 (80.7076) lr 3.9706e-05 eta 0:00:09
epoch [93/100] batch [5/5] time 0.047 (0.300) data 0.000 (0.239) loss 1.4860 (1.5962) acc 78.8462 (76.9776) lr 3.1417e-05 eta 0:00:10
epoch [94/100] batch [5/5] time 0.064 (0.290) data 0.000 (0.226) loss 1.4781 (1.5459) acc 80.8824 (79.4166) lr 2.4083e-05 eta 0:00:08
epoch [95/100] batch [5/5] time 0.044 (0.285) data 0.000 (0.225) loss 1.7842 (1.5536) acc 82.1429 (77.7758) lr 1.7713e-05 eta 0:00:07
epoch [96/100] batch [5/5] time 0.054 (0.239) data 0.000 (0.181) loss 1.4790 (1.5843) acc 92.1875 (82.1554) lr 1.2312e-05 eta 0:00:04
>>> samples [85/160] noisy rate: 0.38 --> 0.25 --> 0.24 <<<
epoch [97/100] batch [5/5] time 0.042 (0.270) data 0.000 (0.214) loss 1.6705 (1.5744) acc 94.2308 (80.0460) lr 7.8853e-06 eta 0:00:04
epoch [98/100] batch [5/5] time 0.050 (0.248) data 0.000 (0.190) loss 1.7102 (1.5635) acc 69.6429 (80.4702) lr 4.4380e-06 eta 0:00:02
epoch [99/100] batch [5/5] time 0.051 (0.251) data 0.001 (0.187) loss 1.4080 (1.5713) acc 81.2500 (75.6225) lr 1.9733e-06 eta 0:00:01
epoch [100/100] batch [5/5] time 0.064 (0.243) data 0.000 (0.182) loss 1.3322 (1.5764) acc 92.5000 (78.4729) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-100
Finish training
* refined noise rate: [0.45, 0.4, 0.39, 0.38, 0.32, 0.31, 0.32, 0.33, 0.32, 0.3, 0.3, 0.3, 0.3, 0.28, 0.29, 0.3, 0.31, 0.28, 0.31, 0.25]
* learned noise rate: [0.25, 0.23, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.23, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<02:42,  2.03s/it]  4%|▎         | 3/81 [00:02<00:44,  1.74it/s]  6%|▌         | 5/81 [00:02<00:23,  3.21it/s]  9%|▊         | 7/81 [00:02<00:15,  4.80it/s] 11%|█         | 9/81 [00:02<00:11,  6.46it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.04it/s] 16%|█▌        | 13/81 [00:02<00:07,  9.47it/s] 19%|█▊        | 15/81 [00:02<00:06, 10.75it/s] 21%|██        | 17/81 [00:03<00:05, 11.78it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.61it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.34it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.52it/s] 31%|███       | 25/81 [00:03<00:03, 14.06it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.41it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.38it/s] 38%|███▊      | 31/81 [00:04<00:03, 14.29it/s] 41%|████      | 33/81 [00:04<00:03, 14.23it/s] 43%|████▎     | 35/81 [00:04<00:03, 14.58it/s] 46%|████▌     | 37/81 [00:04<00:02, 14.86it/s] 48%|████▊     | 39/81 [00:04<00:02, 14.94it/s] 51%|█████     | 41/81 [00:04<00:02, 14.95it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.13it/s] 56%|█████▌    | 45/81 [00:04<00:02, 14.97it/s] 58%|█████▊    | 47/81 [00:05<00:02, 15.17it/s] 60%|██████    | 49/81 [00:05<00:02, 15.09it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.23it/s] 65%|██████▌   | 53/81 [00:05<00:01, 15.34it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.93it/s] 70%|███████   | 57/81 [00:05<00:01, 14.77it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.98it/s] 75%|███████▌  | 61/81 [00:06<00:01, 15.17it/s] 78%|███████▊  | 63/81 [00:06<00:01, 15.24it/s] 80%|████████  | 65/81 [00:06<00:01, 15.32it/s] 83%|████████▎ | 67/81 [00:06<00:00, 15.41it/s] 85%|████████▌ | 69/81 [00:06<00:00, 15.26it/s] 88%|████████▊ | 71/81 [00:06<00:00, 15.39it/s] 90%|█████████ | 73/81 [00:06<00:00, 15.49it/s] 93%|█████████▎| 75/81 [00:06<00:00, 15.54it/s] 95%|█████████▌| 77/81 [00:07<00:00, 15.55it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.54it/s]100%|██████████| 81/81 [00:07<00:00, 15.59it/s]100%|██████████| 81/81 [00:07<00:00, 10.81it/s]
=> result
* total: 8,100
* correct: 4,556
* accuracy: 56.2%
* error: 43.8%
* macro_f1: 53.6%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 447	acc: 49.7%
* class: 1 (Forest)	total: 900	correct: 852	acc: 94.7%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 30	acc: 3.3%
* class: 3 (Highway or Road)	total: 750	correct: 287	acc: 38.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 735	acc: 98.0%
* class: 5 (Pasture Land)	total: 600	correct: 495	acc: 82.5%
* class: 6 (Permanent Crop Land)	total: 750	correct: 520	acc: 69.3%
* class: 7 (Residential Buildings)	total: 900	correct: 386	acc: 42.9%
* class: 8 (River)	total: 750	correct: 390	acc: 52.0%
* class: 9 (Sea or Lake)	total: 900	correct: 414	acc: 46.0%
* average: 57.7%
Elapsed: 0:04:32
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_6-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.093 (0.684) data 0.000 (0.283) loss 1.1228 (1.1306) acc 19.5312 (14.5312) lr 2.0000e-03 eta 0:02:47
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [37/160] noisy rate: 0.38 --> 0.47 --> 0.08 <<<
epoch [2/50] batch [5/5] time 0.191 (0.329) data 0.000 (0.179) loss 2.0163 (2.1055) acc 37.5000 (24.3750) lr 1.9980e-03 eta 0:01:19
epoch [3/50] batch [5/5] time 0.289 (0.402) data 0.000 (0.186) loss 1.8657 (2.1530) acc 34.0909 (18.9015) lr 1.9921e-03 eta 0:01:34
epoch [4/50] batch [5/5] time 0.034 (0.323) data 0.000 (0.279) loss 1.9252 (1.9801) acc 35.0000 (35.7500) lr 1.9823e-03 eta 0:01:14
epoch [5/50] batch [5/5] time 0.172 (0.335) data 0.000 (0.228) loss 1.9369 (1.9059) acc 43.7500 (42.6786) lr 1.9686e-03 eta 0:01:15
epoch [6/50] batch [5/5] time 0.048 (0.279) data 0.000 (0.230) loss 1.6025 (1.7689) acc 66.6667 (57.1111) lr 1.9511e-03 eta 0:01:01
>>> samples [47/160] noisy rate: 0.38 --> 0.43 --> 0.11 <<<
epoch [7/50] batch [5/5] time 0.037 (0.281) data 0.000 (0.230) loss 1.6669 (1.7444) acc 57.5000 (58.6970) lr 1.9298e-03 eta 0:01:00
epoch [8/50] batch [5/5] time 0.043 (0.309) data 0.000 (0.256) loss 1.6690 (1.6654) acc 62.5000 (60.8333) lr 1.9048e-03 eta 0:01:04
epoch [9/50] batch [5/5] time 0.043 (0.267) data 0.000 (0.222) loss 1.8492 (1.6440) acc 50.0000 (62.7987) lr 1.8763e-03 eta 0:00:54
epoch [10/50] batch [5/5] time 0.036 (0.282) data 0.000 (0.182) loss 1.3681 (1.5962) acc 78.1250 (72.1726) lr 1.8443e-03 eta 0:00:56
epoch [11/50] batch [5/5] time 0.036 (0.342) data 0.000 (0.236) loss 1.5868 (1.5743) acc 66.6667 (72.6667) lr 1.8090e-03 eta 0:01:06
>>> samples [53/160] noisy rate: 0.38 --> 0.44 --> 0.13 <<<
epoch [12/50] batch [5/5] time 0.367 (0.288) data 0.000 (0.178) loss 1.4345 (1.5987) acc 80.7692 (74.9341) lr 1.7705e-03 eta 0:00:54
epoch [13/50] batch [5/5] time 0.035 (0.261) data 0.000 (0.220) loss 1.4760 (1.5766) acc 75.0000 (70.5076) lr 1.7290e-03 eta 0:00:48
epoch [14/50] batch [5/5] time 0.048 (0.252) data 0.000 (0.201) loss 1.6355 (1.5486) acc 60.4167 (76.7550) lr 1.6845e-03 eta 0:00:45
epoch [15/50] batch [5/5] time 0.048 (0.252) data 0.000 (0.196) loss 1.5349 (1.5368) acc 64.2857 (72.8066) lr 1.6374e-03 eta 0:00:44
epoch [16/50] batch [5/5] time 0.026 (0.281) data 0.000 (0.233) loss 1.5152 (1.5072) acc 83.3333 (82.1917) lr 1.5878e-03 eta 0:00:47
>>> samples [58/160] noisy rate: 0.38 --> 0.43 --> 0.16 <<<
epoch [17/50] batch [5/5] time 0.353 (0.313) data 0.000 (0.208) loss 1.5499 (1.4940) acc 76.5625 (72.8760) lr 1.5358e-03 eta 0:00:51
epoch [18/50] batch [5/5] time 0.042 (0.337) data 0.000 (0.213) loss 1.4156 (1.5387) acc 55.0000 (69.9636) lr 1.4818e-03 eta 0:00:53
epoch [19/50] batch [5/5] time 0.043 (0.333) data 0.000 (0.288) loss 1.2130 (1.5156) acc 72.9167 (79.3750) lr 1.4258e-03 eta 0:00:51
epoch [20/50] batch [5/5] time 0.042 (0.315) data 0.000 (0.229) loss 1.1664 (1.5077) acc 81.2500 (74.5833) lr 1.3681e-03 eta 0:00:47
epoch [21/50] batch [5/5] time 0.045 (0.276) data 0.000 (0.235) loss 1.4779 (1.5105) acc 66.6667 (75.6962) lr 1.3090e-03 eta 0:00:40
>>> samples [61/160] noisy rate: 0.38 --> 0.43 --> 0.18 <<<
epoch [22/50] batch [5/5] time 0.047 (0.284) data 0.000 (0.236) loss 1.3199 (1.4664) acc 83.3333 (78.2641) lr 1.2487e-03 eta 0:00:39
epoch [23/50] batch [5/5] time 0.048 (0.260) data 0.000 (0.205) loss 1.4125 (1.5036) acc 69.6429 (78.0498) lr 1.1874e-03 eta 0:00:35
epoch [24/50] batch [5/5] time 0.037 (0.249) data 0.000 (0.200) loss 1.7463 (1.4673) acc 70.0000 (77.1310) lr 1.1253e-03 eta 0:00:32
epoch [25/50] batch [5/5] time 0.049 (0.289) data 0.000 (0.234) loss 1.6771 (1.4429) acc 69.6429 (82.0084) lr 1.0628e-03 eta 0:00:36
epoch [26/50] batch [5/5] time 0.037 (0.289) data 0.000 (0.246) loss 1.5148 (1.4612) acc 75.0000 (77.4657) lr 1.0000e-03 eta 0:00:34
>>> samples [72/160] noisy rate: 0.38 --> 0.35 --> 0.22 <<<
epoch [27/50] batch [5/5] time 0.063 (0.412) data 0.000 (0.273) loss 1.3260 (1.4923) acc 85.2941 (71.4831) lr 9.3721e-04 eta 0:00:47
epoch [28/50] batch [5/5] time 0.049 (0.482) data 0.001 (0.345) loss 1.7481 (1.4676) acc 68.1818 (75.8776) lr 8.7467e-04 eta 0:00:52
epoch [29/50] batch [5/5] time 0.036 (0.501) data 0.000 (0.376) loss 1.6394 (1.4538) acc 61.1111 (77.9882) lr 8.1262e-04 eta 0:00:52
epoch [30/50] batch [5/5] time 0.053 (0.306) data 0.000 (0.243) loss 1.3523 (1.4046) acc 76.9231 (78.6971) lr 7.5131e-04 eta 0:00:30
epoch [31/50] batch [5/5] time 0.054 (0.302) data 0.000 (0.247) loss 1.2449 (1.4209) acc 88.4615 (78.3001) lr 6.9098e-04 eta 0:00:28
>>> samples [72/160] noisy rate: 0.38 --> 0.36 --> 0.22 <<<
epoch [32/50] batch [5/5] time 0.064 (0.325) data 0.000 (0.272) loss 1.4715 (1.4160) acc 81.9444 (79.0585) lr 6.3188e-04 eta 0:00:29
epoch [33/50] batch [5/5] time 0.057 (0.303) data 0.000 (0.243) loss 1.4149 (1.4058) acc 83.3333 (79.6307) lr 5.7422e-04 eta 0:00:25
epoch [34/50] batch [5/5] time 0.054 (0.347) data 0.000 (0.283) loss 1.4640 (1.4014) acc 83.9286 (78.3709) lr 5.1825e-04 eta 0:00:27
epoch [35/50] batch [5/5] time 0.062 (0.304) data 0.000 (0.245) loss 1.4965 (1.4201) acc 90.2778 (78.8248) lr 4.6417e-04 eta 0:00:22
epoch [36/50] batch [5/5] time 0.050 (0.260) data 0.000 (0.206) loss 1.5106 (1.3662) acc 71.4286 (79.1575) lr 4.1221e-04 eta 0:00:18
>>> samples [73/160] noisy rate: 0.38 --> 0.36 --> 0.22 <<<
epoch [37/50] batch [5/5] time 0.054 (0.254) data 0.000 (0.191) loss 1.4919 (1.4205) acc 82.1429 (77.9511) lr 3.6258e-04 eta 0:00:16
epoch [38/50] batch [5/5] time 0.493 (0.400) data 0.000 (0.257) loss 1.3830 (1.3840) acc 86.4583 (79.8083) lr 3.1545e-04 eta 0:00:24
epoch [39/50] batch [5/5] time 0.062 (0.460) data 0.000 (0.387) loss 1.3512 (1.4312) acc 82.6923 (75.0422) lr 2.7103e-04 eta 0:00:25
epoch [40/50] batch [5/5] time 0.046 (0.403) data 0.000 (0.344) loss 1.3229 (1.3884) acc 77.2727 (79.0197) lr 2.2949e-04 eta 0:00:20
epoch [41/50] batch [5/5] time 0.058 (0.268) data 0.000 (0.209) loss 1.3659 (1.3720) acc 77.9412 (82.4335) lr 1.9098e-04 eta 0:00:12
>>> samples [74/160] noisy rate: 0.38 --> 0.38 --> 0.22 <<<
epoch [42/50] batch [5/5] time 0.049 (0.364) data 0.001 (0.307) loss 1.1190 (1.3680) acc 78.8462 (78.8348) lr 1.5567e-04 eta 0:00:14
epoch [43/50] batch [5/5] time 0.074 (0.298) data 0.001 (0.235) loss 1.5768 (1.3826) acc 72.3684 (80.8828) lr 1.2369e-04 eta 0:00:10
epoch [44/50] batch [5/5] time 0.046 (0.334) data 0.000 (0.281) loss 1.6773 (1.3837) acc 71.1538 (78.2822) lr 9.5173e-05 eta 0:00:10
epoch [45/50] batch [5/5] time 0.071 (0.314) data 0.000 (0.250) loss 1.4050 (1.3637) acc 75.0000 (81.0577) lr 7.0224e-05 eta 0:00:07
epoch [46/50] batch [5/5] time 0.052 (0.398) data 0.000 (0.344) loss 1.3107 (1.3810) acc 92.8571 (80.9325) lr 4.8943e-05 eta 0:00:07
>>> samples [74/160] noisy rate: 0.38 --> 0.36 --> 0.22 <<<
epoch [47/50] batch [5/5] time 0.050 (0.221) data 0.000 (0.174) loss 1.4092 (1.3541) acc 79.4118 (79.9240) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [5/5] time 0.060 (0.234) data 0.000 (0.181) loss 1.1252 (1.3837) acc 80.5555 (79.2457) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [5/5] time 0.039 (0.212) data 0.000 (0.158) loss 1.1962 (1.3653) acc 90.0000 (80.1008) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/5] time 0.057 (0.254) data 0.000 (0.207) loss 1.2296 (1.3859) acc 88.7500 (80.2118) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_6FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.47, 0.43, 0.44, 0.43, 0.43, 0.35, 0.36, 0.36, 0.38, 0.36]
* learned noise rate: [0.08, 0.11, 0.13, 0.16, 0.18, 0.22, 0.22, 0.22, 0.22, 0.22]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:01<02:30,  1.88s/it]  4%|▎         | 3/81 [00:02<00:41,  1.87it/s]  6%|▌         | 5/81 [00:02<00:22,  3.43it/s]  9%|▊         | 7/81 [00:02<00:14,  5.09it/s] 11%|█         | 9/81 [00:02<00:10,  6.78it/s] 14%|█▎        | 11/81 [00:02<00:08,  8.49it/s] 16%|█▌        | 13/81 [00:02<00:06, 10.07it/s] 19%|█▊        | 15/81 [00:02<00:05, 11.42it/s] 21%|██        | 17/81 [00:02<00:05, 12.25it/s] 23%|██▎       | 19/81 [00:03<00:04, 12.68it/s] 26%|██▌       | 21/81 [00:03<00:04, 13.36it/s] 28%|██▊       | 23/81 [00:03<00:04, 13.51it/s] 31%|███       | 25/81 [00:03<00:03, 14.11it/s] 33%|███▎      | 27/81 [00:03<00:03, 14.52it/s] 36%|███▌      | 29/81 [00:03<00:03, 14.39it/s] 38%|███▊      | 31/81 [00:03<00:03, 14.20it/s] 41%|████      | 33/81 [00:04<00:03, 14.18it/s] 43%|████▎     | 35/81 [00:04<00:03, 14.58it/s] 46%|████▌     | 37/81 [00:04<00:02, 14.93it/s] 48%|████▊     | 39/81 [00:04<00:02, 15.13it/s] 51%|█████     | 41/81 [00:04<00:02, 15.35it/s] 53%|█████▎    | 43/81 [00:04<00:02, 15.52it/s] 56%|█████▌    | 45/81 [00:04<00:02, 15.38it/s] 58%|█████▊    | 47/81 [00:04<00:02, 15.27it/s] 60%|██████    | 49/81 [00:05<00:02, 15.39it/s] 63%|██████▎   | 51/81 [00:05<00:01, 15.03it/s] 65%|██████▌   | 53/81 [00:05<00:01, 14.22it/s] 68%|██████▊   | 55/81 [00:05<00:01, 14.09it/s] 70%|███████   | 57/81 [00:05<00:01, 14.49it/s] 73%|███████▎  | 59/81 [00:05<00:01, 14.17it/s] 75%|███████▌  | 61/81 [00:05<00:01, 14.60it/s] 78%|███████▊  | 63/81 [00:06<00:01, 13.87it/s] 80%|████████  | 65/81 [00:06<00:01, 14.08it/s] 83%|████████▎ | 67/81 [00:06<00:01, 13.52it/s] 85%|████████▌ | 69/81 [00:06<00:00, 14.07it/s] 88%|████████▊ | 71/81 [00:06<00:00, 14.47it/s] 90%|█████████ | 73/81 [00:06<00:00, 14.65it/s] 93%|█████████▎| 75/81 [00:06<00:00, 14.80it/s] 95%|█████████▌| 77/81 [00:07<00:00, 14.74it/s] 98%|█████████▊| 79/81 [00:07<00:00, 15.00it/s]100%|██████████| 81/81 [00:07<00:00, 15.19it/s]100%|██████████| 81/81 [00:07<00:00, 10.64it/s]
=> result
* total: 8,100
* correct: 4,354
* accuracy: 53.8%
* error: 46.2%
* macro_f1: 48.8%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 399	acc: 44.3%
* class: 1 (Forest)	total: 900	correct: 861	acc: 95.7%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 0	acc: 0.0%
* class: 3 (Highway or Road)	total: 750	correct: 455	acc: 60.7%
* class: 4 (Industrial Buildings)	total: 750	correct: 655	acc: 87.3%
* class: 5 (Pasture Land)	total: 600	correct: 448	acc: 74.7%
* class: 6 (Permanent Crop Land)	total: 750	correct: 387	acc: 51.6%
* class: 7 (Residential Buildings)	total: 900	correct: 768	acc: 85.3%
* class: 8 (River)	total: 750	correct: 353	acc: 47.1%
* class: 9 (Sea or Lake)	total: 900	correct: 28	acc: 3.1%
* average: 55.0%
Elapsed: 0:02:37
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_8-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.087 (0.808) data 0.000 (0.300) loss 1.1412 (1.1714) acc 7.8125 (13.4375) lr 2.0000e-03 eta 0:03:18
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> samples [41/160] noisy rate: 0.50 --> 0.51 --> 0.20 <<<
epoch [2/50] batch [5/5] time 0.270 (0.512) data 0.000 (0.197) loss 2.2761 (2.2297) acc 3.5714 (12.3380) lr 1.9980e-03 eta 0:02:02
epoch [3/50] batch [5/5] time 0.259 (0.249) data 0.000 (0.158) loss 1.9156 (2.0666) acc 58.3333 (28.4524) lr 1.9921e-03 eta 0:00:58
epoch [4/50] batch [5/5] time 0.040 (0.297) data 0.000 (0.251) loss 1.9920 (1.9643) acc 35.7143 (33.6833) lr 1.9823e-03 eta 0:01:08
epoch [5/50] batch [5/5] time 0.045 (0.307) data 0.000 (0.258) loss 2.1042 (1.8827) acc 32.5000 (47.1647) lr 1.9686e-03 eta 0:01:09
epoch [6/50] batch [5/5] time 0.036 (0.373) data 0.000 (0.290) loss 1.7322 (1.8098) acc 52.5000 (50.3268) lr 1.9511e-03 eta 0:01:22
>>> samples [47/160] noisy rate: 0.50 --> 0.49 --> 0.17 <<<
epoch [7/50] batch [5/5] time 0.038 (0.367) data 0.000 (0.266) loss 1.8245 (1.8496) acc 50.0000 (38.3858) lr 1.9298e-03 eta 0:01:18
epoch [8/50] batch [5/5] time 0.036 (0.305) data 0.000 (0.187) loss 1.5190 (1.8794) acc 43.7500 (39.9405) lr 1.9048e-03 eta 0:01:04
epoch [9/50] batch [5/5] time 0.048 (0.306) data 0.000 (0.250) loss 1.8358 (1.8153) acc 26.9231 (40.1444) lr 1.8763e-03 eta 0:01:02
epoch [10/50] batch [5/5] time 0.031 (0.283) data 0.000 (0.234) loss 1.5500 (1.6248) acc 58.3333 (58.2900) lr 1.8443e-03 eta 0:00:56
epoch [11/50] batch [5/5] time 0.033 (0.253) data 0.000 (0.210) loss 1.6890 (1.6035) acc 52.7778 (57.4786) lr 1.8090e-03 eta 0:00:49
>>> samples [48/160] noisy rate: 0.50 --> 0.49 --> 0.19 <<<
epoch [12/50] batch [5/5] time 0.044 (0.302) data 0.000 (0.184) loss 1.5548 (1.6377) acc 50.0000 (53.3059) lr 1.7705e-03 eta 0:00:57
epoch [13/50] batch [5/5] time 0.050 (0.322) data 0.000 (0.268) loss 1.7321 (1.4943) acc 50.0000 (65.1667) lr 1.7290e-03 eta 0:00:59
epoch [14/50] batch [5/5] time 0.046 (0.416) data 0.000 (0.291) loss 1.5396 (1.5304) acc 60.0000 (65.0198) lr 1.6845e-03 eta 0:01:14
epoch [15/50] batch [5/5] time 0.053 (0.561) data 0.000 (0.503) loss 1.5730 (1.4709) acc 58.3333 (63.3333) lr 1.6374e-03 eta 0:01:38
epoch [16/50] batch [5/5] time 0.046 (0.289) data 0.000 (0.241) loss 1.6536 (1.4860) acc 61.3636 (63.2866) lr 1.5878e-03 eta 0:00:49
>>> samples [49/160] noisy rate: 0.50 --> 0.46 --> 0.20 <<<
epoch [17/50] batch [5/5] time 0.032 (0.309) data 0.000 (0.262) loss 1.3990 (1.5134) acc 75.0000 (65.5278) lr 1.5358e-03 eta 0:00:50
epoch [18/50] batch [5/5] time 0.035 (0.310) data 0.000 (0.270) loss 1.2673 (1.4217) acc 69.4444 (64.6401) lr 1.4818e-03 eta 0:00:49
epoch [19/50] batch [5/5] time 0.041 (0.359) data 0.000 (0.306) loss 1.8223 (1.4332) acc 58.3333 (67.8755) lr 1.4258e-03 eta 0:00:55
epoch [20/50] batch [5/5] time 0.054 (0.453) data 0.000 (0.332) loss 1.4664 (1.3428) acc 70.4545 (74.9242) lr 1.3681e-03 eta 0:01:07
epoch [21/50] batch [5/5] time 0.059 (0.472) data 0.000 (0.414) loss 1.4157 (1.4072) acc 63.8889 (66.9704) lr 1.3090e-03 eta 0:01:08
>>> samples [54/160] noisy rate: 0.50 --> 0.43 --> 0.22 <<<
epoch [22/50] batch [5/5] time 0.046 (0.258) data 0.000 (0.220) loss 1.5215 (1.3722) acc 71.6667 (68.4747) lr 1.2487e-03 eta 0:00:36
epoch [23/50] batch [5/5] time 0.058 (0.353) data 0.000 (0.304) loss 1.3776 (1.3584) acc 69.2308 (69.9636) lr 1.1874e-03 eta 0:00:47
epoch [24/50] batch [5/5] time 0.048 (0.373) data 0.000 (0.328) loss 1.0476 (1.3986) acc 83.3333 (69.2088) lr 1.1253e-03 eta 0:00:48
epoch [25/50] batch [5/5] time 0.049 (0.356) data 0.000 (0.300) loss 1.3629 (1.3569) acc 73.2143 (64.9326) lr 1.0628e-03 eta 0:00:44
epoch [26/50] batch [5/5] time 0.050 (0.316) data 0.000 (0.257) loss 1.2979 (1.3423) acc 65.3846 (73.1678) lr 1.0000e-03 eta 0:00:37
>>> samples [55/160] noisy rate: 0.50 --> 0.36 --> 0.22 <<<
epoch [27/50] batch [5/5] time 0.099 (0.348) data 0.000 (0.257) loss 1.3099 (1.3231) acc 81.2500 (74.7827) lr 9.3721e-04 eta 0:00:40
epoch [28/50] batch [5/5] time 0.092 (0.351) data 0.000 (0.255) loss 1.2755 (1.2826) acc 83.3333 (75.6074) lr 8.7467e-04 eta 0:00:38
epoch [29/50] batch [5/5] time 0.066 (0.569) data 0.001 (0.476) loss 1.1407 (1.3300) acc 77.7778 (71.3472) lr 8.1262e-04 eta 0:00:59
epoch [30/50] batch [5/5] time 0.074 (0.441) data 0.000 (0.358) loss 1.5677 (1.3107) acc 72.7273 (74.2028) lr 7.5131e-04 eta 0:00:44
epoch [31/50] batch [5/5] time 0.082 (0.270) data 0.000 (0.191) loss 1.0243 (1.2533) acc 100.0000 (77.5833) lr 6.9098e-04 eta 0:00:25
>>> samples [55/160] noisy rate: 0.50 --> 0.32 --> 0.22 <<<
epoch [32/50] batch [5/5] time 0.103 (0.400) data 0.001 (0.309) loss 1.1457 (1.2542) acc 88.3333 (73.0833) lr 6.3188e-04 eta 0:00:36
epoch [33/50] batch [5/5] time 0.084 (0.308) data 0.000 (0.228) loss 1.1088 (1.2595) acc 81.2500 (78.3460) lr 5.7422e-04 eta 0:00:26
epoch [34/50] batch [5/5] time 0.084 (0.317) data 0.000 (0.235) loss 1.1727 (1.2499) acc 76.9231 (73.9957) lr 5.1825e-04 eta 0:00:25
epoch [35/50] batch [5/5] time 0.087 (0.573) data 0.000 (0.358) loss 1.2717 (1.2751) acc 87.5000 (74.3766) lr 4.6417e-04 eta 0:00:43
epoch [36/50] batch [5/5] time 0.062 (0.279) data 0.000 (0.192) loss 1.2992 (1.2402) acc 87.5000 (79.0476) lr 4.1221e-04 eta 0:00:19
>>> samples [56/160] noisy rate: 0.50 --> 0.36 --> 0.21 <<<
epoch [37/50] batch [5/5] time 0.078 (0.307) data 0.000 (0.215) loss 0.8111 (1.1790) acc 90.6250 (79.4345) lr 3.6258e-04 eta 0:00:19
epoch [38/50] batch [5/5] time 0.079 (0.445) data 0.000 (0.364) loss 1.3811 (1.2686) acc 75.0000 (78.4722) lr 3.1545e-04 eta 0:00:26
epoch [39/50] batch [5/5] time 0.068 (0.301) data 0.000 (0.212) loss 1.4200 (1.2467) acc 61.1111 (77.5252) lr 2.7103e-04 eta 0:00:16
epoch [40/50] batch [5/5] time 0.049 (0.292) data 0.000 (0.216) loss 1.0754 (1.2762) acc 88.8889 (78.3611) lr 2.2949e-04 eta 0:00:14
epoch [41/50] batch [5/5] time 0.058 (0.323) data 0.000 (0.249) loss 0.9817 (1.1883) acc 81.2500 (77.9491) lr 1.9098e-04 eta 0:00:14
>>> samples [70/160] noisy rate: 0.50 --> 0.39 --> 0.26 <<<
epoch [42/50] batch [5/5] time 0.101 (0.408) data 0.000 (0.195) loss 1.2227 (1.2317) acc 79.6875 (74.1591) lr 1.5567e-04 eta 0:00:16
epoch [43/50] batch [5/5] time 0.102 (0.259) data 0.000 (0.161) loss 1.2387 (1.2156) acc 73.3333 (74.8974) lr 1.2369e-04 eta 0:00:09
epoch [44/50] batch [5/5] time 0.049 (0.328) data 0.001 (0.250) loss 1.2292 (1.1999) acc 65.9091 (75.3026) lr 9.5173e-05 eta 0:00:09
epoch [45/50] batch [5/5] time 0.043 (0.353) data 0.000 (0.297) loss 1.2461 (1.2221) acc 67.5000 (71.8990) lr 7.0224e-05 eta 0:00:08
epoch [46/50] batch [5/5] time 0.062 (0.447) data 0.000 (0.240) loss 0.9952 (1.1975) acc 81.8182 (76.5564) lr 4.8943e-05 eta 0:00:08
>>> samples [72/160] noisy rate: 0.50 --> 0.36 --> 0.26 <<<
epoch [47/50] batch [5/5] time 0.068 (0.479) data 0.000 (0.312) loss 1.2614 (1.2080) acc 69.1176 (76.7457) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/5] time 0.090 (0.405) data 0.000 (0.310) loss 1.4462 (1.2025) acc 63.6364 (77.7945) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/5] time 0.122 (0.576) data 0.000 (0.467) loss 0.9873 (1.1877) acc 83.3333 (78.9337) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/5] time 0.066 (0.532) data 0.000 (0.475) loss 1.1689 (1.1576) acc 76.3889 (75.7222) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.51, 0.49, 0.49, 0.46, 0.43, 0.36, 0.32, 0.36, 0.39, 0.36]
* learned noise rate: [0.2, 0.17, 0.19, 0.2, 0.22, 0.22, 0.22, 0.21, 0.26, 0.26]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<03:33,  2.67s/it]  2%|▏         | 2/81 [00:02<01:32,  1.18s/it]  4%|▎         | 3/81 [00:02<00:53,  1.45it/s]  5%|▍         | 4/81 [00:03<00:36,  2.12it/s]  7%|▋         | 6/81 [00:03<00:20,  3.58it/s]  9%|▊         | 7/81 [00:03<00:17,  4.22it/s] 10%|▉         | 8/81 [00:03<00:14,  5.00it/s] 11%|█         | 9/81 [00:03<00:12,  5.61it/s] 12%|█▏        | 10/81 [00:03<00:11,  6.31it/s] 14%|█▎        | 11/81 [00:03<00:10,  6.62it/s] 15%|█▍        | 12/81 [00:03<00:09,  7.24it/s] 16%|█▌        | 13/81 [00:04<00:09,  7.35it/s] 17%|█▋        | 14/81 [00:04<00:09,  7.42it/s] 19%|█▊        | 15/81 [00:04<00:08,  7.54it/s] 20%|█▉        | 16/81 [00:04<00:08,  7.35it/s] 21%|██        | 17/81 [00:04<00:08,  7.77it/s] 22%|██▏       | 18/81 [00:04<00:08,  7.80it/s] 23%|██▎       | 19/81 [00:04<00:08,  7.69it/s] 25%|██▍       | 20/81 [00:05<00:08,  7.39it/s] 26%|██▌       | 21/81 [00:05<00:07,  7.60it/s] 27%|██▋       | 22/81 [00:05<00:07,  7.85it/s] 28%|██▊       | 23/81 [00:05<00:07,  7.83it/s] 30%|██▉       | 24/81 [00:05<00:07,  7.67it/s] 31%|███       | 25/81 [00:05<00:07,  7.08it/s] 32%|███▏      | 26/81 [00:05<00:07,  7.65it/s] 33%|███▎      | 27/81 [00:05<00:07,  7.70it/s] 35%|███▍      | 28/81 [00:06<00:06,  7.91it/s] 36%|███▌      | 29/81 [00:06<00:07,  7.33it/s] 37%|███▋      | 30/81 [00:06<00:06,  7.48it/s] 38%|███▊      | 31/81 [00:06<00:06,  7.46it/s] 41%|████      | 33/81 [00:06<00:05,  8.31it/s] 42%|████▏     | 34/81 [00:06<00:05,  8.17it/s] 43%|████▎     | 35/81 [00:06<00:05,  7.96it/s] 44%|████▍     | 36/81 [00:07<00:05,  7.86it/s] 47%|████▋     | 38/81 [00:07<00:04,  9.15it/s] 48%|████▊     | 39/81 [00:07<00:04,  8.64it/s] 49%|████▉     | 40/81 [00:07<00:04,  8.84it/s] 51%|█████     | 41/81 [00:07<00:04,  8.02it/s] 52%|█████▏    | 42/81 [00:07<00:04,  8.13it/s] 53%|█████▎    | 43/81 [00:07<00:04,  8.50it/s] 56%|█████▌    | 45/81 [00:08<00:03,  9.69it/s] 58%|█████▊    | 47/81 [00:08<00:03, 10.28it/s] 60%|██████    | 49/81 [00:08<00:02, 11.03it/s] 63%|██████▎   | 51/81 [00:08<00:02, 11.19it/s] 65%|██████▌   | 53/81 [00:08<00:02, 10.40it/s] 68%|██████▊   | 55/81 [00:09<00:02,  9.56it/s] 69%|██████▉   | 56/81 [00:09<00:02,  9.26it/s] 70%|███████   | 57/81 [00:09<00:02,  9.14it/s] 72%|███████▏  | 58/81 [00:09<00:02,  8.71it/s] 73%|███████▎  | 59/81 [00:09<00:02,  8.22it/s] 75%|███████▌  | 61/81 [00:09<00:02,  8.42it/s] 77%|███████▋  | 62/81 [00:09<00:02,  8.04it/s] 78%|███████▊  | 63/81 [00:10<00:02,  8.10it/s] 79%|███████▉  | 64/81 [00:10<00:02,  8.01it/s] 80%|████████  | 65/81 [00:10<00:02,  7.91it/s] 81%|████████▏ | 66/81 [00:10<00:01,  7.92it/s] 83%|████████▎ | 67/81 [00:10<00:01,  7.52it/s] 84%|████████▍ | 68/81 [00:10<00:01,  7.63it/s] 85%|████████▌ | 69/81 [00:10<00:01,  7.57it/s] 86%|████████▋ | 70/81 [00:10<00:01,  7.60it/s] 88%|████████▊ | 71/81 [00:11<00:01,  7.70it/s] 89%|████████▉ | 72/81 [00:11<00:01,  7.65it/s] 90%|█████████ | 73/81 [00:11<00:01,  7.68it/s] 91%|█████████▏| 74/81 [00:11<00:00,  7.68it/s] 93%|█████████▎| 75/81 [00:11<00:00,  7.68it/s] 94%|█████████▍| 76/81 [00:11<00:00,  7.71it/s] 95%|█████████▌| 77/81 [00:11<00:00,  7.67it/s] 96%|█████████▋| 78/81 [00:11<00:00,  7.54it/s] 99%|█████████▉| 80/81 [00:12<00:00,  9.09it/s]100%|██████████| 81/81 [00:12<00:00,  6.35it/s]
=> result
* total: 8,100
* correct: 4,395
* accuracy: 54.3%
* error: 45.7%
* macro_f1: 51.0%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 656	acc: 72.9%
* class: 1 (Forest)	total: 900	correct: 835	acc: 92.8%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 1	acc: 0.1%
* class: 3 (Highway or Road)	total: 750	correct: 236	acc: 31.5%
* class: 4 (Industrial Buildings)	total: 750	correct: 611	acc: 81.5%
* class: 5 (Pasture Land)	total: 600	correct: 446	acc: 74.3%
* class: 6 (Permanent Crop Land)	total: 750	correct: 550	acc: 73.3%
* class: 7 (Residential Buildings)	total: 900	correct: 371	acc: 41.2%
* class: 8 (River)	total: 750	correct: 487	acc: 64.9%
* class: 9 (Sea or Lake)	total: 900	correct: 202	acc: 22.4%
* average: 55.5%
Elapsed: 0:03:13
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_8-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.179 (1.252) data 0.000 (0.327) loss 1.1738 (1.1644) acc 14.0625 (13.4375) lr 2.0000e-03 eta 0:05:06
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> samples [48/160] noisy rate: 0.50 --> 0.49 --> 0.19 <<<
epoch [2/50] batch [5/5] time 0.082 (0.477) data 0.000 (0.199) loss 2.0380 (2.1353) acc 29.5455 (26.6136) lr 1.9980e-03 eta 0:01:54
epoch [3/50] batch [5/5] time 0.561 (0.826) data 0.000 (0.387) loss 2.0227 (2.1228) acc 16.6667 (24.8840) lr 1.9921e-03 eta 0:03:14
epoch [4/50] batch [5/5] time 0.091 (0.451) data 0.000 (0.372) loss 1.9975 (2.0031) acc 25.0000 (32.5934) lr 1.9823e-03 eta 0:01:43
epoch [5/50] batch [5/5] time 0.098 (0.344) data 0.000 (0.261) loss 2.0496 (2.0382) acc 25.0000 (29.7058) lr 1.9686e-03 eta 0:01:17
epoch [6/50] batch [5/5] time 0.080 (0.581) data 0.000 (0.498) loss 2.0343 (1.9637) acc 27.7778 (38.6454) lr 1.9511e-03 eta 0:02:07
>>> samples [55/160] noisy rate: 0.50 --> 0.51 --> 0.18 <<<
epoch [7/50] batch [5/5] time 0.094 (0.387) data 0.000 (0.310) loss 2.0250 (1.8597) acc 50.0000 (43.5820) lr 1.9298e-03 eta 0:01:23
epoch [8/50] batch [5/5] time 0.682 (0.790) data 0.000 (0.478) loss 1.6329 (1.8365) acc 48.4375 (47.3920) lr 1.9048e-03 eta 0:02:45
epoch [9/50] batch [5/5] time 0.038 (0.368) data 0.000 (0.312) loss 1.6098 (1.7647) acc 70.0000 (51.0559) lr 1.8763e-03 eta 0:01:15
epoch [10/50] batch [5/5] time 0.068 (0.310) data 0.000 (0.252) loss 1.7317 (1.7620) acc 43.7500 (55.7158) lr 1.8443e-03 eta 0:01:01
epoch [11/50] batch [5/5] time 0.056 (0.454) data 0.000 (0.374) loss 1.8297 (1.7365) acc 51.9231 (58.4991) lr 1.8090e-03 eta 0:01:28
>>> samples [61/160] noisy rate: 0.50 --> 0.43 --> 0.16 <<<
epoch [12/50] batch [5/5] time 0.702 (0.642) data 0.000 (0.304) loss 1.5595 (1.6044) acc 66.0714 (68.9716) lr 1.7705e-03 eta 0:02:01
epoch [13/50] batch [5/5] time 0.050 (0.280) data 0.000 (0.224) loss 1.7765 (1.6915) acc 54.5455 (59.0851) lr 1.7290e-03 eta 0:00:51
epoch [14/50] batch [5/5] time 0.079 (0.439) data 0.000 (0.345) loss 1.6835 (1.6145) acc 53.5714 (68.6082) lr 1.6845e-03 eta 0:01:18
epoch [15/50] batch [5/5] time 0.072 (0.426) data 0.000 (0.343) loss 1.4732 (1.5922) acc 68.7500 (66.1845) lr 1.6374e-03 eta 0:01:14
epoch [16/50] batch [5/5] time 0.689 (0.428) data 0.000 (0.223) loss 1.6268 (1.6025) acc 61.1111 (69.5328) lr 1.5878e-03 eta 0:01:12
>>> samples [64/160] noisy rate: 0.50 --> 0.44 --> 0.19 <<<
epoch [17/50] batch [5/5] time 0.034 (0.246) data 0.000 (0.199) loss 1.5018 (1.5813) acc 69.4444 (66.0940) lr 1.5358e-03 eta 0:00:40
epoch [18/50] batch [5/5] time 0.770 (0.527) data 0.000 (0.304) loss 1.5724 (1.5563) acc 67.6471 (71.6560) lr 1.4818e-03 eta 0:01:24
epoch [19/50] batch [5/5] time 0.100 (0.281) data 0.000 (0.188) loss 1.4877 (1.5438) acc 65.3846 (69.4463) lr 1.4258e-03 eta 0:00:43
epoch [20/50] batch [5/5] time 0.090 (0.271) data 0.000 (0.173) loss 1.5269 (1.5036) acc 69.2308 (70.9994) lr 1.3681e-03 eta 0:00:40
epoch [21/50] batch [5/5] time 0.097 (0.291) data 0.000 (0.190) loss 1.4697 (1.5270) acc 62.5000 (67.0128) lr 1.3090e-03 eta 0:00:42
>>> samples [65/160] noisy rate: 0.50 --> 0.42 --> 0.18 <<<
epoch [22/50] batch [5/5] time 0.093 (0.391) data 0.000 (0.296) loss 1.5338 (1.4784) acc 85.4167 (73.3248) lr 1.2487e-03 eta 0:00:54
epoch [23/50] batch [5/5] time 0.102 (0.291) data 0.000 (0.194) loss 1.5301 (1.4854) acc 76.5625 (72.5355) lr 1.1874e-03 eta 0:00:39
epoch [24/50] batch [5/5] time 0.119 (0.385) data 0.001 (0.276) loss 1.3653 (1.4584) acc 73.2143 (70.5141) lr 1.1253e-03 eta 0:00:50
epoch [25/50] batch [5/5] time 0.087 (0.465) data 0.000 (0.365) loss 1.3466 (1.4846) acc 75.0000 (67.9021) lr 1.0628e-03 eta 0:00:58
epoch [26/50] batch [5/5] time 0.031 (0.457) data 0.000 (0.407) loss 1.4678 (1.4345) acc 71.4286 (73.0203) lr 1.0000e-03 eta 0:00:54
>>> samples [66/160] noisy rate: 0.50 --> 0.41 --> 0.20 <<<
epoch [27/50] batch [5/5] time 0.079 (0.574) data 0.001 (0.466) loss 1.2966 (1.4310) acc 85.7143 (72.6200) lr 9.3721e-04 eta 0:01:05
epoch [28/50] batch [5/5] time 0.121 (0.440) data 0.001 (0.350) loss 1.2928 (1.4343) acc 77.9412 (73.5049) lr 8.7467e-04 eta 0:00:48
epoch [29/50] batch [5/5] time 0.064 (0.366) data 0.000 (0.268) loss 1.1516 (1.4287) acc 88.6364 (79.0312) lr 8.1262e-04 eta 0:00:38
epoch [30/50] batch [5/5] time 0.098 (0.325) data 0.000 (0.227) loss 1.1744 (1.4058) acc 79.6875 (79.1683) lr 7.5131e-04 eta 0:00:32
epoch [31/50] batch [5/5] time 0.065 (0.335) data 0.000 (0.230) loss 1.5322 (1.4213) acc 78.1250 (77.3690) lr 6.9098e-04 eta 0:00:31
>>> samples [66/160] noisy rate: 0.50 --> 0.38 --> 0.20 <<<
epoch [32/50] batch [5/5] time 0.095 (0.568) data 0.000 (0.476) loss 1.3351 (1.3768) acc 88.4615 (79.7179) lr 6.3188e-04 eta 0:00:51
epoch [33/50] batch [5/5] time 0.095 (0.337) data 0.000 (0.244) loss 1.2329 (1.3920) acc 76.7857 (78.2005) lr 5.7422e-04 eta 0:00:28
epoch [34/50] batch [5/5] time 0.042 (0.265) data 0.000 (0.213) loss 1.4843 (1.3450) acc 88.6364 (83.0214) lr 5.1825e-04 eta 0:00:21
epoch [35/50] batch [5/5] time 0.098 (0.470) data 0.000 (0.364) loss 1.4801 (1.4036) acc 72.9167 (79.2869) lr 4.6417e-04 eta 0:00:35
epoch [36/50] batch [5/5] time 0.106 (0.651) data 0.000 (0.549) loss 1.7096 (1.3648) acc 65.2778 (77.5427) lr 4.1221e-04 eta 0:00:45
>>> samples [72/160] noisy rate: 0.50 --> 0.42 --> 0.22 <<<
epoch [37/50] batch [5/5] time 0.058 (0.288) data 0.000 (0.236) loss 1.3979 (1.3978) acc 71.8750 (76.0417) lr 3.6258e-04 eta 0:00:18
epoch [38/50] batch [5/5] time 0.114 (0.329) data 0.000 (0.239) loss 1.3052 (1.4014) acc 86.7647 (76.2835) lr 3.1545e-04 eta 0:00:19
epoch [39/50] batch [5/5] time 0.064 (0.587) data 0.001 (0.487) loss 1.1886 (1.3651) acc 87.5000 (80.1667) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [5/5] time 0.481 (0.586) data 0.000 (0.409) loss 1.3223 (1.3885) acc 82.5000 (75.7820) lr 2.2949e-04 eta 0:00:29
epoch [41/50] batch [5/5] time 0.046 (0.557) data 0.001 (0.358) loss 1.4030 (1.3892) acc 71.8750 (79.0326) lr 1.9098e-04 eta 0:00:25
>>> samples [72/160] noisy rate: 0.50 --> 0.38 --> 0.22 <<<
epoch [42/50] batch [5/5] time 0.098 (0.384) data 0.000 (0.278) loss 1.4349 (1.3688) acc 75.0000 (78.4246) lr 1.5567e-04 eta 0:00:15
epoch [43/50] batch [5/5] time 0.103 (0.308) data 0.000 (0.207) loss 1.3682 (1.3874) acc 80.8824 (80.2390) lr 1.2369e-04 eta 0:00:10
epoch [44/50] batch [5/5] time 0.123 (0.339) data 0.000 (0.232) loss 1.4119 (1.3825) acc 75.0000 (78.4167) lr 9.5173e-05 eta 0:00:10
epoch [45/50] batch [5/5] time 0.116 (0.362) data 0.000 (0.262) loss 1.3915 (1.3428) acc 84.3750 (79.0230) lr 7.0224e-05 eta 0:00:09
epoch [46/50] batch [5/5] time 0.092 (0.413) data 0.000 (0.314) loss 1.2790 (1.3639) acc 86.3636 (81.8981) lr 4.8943e-05 eta 0:00:08
>>> samples [72/160] noisy rate: 0.50 --> 0.39 --> 0.22 <<<
epoch [47/50] batch [5/5] time 0.113 (0.316) data 0.000 (0.212) loss 1.4119 (1.4005) acc 82.1429 (75.9487) lr 3.1417e-05 eta 0:00:04
epoch [48/50] batch [5/5] time 0.089 (0.324) data 0.000 (0.223) loss 1.5333 (1.3740) acc 75.0000 (75.7545) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [5/5] time 0.050 (0.340) data 0.000 (0.281) loss 1.2410 (1.3895) acc 75.0000 (78.6896) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [5/5] time 0.112 (0.463) data 0.000 (0.367) loss 1.3508 (1.3647) acc 75.0000 (80.9684) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.49, 0.51, 0.43, 0.44, 0.42, 0.41, 0.38, 0.42, 0.38, 0.39]
* learned noise rate: [0.19, 0.18, 0.16, 0.19, 0.18, 0.2, 0.2, 0.22, 0.22, 0.22]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:02<03:06,  2.33s/it]  2%|▏         | 2/81 [00:02<01:21,  1.03s/it]  4%|▎         | 3/81 [00:02<00:48,  1.61it/s]  5%|▍         | 4/81 [00:02<00:33,  2.32it/s]  6%|▌         | 5/81 [00:02<00:23,  3.19it/s]  7%|▋         | 6/81 [00:02<00:18,  4.03it/s]  9%|▊         | 7/81 [00:03<00:15,  4.91it/s] 10%|▉         | 8/81 [00:03<00:12,  5.66it/s] 11%|█         | 9/81 [00:03<00:12,  5.95it/s] 12%|█▏        | 10/81 [00:03<00:11,  6.30it/s] 15%|█▍        | 12/81 [00:03<00:09,  7.64it/s] 16%|█▌        | 13/81 [00:03<00:08,  7.79it/s] 19%|█▊        | 15/81 [00:04<00:07,  8.39it/s] 20%|█▉        | 16/81 [00:04<00:08,  8.00it/s] 21%|██        | 17/81 [00:04<00:08,  7.72it/s] 23%|██▎       | 19/81 [00:04<00:07,  8.57it/s] 25%|██▍       | 20/81 [00:04<00:07,  8.45it/s] 26%|██▌       | 21/81 [00:04<00:07,  8.32it/s] 27%|██▋       | 22/81 [00:04<00:06,  8.51it/s] 28%|██▊       | 23/81 [00:04<00:07,  8.13it/s] 30%|██▉       | 24/81 [00:05<00:07,  7.57it/s] 31%|███       | 25/81 [00:05<00:07,  7.94it/s] 32%|███▏      | 26/81 [00:05<00:06,  8.27it/s] 33%|███▎      | 27/81 [00:05<00:06,  8.50it/s] 35%|███▍      | 28/81 [00:05<00:06,  8.35it/s] 36%|███▌      | 29/81 [00:05<00:06,  7.70it/s] 37%|███▋      | 30/81 [00:05<00:06,  7.87it/s] 38%|███▊      | 31/81 [00:05<00:06,  8.00it/s] 40%|███▉      | 32/81 [00:06<00:05,  8.25it/s] 41%|████      | 33/81 [00:06<00:05,  8.04it/s] 43%|████▎     | 35/81 [00:06<00:05,  7.97it/s] 44%|████▍     | 36/81 [00:06<00:05,  7.72it/s] 46%|████▌     | 37/81 [00:06<00:05,  7.46it/s] 47%|████▋     | 38/81 [00:06<00:05,  7.26it/s] 48%|████▊     | 39/81 [00:07<00:05,  7.61it/s] 49%|████▉     | 40/81 [00:07<00:05,  7.45it/s] 51%|█████     | 41/81 [00:07<00:05,  7.80it/s] 52%|█████▏    | 42/81 [00:07<00:04,  8.14it/s] 53%|█████▎    | 43/81 [00:07<00:04,  8.24it/s] 54%|█████▍    | 44/81 [00:07<00:04,  8.26it/s] 56%|█████▌    | 45/81 [00:07<00:04,  8.24it/s] 58%|█████▊    | 47/81 [00:07<00:04,  8.33it/s] 59%|█████▉    | 48/81 [00:08<00:04,  7.97it/s] 62%|██████▏   | 50/81 [00:08<00:03,  8.63it/s] 63%|██████▎   | 51/81 [00:08<00:03,  8.49it/s] 64%|██████▍   | 52/81 [00:08<00:03,  8.76it/s] 65%|██████▌   | 53/81 [00:08<00:03,  8.64it/s] 67%|██████▋   | 54/81 [00:08<00:03,  8.16it/s] 68%|██████▊   | 55/81 [00:08<00:03,  7.72it/s] 70%|███████   | 57/81 [00:09<00:02,  8.68it/s] 72%|███████▏  | 58/81 [00:09<00:02,  8.44it/s] 73%|███████▎  | 59/81 [00:09<00:02,  8.75it/s] 74%|███████▍  | 60/81 [00:09<00:02,  8.83it/s] 75%|███████▌  | 61/81 [00:09<00:02,  8.30it/s] 77%|███████▋  | 62/81 [00:09<00:02,  7.97it/s] 79%|███████▉  | 64/81 [00:09<00:01,  8.74it/s] 80%|████████  | 65/81 [00:10<00:01,  8.60it/s] 81%|████████▏ | 66/81 [00:10<00:01,  8.54it/s] 83%|████████▎ | 67/81 [00:10<00:01,  8.65it/s] 84%|████████▍ | 68/81 [00:10<00:01,  8.21it/s] 85%|████████▌ | 69/81 [00:10<00:01,  7.87it/s] 88%|████████▊ | 71/81 [00:10<00:01,  8.86it/s] 89%|████████▉ | 72/81 [00:10<00:01,  8.65it/s] 90%|█████████ | 73/81 [00:11<00:00,  8.88it/s] 91%|█████████▏| 74/81 [00:11<00:00,  9.09it/s] 93%|█████████▎| 75/81 [00:11<00:00,  8.78it/s] 94%|█████████▍| 76/81 [00:11<00:00,  8.31it/s] 95%|█████████▌| 77/81 [00:11<00:00,  8.43it/s] 96%|█████████▋| 78/81 [00:11<00:00,  8.37it/s] 98%|█████████▊| 79/81 [00:11<00:00,  8.49it/s] 99%|█████████▉| 80/81 [00:11<00:00,  8.58it/s]100%|██████████| 81/81 [00:11<00:00,  8.53it/s]100%|██████████| 81/81 [00:12<00:00,  6.63it/s]
=> result
* total: 8,100
* correct: 4,459
* accuracy: 55.0%
* error: 45.0%
* macro_f1: 51.4%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 184	acc: 20.4%
* class: 1 (Forest)	total: 900	correct: 873	acc: 97.0%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 1	acc: 0.1%
* class: 3 (Highway or Road)	total: 750	correct: 400	acc: 53.3%
* class: 4 (Industrial Buildings)	total: 750	correct: 617	acc: 82.3%
* class: 5 (Pasture Land)	total: 600	correct: 379	acc: 63.2%
* class: 6 (Permanent Crop Land)	total: 750	correct: 544	acc: 72.5%
* class: 7 (Residential Buildings)	total: 900	correct: 256	acc: 28.4%
* class: 8 (River)	total: 750	correct: 482	acc: 64.3%
* class: 9 (Sea or Lake)	total: 900	correct: 723	acc: 80.3%
* average: 56.2%
Elapsed: 0:03:44
Run this job and save the output to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'pairflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: EuroSAT
Reading split from /data1/zhli/dpl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/eurosat/split_fewshot/shots_16_pairflip/fp_8-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      40
# test     8,100
---------  -------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: pairflip
  NAME: EuroSAT
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA1: 0.5
    ALPHA2: 0.1
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 1
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/5] time 0.179 (0.999) data 0.000 (0.394) loss 1.1380 (1.1567) acc 19.5312 (13.4375) lr 2.0000e-03 eta 0:04:04
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> samples [36/160] noisy rate: 0.50 --> 0.57 --> 0.17 <<<
epoch [2/50] batch [5/5] time 0.488 (0.822) data 0.000 (0.453) loss 2.1045 (2.1002) acc 27.7778 (25.2341) lr 1.9980e-03 eta 0:03:17
epoch [3/50] batch [5/5] time 0.048 (0.498) data 0.000 (0.305) loss 1.7346 (2.1133) acc 55.5556 (24.8611) lr 1.9921e-03 eta 0:01:56
epoch [4/50] batch [5/5] time 0.064 (0.309) data 0.000 (0.238) loss 2.0763 (2.0515) acc 20.0000 (25.8056) lr 1.9823e-03 eta 0:01:11
epoch [5/50] batch [5/5] time 0.064 (0.397) data 0.000 (0.226) loss 1.9400 (1.9452) acc 70.0000 (43.5833) lr 1.9686e-03 eta 0:01:29
epoch [6/50] batch [5/5] time 0.923 (0.670) data 0.000 (0.435) loss 1.7015 (1.8466) acc 65.9091 (55.8009) lr 1.9511e-03 eta 0:02:27
>>> samples [41/160] noisy rate: 0.50 --> 0.51 --> 0.20 <<<
epoch [7/50] batch [5/5] time 0.083 (0.441) data 0.000 (0.267) loss 1.8828 (1.8258) acc 50.0000 (52.7937) lr 1.9298e-03 eta 0:01:34
epoch [8/50] batch [5/5] time 0.077 (0.422) data 0.000 (0.345) loss 1.7936 (1.7456) acc 52.5000 (53.6944) lr 1.9048e-03 eta 0:01:28
epoch [9/50] batch [5/5] time 0.080 (0.369) data 0.000 (0.293) loss 2.0204 (1.7349) acc 36.3636 (59.8561) lr 1.8763e-03 eta 0:01:15
epoch [10/50] batch [5/5] time 0.033 (0.445) data 0.000 (0.328) loss 1.2464 (1.6459) acc 66.6667 (64.0476) lr 1.8443e-03 eta 0:01:28
epoch [11/50] batch [5/5] time 0.045 (0.278) data 0.000 (0.227) loss 1.6557 (1.6534) acc 70.0000 (67.5238) lr 1.8090e-03 eta 0:00:54
>>> samples [52/160] noisy rate: 0.50 --> 0.51 --> 0.21 <<<
epoch [12/50] batch [5/5] time 0.076 (0.420) data 0.000 (0.339) loss 1.4323 (1.6249) acc 68.1818 (66.0054) lr 1.7705e-03 eta 0:01:19
epoch [13/50] batch [5/5] time 0.079 (0.367) data 0.000 (0.289) loss 1.6534 (1.6242) acc 65.9091 (67.9596) lr 1.7290e-03 eta 0:01:07
epoch [14/50] batch [5/5] time 0.065 (0.692) data 0.000 (0.488) loss 1.6157 (1.6035) acc 63.8889 (72.4973) lr 1.6845e-03 eta 0:02:04
epoch [15/50] batch [5/5] time 0.056 (0.284) data 0.000 (0.203) loss 1.6046 (1.5420) acc 64.5833 (76.4583) lr 1.6374e-03 eta 0:00:49
epoch [16/50] batch [5/5] time 0.065 (0.290) data 0.000 (0.213) loss 1.4822 (1.5557) acc 75.0000 (75.7292) lr 1.5878e-03 eta 0:00:49
>>> samples [53/160] noisy rate: 0.50 --> 0.45 --> 0.21 <<<
epoch [17/50] batch [5/5] time 0.090 (0.684) data 0.000 (0.487) loss 1.4614 (1.5846) acc 80.3571 (72.1281) lr 1.5358e-03 eta 0:01:52
epoch [18/50] batch [5/5] time 0.062 (0.336) data 0.000 (0.254) loss 1.6532 (1.5730) acc 59.3750 (70.1307) lr 1.4818e-03 eta 0:00:53
epoch [19/50] batch [5/5] time 0.055 (0.339) data 0.000 (0.282) loss 1.5122 (1.5309) acc 62.5000 (78.7968) lr 1.4258e-03 eta 0:00:52
epoch [20/50] batch [5/5] time 0.110 (0.446) data 0.000 (0.360) loss 1.4054 (1.5690) acc 68.1818 (74.3750) lr 1.3681e-03 eta 0:01:06
epoch [21/50] batch [5/5] time 0.093 (0.501) data 0.000 (0.422) loss 1.6762 (1.5700) acc 63.4615 (73.4246) lr 1.3090e-03 eta 0:01:12
>>> samples [57/160] noisy rate: 0.50 --> 0.47 --> 0.23 <<<
epoch [22/50] batch [5/5] time 0.048 (0.450) data 0.000 (0.267) loss 1.4716 (1.5902) acc 67.3077 (74.5726) lr 1.2487e-03 eta 0:01:02
epoch [23/50] batch [5/5] time 0.045 (0.246) data 0.000 (0.194) loss 1.4727 (1.5690) acc 70.4545 (69.8962) lr 1.1874e-03 eta 0:00:33
epoch [24/50] batch [5/5] time 0.089 (0.395) data 0.001 (0.298) loss 1.9140 (1.5334) acc 67.5000 (74.5000) lr 1.1253e-03 eta 0:00:51
epoch [25/50] batch [5/5] time 0.109 (0.381) data 0.000 (0.286) loss 1.7856 (1.5418) acc 65.3846 (76.8020) lr 1.0628e-03 eta 0:00:47
epoch [26/50] batch [5/5] time 0.069 (0.271) data 0.000 (0.187) loss 1.6136 (1.5455) acc 62.5000 (73.7202) lr 1.0000e-03 eta 0:00:32
>>> samples [60/160] noisy rate: 0.50 --> 0.49 --> 0.23 <<<
epoch [27/50] batch [5/5] time 0.091 (0.388) data 0.000 (0.314) loss 1.1708 (1.5206) acc 96.4286 (74.6875) lr 9.3721e-04 eta 0:00:44
epoch [28/50] batch [5/5] time 0.077 (0.276) data 0.000 (0.188) loss 1.7149 (1.5347) acc 75.0000 (75.0000) lr 8.7467e-04 eta 0:00:30
epoch [29/50] batch [5/5] time 0.038 (0.262) data 0.000 (0.175) loss 1.6864 (1.5374) acc 60.0000 (73.0208) lr 8.1262e-04 eta 0:00:27
epoch [30/50] batch [5/5] time 0.061 (0.467) data 0.000 (0.239) loss 1.2550 (1.4557) acc 82.1429 (77.0675) lr 7.5131e-04 eta 0:00:46
epoch [31/50] batch [5/5] time 0.070 (0.340) data 0.000 (0.255) loss 1.4533 (1.5091) acc 90.6250 (74.0541) lr 6.9098e-04 eta 0:00:32
>>> samples [62/160] noisy rate: 0.50 --> 0.51 --> 0.24 <<<
epoch [32/50] batch [5/5] time 0.094 (0.377) data 0.000 (0.286) loss 1.3352 (1.5078) acc 79.6875 (73.4940) lr 6.3188e-04 eta 0:00:33
epoch [33/50] batch [5/5] time 0.784 (0.433) data 0.000 (0.211) loss 1.3819 (1.4971) acc 76.4706 (73.3930) lr 5.7422e-04 eta 0:00:36
epoch [34/50] batch [5/5] time 0.093 (0.390) data 0.000 (0.297) loss 1.5366 (1.5069) acc 80.7692 (74.2978) lr 5.1825e-04 eta 0:00:31
epoch [35/50] batch [5/5] time 0.048 (0.359) data 0.000 (0.309) loss 1.4653 (1.5174) acc 87.5000 (73.4280) lr 4.6417e-04 eta 0:00:26
epoch [36/50] batch [5/5] time 0.096 (0.351) data 0.000 (0.293) loss 1.4180 (1.4572) acc 76.9231 (78.2367) lr 4.1221e-04 eta 0:00:24
>>> samples [64/160] noisy rate: 0.50 --> 0.47 --> 0.25 <<<
epoch [37/50] batch [5/5] time 0.053 (0.389) data 0.001 (0.316) loss 1.4316 (1.5086) acc 87.5000 (76.8082) lr 3.6258e-04 eta 0:00:25
epoch [38/50] batch [5/5] time 0.115 (0.328) data 0.000 (0.232) loss 1.3840 (1.4908) acc 80.5555 (73.2401) lr 3.1545e-04 eta 0:00:19
epoch [39/50] batch [5/5] time 0.054 (0.361) data 0.000 (0.272) loss 1.4324 (1.5354) acc 73.0769 (69.8156) lr 2.7103e-04 eta 0:00:19
epoch [40/50] batch [5/5] time 0.068 (0.446) data 0.000 (0.350) loss 1.5325 (1.5025) acc 61.1111 (72.3687) lr 2.2949e-04 eta 0:00:22
epoch [41/50] batch [5/5] time 0.092 (0.375) data 0.000 (0.277) loss 1.4203 (1.4991) acc 78.3333 (74.3611) lr 1.9098e-04 eta 0:00:16
>>> samples [64/160] noisy rate: 0.50 --> 0.49 --> 0.25 <<<
epoch [42/50] batch [5/5] time 0.096 (0.449) data 0.000 (0.357) loss 1.5080 (1.5013) acc 66.0714 (74.0909) lr 1.5567e-04 eta 0:00:17
epoch [43/50] batch [5/5] time 0.116 (0.341) data 0.000 (0.242) loss 1.7058 (1.4914) acc 58.8235 (75.1522) lr 1.2369e-04 eta 0:00:11
epoch [44/50] batch [5/5] time 0.055 (0.348) data 0.000 (0.297) loss 1.8511 (1.4980) acc 51.6667 (75.5534) lr 9.5173e-05 eta 0:00:10
epoch [45/50] batch [5/5] time 0.094 (0.338) data 0.000 (0.241) loss 1.4289 (1.4629) acc 84.6154 (77.7867) lr 7.0224e-05 eta 0:00:08
epoch [46/50] batch [5/5] time 0.081 (0.373) data 0.000 (0.288) loss 1.3697 (1.5257) acc 81.2500 (77.5772) lr 4.8943e-05 eta 0:00:07
>>> samples [65/160] noisy rate: 0.50 --> 0.49 --> 0.25 <<<
epoch [47/50] batch [5/5] time 0.103 (0.484) data 0.000 (0.392) loss 1.5643 (1.4959) acc 75.0000 (74.9643) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [5/5] time 0.052 (0.275) data 0.000 (0.219) loss 1.2471 (1.5319) acc 76.5625 (72.9524) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [5/5] time 0.068 (0.403) data 0.000 (0.303) loss 1.1915 (1.4746) acc 81.2500 (73.2756) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/5] time 0.125 (0.435) data 0.000 (0.334) loss 1.3758 (1.4976) acc 76.4706 (73.3561) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/Chosen/eurosat/rn50_16shots_8FP_pairflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* refined noise rate: [0.57, 0.51, 0.51, 0.45, 0.47, 0.49, 0.51, 0.47, 0.49, 0.49]
* learned noise rate: [0.17, 0.2, 0.21, 0.21, 0.23, 0.23, 0.24, 0.25, 0.25, 0.25]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:01,  3.01s/it]  4%|▎         | 3/81 [00:03<01:04,  1.20it/s]  6%|▌         | 5/81 [00:03<00:33,  2.28it/s]  9%|▊         | 7/81 [00:03<00:20,  3.55it/s] 11%|█         | 9/81 [00:03<00:14,  4.82it/s] 14%|█▎        | 11/81 [00:03<00:11,  6.18it/s] 16%|█▌        | 13/81 [00:03<00:08,  7.59it/s] 19%|█▊        | 15/81 [00:04<00:07,  8.96it/s] 21%|██        | 17/81 [00:04<00:06, 10.17it/s] 23%|██▎       | 19/81 [00:04<00:05, 10.37it/s] 26%|██▌       | 21/81 [00:04<00:06,  9.37it/s] 28%|██▊       | 23/81 [00:04<00:06,  9.00it/s] 31%|███       | 25/81 [00:05<00:06,  8.43it/s] 32%|███▏      | 26/81 [00:05<00:06,  8.36it/s] 33%|███▎      | 27/81 [00:05<00:06,  8.25it/s] 35%|███▍      | 28/81 [00:05<00:06,  8.19it/s] 36%|███▌      | 29/81 [00:05<00:06,  8.49it/s] 37%|███▋      | 30/81 [00:05<00:06,  8.47it/s] 38%|███▊      | 31/81 [00:05<00:05,  8.36it/s] 40%|███▉      | 32/81 [00:05<00:06,  8.00it/s] 41%|████      | 33/81 [00:06<00:05,  8.40it/s] 42%|████▏     | 34/81 [00:06<00:05,  8.03it/s] 43%|████▎     | 35/81 [00:06<00:05,  7.99it/s] 44%|████▍     | 36/81 [00:06<00:05,  7.86it/s] 46%|████▌     | 37/81 [00:06<00:06,  7.31it/s] 47%|████▋     | 38/81 [00:06<00:05,  7.41it/s] 48%|████▊     | 39/81 [00:06<00:05,  7.49it/s] 49%|████▉     | 40/81 [00:07<00:05,  7.31it/s] 51%|█████     | 41/81 [00:07<00:05,  7.49it/s] 52%|█████▏    | 42/81 [00:07<00:05,  7.47it/s] 53%|█████▎    | 43/81 [00:07<00:04,  7.64it/s] 54%|█████▍    | 44/81 [00:07<00:04,  7.73it/s] 56%|█████▌    | 45/81 [00:07<00:04,  7.61it/s] 57%|█████▋    | 46/81 [00:07<00:04,  7.69it/s] 58%|█████▊    | 47/81 [00:07<00:04,  7.74it/s] 59%|█████▉    | 48/81 [00:08<00:04,  7.72it/s] 60%|██████    | 49/81 [00:08<00:04,  7.30it/s] 62%|██████▏   | 50/81 [00:08<00:04,  7.37it/s] 63%|██████▎   | 51/81 [00:08<00:03,  7.70it/s] 64%|██████▍   | 52/81 [00:08<00:03,  8.00it/s] 65%|██████▌   | 53/81 [00:08<00:03,  7.95it/s] 67%|██████▋   | 54/81 [00:08<00:03,  7.67it/s] 68%|██████▊   | 55/81 [00:08<00:03,  8.13it/s] 69%|██████▉   | 56/81 [00:09<00:02,  8.33it/s] 70%|███████   | 57/81 [00:09<00:02,  8.40it/s] 72%|███████▏  | 58/81 [00:09<00:02,  8.04it/s] 73%|███████▎  | 59/81 [00:09<00:02,  7.96it/s] 74%|███████▍  | 60/81 [00:09<00:02,  8.37it/s] 75%|███████▌  | 61/81 [00:09<00:02,  8.22it/s] 77%|███████▋  | 62/81 [00:09<00:02,  7.97it/s] 78%|███████▊  | 63/81 [00:09<00:02,  7.74it/s] 79%|███████▉  | 64/81 [00:10<00:02,  8.11it/s] 80%|████████  | 65/81 [00:10<00:02,  7.88it/s] 81%|████████▏ | 66/81 [00:10<00:01,  7.92it/s] 83%|████████▎ | 67/81 [00:10<00:01,  7.81it/s] 85%|████████▌ | 69/81 [00:10<00:01,  8.58it/s] 86%|████████▋ | 70/81 [00:10<00:01,  8.63it/s] 88%|████████▊ | 71/81 [00:10<00:01,  8.30it/s] 89%|████████▉ | 72/81 [00:11<00:01,  8.55it/s] 90%|█████████ | 73/81 [00:11<00:00,  8.29it/s] 91%|█████████▏| 74/81 [00:11<00:00,  8.33it/s] 93%|█████████▎| 75/81 [00:11<00:00,  8.12it/s] 94%|█████████▍| 76/81 [00:11<00:00,  7.88it/s] 95%|█████████▌| 77/81 [00:11<00:00,  7.97it/s] 96%|█████████▋| 78/81 [00:11<00:00,  7.81it/s] 98%|█████████▊| 79/81 [00:11<00:00,  7.77it/s] 99%|█████████▉| 80/81 [00:12<00:00,  8.32it/s]100%|██████████| 81/81 [00:12<00:00,  8.40it/s]100%|██████████| 81/81 [00:12<00:00,  6.39it/s]
=> result
* total: 8,100
* correct: 3,967
* accuracy: 49.0%
* error: 51.0%
* macro_f1: 42.5%
=> per-class result
* class: 0 (Annual Crop Land)	total: 900	correct: 3	acc: 0.3%
* class: 1 (Forest)	total: 900	correct: 891	acc: 99.0%
* class: 2 (Herbaceous Vegetation Land)	total: 900	correct: 27	acc: 3.0%
* class: 3 (Highway or Road)	total: 750	correct: 442	acc: 58.9%
* class: 4 (Industrial Buildings)	total: 750	correct: 485	acc: 64.7%
* class: 5 (Pasture Land)	total: 600	correct: 435	acc: 72.5%
* class: 6 (Permanent Crop Land)	total: 750	correct: 499	acc: 66.5%
* class: 7 (Residential Buildings)	total: 900	correct: 779	acc: 86.6%
* class: 8 (River)	total: 750	correct: 400	acc: 53.3%
* class: 9 (Sea or Lake)	total: 900	correct: 6	acc: 0.7%
* average: 50.6%
Elapsed: 0:03:40
