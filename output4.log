nohup: ignoring input
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_0-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.269 (0.981) data 0.000 (0.274) loss 4.1233 (4.1354) acc 6.2500 (15.0000) lr 1.0000e-05 eta 0:41:35
epoch [1/50] batch [10/51] time 0.269 (0.621) data 0.000 (0.137) loss 3.6235 (3.9654) acc 25.0000 (18.4375) lr 1.0000e-05 eta 0:26:18
epoch [1/50] batch [15/51] time 0.272 (0.504) data 0.000 (0.091) loss 3.5503 (3.8357) acc 31.2500 (21.0417) lr 1.0000e-05 eta 0:21:16
epoch [1/50] batch [20/51] time 0.262 (0.444) data 0.000 (0.069) loss 2.6400 (3.5461) acc 40.6250 (25.4688) lr 1.0000e-05 eta 0:18:43
epoch [1/50] batch [25/51] time 0.276 (0.410) data 0.000 (0.055) loss 2.3013 (3.3119) acc 43.7500 (29.2500) lr 1.0000e-05 eta 0:17:15
epoch [1/50] batch [30/51] time 0.279 (0.386) data 0.000 (0.046) loss 1.7100 (3.1401) acc 68.7500 (32.8125) lr 1.0000e-05 eta 0:16:13
epoch [1/50] batch [35/51] time 0.282 (0.370) data 0.000 (0.039) loss 2.1377 (2.9957) acc 62.5000 (36.1607) lr 1.0000e-05 eta 0:15:30
epoch [1/50] batch [40/51] time 0.259 (0.356) data 0.000 (0.034) loss 2.1441 (2.9198) acc 65.6250 (38.1250) lr 1.0000e-05 eta 0:14:54
epoch [1/50] batch [45/51] time 0.259 (0.346) data 0.000 (0.031) loss 2.3594 (2.8083) acc 43.7500 (40.0694) lr 1.0000e-05 eta 0:14:25
epoch [1/50] batch [50/51] time 0.259 (0.337) data 0.000 (0.028) loss 1.9350 (2.6989) acc 62.5000 (42.5625) lr 1.0000e-05 eta 0:14:02
epoch [2/50] batch [5/51] time 0.275 (0.576) data 0.000 (0.290) loss 1.6105 (2.2435) acc 53.1250 (46.8750) lr 2.0000e-03 eta 0:23:56
epoch [2/50] batch [10/51] time 0.277 (0.425) data 0.000 (0.146) loss 1.9173 (2.0600) acc 65.6250 (54.0625) lr 2.0000e-03 eta 0:17:37
epoch [2/50] batch [15/51] time 0.275 (0.375) data 0.000 (0.097) loss 1.2793 (1.8841) acc 62.5000 (57.2917) lr 2.0000e-03 eta 0:15:31
epoch [2/50] batch [20/51] time 0.272 (0.349) data 0.000 (0.073) loss 1.4909 (1.7447) acc 56.2500 (59.6875) lr 2.0000e-03 eta 0:14:25
epoch [2/50] batch [25/51] time 0.269 (0.333) data 0.000 (0.058) loss 1.8760 (1.7312) acc 56.2500 (60.0000) lr 2.0000e-03 eta 0:13:43
epoch [2/50] batch [30/51] time 0.274 (0.322) data 0.000 (0.049) loss 1.4694 (1.6756) acc 71.8750 (60.9375) lr 2.0000e-03 eta 0:13:15
epoch [2/50] batch [35/51] time 0.266 (0.315) data 0.000 (0.042) loss 1.1381 (1.6458) acc 75.0000 (61.4286) lr 2.0000e-03 eta 0:12:55
epoch [2/50] batch [40/51] time 0.262 (0.309) data 0.000 (0.037) loss 1.4068 (1.6366) acc 62.5000 (61.4844) lr 2.0000e-03 eta 0:12:40
epoch [2/50] batch [45/51] time 0.259 (0.304) data 0.000 (0.033) loss 1.4532 (1.6129) acc 53.1250 (61.9444) lr 2.0000e-03 eta 0:12:25
epoch [2/50] batch [50/51] time 0.261 (0.299) data 0.000 (0.029) loss 1.3753 (1.5802) acc 59.3750 (62.9375) lr 2.0000e-03 eta 0:12:12
epoch [3/50] batch [5/51] time 0.279 (0.541) data 0.000 (0.265) loss 1.3007 (1.1918) acc 65.6250 (69.3750) lr 1.9980e-03 eta 0:22:02
epoch [3/50] batch [10/51] time 0.270 (0.407) data 0.000 (0.132) loss 1.4469 (1.2285) acc 50.0000 (68.1250) lr 1.9980e-03 eta 0:16:31
epoch [3/50] batch [15/51] time 0.277 (0.360) data 0.000 (0.088) loss 1.2911 (1.2201) acc 71.8750 (66.8750) lr 1.9980e-03 eta 0:14:35
epoch [3/50] batch [20/51] time 0.261 (0.335) data 0.000 (0.066) loss 0.8649 (1.2310) acc 78.1250 (67.0312) lr 1.9980e-03 eta 0:13:34
epoch [3/50] batch [25/51] time 0.261 (0.322) data 0.000 (0.053) loss 0.7615 (1.1997) acc 81.2500 (68.6250) lr 1.9980e-03 eta 0:12:59
epoch [3/50] batch [30/51] time 0.262 (0.312) data 0.000 (0.044) loss 0.8969 (1.1667) acc 75.0000 (69.2708) lr 1.9980e-03 eta 0:12:34
epoch [3/50] batch [35/51] time 0.261 (0.305) data 0.000 (0.038) loss 0.9280 (1.1414) acc 75.0000 (69.6429) lr 1.9980e-03 eta 0:12:16
epoch [3/50] batch [40/51] time 0.260 (0.300) data 0.000 (0.033) loss 1.0412 (1.1265) acc 71.8750 (70.3906) lr 1.9980e-03 eta 0:12:02
epoch [3/50] batch [45/51] time 0.262 (0.296) data 0.000 (0.030) loss 1.1909 (1.1222) acc 65.6250 (70.4861) lr 1.9980e-03 eta 0:11:50
epoch [3/50] batch [50/51] time 0.264 (0.292) data 0.000 (0.027) loss 1.1784 (1.1018) acc 65.6250 (71.1250) lr 1.9980e-03 eta 0:11:41
epoch [4/50] batch [5/51] time 0.279 (0.534) data 0.000 (0.255) loss 0.9667 (0.9294) acc 75.0000 (74.3750) lr 1.9921e-03 eta 0:21:17
epoch [4/50] batch [10/51] time 0.261 (0.398) data 0.000 (0.128) loss 0.9555 (0.9487) acc 71.8750 (74.0625) lr 1.9921e-03 eta 0:15:51
epoch [4/50] batch [15/51] time 0.259 (0.353) data 0.000 (0.085) loss 0.8375 (0.9106) acc 81.2500 (76.2500) lr 1.9921e-03 eta 0:14:01
epoch [4/50] batch [20/51] time 0.263 (0.332) data 0.000 (0.064) loss 0.7995 (0.9005) acc 78.1250 (76.7188) lr 1.9921e-03 eta 0:13:08
epoch [4/50] batch [25/51] time 0.286 (0.320) data 0.000 (0.052) loss 1.2559 (0.9001) acc 75.0000 (77.5000) lr 1.9921e-03 eta 0:12:39
epoch [4/50] batch [30/51] time 0.261 (0.311) data 0.000 (0.043) loss 0.9655 (0.8976) acc 75.0000 (77.5000) lr 1.9921e-03 eta 0:12:16
epoch [4/50] batch [35/51] time 0.260 (0.305) data 0.000 (0.037) loss 0.5773 (0.8687) acc 90.6250 (77.9464) lr 1.9921e-03 eta 0:11:59
epoch [4/50] batch [40/51] time 0.260 (0.300) data 0.000 (0.032) loss 0.7123 (0.8606) acc 84.3750 (78.1250) lr 1.9921e-03 eta 0:11:46
epoch [4/50] batch [45/51] time 0.263 (0.295) data 0.000 (0.029) loss 0.6838 (0.8583) acc 90.6250 (78.4722) lr 1.9921e-03 eta 0:11:34
epoch [4/50] batch [50/51] time 0.260 (0.292) data 0.000 (0.026) loss 0.9446 (0.8561) acc 71.8750 (78.3125) lr 1.9921e-03 eta 0:11:25
epoch [5/50] batch [5/51] time 0.265 (0.554) data 0.000 (0.279) loss 0.7929 (0.6222) acc 78.1250 (83.7500) lr 1.9823e-03 eta 0:21:37
epoch [5/50] batch [10/51] time 0.277 (0.410) data 0.000 (0.140) loss 0.8027 (0.7248) acc 78.1250 (81.2500) lr 1.9823e-03 eta 0:15:58
epoch [5/50] batch [15/51] time 0.262 (0.362) data 0.000 (0.093) loss 0.6972 (0.7581) acc 78.1250 (78.9583) lr 1.9823e-03 eta 0:14:03
epoch [5/50] batch [20/51] time 0.269 (0.338) data 0.000 (0.070) loss 0.5843 (0.7362) acc 87.5000 (80.6250) lr 1.9823e-03 eta 0:13:07
epoch [5/50] batch [25/51] time 0.262 (0.324) data 0.000 (0.056) loss 0.7414 (0.7313) acc 78.1250 (80.7500) lr 1.9823e-03 eta 0:12:32
epoch [5/50] batch [30/51] time 0.262 (0.314) data 0.000 (0.047) loss 1.0281 (0.7320) acc 78.1250 (81.5625) lr 1.9823e-03 eta 0:12:07
epoch [5/50] batch [35/51] time 0.267 (0.307) data 0.000 (0.040) loss 0.5201 (0.7347) acc 90.6250 (81.9643) lr 1.9823e-03 eta 0:11:49
epoch [5/50] batch [40/51] time 0.261 (0.302) data 0.000 (0.035) loss 0.8202 (0.7349) acc 78.1250 (82.4219) lr 1.9823e-03 eta 0:11:35
epoch [5/50] batch [45/51] time 0.261 (0.297) data 0.000 (0.031) loss 0.7473 (0.7263) acc 84.3750 (82.5000) lr 1.9823e-03 eta 0:11:23
epoch [5/50] batch [50/51] time 0.261 (0.293) data 0.000 (0.028) loss 0.6375 (0.7326) acc 78.1250 (82.1250) lr 1.9823e-03 eta 0:11:13
epoch [6/50] batch [5/51] time 0.271 (0.575) data 0.000 (0.294) loss 0.5819 (0.5566) acc 81.2500 (86.8750) lr 1.9686e-03 eta 0:21:56
epoch [6/50] batch [10/51] time 0.269 (0.420) data 0.000 (0.147) loss 0.6327 (0.5820) acc 87.5000 (87.8125) lr 1.9686e-03 eta 0:15:58
epoch [6/50] batch [15/51] time 0.262 (0.368) data 0.000 (0.098) loss 0.6135 (0.5931) acc 81.2500 (86.6667) lr 1.9686e-03 eta 0:14:00
epoch [6/50] batch [20/51] time 0.263 (0.343) data 0.000 (0.074) loss 0.8909 (0.5770) acc 71.8750 (86.8750) lr 1.9686e-03 eta 0:12:59
epoch [6/50] batch [25/51] time 0.282 (0.329) data 0.000 (0.059) loss 0.5020 (0.6042) acc 84.3750 (86.0000) lr 1.9686e-03 eta 0:12:26
epoch [6/50] batch [30/51] time 0.262 (0.318) data 0.000 (0.049) loss 0.7942 (0.6149) acc 84.3750 (85.5208) lr 1.9686e-03 eta 0:12:01
epoch [6/50] batch [35/51] time 0.262 (0.311) data 0.000 (0.042) loss 0.5157 (0.6178) acc 93.7500 (85.5357) lr 1.9686e-03 eta 0:11:43
epoch [6/50] batch [40/51] time 0.262 (0.305) data 0.000 (0.037) loss 0.7757 (0.6450) acc 75.0000 (84.2969) lr 1.9686e-03 eta 0:11:27
epoch [6/50] batch [45/51] time 0.263 (0.300) data 0.000 (0.033) loss 0.5142 (0.6421) acc 93.7500 (84.5139) lr 1.9686e-03 eta 0:11:15
epoch [6/50] batch [50/51] time 0.261 (0.296) data 0.000 (0.030) loss 0.7255 (0.6345) acc 71.8750 (84.6875) lr 1.9686e-03 eta 0:11:05
epoch [7/50] batch [5/51] time 0.275 (0.534) data 0.000 (0.250) loss 0.8481 (0.5613) acc 75.0000 (85.6250) lr 1.9511e-03 eta 0:19:55
epoch [7/50] batch [10/51] time 0.271 (0.403) data 0.000 (0.125) loss 0.6072 (0.5581) acc 78.1250 (86.2500) lr 1.9511e-03 eta 0:15:01
epoch [7/50] batch [15/51] time 0.274 (0.359) data 0.000 (0.083) loss 0.5551 (0.5611) acc 78.1250 (86.4583) lr 1.9511e-03 eta 0:13:19
epoch [7/50] batch [20/51] time 0.267 (0.336) data 0.000 (0.063) loss 0.6422 (0.5629) acc 87.5000 (86.2500) lr 1.9511e-03 eta 0:12:26
epoch [7/50] batch [25/51] time 0.263 (0.322) data 0.000 (0.050) loss 0.5752 (0.5597) acc 87.5000 (87.0000) lr 1.9511e-03 eta 0:11:54
epoch [7/50] batch [30/51] time 0.267 (0.314) data 0.000 (0.042) loss 0.7024 (0.5587) acc 68.7500 (86.2500) lr 1.9511e-03 eta 0:11:35
epoch [7/50] batch [35/51] time 0.278 (0.308) data 0.000 (0.036) loss 0.5111 (0.5584) acc 84.3750 (86.5179) lr 1.9511e-03 eta 0:11:21
epoch [7/50] batch [40/51] time 0.262 (0.302) data 0.000 (0.032) loss 0.5095 (0.5608) acc 87.5000 (86.6406) lr 1.9511e-03 eta 0:11:06
epoch [7/50] batch [45/51] time 0.262 (0.298) data 0.000 (0.028) loss 0.6563 (0.5727) acc 81.2500 (86.6667) lr 1.9511e-03 eta 0:10:55
epoch [7/50] batch [50/51] time 0.262 (0.294) data 0.000 (0.025) loss 0.5836 (0.5779) acc 84.3750 (86.1250) lr 1.9511e-03 eta 0:10:45
epoch [8/50] batch [5/51] time 0.270 (0.565) data 0.000 (0.286) loss 0.5289 (0.5835) acc 87.5000 (88.7500) lr 1.9298e-03 eta 0:20:36
epoch [8/50] batch [10/51] time 0.267 (0.416) data 0.000 (0.143) loss 0.4744 (0.5323) acc 87.5000 (88.7500) lr 1.9298e-03 eta 0:15:08
epoch [8/50] batch [15/51] time 0.271 (0.366) data 0.000 (0.095) loss 0.4645 (0.5144) acc 87.5000 (88.5417) lr 1.9298e-03 eta 0:13:16
epoch [8/50] batch [20/51] time 0.268 (0.340) data 0.000 (0.072) loss 0.4659 (0.5083) acc 84.3750 (88.1250) lr 1.9298e-03 eta 0:12:19
epoch [8/50] batch [25/51] time 0.263 (0.325) data 0.000 (0.057) loss 0.4544 (0.5185) acc 90.6250 (87.7500) lr 1.9298e-03 eta 0:11:44
epoch [8/50] batch [30/51] time 0.263 (0.316) data 0.000 (0.048) loss 0.2551 (0.5225) acc 96.8750 (88.1250) lr 1.9298e-03 eta 0:11:24
epoch [8/50] batch [35/51] time 0.271 (0.309) data 0.000 (0.041) loss 0.3841 (0.5304) acc 93.7500 (88.0357) lr 1.9298e-03 eta 0:11:07
epoch [8/50] batch [40/51] time 0.259 (0.304) data 0.000 (0.036) loss 0.5251 (0.5252) acc 87.5000 (88.0469) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [45/51] time 0.263 (0.299) data 0.000 (0.032) loss 0.5053 (0.5244) acc 84.3750 (87.9861) lr 1.9298e-03 eta 0:10:42
epoch [8/50] batch [50/51] time 0.266 (0.295) data 0.000 (0.029) loss 0.4763 (0.5204) acc 90.6250 (88.1250) lr 1.9298e-03 eta 0:10:33
epoch [9/50] batch [5/51] time 0.262 (0.627) data 0.000 (0.346) loss 0.4581 (0.4155) acc 93.7500 (93.1250) lr 1.9048e-03 eta 0:22:20
epoch [9/50] batch [10/51] time 0.263 (0.448) data 0.000 (0.173) loss 0.4481 (0.4429) acc 93.7500 (92.5000) lr 1.9048e-03 eta 0:15:55
epoch [9/50] batch [15/51] time 0.262 (0.387) data 0.000 (0.116) loss 0.5820 (0.4842) acc 90.6250 (91.6667) lr 1.9048e-03 eta 0:13:42
epoch [9/50] batch [20/51] time 0.274 (0.358) data 0.000 (0.087) loss 0.3934 (0.4636) acc 93.7500 (91.5625) lr 1.9048e-03 eta 0:12:38
epoch [9/50] batch [25/51] time 0.269 (0.340) data 0.000 (0.069) loss 0.3743 (0.4755) acc 93.7500 (91.0000) lr 1.9048e-03 eta 0:11:59
epoch [9/50] batch [30/51] time 0.263 (0.328) data 0.000 (0.058) loss 0.5954 (0.4814) acc 81.2500 (90.1042) lr 1.9048e-03 eta 0:11:32
epoch [9/50] batch [35/51] time 0.262 (0.319) data 0.000 (0.050) loss 0.5365 (0.4865) acc 90.6250 (90.2679) lr 1.9048e-03 eta 0:11:12
epoch [9/50] batch [40/51] time 0.260 (0.312) data 0.000 (0.043) loss 0.5855 (0.4862) acc 90.6250 (90.4688) lr 1.9048e-03 eta 0:10:56
epoch [9/50] batch [45/51] time 0.259 (0.306) data 0.000 (0.039) loss 0.5095 (0.4849) acc 87.5000 (90.2083) lr 1.9048e-03 eta 0:10:42
epoch [9/50] batch [50/51] time 0.260 (0.302) data 0.000 (0.035) loss 0.4755 (0.4945) acc 90.6250 (89.7500) lr 1.9048e-03 eta 0:10:31
epoch [10/50] batch [5/51] time 0.273 (0.546) data 0.000 (0.262) loss 0.3893 (0.3617) acc 93.7500 (93.7500) lr 1.8763e-03 eta 0:18:59
epoch [10/50] batch [10/51] time 0.268 (0.406) data 0.000 (0.131) loss 0.5308 (0.4030) acc 84.3750 (92.5000) lr 1.8763e-03 eta 0:14:04
epoch [10/50] batch [15/51] time 0.271 (0.359) data 0.000 (0.087) loss 0.3157 (0.4220) acc 96.8750 (92.2917) lr 1.8763e-03 eta 0:12:25
epoch [10/50] batch [20/51] time 0.271 (0.336) data 0.000 (0.066) loss 0.4906 (0.4336) acc 87.5000 (91.7188) lr 1.8763e-03 eta 0:11:36
epoch [10/50] batch [25/51] time 0.263 (0.322) data 0.000 (0.053) loss 0.5322 (0.4347) acc 90.6250 (91.7500) lr 1.8763e-03 eta 0:11:05
epoch [10/50] batch [30/51] time 0.263 (0.313) data 0.000 (0.044) loss 0.3074 (0.4378) acc 93.7500 (91.2500) lr 1.8763e-03 eta 0:10:44
epoch [10/50] batch [35/51] time 0.276 (0.307) data 0.000 (0.038) loss 0.4500 (0.4359) acc 81.2500 (90.6250) lr 1.8763e-03 eta 0:10:30
epoch [10/50] batch [40/51] time 0.260 (0.301) data 0.000 (0.033) loss 0.5380 (0.4466) acc 93.7500 (90.6250) lr 1.8763e-03 eta 0:10:17
epoch [10/50] batch [45/51] time 0.262 (0.296) data 0.000 (0.029) loss 0.4651 (0.4501) acc 100.0000 (90.9028) lr 1.8763e-03 eta 0:10:06
epoch [10/50] batch [50/51] time 0.262 (0.293) data 0.000 (0.026) loss 0.5144 (0.4548) acc 87.5000 (90.7500) lr 1.8763e-03 eta 0:09:57
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.238  alpha2: -0.098 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [11/50] batch [5/51] time 0.173 (1.028) data 0.000 (0.317) loss 0.6599 (0.7581) acc 81.6327 (83.0470) lr 1.8443e-03 eta 0:34:51
epoch [11/50] batch [10/51] time 0.158 (0.667) data 0.000 (0.159) loss 0.3992 (0.6844) acc 91.6667 (84.7110) lr 1.8443e-03 eta 0:22:33
epoch [11/50] batch [15/51] time 0.157 (0.535) data 0.000 (0.106) loss 1.0342 (0.6834) acc 73.3333 (83.6285) lr 1.8443e-03 eta 0:18:03
epoch [11/50] batch [20/51] time 0.164 (0.471) data 0.000 (0.080) loss 0.6667 (0.7060) acc 81.7708 (83.1409) lr 1.8443e-03 eta 0:15:50
epoch [11/50] batch [25/51] time 0.169 (0.410) data 0.000 (0.064) loss 0.4833 (0.7194) acc 88.0000 (83.0449) lr 1.8443e-03 eta 0:13:45
epoch [11/50] batch [30/51] time 0.197 (0.393) data 0.000 (0.053) loss 0.6704 (0.7128) acc 83.8235 (83.5148) lr 1.8443e-03 eta 0:13:09
epoch [11/50] batch [35/51] time 0.165 (0.376) data 0.000 (0.046) loss 0.8793 (0.7073) acc 81.2500 (83.6023) lr 1.8443e-03 eta 0:12:34
epoch [11/50] batch [40/51] time 0.815 (0.380) data 0.000 (0.040) loss 0.5802 (0.6888) acc 90.4546 (84.0369) lr 1.8443e-03 eta 0:12:40
epoch [11/50] batch [45/51] time 0.177 (0.357) data 0.000 (0.036) loss 0.6767 (0.6981) acc 80.3922 (83.7202) lr 1.8443e-03 eta 0:11:51
epoch [11/50] batch [50/51] time 0.162 (0.348) data 0.000 (0.032) loss 0.8327 (0.6962) acc 73.4043 (83.5877) lr 1.8443e-03 eta 0:11:32
>>> alpha1: 0.247  alpha2: -0.096 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.11 <<<
epoch [12/50] batch [5/51] time 0.176 (0.604) data 0.000 (0.307) loss 0.6272 (0.6664) acc 83.0000 (83.6008) lr 1.8090e-03 eta 0:19:59
epoch [12/50] batch [10/51] time 0.186 (0.388) data 0.000 (0.154) loss 0.6867 (0.6171) acc 81.1225 (84.6859) lr 1.8090e-03 eta 0:12:48
epoch [12/50] batch [15/51] time 0.791 (0.356) data 0.000 (0.103) loss 0.4779 (0.6255) acc 88.8889 (85.0619) lr 1.8090e-03 eta 0:11:42
epoch [12/50] batch [20/51] time 0.170 (0.309) data 0.000 (0.077) loss 0.7648 (0.6363) acc 79.5918 (84.7801) lr 1.8090e-03 eta 0:10:08
epoch [12/50] batch [25/51] time 0.168 (0.282) data 0.000 (0.062) loss 0.5498 (0.6431) acc 85.7143 (84.6586) lr 1.8090e-03 eta 0:09:12
epoch [12/50] batch [30/51] time 0.177 (0.263) data 0.000 (0.052) loss 0.6983 (0.6512) acc 89.0000 (84.3891) lr 1.8090e-03 eta 0:08:35
epoch [12/50] batch [35/51] time 0.177 (0.249) data 0.001 (0.045) loss 0.7312 (0.6617) acc 83.0189 (83.9885) lr 1.8090e-03 eta 0:08:07
epoch [12/50] batch [40/51] time 0.184 (0.256) data 0.000 (0.039) loss 0.4156 (0.6637) acc 90.2778 (83.9994) lr 1.8090e-03 eta 0:08:19
epoch [12/50] batch [45/51] time 0.165 (0.247) data 0.000 (0.035) loss 0.6847 (0.6517) acc 81.7708 (84.3469) lr 1.8090e-03 eta 0:08:00
epoch [12/50] batch [50/51] time 0.169 (0.239) data 0.000 (0.031) loss 0.5891 (0.6500) acc 83.8235 (84.3855) lr 1.8090e-03 eta 0:07:43
>>> alpha1: 0.248  alpha2: -0.097 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.14 <<<
epoch [13/50] batch [5/51] time 0.185 (0.427) data 0.001 (0.246) loss 0.5543 (0.6080) acc 87.7451 (86.2669) lr 1.7705e-03 eta 0:13:44
epoch [13/50] batch [10/51] time 0.174 (0.301) data 0.000 (0.123) loss 0.6431 (0.6214) acc 85.9375 (85.1647) lr 1.7705e-03 eta 0:09:41
epoch [13/50] batch [15/51] time 0.168 (0.257) data 0.000 (0.082) loss 0.6851 (0.6045) acc 82.1429 (85.9174) lr 1.7705e-03 eta 0:08:15
epoch [13/50] batch [20/51] time 0.166 (0.269) data 0.000 (0.062) loss 0.6664 (0.5977) acc 84.8958 (86.5722) lr 1.7705e-03 eta 0:08:35
epoch [13/50] batch [25/51] time 0.165 (0.250) data 0.000 (0.050) loss 0.9167 (0.5977) acc 75.5319 (86.2220) lr 1.7705e-03 eta 0:07:57
epoch [13/50] batch [30/51] time 0.185 (0.238) data 0.000 (0.041) loss 0.4449 (0.5885) acc 91.1765 (86.3493) lr 1.7705e-03 eta 0:07:34
epoch [13/50] batch [35/51] time 0.183 (0.229) data 0.000 (0.035) loss 0.7240 (0.6019) acc 81.9149 (85.6877) lr 1.7705e-03 eta 0:07:16
epoch [13/50] batch [40/51] time 0.168 (0.222) data 0.000 (0.031) loss 0.6512 (0.6111) acc 84.1837 (85.5627) lr 1.7705e-03 eta 0:07:02
epoch [13/50] batch [45/51] time 0.162 (0.216) data 0.000 (0.028) loss 0.8669 (0.6158) acc 79.5455 (85.4735) lr 1.7705e-03 eta 0:06:48
epoch [13/50] batch [50/51] time 0.163 (0.211) data 0.000 (0.025) loss 0.7427 (0.6154) acc 82.4468 (85.4698) lr 1.7705e-03 eta 0:06:38
>>> alpha1: 0.242  alpha2: -0.098 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.13 <<<
epoch [14/50] batch [5/51] time 0.169 (0.470) data 0.000 (0.296) loss 0.4576 (0.4779) acc 90.0000 (90.1867) lr 1.7290e-03 eta 0:14:44
epoch [14/50] batch [10/51] time 0.185 (0.324) data 0.001 (0.148) loss 0.3211 (0.5510) acc 95.0980 (87.3326) lr 1.7290e-03 eta 0:10:07
epoch [14/50] batch [15/51] time 0.181 (0.275) data 0.000 (0.099) loss 0.7574 (0.5430) acc 80.1020 (87.2883) lr 1.7290e-03 eta 0:08:34
epoch [14/50] batch [20/51] time 0.169 (0.251) data 0.000 (0.074) loss 0.6378 (0.5572) acc 83.5000 (86.5191) lr 1.7290e-03 eta 0:07:48
epoch [14/50] batch [25/51] time 0.163 (0.236) data 0.000 (0.059) loss 0.6470 (0.5639) acc 84.5745 (86.2471) lr 1.7290e-03 eta 0:07:19
epoch [14/50] batch [30/51] time 0.188 (0.227) data 0.001 (0.050) loss 0.6730 (0.5701) acc 81.7308 (86.0802) lr 1.7290e-03 eta 0:07:01
epoch [14/50] batch [35/51] time 0.176 (0.219) data 0.000 (0.043) loss 0.5281 (0.5697) acc 89.0000 (86.1278) lr 1.7290e-03 eta 0:06:46
epoch [14/50] batch [40/51] time 0.161 (0.212) data 0.000 (0.037) loss 0.5116 (0.5740) acc 89.3617 (86.0903) lr 1.7290e-03 eta 0:06:31
epoch [14/50] batch [45/51] time 0.164 (0.207) data 0.000 (0.033) loss 0.6001 (0.5654) acc 88.5417 (86.3404) lr 1.7290e-03 eta 0:06:21
epoch [14/50] batch [50/51] time 0.167 (0.203) data 0.000 (0.030) loss 0.5273 (0.5681) acc 87.5000 (86.2769) lr 1.7290e-03 eta 0:06:13
>>> alpha1: 0.241  alpha2: -0.081 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.14 <<<
epoch [15/50] batch [5/51] time 0.164 (0.450) data 0.000 (0.268) loss 0.6865 (0.4961) acc 82.9787 (88.6232) lr 1.6845e-03 eta 0:13:44
epoch [15/50] batch [10/51] time 0.169 (0.311) data 0.000 (0.134) loss 0.4253 (0.5066) acc 91.5000 (87.8146) lr 1.6845e-03 eta 0:09:27
epoch [15/50] batch [15/51] time 0.202 (0.267) data 0.000 (0.090) loss 0.2961 (0.4939) acc 94.5000 (88.2723) lr 1.6845e-03 eta 0:08:06
epoch [15/50] batch [20/51] time 0.179 (0.245) data 0.000 (0.067) loss 0.4955 (0.5230) acc 88.7755 (87.5733) lr 1.6845e-03 eta 0:07:25
epoch [15/50] batch [25/51] time 0.165 (0.233) data 0.000 (0.054) loss 0.3811 (0.5248) acc 86.9792 (87.4447) lr 1.6845e-03 eta 0:07:01
epoch [15/50] batch [30/51] time 0.168 (0.223) data 0.001 (0.045) loss 0.6282 (0.5329) acc 86.2245 (87.4587) lr 1.6845e-03 eta 0:06:42
epoch [15/50] batch [35/51] time 0.172 (0.231) data 0.000 (0.039) loss 0.3450 (0.5345) acc 95.0980 (87.5988) lr 1.6845e-03 eta 0:06:55
epoch [15/50] batch [40/51] time 0.166 (0.223) data 0.000 (0.034) loss 0.4561 (0.5304) acc 88.7755 (87.5805) lr 1.6845e-03 eta 0:06:40
epoch [15/50] batch [45/51] time 0.156 (0.216) data 0.000 (0.030) loss 0.6263 (0.5531) acc 86.1111 (86.9706) lr 1.6845e-03 eta 0:06:27
epoch [15/50] batch [50/51] time 0.172 (0.212) data 0.000 (0.027) loss 0.5469 (0.5516) acc 86.5385 (86.9368) lr 1.6845e-03 eta 0:06:18
>>> alpha1: 0.221  alpha2: -0.131 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.15 <<<
epoch [16/50] batch [5/51] time 0.164 (0.470) data 0.000 (0.293) loss 0.5816 (0.5757) acc 84.8958 (87.2222) lr 1.6374e-03 eta 0:13:56
epoch [16/50] batch [10/51] time 0.168 (0.321) data 0.000 (0.146) loss 0.4948 (0.5735) acc 90.5000 (88.0129) lr 1.6374e-03 eta 0:09:28
epoch [16/50] batch [15/51] time 0.170 (0.272) data 0.000 (0.098) loss 0.4993 (0.5497) acc 91.5000 (88.0803) lr 1.6374e-03 eta 0:08:01
epoch [16/50] batch [20/51] time 0.167 (0.247) data 0.000 (0.073) loss 0.5424 (0.5328) acc 88.7755 (88.5465) lr 1.6374e-03 eta 0:07:16
epoch [16/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.059) loss 0.6825 (0.5303) acc 86.1702 (88.3580) lr 1.6374e-03 eta 0:06:50
epoch [16/50] batch [30/51] time 0.179 (0.224) data 0.000 (0.049) loss 0.5817 (0.5364) acc 81.5000 (88.0271) lr 1.6374e-03 eta 0:06:32
epoch [16/50] batch [35/51] time 0.183 (0.216) data 0.000 (0.042) loss 0.2843 (0.5398) acc 93.3673 (87.5384) lr 1.6374e-03 eta 0:06:18
epoch [16/50] batch [40/51] time 0.167 (0.211) data 0.000 (0.037) loss 0.7424 (0.5401) acc 84.0000 (87.5624) lr 1.6374e-03 eta 0:06:07
epoch [16/50] batch [45/51] time 0.176 (0.206) data 0.000 (0.033) loss 0.4580 (0.5412) acc 90.5660 (87.5408) lr 1.6374e-03 eta 0:05:58
epoch [16/50] batch [50/51] time 0.174 (0.203) data 0.001 (0.029) loss 0.5377 (0.5450) acc 87.5000 (87.4270) lr 1.6374e-03 eta 0:05:51
>>> alpha1: 0.214  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [17/50] batch [5/51] time 0.168 (0.437) data 0.000 (0.259) loss 0.6175 (0.5967) acc 88.7755 (85.7848) lr 1.5878e-03 eta 0:12:35
epoch [17/50] batch [10/51] time 0.210 (0.309) data 0.001 (0.130) loss 0.4291 (0.5608) acc 89.9038 (86.4718) lr 1.5878e-03 eta 0:08:53
epoch [17/50] batch [15/51] time 0.180 (0.264) data 0.000 (0.087) loss 0.7403 (0.5644) acc 79.5918 (86.4236) lr 1.5878e-03 eta 0:07:33
epoch [17/50] batch [20/51] time 0.167 (0.243) data 0.000 (0.065) loss 0.9015 (0.5747) acc 81.7708 (86.6842) lr 1.5878e-03 eta 0:06:56
epoch [17/50] batch [25/51] time 0.168 (0.229) data 0.000 (0.052) loss 0.5072 (0.5639) acc 87.7778 (86.9623) lr 1.5878e-03 eta 0:06:31
epoch [17/50] batch [30/51] time 0.177 (0.220) data 0.000 (0.043) loss 0.6561 (0.5567) acc 80.5000 (87.0504) lr 1.5878e-03 eta 0:06:15
epoch [17/50] batch [35/51] time 0.181 (0.214) data 0.000 (0.038) loss 0.3806 (0.5415) acc 92.3077 (87.3000) lr 1.5878e-03 eta 0:06:03
epoch [17/50] batch [40/51] time 0.176 (0.209) data 0.000 (0.033) loss 0.4345 (0.5372) acc 90.5660 (87.4197) lr 1.5878e-03 eta 0:05:53
epoch [17/50] batch [45/51] time 0.180 (0.204) data 0.000 (0.029) loss 0.5559 (0.5430) acc 90.0000 (87.3358) lr 1.5878e-03 eta 0:05:44
epoch [17/50] batch [50/51] time 0.170 (0.201) data 0.000 (0.026) loss 0.4849 (0.5558) acc 88.7255 (87.0769) lr 1.5878e-03 eta 0:05:37
>>> alpha1: 0.207  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [18/50] batch [5/51] time 0.179 (0.458) data 0.000 (0.271) loss 0.5786 (0.5128) acc 84.2593 (88.0792) lr 1.5358e-03 eta 0:12:47
epoch [18/50] batch [10/51] time 0.183 (0.315) data 0.000 (0.136) loss 0.4575 (0.5082) acc 88.0000 (88.2148) lr 1.5358e-03 eta 0:08:46
epoch [18/50] batch [15/51] time 0.179 (0.268) data 0.000 (0.091) loss 0.3208 (0.5126) acc 92.9245 (87.4113) lr 1.5358e-03 eta 0:07:27
epoch [18/50] batch [20/51] time 0.171 (0.245) data 0.000 (0.068) loss 0.6458 (0.5189) acc 86.7647 (87.7601) lr 1.5358e-03 eta 0:06:48
epoch [18/50] batch [25/51] time 0.174 (0.232) data 0.000 (0.055) loss 0.5041 (0.5157) acc 90.3846 (87.9498) lr 1.5358e-03 eta 0:06:24
epoch [18/50] batch [30/51] time 0.200 (0.222) data 0.000 (0.045) loss 0.4329 (0.5429) acc 90.7407 (87.3939) lr 1.5358e-03 eta 0:06:07
epoch [18/50] batch [35/51] time 0.177 (0.215) data 0.000 (0.039) loss 0.4882 (0.5405) acc 90.0943 (87.3748) lr 1.5358e-03 eta 0:05:54
epoch [18/50] batch [40/51] time 0.180 (0.210) data 0.000 (0.034) loss 0.4678 (0.5411) acc 88.1818 (87.3916) lr 1.5358e-03 eta 0:05:44
epoch [18/50] batch [45/51] time 0.172 (0.205) data 0.000 (0.030) loss 0.5291 (0.5344) acc 88.4615 (87.6690) lr 1.5358e-03 eta 0:05:36
epoch [18/50] batch [50/51] time 0.161 (0.202) data 0.000 (0.027) loss 0.5719 (0.5419) acc 86.1702 (87.5715) lr 1.5358e-03 eta 0:05:29
>>> alpha1: 0.201  alpha2: -0.146 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [19/50] batch [5/51] time 0.182 (0.450) data 0.000 (0.269) loss 0.6924 (0.5819) acc 84.2593 (85.7757) lr 1.4818e-03 eta 0:12:12
epoch [19/50] batch [10/51] time 0.177 (0.317) data 0.000 (0.135) loss 0.5548 (0.5614) acc 89.1509 (87.0174) lr 1.4818e-03 eta 0:08:33
epoch [19/50] batch [15/51] time 0.191 (0.271) data 0.000 (0.090) loss 0.6164 (0.5662) acc 84.8039 (86.9984) lr 1.4818e-03 eta 0:07:17
epoch [19/50] batch [20/51] time 0.184 (0.250) data 0.000 (0.067) loss 0.6043 (0.5465) acc 86.3208 (87.2715) lr 1.4818e-03 eta 0:06:42
epoch [19/50] batch [25/51] time 0.203 (0.237) data 0.000 (0.054) loss 0.6327 (0.5389) acc 83.9623 (87.4565) lr 1.4818e-03 eta 0:06:21
epoch [19/50] batch [30/51] time 0.169 (0.227) data 0.000 (0.045) loss 0.4242 (0.5334) acc 90.1042 (87.5145) lr 1.4818e-03 eta 0:06:04
epoch [19/50] batch [35/51] time 0.181 (0.220) data 0.000 (0.039) loss 0.3945 (0.5316) acc 91.6667 (87.7429) lr 1.4818e-03 eta 0:05:51
epoch [19/50] batch [40/51] time 0.153 (0.213) data 0.000 (0.034) loss 0.6156 (0.5311) acc 83.1395 (87.7311) lr 1.4818e-03 eta 0:05:39
epoch [19/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.030) loss 0.4973 (0.5312) acc 90.3061 (87.8185) lr 1.4818e-03 eta 0:05:30
epoch [19/50] batch [50/51] time 0.170 (0.204) data 0.000 (0.027) loss 0.6046 (0.5347) acc 85.2941 (87.7193) lr 1.4818e-03 eta 0:05:22
>>> alpha1: 0.198  alpha2: -0.149 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [20/50] batch [5/51] time 0.163 (0.510) data 0.000 (0.332) loss 0.4654 (0.4935) acc 89.2045 (87.9943) lr 1.4258e-03 eta 0:13:23
epoch [20/50] batch [10/51] time 0.190 (0.349) data 0.000 (0.166) loss 0.4693 (0.4854) acc 89.0000 (88.4599) lr 1.4258e-03 eta 0:09:07
epoch [20/50] batch [15/51] time 0.172 (0.291) data 0.001 (0.111) loss 0.3077 (0.4914) acc 92.1569 (88.4812) lr 1.4258e-03 eta 0:07:36
epoch [20/50] batch [20/51] time 0.173 (0.262) data 0.000 (0.083) loss 0.4716 (0.5056) acc 86.5385 (88.2824) lr 1.4258e-03 eta 0:06:49
epoch [20/50] batch [25/51] time 0.177 (0.273) data 0.000 (0.067) loss 0.7952 (0.5078) acc 80.0000 (88.0270) lr 1.4258e-03 eta 0:07:04
epoch [20/50] batch [30/51] time 0.168 (0.257) data 0.000 (0.056) loss 0.5563 (0.5200) acc 86.1702 (87.8387) lr 1.4258e-03 eta 0:06:38
epoch [20/50] batch [35/51] time 0.171 (0.245) data 0.000 (0.048) loss 0.3803 (0.5108) acc 90.1961 (88.1077) lr 1.4258e-03 eta 0:06:19
epoch [20/50] batch [40/51] time 0.170 (0.236) data 0.000 (0.042) loss 0.5352 (0.5020) acc 86.7647 (88.2800) lr 1.4258e-03 eta 0:06:04
epoch [20/50] batch [45/51] time 0.178 (0.230) data 0.000 (0.037) loss 0.4781 (0.5008) acc 90.2778 (88.1851) lr 1.4258e-03 eta 0:05:52
epoch [20/50] batch [50/51] time 0.180 (0.224) data 0.000 (0.033) loss 0.2738 (0.4957) acc 93.6364 (88.4468) lr 1.4258e-03 eta 0:05:42
>>> alpha1: 0.188  alpha2: -0.152 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [21/50] batch [5/51] time 0.165 (0.451) data 0.000 (0.275) loss 0.5034 (0.5141) acc 88.5417 (88.4643) lr 1.3681e-03 eta 0:11:28
epoch [21/50] batch [10/51] time 0.173 (0.314) data 0.000 (0.138) loss 0.4634 (0.5061) acc 89.5000 (89.2485) lr 1.3681e-03 eta 0:07:57
epoch [21/50] batch [15/51] time 0.177 (0.313) data 0.000 (0.092) loss 0.4918 (0.5004) acc 90.0943 (89.1339) lr 1.3681e-03 eta 0:07:53
epoch [21/50] batch [20/51] time 0.191 (0.280) data 0.000 (0.069) loss 0.4472 (0.5060) acc 91.5094 (88.9003) lr 1.3681e-03 eta 0:07:02
epoch [21/50] batch [25/51] time 0.165 (0.259) data 0.000 (0.055) loss 0.7942 (0.5132) acc 79.1667 (88.6196) lr 1.3681e-03 eta 0:06:29
epoch [21/50] batch [30/51] time 0.171 (0.246) data 0.000 (0.046) loss 0.5600 (0.5036) acc 90.6250 (89.0011) lr 1.3681e-03 eta 0:06:09
epoch [21/50] batch [35/51] time 0.159 (0.236) data 0.000 (0.040) loss 0.5798 (0.5018) acc 87.2222 (88.8427) lr 1.3681e-03 eta 0:05:52
epoch [21/50] batch [40/51] time 0.172 (0.228) data 0.000 (0.035) loss 0.3966 (0.5024) acc 89.9038 (88.8672) lr 1.3681e-03 eta 0:05:40
epoch [21/50] batch [45/51] time 0.159 (0.222) data 0.000 (0.031) loss 0.4973 (0.5024) acc 89.1304 (88.8842) lr 1.3681e-03 eta 0:05:29
epoch [21/50] batch [50/51] time 0.167 (0.217) data 0.000 (0.028) loss 0.4045 (0.5008) acc 87.5000 (88.7973) lr 1.3681e-03 eta 0:05:20
>>> alpha1: 0.186  alpha2: -0.153 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [22/50] batch [5/51] time 0.181 (0.495) data 0.000 (0.308) loss 0.2058 (0.4528) acc 94.1176 (90.2513) lr 1.3090e-03 eta 0:12:09
epoch [22/50] batch [10/51] time 0.182 (0.335) data 0.000 (0.154) loss 0.5271 (0.4795) acc 87.5000 (89.1571) lr 1.3090e-03 eta 0:08:12
epoch [22/50] batch [15/51] time 0.166 (0.281) data 0.000 (0.103) loss 0.3805 (0.4732) acc 90.3061 (89.5244) lr 1.3090e-03 eta 0:06:51
epoch [22/50] batch [20/51] time 0.170 (0.253) data 0.000 (0.077) loss 0.5277 (0.4738) acc 84.3137 (89.2541) lr 1.3090e-03 eta 0:06:09
epoch [22/50] batch [25/51] time 0.181 (0.239) data 0.000 (0.062) loss 0.7105 (0.4828) acc 80.1020 (88.7381) lr 1.3090e-03 eta 0:05:47
epoch [22/50] batch [30/51] time 0.169 (0.229) data 0.000 (0.052) loss 0.4868 (0.4713) acc 90.0000 (89.0485) lr 1.3090e-03 eta 0:05:31
epoch [22/50] batch [35/51] time 0.187 (0.222) data 0.001 (0.044) loss 0.3141 (0.4746) acc 92.6471 (88.9226) lr 1.3090e-03 eta 0:05:20
epoch [22/50] batch [40/51] time 0.165 (0.216) data 0.000 (0.039) loss 0.6171 (0.4810) acc 87.2449 (88.8599) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [45/51] time 0.157 (0.211) data 0.000 (0.034) loss 0.6030 (0.4826) acc 90.0000 (88.9717) lr 1.3090e-03 eta 0:05:03
epoch [22/50] batch [50/51] time 0.172 (0.207) data 0.000 (0.031) loss 0.5435 (0.4729) acc 87.9808 (89.1329) lr 1.3090e-03 eta 0:04:56
>>> alpha1: 0.182  alpha2: -0.156 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [23/50] batch [5/51] time 0.175 (0.457) data 0.001 (0.278) loss 0.3928 (0.8391) acc 89.5000 (83.8749) lr 1.2487e-03 eta 0:10:50
epoch [23/50] batch [10/51] time 0.183 (0.319) data 0.000 (0.139) loss 0.3340 (0.6169) acc 94.8113 (87.9690) lr 1.2487e-03 eta 0:07:31
epoch [23/50] batch [15/51] time 0.199 (0.273) data 0.000 (0.093) loss 0.3770 (0.5612) acc 91.3462 (88.3325) lr 1.2487e-03 eta 0:06:25
epoch [23/50] batch [20/51] time 0.172 (0.250) data 0.000 (0.070) loss 0.5105 (0.5398) acc 90.1961 (88.6622) lr 1.2487e-03 eta 0:05:52
epoch [23/50] batch [25/51] time 0.174 (0.236) data 0.000 (0.056) loss 0.6497 (0.5247) acc 85.0962 (88.6842) lr 1.2487e-03 eta 0:05:31
epoch [23/50] batch [30/51] time 0.170 (0.226) data 0.000 (0.047) loss 0.3303 (0.5108) acc 92.6471 (88.9572) lr 1.2487e-03 eta 0:05:15
epoch [23/50] batch [35/51] time 0.193 (0.219) data 0.000 (0.040) loss 0.3408 (0.5000) acc 91.6667 (89.1851) lr 1.2487e-03 eta 0:05:05
epoch [23/50] batch [40/51] time 0.164 (0.213) data 0.000 (0.035) loss 0.4230 (0.4850) acc 90.6250 (89.4353) lr 1.2487e-03 eta 0:04:55
epoch [23/50] batch [45/51] time 0.161 (0.208) data 0.000 (0.031) loss 0.3755 (0.4827) acc 92.0213 (89.3902) lr 1.2487e-03 eta 0:04:47
epoch [23/50] batch [50/51] time 0.158 (0.204) data 0.000 (0.028) loss 0.7293 (0.4955) acc 84.2391 (89.1381) lr 1.2487e-03 eta 0:04:40
>>> alpha1: 0.180  alpha2: -0.165 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [24/50] batch [5/51] time 0.169 (0.474) data 0.000 (0.290) loss 0.4429 (0.5733) acc 90.0000 (89.5812) lr 1.1874e-03 eta 0:10:50
epoch [24/50] batch [10/51] time 0.192 (0.331) data 0.000 (0.145) loss 0.4258 (0.5401) acc 86.7924 (88.8871) lr 1.1874e-03 eta 0:07:32
epoch [24/50] batch [15/51] time 0.189 (0.281) data 0.000 (0.097) loss 0.5012 (0.4929) acc 92.6471 (90.0457) lr 1.1874e-03 eta 0:06:22
epoch [24/50] batch [20/51] time 0.173 (0.256) data 0.000 (0.073) loss 0.3785 (0.4748) acc 95.0980 (90.0807) lr 1.1874e-03 eta 0:05:46
epoch [24/50] batch [25/51] time 0.197 (0.241) data 0.000 (0.058) loss 0.3554 (0.4576) acc 91.0714 (90.4649) lr 1.1874e-03 eta 0:05:25
epoch [24/50] batch [30/51] time 0.198 (0.230) data 0.000 (0.049) loss 0.3833 (0.4591) acc 90.2778 (90.2993) lr 1.1874e-03 eta 0:05:10
epoch [24/50] batch [35/51] time 0.164 (0.223) data 0.000 (0.042) loss 0.6141 (0.4600) acc 84.6591 (90.1571) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [40/51] time 0.176 (0.217) data 0.000 (0.036) loss 0.3830 (0.4770) acc 92.9245 (90.0794) lr 1.1874e-03 eta 0:04:50
epoch [24/50] batch [45/51] time 0.171 (0.212) data 0.000 (0.032) loss 0.4189 (0.4728) acc 84.8039 (90.0052) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [50/51] time 0.163 (0.208) data 0.000 (0.029) loss 0.4527 (0.4690) acc 89.3617 (89.9745) lr 1.1874e-03 eta 0:04:35
>>> alpha1: 0.176  alpha2: -0.174 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [25/50] batch [5/51] time 0.172 (0.411) data 0.000 (0.232) loss 0.5785 (0.4737) acc 90.3061 (89.8695) lr 1.1253e-03 eta 0:09:03
epoch [25/50] batch [10/51] time 0.190 (0.296) data 0.000 (0.116) loss 0.4450 (0.4971) acc 91.6667 (88.5453) lr 1.1253e-03 eta 0:06:29
epoch [25/50] batch [15/51] time 0.174 (0.256) data 0.000 (0.078) loss 0.6481 (0.5096) acc 86.7347 (89.0170) lr 1.1253e-03 eta 0:05:35
epoch [25/50] batch [20/51] time 0.183 (0.235) data 0.000 (0.058) loss 0.3999 (0.4806) acc 92.4528 (89.7329) lr 1.1253e-03 eta 0:05:06
epoch [25/50] batch [25/51] time 0.187 (0.223) data 0.000 (0.047) loss 0.4814 (0.4703) acc 88.6364 (89.7583) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [30/51] time 0.182 (0.216) data 0.000 (0.039) loss 0.4574 (0.4655) acc 87.0000 (89.6078) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [35/51] time 0.180 (0.210) data 0.000 (0.033) loss 0.4861 (0.4584) acc 88.4259 (89.6876) lr 1.1253e-03 eta 0:04:31
epoch [25/50] batch [40/51] time 0.178 (0.205) data 0.000 (0.029) loss 0.2928 (0.4559) acc 93.9815 (89.7762) lr 1.1253e-03 eta 0:04:24
epoch [25/50] batch [45/51] time 0.174 (0.202) data 0.000 (0.026) loss 0.4695 (0.4521) acc 91.3462 (89.8173) lr 1.1253e-03 eta 0:04:18
epoch [25/50] batch [50/51] time 0.165 (0.199) data 0.000 (0.023) loss 0.4677 (0.4572) acc 91.1458 (89.7422) lr 1.1253e-03 eta 0:04:13
>>> alpha1: 0.176  alpha2: -0.182 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [26/50] batch [5/51] time 0.171 (0.460) data 0.000 (0.284) loss 0.4062 (0.4191) acc 92.1569 (89.9730) lr 1.0628e-03 eta 0:09:44
epoch [26/50] batch [10/51] time 0.183 (0.315) data 0.000 (0.142) loss 0.4236 (0.4547) acc 91.1765 (89.8339) lr 1.0628e-03 eta 0:06:38
epoch [26/50] batch [15/51] time 0.162 (0.269) data 0.000 (0.095) loss 0.5028 (0.4366) acc 89.3617 (90.3939) lr 1.0628e-03 eta 0:05:38
epoch [26/50] batch [20/51] time 0.174 (0.246) data 0.000 (0.071) loss 0.3827 (0.4365) acc 92.7083 (90.6480) lr 1.0628e-03 eta 0:05:09
epoch [26/50] batch [25/51] time 0.184 (0.234) data 0.000 (0.057) loss 0.4633 (0.5258) acc 87.9630 (89.2597) lr 1.0628e-03 eta 0:04:52
epoch [26/50] batch [30/51] time 0.162 (0.224) data 0.000 (0.048) loss 0.4569 (0.5134) acc 90.4255 (89.1767) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [35/51] time 0.166 (0.218) data 0.001 (0.041) loss 0.2715 (0.4911) acc 97.0930 (89.9016) lr 1.0628e-03 eta 0:04:30
epoch [26/50] batch [40/51] time 0.164 (0.213) data 0.000 (0.036) loss 0.6398 (0.4945) acc 85.4167 (89.7569) lr 1.0628e-03 eta 0:04:22
epoch [26/50] batch [45/51] time 0.175 (0.208) data 0.000 (0.032) loss 0.4897 (0.4916) acc 90.1961 (89.7887) lr 1.0628e-03 eta 0:04:16
epoch [26/50] batch [50/51] time 0.167 (0.204) data 0.000 (0.029) loss 0.4599 (0.5112) acc 88.0000 (89.6692) lr 1.0628e-03 eta 0:04:10
>>> alpha1: 0.174  alpha2: -0.186 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [27/50] batch [5/51] time 0.165 (0.445) data 0.000 (0.267) loss 0.4631 (0.3719) acc 85.4167 (91.1534) lr 1.0000e-03 eta 0:09:01
epoch [27/50] batch [10/51] time 0.173 (0.309) data 0.000 (0.134) loss 0.2812 (0.3874) acc 95.0980 (91.2554) lr 1.0000e-03 eta 0:06:14
epoch [27/50] batch [15/51] time 0.162 (0.263) data 0.000 (0.089) loss 0.5219 (0.3995) acc 88.8298 (91.3852) lr 1.0000e-03 eta 0:05:18
epoch [27/50] batch [20/51] time 0.162 (0.239) data 0.000 (0.067) loss 0.4844 (0.4153) acc 89.8936 (91.0875) lr 1.0000e-03 eta 0:04:47
epoch [27/50] batch [25/51] time 0.191 (0.227) data 0.000 (0.054) loss 0.3938 (0.4183) acc 89.9038 (90.9014) lr 1.0000e-03 eta 0:04:32
epoch [27/50] batch [30/51] time 0.185 (0.218) data 0.000 (0.045) loss 0.3351 (0.4133) acc 94.1176 (91.0525) lr 1.0000e-03 eta 0:04:20
epoch [27/50] batch [35/51] time 0.164 (0.211) data 0.000 (0.039) loss 0.4437 (0.4114) acc 90.2174 (91.0726) lr 1.0000e-03 eta 0:04:11
epoch [27/50] batch [40/51] time 0.172 (0.207) data 0.000 (0.034) loss 0.4152 (0.4046) acc 89.4231 (91.2255) lr 1.0000e-03 eta 0:04:05
epoch [27/50] batch [45/51] time 0.172 (0.204) data 0.000 (0.030) loss 0.4506 (0.4072) acc 88.9423 (91.0069) lr 1.0000e-03 eta 0:04:00
epoch [27/50] batch [50/51] time 0.166 (0.200) data 0.000 (0.027) loss 0.4768 (0.4176) acc 88.7755 (90.7623) lr 1.0000e-03 eta 0:03:54
>>> alpha1: 0.170  alpha2: -0.189 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [28/50] batch [5/51] time 0.170 (0.460) data 0.001 (0.283) loss 0.7189 (0.5635) acc 84.5000 (87.6540) lr 9.3721e-04 eta 0:08:57
epoch [28/50] batch [10/51] time 0.175 (0.318) data 0.000 (0.142) loss 0.3340 (0.4689) acc 95.0980 (89.3913) lr 9.3721e-04 eta 0:06:10
epoch [28/50] batch [15/51] time 0.172 (0.269) data 0.000 (0.094) loss 0.4953 (0.4581) acc 86.2745 (89.3546) lr 9.3721e-04 eta 0:05:11
epoch [28/50] batch [20/51] time 0.188 (0.246) data 0.000 (0.071) loss 0.2779 (0.4563) acc 93.8679 (89.4754) lr 9.3721e-04 eta 0:04:43
epoch [28/50] batch [25/51] time 0.175 (0.232) data 0.000 (0.057) loss 0.4101 (0.4528) acc 89.2857 (89.5436) lr 9.3721e-04 eta 0:04:26
epoch [28/50] batch [30/51] time 0.196 (0.223) data 0.000 (0.047) loss 0.3856 (0.4476) acc 90.3061 (89.3966) lr 9.3721e-04 eta 0:04:14
epoch [28/50] batch [35/51] time 0.190 (0.218) data 0.000 (0.041) loss 0.4533 (0.4376) acc 86.7647 (89.5962) lr 9.3721e-04 eta 0:04:08
epoch [28/50] batch [40/51] time 0.187 (0.213) data 0.000 (0.036) loss 0.3720 (0.4403) acc 90.0000 (89.5976) lr 9.3721e-04 eta 0:04:01
epoch [28/50] batch [45/51] time 0.164 (0.209) data 0.000 (0.032) loss 0.3853 (0.4321) acc 90.1042 (89.8409) lr 9.3721e-04 eta 0:03:55
epoch [28/50] batch [50/51] time 0.179 (0.205) data 0.000 (0.029) loss 0.3869 (0.4316) acc 90.7407 (89.9227) lr 9.3721e-04 eta 0:03:50
>>> alpha1: 0.166  alpha2: -0.188 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [29/50] batch [5/51] time 0.170 (0.400) data 0.000 (0.220) loss 0.5247 (0.4312) acc 84.0000 (89.8255) lr 8.7467e-04 eta 0:07:26
epoch [29/50] batch [10/51] time 0.172 (0.288) data 0.000 (0.110) loss 0.8104 (0.4724) acc 83.3333 (89.8572) lr 8.7467e-04 eta 0:05:20
epoch [29/50] batch [15/51] time 0.184 (0.250) data 0.000 (0.073) loss 0.4013 (0.4473) acc 89.4231 (90.5247) lr 8.7467e-04 eta 0:04:36
epoch [29/50] batch [20/51] time 0.165 (0.232) data 0.000 (0.055) loss 0.2952 (0.4650) acc 93.2292 (89.6230) lr 8.7467e-04 eta 0:04:15
epoch [29/50] batch [25/51] time 0.193 (0.221) data 0.000 (0.044) loss 0.3000 (0.4732) acc 93.0556 (89.4927) lr 8.7467e-04 eta 0:04:02
epoch [29/50] batch [30/51] time 0.168 (0.214) data 0.000 (0.037) loss 0.4376 (0.4606) acc 93.3673 (89.9215) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [35/51] time 0.165 (0.209) data 0.000 (0.032) loss 0.5056 (0.4570) acc 88.5417 (89.8907) lr 8.7467e-04 eta 0:03:46
epoch [29/50] batch [40/51] time 0.160 (0.205) data 0.000 (0.028) loss 0.6741 (0.4628) acc 84.5745 (89.7884) lr 8.7467e-04 eta 0:03:41
epoch [29/50] batch [45/51] time 0.185 (0.201) data 0.000 (0.025) loss 0.3350 (0.4557) acc 94.7368 (89.9330) lr 8.7467e-04 eta 0:03:36
epoch [29/50] batch [50/51] time 0.163 (0.197) data 0.000 (0.022) loss 0.4002 (0.4506) acc 92.1875 (90.0285) lr 8.7467e-04 eta 0:03:31
>>> alpha1: 0.167  alpha2: -0.186 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [30/50] batch [5/51] time 0.159 (0.431) data 0.000 (0.261) loss 0.4477 (0.4786) acc 90.5556 (88.6512) lr 8.1262e-04 eta 0:07:39
epoch [30/50] batch [10/51] time 0.169 (0.306) data 0.000 (0.131) loss 0.4113 (0.4700) acc 88.8298 (89.1475) lr 8.1262e-04 eta 0:05:24
epoch [30/50] batch [15/51] time 0.181 (0.262) data 0.000 (0.087) loss 0.3374 (0.4562) acc 97.1698 (89.9933) lr 8.1262e-04 eta 0:04:36
epoch [30/50] batch [20/51] time 0.181 (0.240) data 0.000 (0.065) loss 0.3066 (0.4524) acc 93.1818 (89.7746) lr 8.1262e-04 eta 0:04:12
epoch [30/50] batch [25/51] time 0.183 (0.228) data 0.000 (0.052) loss 0.3077 (0.4352) acc 94.6429 (90.2267) lr 8.1262e-04 eta 0:03:58
epoch [30/50] batch [30/51] time 0.181 (0.219) data 0.000 (0.044) loss 0.3297 (0.4219) acc 95.9091 (90.5771) lr 8.1262e-04 eta 0:03:48
epoch [30/50] batch [35/51] time 0.175 (0.213) data 0.000 (0.038) loss 0.5401 (0.4374) acc 86.4583 (90.1040) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [40/51] time 0.171 (0.209) data 0.000 (0.033) loss 0.3649 (0.4304) acc 94.5000 (90.3967) lr 8.1262e-04 eta 0:03:35
epoch [30/50] batch [45/51] time 0.166 (0.204) data 0.000 (0.029) loss 0.4722 (0.4242) acc 86.2245 (90.5375) lr 8.1262e-04 eta 0:03:29
epoch [30/50] batch [50/51] time 0.179 (0.201) data 0.000 (0.026) loss 0.3683 (0.4201) acc 93.8679 (90.6275) lr 8.1262e-04 eta 0:03:25
>>> alpha1: 0.163  alpha2: -0.187 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [31/50] batch [5/51] time 0.182 (0.452) data 0.000 (0.270) loss 0.4606 (0.3781) acc 87.7551 (91.2614) lr 7.5131e-04 eta 0:07:38
epoch [31/50] batch [10/51] time 0.175 (0.315) data 0.000 (0.135) loss 0.4588 (0.3910) acc 92.5532 (91.5784) lr 7.5131e-04 eta 0:05:18
epoch [31/50] batch [15/51] time 0.171 (0.269) data 0.000 (0.090) loss 0.3891 (0.4025) acc 89.7059 (91.0403) lr 7.5131e-04 eta 0:04:30
epoch [31/50] batch [20/51] time 0.177 (0.248) data 0.000 (0.068) loss 0.5024 (0.4044) acc 86.9792 (90.8520) lr 7.5131e-04 eta 0:04:07
epoch [31/50] batch [25/51] time 0.177 (0.234) data 0.000 (0.054) loss 0.2400 (0.3839) acc 94.8113 (91.4577) lr 7.5131e-04 eta 0:03:53
epoch [31/50] batch [30/51] time 0.170 (0.225) data 0.000 (0.045) loss 0.4154 (0.3982) acc 92.0000 (91.2001) lr 7.5131e-04 eta 0:03:42
epoch [31/50] batch [35/51] time 0.171 (0.218) data 0.000 (0.039) loss 0.3472 (0.4037) acc 92.8571 (90.8269) lr 7.5131e-04 eta 0:03:34
epoch [31/50] batch [40/51] time 0.166 (0.212) data 0.000 (0.034) loss 0.6673 (0.4133) acc 84.1837 (90.6420) lr 7.5131e-04 eta 0:03:27
epoch [31/50] batch [45/51] time 0.178 (0.208) data 0.000 (0.030) loss 0.3764 (0.4115) acc 91.2037 (90.6386) lr 7.5131e-04 eta 0:03:22
epoch [31/50] batch [50/51] time 0.167 (0.204) data 0.001 (0.027) loss 0.4515 (0.4182) acc 88.7755 (90.4355) lr 7.5131e-04 eta 0:03:17
>>> alpha1: 0.165  alpha2: -0.188 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [32/50] batch [5/51] time 0.188 (0.462) data 0.000 (0.275) loss 0.2065 (0.3352) acc 96.2264 (92.5816) lr 6.9098e-04 eta 0:07:25
epoch [32/50] batch [10/51] time 0.166 (0.317) data 0.000 (0.138) loss 0.4632 (0.3834) acc 88.0208 (91.4734) lr 6.9098e-04 eta 0:05:04
epoch [32/50] batch [15/51] time 0.175 (0.270) data 0.001 (0.092) loss 0.3568 (0.3899) acc 93.1373 (91.5201) lr 6.9098e-04 eta 0:04:17
epoch [32/50] batch [20/51] time 0.163 (0.246) data 0.000 (0.069) loss 0.2725 (0.3790) acc 94.2708 (91.7077) lr 6.9098e-04 eta 0:03:53
epoch [32/50] batch [25/51] time 0.185 (0.234) data 0.001 (0.055) loss 0.2659 (0.3815) acc 93.1373 (91.9654) lr 6.9098e-04 eta 0:03:40
epoch [32/50] batch [30/51] time 0.173 (0.225) data 0.000 (0.046) loss 0.5934 (0.3861) acc 87.2549 (91.6986) lr 6.9098e-04 eta 0:03:31
epoch [32/50] batch [35/51] time 0.177 (0.218) data 0.000 (0.040) loss 0.5416 (0.3946) acc 86.7347 (91.3598) lr 6.9098e-04 eta 0:03:23
epoch [32/50] batch [40/51] time 0.164 (0.212) data 0.000 (0.035) loss 0.4148 (0.3967) acc 92.1875 (91.5209) lr 6.9098e-04 eta 0:03:17
epoch [32/50] batch [45/51] time 0.176 (0.208) data 0.000 (0.031) loss 0.4534 (0.4036) acc 90.0943 (91.3210) lr 6.9098e-04 eta 0:03:11
epoch [32/50] batch [50/51] time 0.170 (0.204) data 0.000 (0.028) loss 0.4764 (0.4035) acc 88.2353 (91.2686) lr 6.9098e-04 eta 0:03:07
>>> alpha1: 0.165  alpha2: -0.188 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [33/50] batch [5/51] time 0.166 (0.428) data 0.000 (0.252) loss 0.4542 (0.4218) acc 88.7755 (91.4264) lr 6.3188e-04 eta 0:06:31
epoch [33/50] batch [10/51] time 0.178 (0.304) data 0.000 (0.126) loss 0.3211 (0.4174) acc 93.6274 (90.9432) lr 6.3188e-04 eta 0:04:36
epoch [33/50] batch [15/51] time 0.166 (0.261) data 0.000 (0.084) loss 0.2753 (0.3926) acc 93.7500 (91.3482) lr 6.3188e-04 eta 0:03:55
epoch [33/50] batch [20/51] time 0.170 (0.275) data 0.000 (0.063) loss 0.4353 (0.3880) acc 89.0625 (91.4656) lr 6.3188e-04 eta 0:04:07
epoch [33/50] batch [25/51] time 0.183 (0.256) data 0.000 (0.051) loss 0.2564 (0.3804) acc 94.8113 (91.8301) lr 6.3188e-04 eta 0:03:48
epoch [33/50] batch [30/51] time 0.168 (0.242) data 0.000 (0.042) loss 0.3653 (0.3912) acc 92.8571 (91.5418) lr 6.3188e-04 eta 0:03:34
epoch [33/50] batch [35/51] time 0.177 (0.233) data 0.000 (0.036) loss 0.4127 (0.3984) acc 91.5094 (91.2859) lr 6.3188e-04 eta 0:03:25
epoch [33/50] batch [40/51] time 0.169 (0.225) data 0.000 (0.032) loss 0.3524 (0.4035) acc 91.5000 (90.9729) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [45/51] time 0.164 (0.219) data 0.000 (0.028) loss 0.3197 (0.3972) acc 90.1042 (90.9564) lr 6.3188e-04 eta 0:03:11
epoch [33/50] batch [50/51] time 0.173 (0.214) data 0.000 (0.025) loss 0.4903 (0.4051) acc 85.0962 (90.6984) lr 6.3188e-04 eta 0:03:06
>>> alpha1: 0.166  alpha2: -0.193 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [34/50] batch [5/51] time 0.167 (0.444) data 0.000 (0.268) loss 0.2841 (0.3247) acc 94.8980 (92.5760) lr 5.7422e-04 eta 0:06:22
epoch [34/50] batch [10/51] time 0.178 (0.312) data 0.000 (0.134) loss 0.5418 (0.3828) acc 87.0192 (90.8893) lr 5.7422e-04 eta 0:04:27
epoch [34/50] batch [15/51] time 0.173 (0.267) data 0.000 (0.089) loss 0.3510 (0.3684) acc 93.2692 (91.5649) lr 5.7422e-04 eta 0:03:47
epoch [34/50] batch [20/51] time 0.164 (0.244) data 0.000 (0.067) loss 0.6143 (0.3849) acc 86.7021 (90.9932) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [25/51] time 0.167 (0.230) data 0.000 (0.054) loss 0.3132 (0.3770) acc 91.6667 (90.9250) lr 5.7422e-04 eta 0:03:13
epoch [34/50] batch [30/51] time 0.176 (0.221) data 0.000 (0.045) loss 0.4137 (0.3915) acc 91.0377 (90.7213) lr 5.7422e-04 eta 0:03:05
epoch [34/50] batch [35/51] time 0.173 (0.215) data 0.000 (0.038) loss 0.3965 (0.3877) acc 87.5000 (90.7700) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [40/51] time 0.169 (0.210) data 0.000 (0.034) loss 0.3122 (0.3825) acc 94.1176 (91.1379) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [45/51] time 0.169 (0.205) data 0.000 (0.030) loss 0.4217 (0.3843) acc 91.6667 (91.1407) lr 5.7422e-04 eta 0:02:48
epoch [34/50] batch [50/51] time 0.172 (0.201) data 0.000 (0.027) loss 0.4554 (0.3888) acc 87.9808 (91.0864) lr 5.7422e-04 eta 0:02:44
>>> alpha1: 0.165  alpha2: -0.199 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [35/50] batch [5/51] time 0.174 (0.461) data 0.000 (0.281) loss 0.2860 (0.3369) acc 94.2308 (92.7643) lr 5.1825e-04 eta 0:06:13
epoch [35/50] batch [10/51] time 0.177 (0.319) data 0.000 (0.141) loss 0.3019 (0.4250) acc 91.5094 (91.4769) lr 5.1825e-04 eta 0:04:16
epoch [35/50] batch [15/51] time 0.189 (0.272) data 0.000 (0.094) loss 0.2828 (0.3945) acc 94.5000 (91.9017) lr 5.1825e-04 eta 0:03:37
epoch [35/50] batch [20/51] time 0.196 (0.247) data 0.000 (0.070) loss 0.3268 (0.3888) acc 92.7885 (91.8552) lr 5.1825e-04 eta 0:03:16
epoch [35/50] batch [25/51] time 0.169 (0.232) data 0.000 (0.056) loss 0.3789 (0.4322) acc 93.8775 (91.5338) lr 5.1825e-04 eta 0:03:03
epoch [35/50] batch [30/51] time 0.181 (0.223) data 0.000 (0.047) loss 0.4384 (0.4364) acc 90.4546 (91.3836) lr 5.1825e-04 eta 0:02:55
epoch [35/50] batch [35/51] time 0.168 (0.216) data 0.000 (0.040) loss 0.4603 (0.4312) acc 91.5000 (91.4286) lr 5.1825e-04 eta 0:02:48
epoch [35/50] batch [40/51] time 0.163 (0.211) data 0.000 (0.035) loss 0.2805 (0.4238) acc 93.7500 (91.6101) lr 5.1825e-04 eta 0:02:43
epoch [35/50] batch [45/51] time 0.176 (0.206) data 0.000 (0.031) loss 0.5050 (0.4252) acc 88.7755 (91.4408) lr 5.1825e-04 eta 0:02:38
epoch [35/50] batch [50/51] time 0.181 (0.203) data 0.000 (0.028) loss 0.4338 (0.4227) acc 86.5741 (91.2871) lr 5.1825e-04 eta 0:02:35
>>> alpha1: 0.164  alpha2: -0.198 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [36/50] batch [5/51] time 0.176 (0.462) data 0.001 (0.262) loss 0.5571 (0.4128) acc 88.2979 (91.6676) lr 4.6417e-04 eta 0:05:51
epoch [36/50] batch [10/51] time 0.186 (0.320) data 0.001 (0.131) loss 0.3421 (0.3754) acc 93.2692 (92.5396) lr 4.6417e-04 eta 0:04:01
epoch [36/50] batch [15/51] time 0.174 (0.274) data 0.000 (0.088) loss 0.3907 (0.3800) acc 91.8269 (92.1444) lr 4.6417e-04 eta 0:03:25
epoch [36/50] batch [20/51] time 0.169 (0.250) data 0.000 (0.066) loss 0.5493 (0.3890) acc 86.7347 (91.5650) lr 4.6417e-04 eta 0:03:05
epoch [36/50] batch [25/51] time 0.167 (0.235) data 0.000 (0.053) loss 0.4584 (0.3922) acc 93.3673 (91.5432) lr 4.6417e-04 eta 0:02:53
epoch [36/50] batch [30/51] time 0.171 (0.225) data 0.000 (0.044) loss 0.2969 (0.3954) acc 92.5000 (91.3540) lr 4.6417e-04 eta 0:02:45
epoch [36/50] batch [35/51] time 0.176 (0.218) data 0.000 (0.038) loss 0.3994 (0.3919) acc 90.0943 (91.4465) lr 4.6417e-04 eta 0:02:38
epoch [36/50] batch [40/51] time 0.169 (0.212) data 0.000 (0.033) loss 0.4249 (0.3946) acc 90.1961 (91.3719) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [45/51] time 0.163 (0.208) data 0.000 (0.029) loss 0.5124 (0.3933) acc 84.0425 (91.2428) lr 4.6417e-04 eta 0:02:29
epoch [36/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.027) loss 0.3547 (0.3865) acc 92.0000 (91.4170) lr 4.6417e-04 eta 0:02:26
>>> alpha1: 0.158  alpha2: -0.204 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [37/50] batch [5/51] time 0.171 (0.507) data 0.000 (0.329) loss 0.2412 (0.4465) acc 95.5000 (90.8984) lr 4.1221e-04 eta 0:05:59
epoch [37/50] batch [10/51] time 0.187 (0.346) data 0.000 (0.165) loss 0.2070 (0.3767) acc 96.7593 (91.6578) lr 4.1221e-04 eta 0:04:03
epoch [37/50] batch [15/51] time 0.173 (0.290) data 0.001 (0.110) loss 0.4116 (0.4249) acc 91.1765 (90.6388) lr 4.1221e-04 eta 0:03:22
epoch [37/50] batch [20/51] time 0.190 (0.263) data 0.001 (0.082) loss 0.5787 (0.4442) acc 87.7358 (90.1345) lr 4.1221e-04 eta 0:03:02
epoch [37/50] batch [25/51] time 0.162 (0.245) data 0.000 (0.066) loss 0.4243 (0.4258) acc 89.1304 (90.7402) lr 4.1221e-04 eta 0:02:48
epoch [37/50] batch [30/51] time 0.170 (0.233) data 0.000 (0.055) loss 0.2360 (0.4063) acc 93.1373 (91.1665) lr 4.1221e-04 eta 0:02:39
epoch [37/50] batch [35/51] time 0.191 (0.226) data 0.000 (0.047) loss 0.2494 (0.3919) acc 95.3704 (91.5962) lr 4.1221e-04 eta 0:02:33
epoch [37/50] batch [40/51] time 0.186 (0.236) data 0.000 (0.041) loss 0.3557 (0.3807) acc 91.8182 (91.8197) lr 4.1221e-04 eta 0:02:39
epoch [37/50] batch [45/51] time 0.167 (0.229) data 0.000 (0.037) loss 0.3646 (0.3810) acc 94.3878 (91.8030) lr 4.1221e-04 eta 0:02:33
epoch [37/50] batch [50/51] time 0.177 (0.223) data 0.000 (0.033) loss 0.3075 (0.3836) acc 93.8679 (91.7716) lr 4.1221e-04 eta 0:02:28
>>> alpha1: 0.158  alpha2: -0.207 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [38/50] batch [5/51] time 0.171 (0.456) data 0.000 (0.278) loss 0.3594 (0.3737) acc 93.0000 (91.2063) lr 3.6258e-04 eta 0:04:59
epoch [38/50] batch [10/51] time 0.180 (0.319) data 0.000 (0.139) loss 0.3966 (0.3757) acc 90.7407 (90.7841) lr 3.6258e-04 eta 0:03:27
epoch [38/50] batch [15/51] time 0.176 (0.271) data 0.000 (0.093) loss 0.4206 (0.3904) acc 90.3846 (90.2063) lr 3.6258e-04 eta 0:02:55
epoch [38/50] batch [20/51] time 0.171 (0.247) data 0.000 (0.070) loss 0.4651 (0.3813) acc 89.7059 (90.6457) lr 3.6258e-04 eta 0:02:39
epoch [38/50] batch [25/51] time 0.189 (0.235) data 0.000 (0.056) loss 0.3212 (0.3793) acc 93.7500 (91.0862) lr 3.6258e-04 eta 0:02:29
epoch [38/50] batch [30/51] time 0.186 (0.227) data 0.000 (0.047) loss 0.3107 (0.3769) acc 92.9825 (91.2661) lr 3.6258e-04 eta 0:02:23
epoch [38/50] batch [35/51] time 0.171 (0.219) data 0.000 (0.040) loss 0.2675 (0.3806) acc 93.1373 (91.2990) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.035) loss 0.4344 (0.3870) acc 90.6863 (91.2288) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [45/51] time 0.172 (0.209) data 0.000 (0.031) loss 0.3491 (0.3849) acc 93.7500 (91.2859) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [50/51] time 0.176 (0.205) data 0.000 (0.028) loss 0.3659 (0.3902) acc 91.9811 (91.1806) lr 3.6258e-04 eta 0:02:05
>>> alpha1: 0.157  alpha2: -0.204 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [39/50] batch [5/51] time 0.167 (0.457) data 0.000 (0.279) loss 0.4524 (0.4753) acc 86.9792 (88.6958) lr 3.1545e-04 eta 0:04:37
epoch [39/50] batch [10/51] time 0.172 (0.318) data 0.001 (0.140) loss 0.3024 (0.4046) acc 94.3878 (90.7378) lr 3.1545e-04 eta 0:03:11
epoch [39/50] batch [15/51] time 0.176 (0.273) data 0.000 (0.093) loss 0.2784 (0.3892) acc 92.4528 (91.1614) lr 3.1545e-04 eta 0:02:42
epoch [39/50] batch [20/51] time 0.174 (0.249) data 0.000 (0.070) loss 0.4598 (0.3843) acc 88.9423 (91.1802) lr 3.1545e-04 eta 0:02:27
epoch [39/50] batch [25/51] time 0.167 (0.235) data 0.000 (0.056) loss 0.2743 (0.3860) acc 92.8571 (91.3514) lr 3.1545e-04 eta 0:02:18
epoch [39/50] batch [30/51] time 0.170 (0.225) data 0.000 (0.047) loss 0.5018 (0.3945) acc 86.7647 (91.0675) lr 3.1545e-04 eta 0:02:11
epoch [39/50] batch [35/51] time 0.187 (0.219) data 0.000 (0.041) loss 0.3587 (0.3960) acc 91.8103 (90.8592) lr 3.1545e-04 eta 0:02:06
epoch [39/50] batch [40/51] time 0.182 (0.213) data 0.000 (0.036) loss 0.2234 (0.3908) acc 96.5000 (90.8455) lr 3.1545e-04 eta 0:02:02
epoch [39/50] batch [45/51] time 0.177 (0.209) data 0.000 (0.032) loss 0.2017 (0.3827) acc 96.6981 (91.1598) lr 3.1545e-04 eta 0:01:58
epoch [39/50] batch [50/51] time 0.173 (0.206) data 0.000 (0.029) loss 0.2955 (0.3834) acc 91.1765 (91.0713) lr 3.1545e-04 eta 0:01:55
>>> alpha1: 0.153  alpha2: -0.198 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [40/50] batch [5/51] time 0.175 (0.508) data 0.000 (0.324) loss 0.4247 (0.3787) acc 89.4231 (90.5366) lr 2.7103e-04 eta 0:04:42
epoch [40/50] batch [10/51] time 0.194 (0.344) data 0.000 (0.162) loss 0.3682 (0.3776) acc 90.3061 (91.2582) lr 2.7103e-04 eta 0:03:09
epoch [40/50] batch [15/51] time 0.173 (0.287) data 0.000 (0.108) loss 0.4379 (0.3757) acc 90.3846 (91.7045) lr 2.7103e-04 eta 0:02:36
epoch [40/50] batch [20/51] time 0.183 (0.260) data 0.000 (0.081) loss 0.3134 (0.3634) acc 94.5000 (91.9886) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [25/51] time 0.174 (0.243) data 0.000 (0.065) loss 0.4354 (0.3643) acc 88.4615 (91.9625) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [30/51] time 0.178 (0.231) data 0.000 (0.054) loss 0.2051 (0.3683) acc 94.0000 (91.6652) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [35/51] time 0.175 (0.223) data 0.001 (0.047) loss 0.3517 (0.3619) acc 92.7885 (91.8696) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [40/51] time 0.170 (0.217) data 0.000 (0.041) loss 0.3365 (0.3604) acc 93.1373 (91.8511) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [45/51] time 0.187 (0.213) data 0.000 (0.036) loss 0.3296 (0.3652) acc 92.3077 (91.5798) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.172 (0.209) data 0.000 (0.033) loss 0.4513 (0.3669) acc 92.3077 (91.6292) lr 2.7103e-04 eta 0:01:46
>>> alpha1: 0.152  alpha2: -0.199 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [41/50] batch [5/51] time 0.191 (0.431) data 0.001 (0.234) loss 0.4205 (0.4120) acc 87.7193 (91.3073) lr 2.2949e-04 eta 0:03:37
epoch [41/50] batch [10/51] time 0.180 (0.305) data 0.000 (0.118) loss 0.4036 (0.4023) acc 90.8654 (91.0311) lr 2.2949e-04 eta 0:02:32
epoch [41/50] batch [15/51] time 0.170 (0.262) data 0.000 (0.079) loss 0.3522 (0.3713) acc 94.8980 (91.8398) lr 2.2949e-04 eta 0:02:09
epoch [41/50] batch [20/51] time 0.173 (0.240) data 0.001 (0.059) loss 0.4195 (0.3585) acc 88.7255 (92.1664) lr 2.2949e-04 eta 0:01:57
epoch [41/50] batch [25/51] time 0.167 (0.227) data 0.000 (0.047) loss 0.4014 (0.3638) acc 94.6808 (92.1005) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [30/51] time 0.168 (0.218) data 0.000 (0.039) loss 0.4070 (0.3713) acc 91.0000 (91.8967) lr 2.2949e-04 eta 0:01:44
epoch [41/50] batch [35/51] time 0.177 (0.212) data 0.000 (0.034) loss 0.2587 (0.3776) acc 94.8113 (91.6949) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [40/51] time 0.174 (0.208) data 0.000 (0.030) loss 0.4528 (0.3786) acc 89.4231 (91.6815) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [45/51] time 0.171 (0.204) data 0.000 (0.026) loss 0.4169 (0.3848) acc 93.1373 (91.5771) lr 2.2949e-04 eta 0:01:34
epoch [41/50] batch [50/51] time 0.167 (0.200) data 0.000 (0.024) loss 0.4424 (0.3820) acc 90.3061 (91.6498) lr 2.2949e-04 eta 0:01:32
>>> alpha1: 0.156  alpha2: -0.202 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [42/50] batch [5/51] time 0.185 (0.443) data 0.001 (0.251) loss 0.3162 (0.2788) acc 92.2727 (93.0257) lr 1.9098e-04 eta 0:03:21
epoch [42/50] batch [10/51] time 0.182 (0.312) data 0.000 (0.126) loss 0.3525 (0.3299) acc 92.6471 (92.4785) lr 1.9098e-04 eta 0:02:20
epoch [42/50] batch [15/51] time 0.176 (0.267) data 0.000 (0.084) loss 0.4827 (0.3737) acc 87.9808 (92.4890) lr 1.9098e-04 eta 0:01:58
epoch [42/50] batch [20/51] time 0.179 (0.245) data 0.000 (0.063) loss 0.2795 (0.3689) acc 91.6667 (92.2157) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [25/51] time 0.188 (0.231) data 0.000 (0.050) loss 0.2383 (0.3736) acc 95.5357 (92.0655) lr 1.9098e-04 eta 0:01:40
epoch [42/50] batch [30/51] time 0.188 (0.223) data 0.000 (0.042) loss 0.3710 (0.3659) acc 91.3462 (92.3963) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [35/51] time 0.168 (0.216) data 0.000 (0.036) loss 0.3940 (0.3668) acc 93.8775 (92.2115) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.170 (0.210) data 0.000 (0.032) loss 0.2684 (0.3671) acc 96.9388 (92.2150) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.156 (0.205) data 0.000 (0.028) loss 0.3989 (0.3654) acc 92.2222 (92.2934) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.175 (0.202) data 0.000 (0.025) loss 0.5813 (0.3706) acc 87.7358 (92.2026) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.154  alpha2: -0.201 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [43/50] batch [5/51] time 0.174 (0.454) data 0.000 (0.271) loss 0.2726 (0.3023) acc 95.0980 (93.7318) lr 1.5567e-04 eta 0:03:02
epoch [43/50] batch [10/51] time 0.171 (0.318) data 0.001 (0.136) loss 0.3935 (0.3374) acc 91.8367 (93.0444) lr 1.5567e-04 eta 0:02:06
epoch [43/50] batch [15/51] time 0.198 (0.274) data 0.000 (0.091) loss 0.1634 (0.3485) acc 99.0385 (93.0774) lr 1.5567e-04 eta 0:01:47
epoch [43/50] batch [20/51] time 0.195 (0.253) data 0.001 (0.068) loss 0.2168 (0.3557) acc 95.3704 (92.6300) lr 1.5567e-04 eta 0:01:38
epoch [43/50] batch [25/51] time 0.190 (0.237) data 0.000 (0.055) loss 0.5288 (0.3682) acc 88.3929 (92.1204) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.181 (0.228) data 0.000 (0.045) loss 0.1613 (0.3558) acc 98.1818 (92.4113) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [35/51] time 0.164 (0.222) data 0.000 (0.039) loss 0.2002 (0.3481) acc 94.1489 (92.5870) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.167 (0.216) data 0.000 (0.034) loss 0.3290 (0.3504) acc 92.7083 (92.4254) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.169 (0.211) data 0.000 (0.030) loss 0.3126 (0.3557) acc 93.3673 (92.2447) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.176 (0.208) data 0.000 (0.027) loss 0.3780 (0.3539) acc 91.5094 (92.2598) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.158  alpha2: -0.206 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [44/50] batch [5/51] time 0.187 (0.411) data 0.001 (0.225) loss 0.2623 (0.3164) acc 96.1538 (91.8280) lr 1.2369e-04 eta 0:02:24
epoch [44/50] batch [10/51] time 0.174 (0.295) data 0.000 (0.113) loss 0.2855 (0.3333) acc 94.5000 (92.0876) lr 1.2369e-04 eta 0:01:42
epoch [44/50] batch [15/51] time 0.164 (0.256) data 0.000 (0.075) loss 0.3297 (0.3350) acc 89.8936 (91.8738) lr 1.2369e-04 eta 0:01:27
epoch [44/50] batch [20/51] time 0.179 (0.238) data 0.000 (0.057) loss 0.3550 (0.3499) acc 91.9811 (91.5174) lr 1.2369e-04 eta 0:01:20
epoch [44/50] batch [25/51] time 0.177 (0.225) data 0.000 (0.045) loss 0.2469 (0.3400) acc 96.9388 (91.9603) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [30/51] time 0.181 (0.217) data 0.000 (0.038) loss 0.3508 (0.3351) acc 94.3396 (91.9892) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [35/51] time 0.178 (0.212) data 0.001 (0.032) loss 0.3887 (0.3323) acc 91.5000 (92.1303) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [40/51] time 0.173 (0.208) data 0.000 (0.028) loss 0.2674 (0.3325) acc 93.1373 (92.0899) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [45/51] time 0.170 (0.204) data 0.000 (0.025) loss 0.4154 (0.3355) acc 89.7059 (92.0037) lr 1.2369e-04 eta 0:01:03
epoch [44/50] batch [50/51] time 0.178 (0.201) data 0.000 (0.023) loss 0.4334 (0.3370) acc 90.5660 (92.0388) lr 1.2369e-04 eta 0:01:01
>>> alpha1: 0.159  alpha2: -0.214 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [45/50] batch [5/51] time 0.199 (0.505) data 0.000 (0.307) loss 0.4349 (0.3502) acc 88.8889 (92.5304) lr 9.5173e-05 eta 0:02:31
epoch [45/50] batch [10/51] time 0.185 (0.347) data 0.000 (0.154) loss 0.3251 (0.3490) acc 92.5926 (91.7958) lr 9.5173e-05 eta 0:01:42
epoch [45/50] batch [15/51] time 0.180 (0.289) data 0.001 (0.103) loss 0.5311 (0.3704) acc 87.5000 (91.1029) lr 9.5173e-05 eta 0:01:24
epoch [45/50] batch [20/51] time 0.165 (0.262) data 0.000 (0.077) loss 0.4770 (0.3810) acc 89.0625 (91.1672) lr 9.5173e-05 eta 0:01:14
epoch [45/50] batch [25/51] time 0.186 (0.245) data 0.000 (0.062) loss 0.5021 (0.3777) acc 88.1818 (91.1818) lr 9.5173e-05 eta 0:01:08
epoch [45/50] batch [30/51] time 0.188 (0.234) data 0.000 (0.051) loss 0.3261 (0.3733) acc 94.4444 (91.4322) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [35/51] time 0.187 (0.226) data 0.000 (0.044) loss 0.4165 (0.3679) acc 91.3462 (91.6804) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [40/51] time 0.171 (0.220) data 0.000 (0.039) loss 0.2919 (0.3656) acc 96.0000 (91.8485) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.178 (0.215) data 0.000 (0.034) loss 0.3781 (0.3708) acc 90.5660 (91.7657) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.194 (0.211) data 0.000 (0.031) loss 0.4830 (0.3664) acc 87.9808 (91.8297) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.160  alpha2: -0.209 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [46/50] batch [5/51] time 0.180 (0.412) data 0.000 (0.234) loss 0.4105 (0.4233) acc 89.7959 (89.0864) lr 7.0224e-05 eta 0:01:43
epoch [46/50] batch [10/51] time 0.180 (0.292) data 0.000 (0.117) loss 0.4391 (0.4171) acc 88.7755 (89.4627) lr 7.0224e-05 eta 0:01:11
epoch [46/50] batch [15/51] time 0.178 (0.254) data 0.000 (0.078) loss 0.4545 (0.4130) acc 87.5000 (89.6865) lr 7.0224e-05 eta 0:01:01
epoch [46/50] batch [20/51] time 0.199 (0.235) data 0.000 (0.059) loss 0.3320 (0.3970) acc 90.8654 (90.4070) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [25/51] time 0.179 (0.223) data 0.001 (0.047) loss 0.2540 (0.3849) acc 97.6852 (91.1208) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [30/51] time 0.167 (0.215) data 0.000 (0.039) loss 0.2807 (0.3696) acc 94.8980 (91.6090) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [35/51] time 0.185 (0.210) data 0.000 (0.034) loss 0.3558 (0.3709) acc 91.9643 (91.6118) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [40/51] time 0.166 (0.206) data 0.000 (0.030) loss 0.3546 (0.3672) acc 91.6667 (91.6314) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.169 (0.202) data 0.000 (0.026) loss 0.5552 (0.3579) acc 85.0000 (91.9363) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [50/51] time 0.167 (0.199) data 0.000 (0.024) loss 0.3159 (0.3571) acc 94.3878 (91.9715) lr 7.0224e-05 eta 0:00:40
>>> alpha1: 0.156  alpha2: -0.212 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [47/50] batch [5/51] time 0.182 (0.459) data 0.000 (0.275) loss 0.2961 (0.3178) acc 92.7273 (93.6813) lr 4.8943e-05 eta 0:01:31
epoch [47/50] batch [10/51] time 0.170 (0.317) data 0.000 (0.138) loss 0.2430 (0.3309) acc 97.4490 (93.2063) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [15/51] time 0.185 (0.269) data 0.000 (0.092) loss 0.2636 (0.3449) acc 96.5000 (93.6322) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [20/51] time 0.194 (0.245) data 0.001 (0.069) loss 0.3315 (0.3461) acc 93.0556 (93.3974) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [25/51] time 0.169 (0.231) data 0.000 (0.055) loss 0.3390 (0.3539) acc 91.0000 (92.8483) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.171 (0.222) data 0.000 (0.046) loss 0.3256 (0.3669) acc 93.6274 (92.4511) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [35/51] time 0.181 (0.215) data 0.000 (0.040) loss 0.3842 (0.3702) acc 91.8269 (92.3367) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.167 (0.210) data 0.000 (0.035) loss 0.2463 (0.3614) acc 95.9184 (92.5631) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.194 (0.206) data 0.000 (0.031) loss 0.4010 (0.3635) acc 91.5000 (92.4742) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.176 (0.203) data 0.000 (0.028) loss 0.4742 (0.3580) acc 85.5000 (92.4110) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.156  alpha2: -0.218 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [48/50] batch [5/51] time 0.172 (0.502) data 0.000 (0.319) loss 0.5124 (0.3827) acc 87.2549 (92.1510) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [10/51] time 0.176 (0.339) data 0.000 (0.160) loss 0.3533 (0.3705) acc 93.1373 (92.1916) lr 3.1417e-05 eta 0:00:48
epoch [48/50] batch [15/51] time 0.195 (0.286) data 0.000 (0.107) loss 0.3270 (0.3320) acc 91.1765 (92.6633) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.191 (0.263) data 0.000 (0.080) loss 0.3160 (0.3412) acc 94.0000 (92.4989) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.192 (0.248) data 0.000 (0.064) loss 0.3920 (0.3530) acc 90.7407 (92.2706) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.178 (0.236) data 0.000 (0.053) loss 0.3746 (0.3535) acc 91.0377 (92.1403) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [35/51] time 0.183 (0.228) data 0.001 (0.046) loss 0.5243 (0.3612) acc 88.2653 (92.1404) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.168 (0.221) data 0.000 (0.040) loss 0.4029 (0.3600) acc 90.6863 (92.1400) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [45/51] time 0.163 (0.215) data 0.000 (0.036) loss 0.2929 (0.3612) acc 94.7917 (92.3122) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.191 (0.211) data 0.000 (0.032) loss 0.2806 (0.3566) acc 95.8333 (92.3860) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.155  alpha2: -0.208 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [49/50] batch [5/51] time 0.177 (0.467) data 0.000 (0.285) loss 0.3102 (0.3053) acc 94.3396 (93.2459) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.185 (0.323) data 0.000 (0.143) loss 0.4579 (0.3151) acc 90.1961 (93.5384) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.184 (0.275) data 0.000 (0.095) loss 0.2906 (0.3249) acc 92.7885 (93.1103) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.168 (0.251) data 0.000 (0.071) loss 0.6512 (0.3464) acc 84.7826 (92.5429) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.170 (0.236) data 0.000 (0.057) loss 0.2793 (0.3413) acc 95.5000 (92.7824) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.171 (0.226) data 0.000 (0.048) loss 0.4458 (0.3605) acc 91.1765 (92.4816) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.172 (0.219) data 0.000 (0.041) loss 0.4695 (0.3634) acc 87.0192 (92.2292) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.036) loss 0.3610 (0.3647) acc 93.1373 (92.2038) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.180 (0.210) data 0.000 (0.032) loss 0.4339 (0.3663) acc 92.1296 (92.1682) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.185 (0.207) data 0.000 (0.029) loss 0.2420 (0.3559) acc 94.7368 (92.3958) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.153  alpha2: -0.207 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [50/50] batch [5/51] time 0.181 (0.460) data 0.001 (0.273) loss 0.2711 (0.3344) acc 94.8113 (93.1899) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.176 (0.322) data 0.000 (0.137) loss 0.2718 (0.3751) acc 93.5000 (92.1035) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.173 (0.275) data 0.000 (0.091) loss 0.3809 (0.3621) acc 92.6471 (92.0451) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.171 (0.250) data 0.000 (0.069) loss 0.3009 (0.3576) acc 90.1961 (92.3734) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.174 (0.236) data 0.000 (0.055) loss 0.2433 (0.3478) acc 95.7447 (92.5963) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.168 (0.225) data 0.000 (0.046) loss 0.5302 (0.3518) acc 89.7959 (92.4801) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.174 (0.219) data 0.000 (0.039) loss 0.3612 (0.3483) acc 91.8269 (92.4817) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.168 (0.213) data 0.000 (0.034) loss 0.3348 (0.3491) acc 90.8163 (92.5668) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.170 (0.209) data 0.000 (0.031) loss 0.3961 (0.3531) acc 88.5000 (92.3364) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.173 (0.205) data 0.000 (0.028) loss 0.2890 (0.3499) acc 92.5000 (92.4181) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.05, 0.06, 0.07, 0.07, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.07, 0.08, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08]
* matched noise rate: [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]
* unmatched noise rate: [0.08, 0.11, 0.14, 0.13, 0.14, 0.15, 0.14, 0.15, 0.15, 0.15, 0.14, 0.14, 0.15, 0.14, 0.14, 0.15, 0.13, 0.14, 0.15, 0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15, 0.14, 0.13, 0.14, 0.13, 0.14, 0.13, 0.13, 0.15, 0.14, 0.13, 0.13, 0.14, 0.13]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:04,  2.69s/it]  8%|▊         | 2/25 [00:02<00:26,  1.17s/it] 12%|█▏        | 3/25 [00:02<00:15,  1.44it/s] 20%|██        | 5/25 [00:03<00:06,  2.86it/s] 28%|██▊       | 7/25 [00:03<00:04,  4.35it/s] 36%|███▌      | 9/25 [00:03<00:02,  5.82it/s] 44%|████▍     | 11/25 [00:03<00:01,  7.17it/s] 52%|█████▏    | 13/25 [00:03<00:01,  8.33it/s] 60%|██████    | 15/25 [00:03<00:01,  9.30it/s] 68%|██████▊   | 17/25 [00:04<00:00, 10.09it/s] 76%|███████▌  | 19/25 [00:04<00:00, 10.70it/s] 84%|████████▍ | 21/25 [00:04<00:00, 11.12it/s] 92%|█████████▏| 23/25 [00:04<00:00, 11.49it/s]100%|██████████| 25/25 [00:04<00:00,  8.19it/s]100%|██████████| 25/25 [00:05<00:00,  4.90it/s]
=> result
* total: 2,463
* correct: 2,150
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.1%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 5	acc: 41.7%
* class: 3 (sweet pea)	total: 17	correct: 10	acc: 58.8%
* class: 4 (english marigold)	total: 20	correct: 15	acc: 75.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 23	acc: 88.5%
* class: 11 (colt's foot)	total: 26	correct: 24	acc: 92.3%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 14	acc: 93.3%
* class: 15 (globe-flower)	total: 13	correct: 11	acc: 84.6%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 18	acc: 72.0%
* class: 18 (balloon flower)	total: 15	correct: 11	acc: 73.3%
* class: 19 (giant white arum lily)	total: 17	correct: 16	acc: 94.1%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 23	acc: 88.5%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 20	acc: 90.9%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 19	acc: 50.0%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 3	acc: 25.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 46	acc: 59.7%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 13	acc: 81.2%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 30	acc: 96.8%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 6	acc: 37.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 18	acc: 94.7%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 34	acc: 94.4%
* class: 75 (morning glory)	total: 32	correct: 28	acc: 87.5%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 37	acc: 94.9%
* class: 83 (columbine)	total: 26	correct: 25	acc: 96.2%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 15	acc: 83.3%
* class: 87 (cyclamen)	total: 46	correct: 30	acc: 65.2%
* class: 88 (watercress)	total: 55	correct: 9	acc: 16.4%
* class: 89 (canna lily)	total: 25	correct: 23	acc: 92.0%
* class: 90 (hippeastrum)	total: 23	correct: 18	acc: 78.3%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 14	acc: 100.0%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 34	acc: 89.5%
* class: 95 (camellia)	total: 27	correct: 21	acc: 77.8%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 23	acc: 92.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 14	acc: 82.4%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 88.6%
Elapsed: 0:27:57
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_0-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.267 (1.092) data 0.000 (0.342) loss 3.8032 (4.0671) acc 21.8750 (10.6250) lr 1.0000e-05 eta 0:46:20
epoch [1/50] batch [10/51] time 0.266 (0.678) data 0.000 (0.171) loss 3.7078 (4.0052) acc 18.7500 (14.3750) lr 1.0000e-05 eta 0:28:42
epoch [1/50] batch [15/51] time 0.265 (0.539) data 0.000 (0.114) loss 3.2408 (3.8961) acc 31.2500 (16.2500) lr 1.0000e-05 eta 0:22:47
epoch [1/50] batch [20/51] time 0.259 (0.470) data 0.000 (0.086) loss 3.0952 (3.7764) acc 31.2500 (17.8125) lr 1.0000e-05 eta 0:19:48
epoch [1/50] batch [25/51] time 0.262 (0.428) data 0.000 (0.069) loss 2.9830 (3.5855) acc 40.6250 (22.5000) lr 1.0000e-05 eta 0:18:01
epoch [1/50] batch [30/51] time 0.259 (0.400) data 0.000 (0.057) loss 2.8647 (3.4589) acc 53.1250 (26.3542) lr 1.0000e-05 eta 0:16:48
epoch [1/50] batch [35/51] time 0.270 (0.381) data 0.000 (0.049) loss 2.3564 (3.3697) acc 40.6250 (28.3036) lr 1.0000e-05 eta 0:15:57
epoch [1/50] batch [40/51] time 0.261 (0.366) data 0.000 (0.043) loss 2.2144 (3.2327) acc 50.0000 (30.7031) lr 1.0000e-05 eta 0:15:19
epoch [1/50] batch [45/51] time 0.259 (0.354) data 0.000 (0.038) loss 2.0498 (3.1222) acc 59.3750 (32.9167) lr 1.0000e-05 eta 0:14:47
epoch [1/50] batch [50/51] time 0.257 (0.345) data 0.000 (0.034) loss 2.4257 (3.0322) acc 50.0000 (35.2500) lr 1.0000e-05 eta 0:14:21
epoch [2/50] batch [5/51] time 0.264 (0.524) data 0.000 (0.236) loss 1.5365 (2.0711) acc 65.6250 (54.3750) lr 2.0000e-03 eta 0:21:46
epoch [2/50] batch [10/51] time 0.264 (0.398) data 0.000 (0.118) loss 1.5394 (1.8960) acc 59.3750 (57.1875) lr 2.0000e-03 eta 0:16:30
epoch [2/50] batch [15/51] time 0.259 (0.354) data 0.000 (0.079) loss 1.3894 (1.8463) acc 62.5000 (58.3333) lr 2.0000e-03 eta 0:14:40
epoch [2/50] batch [20/51] time 0.285 (0.333) data 0.000 (0.059) loss 1.4523 (1.8111) acc 62.5000 (59.2188) lr 2.0000e-03 eta 0:13:45
epoch [2/50] batch [25/51] time 0.261 (0.319) data 0.000 (0.047) loss 1.0947 (1.7595) acc 71.8750 (60.1250) lr 2.0000e-03 eta 0:13:10
epoch [2/50] batch [30/51] time 0.269 (0.311) data 0.000 (0.040) loss 1.9644 (1.7679) acc 68.7500 (60.5208) lr 2.0000e-03 eta 0:12:47
epoch [2/50] batch [35/51] time 0.263 (0.304) data 0.000 (0.034) loss 1.2025 (1.7364) acc 71.8750 (61.5179) lr 2.0000e-03 eta 0:12:29
epoch [2/50] batch [40/51] time 0.258 (0.299) data 0.000 (0.030) loss 1.6446 (1.7160) acc 59.3750 (62.1094) lr 2.0000e-03 eta 0:12:16
epoch [2/50] batch [45/51] time 0.258 (0.295) data 0.000 (0.026) loss 1.3664 (1.6730) acc 62.5000 (62.5000) lr 2.0000e-03 eta 0:12:03
epoch [2/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.024) loss 1.4513 (1.6212) acc 65.6250 (63.6875) lr 2.0000e-03 eta 0:11:53
epoch [3/50] batch [5/51] time 0.299 (0.562) data 0.000 (0.271) loss 0.9442 (1.2391) acc 84.3750 (73.7500) lr 1.9980e-03 eta 0:22:52
epoch [3/50] batch [10/51] time 0.272 (0.414) data 0.000 (0.136) loss 1.6322 (1.3388) acc 56.2500 (68.7500) lr 1.9980e-03 eta 0:16:48
epoch [3/50] batch [15/51] time 0.263 (0.369) data 0.000 (0.092) loss 1.2787 (1.3416) acc 75.0000 (68.1250) lr 1.9980e-03 eta 0:14:57
epoch [3/50] batch [20/51] time 0.260 (0.344) data 0.000 (0.069) loss 1.2422 (1.3073) acc 71.8750 (68.5938) lr 1.9980e-03 eta 0:13:55
epoch [3/50] batch [25/51] time 0.294 (0.330) data 0.000 (0.055) loss 1.3662 (1.3022) acc 68.7500 (68.8750) lr 1.9980e-03 eta 0:13:19
epoch [3/50] batch [30/51] time 0.260 (0.319) data 0.000 (0.046) loss 0.9536 (1.2673) acc 78.1250 (69.4792) lr 1.9980e-03 eta 0:12:51
epoch [3/50] batch [35/51] time 0.260 (0.311) data 0.000 (0.039) loss 1.3759 (1.2429) acc 75.0000 (69.9107) lr 1.9980e-03 eta 0:12:30
epoch [3/50] batch [40/51] time 0.260 (0.305) data 0.000 (0.034) loss 1.3133 (1.2368) acc 71.8750 (70.0781) lr 1.9980e-03 eta 0:12:13
epoch [3/50] batch [45/51] time 0.262 (0.300) data 0.000 (0.031) loss 0.8789 (1.2244) acc 81.2500 (70.4167) lr 1.9980e-03 eta 0:12:00
epoch [3/50] batch [50/51] time 0.257 (0.296) data 0.000 (0.028) loss 0.9578 (1.2153) acc 68.7500 (70.6250) lr 1.9980e-03 eta 0:11:49
epoch [4/50] batch [5/51] time 0.292 (0.575) data 0.001 (0.288) loss 0.6924 (0.9210) acc 87.5000 (80.0000) lr 1.9921e-03 eta 0:22:54
epoch [4/50] batch [10/51] time 0.260 (0.421) data 0.000 (0.144) loss 1.0040 (0.9239) acc 75.0000 (78.4375) lr 1.9921e-03 eta 0:16:44
epoch [4/50] batch [15/51] time 0.261 (0.370) data 0.000 (0.096) loss 1.0244 (0.9625) acc 71.8750 (75.6250) lr 1.9921e-03 eta 0:14:41
epoch [4/50] batch [20/51] time 0.259 (0.343) data 0.000 (0.072) loss 1.0437 (1.0107) acc 68.7500 (74.5312) lr 1.9921e-03 eta 0:13:36
epoch [4/50] batch [25/51] time 0.263 (0.328) data 0.000 (0.058) loss 0.8717 (1.0002) acc 81.2500 (74.6250) lr 1.9921e-03 eta 0:12:57
epoch [4/50] batch [30/51] time 0.259 (0.317) data 0.000 (0.048) loss 1.1576 (1.0104) acc 65.6250 (74.0625) lr 1.9921e-03 eta 0:12:29
epoch [4/50] batch [35/51] time 0.271 (0.309) data 0.000 (0.041) loss 1.1537 (0.9999) acc 81.2500 (74.8214) lr 1.9921e-03 eta 0:12:09
epoch [4/50] batch [40/51] time 0.258 (0.303) data 0.000 (0.036) loss 0.8446 (0.9934) acc 81.2500 (75.1562) lr 1.9921e-03 eta 0:11:53
epoch [4/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.032) loss 0.8972 (0.9899) acc 71.8750 (75.0694) lr 1.9921e-03 eta 0:11:39
epoch [4/50] batch [50/51] time 0.259 (0.294) data 0.000 (0.029) loss 0.8816 (0.9884) acc 81.2500 (74.7500) lr 1.9921e-03 eta 0:11:29
epoch [5/50] batch [5/51] time 0.266 (0.528) data 0.000 (0.247) loss 0.8057 (0.9112) acc 84.3750 (76.8750) lr 1.9823e-03 eta 0:20:36
epoch [5/50] batch [10/51] time 0.259 (0.395) data 0.000 (0.124) loss 0.5993 (0.8322) acc 87.5000 (79.6875) lr 1.9823e-03 eta 0:15:22
epoch [5/50] batch [15/51] time 0.259 (0.350) data 0.000 (0.082) loss 0.4498 (0.7940) acc 90.6250 (80.4167) lr 1.9823e-03 eta 0:13:35
epoch [5/50] batch [20/51] time 0.268 (0.328) data 0.000 (0.062) loss 0.5685 (0.8193) acc 87.5000 (80.1562) lr 1.9823e-03 eta 0:12:43
epoch [5/50] batch [25/51] time 0.270 (0.315) data 0.000 (0.050) loss 0.7464 (0.8289) acc 84.3750 (79.7500) lr 1.9823e-03 eta 0:12:12
epoch [5/50] batch [30/51] time 0.262 (0.306) data 0.000 (0.041) loss 0.9581 (0.8407) acc 78.1250 (79.1667) lr 1.9823e-03 eta 0:11:49
epoch [5/50] batch [35/51] time 0.261 (0.301) data 0.000 (0.035) loss 0.5444 (0.8165) acc 93.7500 (80.6250) lr 1.9823e-03 eta 0:11:34
epoch [5/50] batch [40/51] time 0.259 (0.296) data 0.000 (0.031) loss 0.6629 (0.8038) acc 84.3750 (81.3281) lr 1.9823e-03 eta 0:11:21
epoch [5/50] batch [45/51] time 0.257 (0.291) data 0.000 (0.028) loss 1.0427 (0.8117) acc 68.7500 (80.6250) lr 1.9823e-03 eta 0:11:10
epoch [5/50] batch [50/51] time 0.256 (0.288) data 0.000 (0.025) loss 0.6518 (0.8065) acc 78.1250 (80.6250) lr 1.9823e-03 eta 0:11:00
epoch [6/50] batch [5/51] time 0.271 (0.538) data 0.000 (0.273) loss 0.7340 (0.7858) acc 81.2500 (80.6250) lr 1.9686e-03 eta 0:20:33
epoch [6/50] batch [10/51] time 0.260 (0.399) data 0.000 (0.137) loss 0.7220 (0.7196) acc 81.2500 (83.7500) lr 1.9686e-03 eta 0:15:11
epoch [6/50] batch [15/51] time 0.259 (0.354) data 0.000 (0.091) loss 0.8556 (0.7316) acc 84.3750 (82.2917) lr 1.9686e-03 eta 0:13:26
epoch [6/50] batch [20/51] time 0.272 (0.331) data 0.000 (0.068) loss 0.7789 (0.7456) acc 84.3750 (81.5625) lr 1.9686e-03 eta 0:12:32
epoch [6/50] batch [25/51] time 0.261 (0.317) data 0.000 (0.055) loss 0.6669 (0.7245) acc 90.6250 (82.2500) lr 1.9686e-03 eta 0:11:59
epoch [6/50] batch [30/51] time 0.260 (0.308) data 0.000 (0.046) loss 0.5964 (0.7084) acc 81.2500 (82.3958) lr 1.9686e-03 eta 0:11:37
epoch [6/50] batch [35/51] time 0.259 (0.302) data 0.000 (0.039) loss 0.5983 (0.7033) acc 87.5000 (82.5893) lr 1.9686e-03 eta 0:11:21
epoch [6/50] batch [40/51] time 0.259 (0.297) data 0.000 (0.034) loss 0.6642 (0.7148) acc 84.3750 (81.7969) lr 1.9686e-03 eta 0:11:08
epoch [6/50] batch [45/51] time 0.260 (0.292) data 0.000 (0.031) loss 0.4375 (0.6988) acc 93.7500 (82.4306) lr 1.9686e-03 eta 0:10:57
epoch [6/50] batch [50/51] time 0.259 (0.289) data 0.000 (0.027) loss 0.6458 (0.6932) acc 81.2500 (82.7500) lr 1.9686e-03 eta 0:10:48
epoch [7/50] batch [5/51] time 0.267 (0.570) data 0.000 (0.295) loss 0.5851 (0.6253) acc 90.6250 (87.5000) lr 1.9511e-03 eta 0:21:15
epoch [7/50] batch [10/51] time 0.271 (0.418) data 0.000 (0.148) loss 0.4433 (0.6048) acc 84.3750 (85.0000) lr 1.9511e-03 eta 0:15:33
epoch [7/50] batch [15/51] time 0.259 (0.365) data 0.000 (0.098) loss 0.6344 (0.6068) acc 87.5000 (85.4167) lr 1.9511e-03 eta 0:13:34
epoch [7/50] batch [20/51] time 0.259 (0.340) data 0.000 (0.074) loss 0.8237 (0.6171) acc 78.1250 (85.4688) lr 1.9511e-03 eta 0:12:35
epoch [7/50] batch [25/51] time 0.258 (0.324) data 0.000 (0.059) loss 0.5291 (0.6103) acc 87.5000 (85.3750) lr 1.9511e-03 eta 0:11:59
epoch [7/50] batch [30/51] time 0.262 (0.314) data 0.000 (0.049) loss 0.6611 (0.5954) acc 84.3750 (85.5208) lr 1.9511e-03 eta 0:11:34
epoch [7/50] batch [35/51] time 0.262 (0.306) data 0.000 (0.042) loss 0.7158 (0.6076) acc 84.3750 (85.4464) lr 1.9511e-03 eta 0:11:16
epoch [7/50] batch [40/51] time 0.257 (0.300) data 0.000 (0.037) loss 0.3592 (0.6007) acc 96.8750 (85.5469) lr 1.9511e-03 eta 0:11:01
epoch [7/50] batch [45/51] time 0.257 (0.295) data 0.000 (0.033) loss 1.1247 (0.6028) acc 71.8750 (85.6250) lr 1.9511e-03 eta 0:10:49
epoch [7/50] batch [50/51] time 0.257 (0.292) data 0.000 (0.030) loss 0.7772 (0.6015) acc 78.1250 (85.5625) lr 1.9511e-03 eta 0:10:39
epoch [8/50] batch [5/51] time 0.273 (0.559) data 0.000 (0.285) loss 0.5179 (0.4235) acc 87.5000 (92.5000) lr 1.9298e-03 eta 0:20:23
epoch [8/50] batch [10/51] time 0.259 (0.409) data 0.000 (0.143) loss 0.4029 (0.4352) acc 93.7500 (91.8750) lr 1.9298e-03 eta 0:14:53
epoch [8/50] batch [15/51] time 0.260 (0.360) data 0.000 (0.095) loss 0.4981 (0.4406) acc 93.7500 (92.0833) lr 1.9298e-03 eta 0:13:04
epoch [8/50] batch [20/51] time 0.263 (0.336) data 0.000 (0.071) loss 0.5926 (0.4802) acc 84.3750 (91.2500) lr 1.9298e-03 eta 0:12:09
epoch [8/50] batch [25/51] time 0.276 (0.323) data 0.000 (0.057) loss 0.4276 (0.4715) acc 90.6250 (91.0000) lr 1.9298e-03 eta 0:11:39
epoch [8/50] batch [30/51] time 0.261 (0.313) data 0.000 (0.048) loss 0.4978 (0.4797) acc 87.5000 (90.6250) lr 1.9298e-03 eta 0:11:18
epoch [8/50] batch [35/51] time 0.260 (0.306) data 0.000 (0.041) loss 0.5494 (0.4963) acc 87.5000 (89.9107) lr 1.9298e-03 eta 0:11:00
epoch [8/50] batch [40/51] time 0.260 (0.301) data 0.000 (0.036) loss 0.3045 (0.4892) acc 93.7500 (90.1562) lr 1.9298e-03 eta 0:10:48
epoch [8/50] batch [45/51] time 0.258 (0.296) data 0.000 (0.032) loss 0.7288 (0.4884) acc 78.1250 (89.9306) lr 1.9298e-03 eta 0:10:36
epoch [8/50] batch [50/51] time 0.259 (0.293) data 0.000 (0.029) loss 0.7101 (0.4945) acc 87.5000 (89.8750) lr 1.9298e-03 eta 0:10:27
epoch [9/50] batch [5/51] time 0.271 (0.553) data 0.000 (0.271) loss 0.4166 (0.4027) acc 90.6250 (92.5000) lr 1.9048e-03 eta 0:19:41
epoch [9/50] batch [10/51] time 0.269 (0.409) data 0.000 (0.136) loss 0.5369 (0.4487) acc 93.7500 (91.5625) lr 1.9048e-03 eta 0:14:31
epoch [9/50] batch [15/51] time 0.262 (0.360) data 0.000 (0.091) loss 0.4567 (0.4815) acc 93.7500 (90.4167) lr 1.9048e-03 eta 0:12:45
epoch [9/50] batch [20/51] time 0.261 (0.337) data 0.000 (0.068) loss 0.4289 (0.4549) acc 90.6250 (91.2500) lr 1.9048e-03 eta 0:11:54
epoch [9/50] batch [25/51] time 0.260 (0.321) data 0.000 (0.054) loss 0.4286 (0.4384) acc 87.5000 (91.6250) lr 1.9048e-03 eta 0:11:20
epoch [9/50] batch [30/51] time 0.262 (0.312) data 0.000 (0.045) loss 0.2680 (0.4377) acc 96.8750 (91.8750) lr 1.9048e-03 eta 0:10:58
epoch [9/50] batch [35/51] time 0.260 (0.305) data 0.000 (0.039) loss 0.3564 (0.4378) acc 93.7500 (91.3393) lr 1.9048e-03 eta 0:10:43
epoch [9/50] batch [40/51] time 0.257 (0.299) data 0.000 (0.034) loss 0.4998 (0.4455) acc 93.7500 (90.7812) lr 1.9048e-03 eta 0:10:29
epoch [9/50] batch [45/51] time 0.257 (0.295) data 0.000 (0.030) loss 0.2239 (0.4447) acc 96.8750 (90.9028) lr 1.9048e-03 eta 0:10:17
epoch [9/50] batch [50/51] time 0.257 (0.291) data 0.000 (0.027) loss 0.4358 (0.4390) acc 90.6250 (91.0000) lr 1.9048e-03 eta 0:10:08
epoch [10/50] batch [5/51] time 0.268 (0.550) data 0.000 (0.281) loss 0.4527 (0.4491) acc 93.7500 (94.3750) lr 1.8763e-03 eta 0:19:07
epoch [10/50] batch [10/51] time 0.270 (0.407) data 0.000 (0.141) loss 0.4084 (0.3936) acc 93.7500 (95.0000) lr 1.8763e-03 eta 0:14:07
epoch [10/50] batch [15/51] time 0.263 (0.360) data 0.000 (0.094) loss 0.3860 (0.4244) acc 93.7500 (92.9167) lr 1.8763e-03 eta 0:12:27
epoch [10/50] batch [20/51] time 0.267 (0.337) data 0.000 (0.070) loss 0.3972 (0.4083) acc 96.8750 (93.1250) lr 1.8763e-03 eta 0:11:37
epoch [10/50] batch [25/51] time 0.260 (0.322) data 0.000 (0.056) loss 0.3403 (0.4046) acc 90.6250 (93.0000) lr 1.8763e-03 eta 0:11:04
epoch [10/50] batch [30/51] time 0.263 (0.313) data 0.000 (0.047) loss 0.6199 (0.4075) acc 84.3750 (93.2292) lr 1.8763e-03 eta 0:10:44
epoch [10/50] batch [35/51] time 0.264 (0.306) data 0.000 (0.040) loss 0.3371 (0.3976) acc 90.6250 (92.9464) lr 1.8763e-03 eta 0:10:28
epoch [10/50] batch [40/51] time 0.258 (0.300) data 0.000 (0.035) loss 0.4130 (0.4075) acc 93.7500 (92.6562) lr 1.8763e-03 eta 0:10:15
epoch [10/50] batch [45/51] time 0.256 (0.295) data 0.000 (0.031) loss 0.5617 (0.4025) acc 90.6250 (92.7778) lr 1.8763e-03 eta 0:10:03
epoch [10/50] batch [50/51] time 0.256 (0.291) data 0.000 (0.028) loss 0.6586 (0.4099) acc 81.2500 (92.3750) lr 1.8763e-03 eta 0:09:54
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.235  alpha2: -0.116 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.06 <<<
epoch [11/50] batch [5/51] time 0.166 (0.834) data 0.001 (0.296) loss 0.5017 (0.6896) acc 90.7609 (85.8719) lr 1.8443e-03 eta 0:28:16
epoch [11/50] batch [10/51] time 0.165 (0.685) data 0.000 (0.148) loss 0.6424 (0.6255) acc 87.5000 (86.8459) lr 1.8443e-03 eta 0:23:09
epoch [11/50] batch [15/51] time 0.885 (0.645) data 0.000 (0.099) loss 0.5415 (0.6337) acc 90.8654 (86.1355) lr 1.8443e-03 eta 0:21:45
epoch [11/50] batch [20/51] time 0.173 (0.558) data 0.000 (0.074) loss 0.6964 (0.6400) acc 82.6923 (85.9175) lr 1.8443e-03 eta 0:18:47
epoch [11/50] batch [25/51] time 0.155 (0.480) data 0.000 (0.059) loss 0.7194 (0.6567) acc 84.5238 (85.2635) lr 1.8443e-03 eta 0:16:06
epoch [11/50] batch [30/51] time 0.164 (0.452) data 0.000 (0.049) loss 0.5194 (0.6617) acc 91.6667 (85.0438) lr 1.8443e-03 eta 0:15:08
epoch [11/50] batch [35/51] time 0.162 (0.411) data 0.000 (0.042) loss 0.5065 (0.6615) acc 90.4255 (85.0030) lr 1.8443e-03 eta 0:13:44
epoch [11/50] batch [40/51] time 0.870 (0.399) data 0.000 (0.037) loss 0.7946 (0.6694) acc 81.8627 (84.8368) lr 1.8443e-03 eta 0:13:18
epoch [11/50] batch [45/51] time 0.167 (0.374) data 0.000 (0.033) loss 0.6830 (0.6625) acc 92.3913 (85.1936) lr 1.8443e-03 eta 0:12:25
epoch [11/50] batch [50/51] time 0.187 (0.353) data 0.000 (0.030) loss 0.6723 (0.6654) acc 84.9057 (85.0686) lr 1.8443e-03 eta 0:11:43
>>> alpha1: 0.255  alpha2: -0.086 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [12/50] batch [5/51] time 0.171 (0.753) data 0.000 (0.267) loss 0.7418 (0.6685) acc 79.9020 (83.6954) lr 1.8090e-03 eta 0:24:54
epoch [12/50] batch [10/51] time 0.189 (0.464) data 0.000 (0.133) loss 0.5280 (0.6363) acc 90.3846 (84.5587) lr 1.8090e-03 eta 0:15:19
epoch [12/50] batch [15/51] time 0.171 (0.370) data 0.000 (0.089) loss 0.6476 (0.6112) acc 86.5000 (85.3379) lr 1.8090e-03 eta 0:12:09
epoch [12/50] batch [20/51] time 0.164 (0.321) data 0.000 (0.067) loss 0.8465 (0.6218) acc 81.9149 (85.6723) lr 1.8090e-03 eta 0:10:32
epoch [12/50] batch [25/51] time 0.184 (0.293) data 0.000 (0.054) loss 0.4931 (0.6129) acc 89.9038 (85.5099) lr 1.8090e-03 eta 0:09:35
epoch [12/50] batch [30/51] time 0.172 (0.273) data 0.000 (0.045) loss 0.7674 (0.6402) acc 83.0000 (84.7045) lr 1.8090e-03 eta 0:08:54
epoch [12/50] batch [35/51] time 0.175 (0.259) data 0.000 (0.038) loss 0.7629 (0.7068) acc 79.9020 (83.6510) lr 1.8090e-03 eta 0:08:25
epoch [12/50] batch [40/51] time 0.183 (0.268) data 0.001 (0.034) loss 0.5017 (0.6813) acc 89.6226 (84.2378) lr 1.8090e-03 eta 0:08:41
epoch [12/50] batch [45/51] time 0.168 (0.257) data 0.000 (0.030) loss 0.6114 (0.6722) acc 83.1633 (84.4730) lr 1.8090e-03 eta 0:08:18
epoch [12/50] batch [50/51] time 0.171 (0.263) data 0.000 (0.027) loss 0.3816 (0.6605) acc 91.1765 (84.7982) lr 1.8090e-03 eta 0:08:29
>>> alpha1: 0.252  alpha2: -0.089 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [13/50] batch [5/51] time 0.197 (0.490) data 0.000 (0.315) loss 0.5796 (0.5633) acc 85.7143 (85.5959) lr 1.7705e-03 eta 0:15:47
epoch [13/50] batch [10/51] time 0.182 (0.334) data 0.000 (0.158) loss 0.5580 (0.6271) acc 83.0000 (84.2862) lr 1.7705e-03 eta 0:10:43
epoch [13/50] batch [15/51] time 0.175 (0.280) data 0.000 (0.105) loss 0.6176 (0.6157) acc 83.0000 (84.3807) lr 1.7705e-03 eta 0:08:58
epoch [13/50] batch [20/51] time 0.171 (0.253) data 0.001 (0.079) loss 0.6416 (0.5884) acc 82.8125 (84.9680) lr 1.7705e-03 eta 0:08:05
epoch [13/50] batch [25/51] time 0.164 (0.237) data 0.000 (0.063) loss 0.7838 (0.5849) acc 77.1277 (85.0751) lr 1.7705e-03 eta 0:07:33
epoch [13/50] batch [30/51] time 0.196 (0.228) data 0.000 (0.053) loss 0.6654 (0.5831) acc 86.0577 (85.3428) lr 1.7705e-03 eta 0:07:14
epoch [13/50] batch [35/51] time 0.190 (0.221) data 0.000 (0.045) loss 0.7044 (0.5956) acc 77.8846 (85.0295) lr 1.7705e-03 eta 0:06:59
epoch [13/50] batch [40/51] time 0.160 (0.214) data 0.000 (0.040) loss 0.6073 (0.5947) acc 87.5000 (85.2317) lr 1.7705e-03 eta 0:06:46
epoch [13/50] batch [45/51] time 0.171 (0.210) data 0.000 (0.035) loss 0.6606 (0.5891) acc 82.8431 (85.5612) lr 1.7705e-03 eta 0:06:36
epoch [13/50] batch [50/51] time 0.184 (0.206) data 0.000 (0.032) loss 0.4999 (0.5926) acc 87.7358 (85.5064) lr 1.7705e-03 eta 0:06:29
>>> alpha1: 0.245  alpha2: -0.083 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [14/50] batch [5/51] time 0.194 (0.508) data 0.015 (0.325) loss 0.5764 (0.5952) acc 88.6792 (87.6364) lr 1.7290e-03 eta 0:15:55
epoch [14/50] batch [10/51] time 0.176 (0.343) data 0.000 (0.162) loss 0.5002 (0.5829) acc 90.6250 (87.3394) lr 1.7290e-03 eta 0:10:43
epoch [14/50] batch [15/51] time 0.178 (0.288) data 0.000 (0.108) loss 0.5814 (0.5823) acc 85.6383 (87.1074) lr 1.7290e-03 eta 0:08:58
epoch [14/50] batch [20/51] time 0.160 (0.257) data 0.000 (0.081) loss 0.8141 (0.5777) acc 82.6087 (87.1106) lr 1.7290e-03 eta 0:08:00
epoch [14/50] batch [25/51] time 0.184 (0.243) data 0.000 (0.065) loss 0.5848 (0.5731) acc 84.9057 (86.8808) lr 1.7290e-03 eta 0:07:31
epoch [14/50] batch [30/51] time 0.174 (0.257) data 0.000 (0.054) loss 0.5038 (0.5663) acc 87.7451 (87.1937) lr 1.7290e-03 eta 0:07:56
epoch [14/50] batch [35/51] time 0.172 (0.246) data 0.000 (0.047) loss 0.6321 (0.5628) acc 84.8039 (87.3307) lr 1.7290e-03 eta 0:07:34
epoch [14/50] batch [40/51] time 0.160 (0.236) data 0.000 (0.041) loss 0.8035 (0.5608) acc 82.0652 (87.3175) lr 1.7290e-03 eta 0:07:15
epoch [14/50] batch [45/51] time 0.173 (0.228) data 0.000 (0.036) loss 0.6068 (0.5578) acc 89.4231 (87.4774) lr 1.7290e-03 eta 0:07:00
epoch [14/50] batch [50/51] time 0.164 (0.223) data 0.000 (0.033) loss 0.6092 (0.5618) acc 87.5000 (87.4069) lr 1.7290e-03 eta 0:06:48
>>> alpha1: 0.233  alpha2: -0.084 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.10 <<<
epoch [15/50] batch [5/51] time 0.180 (0.469) data 0.016 (0.293) loss 0.4554 (0.4941) acc 90.2174 (88.2023) lr 1.6845e-03 eta 0:14:18
epoch [15/50] batch [10/51] time 0.169 (0.322) data 0.000 (0.146) loss 0.5976 (0.5515) acc 84.0000 (87.4528) lr 1.6845e-03 eta 0:09:47
epoch [15/50] batch [15/51] time 0.171 (0.272) data 0.000 (0.098) loss 0.4560 (0.5688) acc 90.5556 (87.0832) lr 1.6845e-03 eta 0:08:15
epoch [15/50] batch [20/51] time 0.165 (0.247) data 0.000 (0.073) loss 0.5597 (0.5618) acc 87.5000 (87.2331) lr 1.6845e-03 eta 0:07:28
epoch [15/50] batch [25/51] time 0.169 (0.232) data 0.000 (0.059) loss 0.3463 (0.5368) acc 95.0000 (87.7517) lr 1.6845e-03 eta 0:06:59
epoch [15/50] batch [30/51] time 0.177 (0.223) data 0.000 (0.049) loss 0.4959 (0.5381) acc 86.7924 (87.6669) lr 1.6845e-03 eta 0:06:42
epoch [15/50] batch [35/51] time 0.179 (0.216) data 0.001 (0.042) loss 0.7420 (0.5415) acc 87.5000 (87.8671) lr 1.6845e-03 eta 0:06:29
epoch [15/50] batch [40/51] time 0.190 (0.214) data 0.001 (0.037) loss 0.5704 (0.5537) acc 84.1346 (87.3170) lr 1.6845e-03 eta 0:06:23
epoch [15/50] batch [45/51] time 0.171 (0.210) data 0.001 (0.033) loss 0.6690 (0.5553) acc 90.0000 (87.5549) lr 1.6845e-03 eta 0:06:16
epoch [15/50] batch [50/51] time 0.174 (0.207) data 0.000 (0.030) loss 0.5079 (0.5533) acc 86.5385 (87.4938) lr 1.6845e-03 eta 0:06:09
>>> alpha1: 0.228  alpha2: -0.089 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [16/50] batch [5/51] time 0.198 (0.519) data 0.001 (0.332) loss 0.4421 (0.4685) acc 91.0377 (91.0414) lr 1.6374e-03 eta 0:15:24
epoch [16/50] batch [10/51] time 0.165 (0.350) data 0.001 (0.166) loss 0.4820 (0.4679) acc 90.1042 (90.5349) lr 1.6374e-03 eta 0:10:20
epoch [16/50] batch [15/51] time 0.186 (0.291) data 0.001 (0.111) loss 0.4822 (0.5111) acc 86.7647 (88.6992) lr 1.6374e-03 eta 0:08:35
epoch [16/50] batch [20/51] time 0.160 (0.261) data 0.000 (0.083) loss 0.4537 (0.5172) acc 90.5556 (88.2501) lr 1.6374e-03 eta 0:07:40
epoch [16/50] batch [25/51] time 0.172 (0.246) data 0.000 (0.067) loss 0.4135 (0.4977) acc 91.6667 (88.6333) lr 1.6374e-03 eta 0:07:13
epoch [16/50] batch [30/51] time 0.181 (0.236) data 0.000 (0.056) loss 0.5263 (0.4965) acc 88.7755 (88.5735) lr 1.6374e-03 eta 0:06:53
epoch [16/50] batch [35/51] time 0.187 (0.227) data 0.000 (0.048) loss 0.3943 (0.5074) acc 91.2281 (88.1646) lr 1.6374e-03 eta 0:06:37
epoch [16/50] batch [40/51] time 0.167 (0.220) data 0.000 (0.042) loss 0.6124 (0.5188) acc 88.7755 (87.9769) lr 1.6374e-03 eta 0:06:23
epoch [16/50] batch [45/51] time 0.166 (0.214) data 0.000 (0.037) loss 0.5495 (0.5185) acc 85.7143 (87.9738) lr 1.6374e-03 eta 0:06:13
epoch [16/50] batch [50/51] time 0.166 (0.210) data 0.000 (0.034) loss 0.5022 (0.5185) acc 86.2245 (87.9389) lr 1.6374e-03 eta 0:06:03
>>> alpha1: 0.215  alpha2: -0.130 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [17/50] batch [5/51] time 0.176 (0.490) data 0.000 (0.311) loss 0.6188 (0.5249) acc 87.0192 (88.0562) lr 1.5878e-03 eta 0:14:06
epoch [17/50] batch [10/51] time 0.185 (0.333) data 0.000 (0.156) loss 0.5020 (0.5388) acc 83.3333 (86.8454) lr 1.5878e-03 eta 0:09:34
epoch [17/50] batch [15/51] time 0.171 (0.282) data 0.000 (0.104) loss 0.4993 (0.5143) acc 85.7843 (87.4498) lr 1.5878e-03 eta 0:08:03
epoch [17/50] batch [20/51] time 0.174 (0.255) data 0.000 (0.078) loss 0.3019 (0.4866) acc 94.7115 (88.3005) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [25/51] time 0.172 (0.238) data 0.000 (0.062) loss 0.3638 (0.4869) acc 90.8163 (88.2014) lr 1.5878e-03 eta 0:06:47
epoch [17/50] batch [30/51] time 0.166 (0.228) data 0.000 (0.052) loss 0.7414 (0.5128) acc 76.0417 (87.5428) lr 1.5878e-03 eta 0:06:27
epoch [17/50] batch [35/51] time 0.186 (0.222) data 0.000 (0.045) loss 0.4465 (0.5072) acc 91.3462 (87.5726) lr 1.5878e-03 eta 0:06:16
epoch [17/50] batch [40/51] time 0.175 (0.217) data 0.000 (0.039) loss 0.4967 (0.5202) acc 91.1458 (87.4046) lr 1.5878e-03 eta 0:06:07
epoch [17/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.035) loss 0.6927 (0.5220) acc 81.9149 (87.4397) lr 1.5878e-03 eta 0:05:57
epoch [17/50] batch [50/51] time 0.165 (0.208) data 0.000 (0.031) loss 0.8449 (0.5356) acc 81.7708 (87.2190) lr 1.5878e-03 eta 0:05:49
>>> alpha1: 0.210  alpha2: -0.131 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [18/50] batch [5/51] time 0.172 (0.480) data 0.000 (0.308) loss 0.5091 (0.5859) acc 87.5000 (87.2180) lr 1.5358e-03 eta 0:13:26
epoch [18/50] batch [10/51] time 0.179 (0.327) data 0.000 (0.154) loss 0.5206 (0.5327) acc 87.7358 (87.8014) lr 1.5358e-03 eta 0:09:07
epoch [18/50] batch [15/51] time 0.168 (0.276) data 0.001 (0.103) loss 0.4506 (0.5012) acc 88.2653 (88.4417) lr 1.5358e-03 eta 0:07:40
epoch [18/50] batch [20/51] time 0.185 (0.251) data 0.000 (0.077) loss 0.5763 (0.5186) acc 85.5263 (87.7797) lr 1.5358e-03 eta 0:06:57
epoch [18/50] batch [25/51] time 0.184 (0.236) data 0.000 (0.062) loss 0.5761 (0.5162) acc 87.7358 (87.8767) lr 1.5358e-03 eta 0:06:31
epoch [18/50] batch [30/51] time 0.176 (0.226) data 0.000 (0.052) loss 0.3038 (0.5087) acc 93.3962 (88.1420) lr 1.5358e-03 eta 0:06:13
epoch [18/50] batch [35/51] time 0.161 (0.219) data 0.001 (0.044) loss 0.6365 (0.5146) acc 84.2391 (87.7894) lr 1.5358e-03 eta 0:06:00
epoch [18/50] batch [40/51] time 0.173 (0.214) data 0.000 (0.039) loss 0.6516 (0.5151) acc 85.5769 (87.9266) lr 1.5358e-03 eta 0:05:51
epoch [18/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.034) loss 0.5475 (0.5157) acc 80.0000 (87.7058) lr 1.5358e-03 eta 0:05:43
epoch [18/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.031) loss 0.6059 (0.5145) acc 87.5000 (87.7934) lr 1.5358e-03 eta 0:05:35
>>> alpha1: 0.205  alpha2: -0.142 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [19/50] batch [5/51] time 0.164 (0.512) data 0.000 (0.334) loss 0.6736 (0.5559) acc 81.3830 (86.2656) lr 1.4818e-03 eta 0:13:53
epoch [19/50] batch [10/51] time 0.171 (0.348) data 0.000 (0.167) loss 0.5408 (0.5370) acc 87.7551 (87.5152) lr 1.4818e-03 eta 0:09:24
epoch [19/50] batch [15/51] time 0.169 (0.290) data 0.000 (0.111) loss 0.6153 (0.5271) acc 85.2041 (87.8688) lr 1.4818e-03 eta 0:07:49
epoch [19/50] batch [20/51] time 0.166 (0.261) data 0.000 (0.084) loss 0.4939 (0.5255) acc 89.7959 (88.1479) lr 1.4818e-03 eta 0:07:01
epoch [19/50] batch [25/51] time 0.171 (0.244) data 0.000 (0.067) loss 0.4244 (0.5349) acc 91.6667 (87.9171) lr 1.4818e-03 eta 0:06:31
epoch [19/50] batch [30/51] time 0.170 (0.233) data 0.000 (0.056) loss 0.5848 (0.5193) acc 87.0000 (88.1560) lr 1.4818e-03 eta 0:06:12
epoch [19/50] batch [35/51] time 0.192 (0.225) data 0.000 (0.048) loss 0.3154 (0.5093) acc 92.2727 (88.2899) lr 1.4818e-03 eta 0:05:58
epoch [19/50] batch [40/51] time 0.172 (0.218) data 0.000 (0.042) loss 0.5642 (0.5154) acc 84.3137 (88.0648) lr 1.4818e-03 eta 0:05:46
epoch [19/50] batch [45/51] time 0.165 (0.213) data 0.000 (0.037) loss 0.4479 (0.5091) acc 89.7959 (88.0816) lr 1.4818e-03 eta 0:05:37
epoch [19/50] batch [50/51] time 0.164 (0.208) data 0.000 (0.034) loss 0.3690 (0.4984) acc 91.1458 (88.2577) lr 1.4818e-03 eta 0:05:29
>>> alpha1: 0.202  alpha2: -0.150 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [20/50] batch [5/51] time 0.207 (0.470) data 0.000 (0.280) loss 0.3934 (0.4763) acc 93.1818 (89.4890) lr 1.4258e-03 eta 0:12:20
epoch [20/50] batch [10/51] time 0.181 (0.326) data 0.000 (0.140) loss 0.3413 (0.4662) acc 88.8889 (89.3201) lr 1.4258e-03 eta 0:08:32
epoch [20/50] batch [15/51] time 0.173 (0.276) data 0.000 (0.094) loss 0.5892 (0.4849) acc 84.8039 (88.6223) lr 1.4258e-03 eta 0:07:12
epoch [20/50] batch [20/51] time 0.187 (0.252) data 0.000 (0.070) loss 0.4780 (0.5009) acc 89.2157 (88.2832) lr 1.4258e-03 eta 0:06:33
epoch [20/50] batch [25/51] time 0.206 (0.237) data 0.000 (0.056) loss 0.6987 (0.5108) acc 83.6735 (88.0253) lr 1.4258e-03 eta 0:06:09
epoch [20/50] batch [30/51] time 0.172 (0.227) data 0.000 (0.047) loss 0.4709 (0.5106) acc 87.7660 (87.7030) lr 1.4258e-03 eta 0:05:51
epoch [20/50] batch [35/51] time 0.171 (0.221) data 0.000 (0.040) loss 0.4706 (0.5030) acc 85.5000 (87.9206) lr 1.4258e-03 eta 0:05:41
epoch [20/50] batch [40/51] time 0.163 (0.214) data 0.000 (0.035) loss 0.5335 (0.4987) acc 89.5833 (88.0717) lr 1.4258e-03 eta 0:05:30
epoch [20/50] batch [45/51] time 0.163 (0.209) data 0.000 (0.031) loss 0.4572 (0.4942) acc 90.1042 (88.1621) lr 1.4258e-03 eta 0:05:20
epoch [20/50] batch [50/51] time 0.178 (0.205) data 0.000 (0.028) loss 0.4893 (0.4937) acc 90.7407 (88.2561) lr 1.4258e-03 eta 0:05:14
>>> alpha1: 0.198  alpha2: -0.154 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [21/50] batch [5/51] time 0.180 (0.475) data 0.000 (0.290) loss 0.3116 (0.4369) acc 93.7500 (90.9489) lr 1.3681e-03 eta 0:12:04
epoch [21/50] batch [10/51] time 0.171 (0.321) data 0.000 (0.145) loss 0.4404 (0.4575) acc 87.7451 (89.9893) lr 1.3681e-03 eta 0:08:08
epoch [21/50] batch [15/51] time 0.186 (0.274) data 0.001 (0.097) loss 0.3327 (0.4289) acc 93.5185 (90.7733) lr 1.3681e-03 eta 0:06:55
epoch [21/50] batch [20/51] time 0.181 (0.249) data 0.001 (0.073) loss 0.4935 (0.4475) acc 90.8163 (90.4698) lr 1.3681e-03 eta 0:06:16
epoch [21/50] batch [25/51] time 0.177 (0.234) data 0.000 (0.058) loss 0.7802 (0.4656) acc 82.0755 (89.5468) lr 1.3681e-03 eta 0:05:52
epoch [21/50] batch [30/51] time 0.174 (0.225) data 0.000 (0.049) loss 0.4920 (0.4644) acc 88.4615 (89.5115) lr 1.3681e-03 eta 0:05:37
epoch [21/50] batch [35/51] time 0.181 (0.218) data 0.000 (0.042) loss 0.5752 (0.4689) acc 82.3529 (89.1995) lr 1.3681e-03 eta 0:05:25
epoch [21/50] batch [40/51] time 0.170 (0.212) data 0.000 (0.037) loss 0.4705 (0.4695) acc 89.2157 (89.2738) lr 1.3681e-03 eta 0:05:15
epoch [21/50] batch [45/51] time 0.169 (0.207) data 0.000 (0.033) loss 0.4850 (0.4748) acc 87.7451 (89.2574) lr 1.3681e-03 eta 0:05:07
epoch [21/50] batch [50/51] time 0.172 (0.203) data 0.000 (0.029) loss 0.5197 (0.4726) acc 88.7255 (89.2173) lr 1.3681e-03 eta 0:05:00
>>> alpha1: 0.190  alpha2: -0.153 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [22/50] batch [5/51] time 0.169 (0.485) data 0.000 (0.310) loss 0.4625 (0.4544) acc 93.2292 (91.3090) lr 1.3090e-03 eta 0:11:54
epoch [22/50] batch [10/51] time 0.169 (0.330) data 0.000 (0.155) loss 0.5094 (0.4773) acc 90.5000 (89.8177) lr 1.3090e-03 eta 0:08:04
epoch [22/50] batch [15/51] time 0.173 (0.278) data 0.000 (0.103) loss 0.4398 (0.4819) acc 84.8039 (89.0183) lr 1.3090e-03 eta 0:06:47
epoch [22/50] batch [20/51] time 0.169 (0.252) data 0.001 (0.078) loss 0.2689 (0.4785) acc 95.0000 (89.1563) lr 1.3090e-03 eta 0:06:07
epoch [22/50] batch [25/51] time 0.182 (0.238) data 0.000 (0.062) loss 0.5584 (0.4796) acc 81.4815 (88.8229) lr 1.3090e-03 eta 0:05:46
epoch [22/50] batch [30/51] time 0.183 (0.228) data 0.000 (0.052) loss 0.4233 (0.4748) acc 93.7500 (89.2210) lr 1.3090e-03 eta 0:05:30
epoch [22/50] batch [35/51] time 0.166 (0.222) data 0.001 (0.045) loss 0.5074 (0.4823) acc 90.4255 (88.9854) lr 1.3090e-03 eta 0:05:19
epoch [22/50] batch [40/51] time 0.177 (0.216) data 0.000 (0.039) loss 0.2789 (0.4726) acc 95.2830 (89.2231) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [45/51] time 0.159 (0.211) data 0.000 (0.035) loss 0.3420 (0.4684) acc 92.9348 (89.2348) lr 1.3090e-03 eta 0:05:02
epoch [22/50] batch [50/51] time 0.175 (0.207) data 0.000 (0.031) loss 0.5346 (0.4656) acc 83.9623 (89.2554) lr 1.3090e-03 eta 0:04:55
>>> alpha1: 0.182  alpha2: -0.159 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [23/50] batch [5/51] time 0.196 (0.462) data 0.000 (0.281) loss 0.4429 (0.4041) acc 90.0943 (90.8742) lr 1.2487e-03 eta 0:10:57
epoch [23/50] batch [10/51] time 0.195 (0.320) data 0.000 (0.141) loss 0.3768 (0.4597) acc 88.7255 (88.7746) lr 1.2487e-03 eta 0:07:34
epoch [23/50] batch [15/51] time 0.181 (0.273) data 0.001 (0.094) loss 0.4620 (0.5999) acc 89.0625 (86.8922) lr 1.2487e-03 eta 0:06:25
epoch [23/50] batch [20/51] time 0.167 (0.247) data 0.001 (0.070) loss 0.6122 (0.5704) acc 83.6735 (87.1007) lr 1.2487e-03 eta 0:05:47
epoch [23/50] batch [25/51] time 0.173 (0.233) data 0.000 (0.056) loss 0.3686 (0.5275) acc 91.3462 (88.2216) lr 1.2487e-03 eta 0:05:27
epoch [23/50] batch [30/51] time 0.170 (0.224) data 0.000 (0.047) loss 0.5673 (0.5235) acc 85.7143 (88.2912) lr 1.2487e-03 eta 0:05:12
epoch [23/50] batch [35/51] time 0.170 (0.217) data 0.000 (0.040) loss 0.5345 (0.5208) acc 87.7551 (88.3329) lr 1.2487e-03 eta 0:05:02
epoch [23/50] batch [40/51] time 0.169 (0.213) data 0.001 (0.035) loss 0.3889 (0.5126) acc 90.5000 (88.5269) lr 1.2487e-03 eta 0:04:55
epoch [23/50] batch [45/51] time 0.176 (0.209) data 0.000 (0.032) loss 0.4989 (0.5026) acc 86.4583 (88.7377) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.028) loss 0.5343 (0.5088) acc 86.6667 (88.5854) lr 1.2487e-03 eta 0:04:42
>>> alpha1: 0.177  alpha2: -0.159 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [24/50] batch [5/51] time 0.199 (0.495) data 0.000 (0.307) loss 0.4020 (0.4363) acc 89.2857 (89.6186) lr 1.1874e-03 eta 0:11:18
epoch [24/50] batch [10/51] time 0.171 (0.336) data 0.000 (0.154) loss 0.4533 (0.4448) acc 86.2745 (89.1633) lr 1.1874e-03 eta 0:07:39
epoch [24/50] batch [15/51] time 0.172 (0.283) data 0.000 (0.103) loss 0.2729 (0.4308) acc 96.0784 (89.8813) lr 1.1874e-03 eta 0:06:25
epoch [24/50] batch [20/51] time 0.172 (0.256) data 0.000 (0.077) loss 0.4146 (0.4303) acc 93.0000 (90.2537) lr 1.1874e-03 eta 0:05:47
epoch [24/50] batch [25/51] time 0.180 (0.240) data 0.000 (0.062) loss 0.3194 (0.4288) acc 89.7959 (90.1310) lr 1.1874e-03 eta 0:05:24
epoch [24/50] batch [30/51] time 0.173 (0.229) data 0.001 (0.052) loss 0.3978 (0.4336) acc 93.1373 (90.2001) lr 1.1874e-03 eta 0:05:09
epoch [24/50] batch [35/51] time 0.175 (0.222) data 0.001 (0.044) loss 0.3813 (0.4365) acc 90.3846 (89.9479) lr 1.1874e-03 eta 0:04:58
epoch [24/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.039) loss 0.4884 (0.4433) acc 87.0000 (89.6251) lr 1.1874e-03 eta 0:04:49
epoch [24/50] batch [45/51] time 0.165 (0.211) data 0.000 (0.035) loss 0.5847 (0.4522) acc 89.0625 (89.4875) lr 1.1874e-03 eta 0:04:40
epoch [24/50] batch [50/51] time 0.168 (0.207) data 0.000 (0.031) loss 0.3682 (0.4476) acc 90.5000 (89.6713) lr 1.1874e-03 eta 0:04:34
>>> alpha1: 0.175  alpha2: -0.165 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [25/50] batch [5/51] time 0.185 (0.470) data 0.001 (0.293) loss 0.4605 (0.4820) acc 90.1961 (89.0536) lr 1.1253e-03 eta 0:10:20
epoch [25/50] batch [10/51] time 0.190 (0.325) data 0.000 (0.147) loss 0.3906 (0.4360) acc 90.0000 (89.5157) lr 1.1253e-03 eta 0:07:07
epoch [25/50] batch [15/51] time 0.185 (0.277) data 0.000 (0.098) loss 0.5307 (0.4601) acc 87.5000 (89.4030) lr 1.1253e-03 eta 0:06:02
epoch [25/50] batch [20/51] time 0.182 (0.251) data 0.000 (0.073) loss 0.2344 (0.4471) acc 96.4286 (89.5035) lr 1.1253e-03 eta 0:05:28
epoch [25/50] batch [25/51] time 0.170 (0.236) data 0.000 (0.059) loss 0.5281 (0.4534) acc 85.5000 (89.3110) lr 1.1253e-03 eta 0:05:07
epoch [25/50] batch [30/51] time 0.182 (0.228) data 0.000 (0.049) loss 0.6043 (0.4573) acc 85.4167 (89.1831) lr 1.1253e-03 eta 0:04:55
epoch [25/50] batch [35/51] time 0.178 (0.220) data 0.001 (0.042) loss 0.4786 (0.4587) acc 90.4255 (89.1332) lr 1.1253e-03 eta 0:04:44
epoch [25/50] batch [40/51] time 0.173 (0.215) data 0.000 (0.037) loss 0.4217 (0.4593) acc 87.5000 (88.9266) lr 1.1253e-03 eta 0:04:36
epoch [25/50] batch [45/51] time 0.182 (0.212) data 0.000 (0.033) loss 0.4848 (0.4715) acc 90.0000 (88.8428) lr 1.1253e-03 eta 0:04:31
epoch [25/50] batch [50/51] time 0.170 (0.208) data 0.000 (0.030) loss 0.4016 (0.4603) acc 95.8333 (89.1550) lr 1.1253e-03 eta 0:04:25
>>> alpha1: 0.177  alpha2: -0.172 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [26/50] batch [5/51] time 0.162 (0.438) data 0.001 (0.266) loss 0.8230 (0.5146) acc 82.6087 (88.7255) lr 1.0628e-03 eta 0:09:16
epoch [26/50] batch [10/51] time 0.178 (0.311) data 0.000 (0.133) loss 0.3994 (0.4716) acc 90.0943 (89.6186) lr 1.0628e-03 eta 0:06:33
epoch [26/50] batch [15/51] time 0.184 (0.266) data 0.000 (0.089) loss 0.3155 (0.4577) acc 91.9643 (89.9526) lr 1.0628e-03 eta 0:05:35
epoch [26/50] batch [20/51] time 0.173 (0.244) data 0.000 (0.067) loss 0.4531 (0.4636) acc 89.3617 (90.0124) lr 1.0628e-03 eta 0:05:06
epoch [26/50] batch [25/51] time 0.183 (0.232) data 0.001 (0.053) loss 0.3429 (0.4676) acc 91.6667 (89.8748) lr 1.0628e-03 eta 0:04:50
epoch [26/50] batch [30/51] time 0.180 (0.223) data 0.000 (0.045) loss 0.2893 (0.4583) acc 93.2692 (90.0151) lr 1.0628e-03 eta 0:04:37
epoch [26/50] batch [35/51] time 0.193 (0.217) data 0.000 (0.038) loss 0.3732 (0.4661) acc 90.0943 (89.5630) lr 1.0628e-03 eta 0:04:29
epoch [26/50] batch [40/51] time 0.178 (0.212) data 0.000 (0.033) loss 0.3712 (0.4664) acc 90.2778 (89.5354) lr 1.0628e-03 eta 0:04:21
epoch [26/50] batch [45/51] time 0.168 (0.208) data 0.000 (0.030) loss 0.3876 (0.4583) acc 87.5000 (89.6421) lr 1.0628e-03 eta 0:04:15
epoch [26/50] batch [50/51] time 0.170 (0.204) data 0.000 (0.027) loss 0.4038 (0.4515) acc 91.6667 (89.8567) lr 1.0628e-03 eta 0:04:09
>>> alpha1: 0.177  alpha2: -0.179 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [27/50] batch [5/51] time 0.185 (0.460) data 0.000 (0.275) loss 0.5219 (0.9570) acc 85.1852 (82.9657) lr 1.0000e-03 eta 0:09:20
epoch [27/50] batch [10/51] time 0.175 (0.321) data 0.001 (0.138) loss 0.2708 (0.6929) acc 92.3077 (86.4238) lr 1.0000e-03 eta 0:06:29
epoch [27/50] batch [15/51] time 0.186 (0.274) data 0.001 (0.093) loss 0.5540 (0.6175) acc 86.4035 (87.2206) lr 1.0000e-03 eta 0:05:31
epoch [27/50] batch [20/51] time 0.169 (0.249) data 0.000 (0.070) loss 0.3083 (0.5665) acc 92.5000 (87.9034) lr 1.0000e-03 eta 0:04:59
epoch [27/50] batch [25/51] time 0.186 (0.235) data 0.001 (0.056) loss 0.2657 (0.5530) acc 94.7368 (88.1236) lr 1.0000e-03 eta 0:04:41
epoch [27/50] batch [30/51] time 0.181 (0.225) data 0.000 (0.047) loss 0.4297 (0.5337) acc 92.0000 (88.4982) lr 1.0000e-03 eta 0:04:28
epoch [27/50] batch [35/51] time 0.184 (0.218) data 0.000 (0.040) loss 0.3750 (0.5157) acc 92.1296 (88.8980) lr 1.0000e-03 eta 0:04:19
epoch [27/50] batch [40/51] time 0.167 (0.213) data 0.000 (0.035) loss 0.3731 (0.5008) acc 93.0000 (89.3332) lr 1.0000e-03 eta 0:04:12
epoch [27/50] batch [45/51] time 0.164 (0.208) data 0.000 (0.031) loss 0.5680 (0.4896) acc 88.5417 (89.5034) lr 1.0000e-03 eta 0:04:05
epoch [27/50] batch [50/51] time 0.174 (0.205) data 0.000 (0.028) loss 0.5803 (0.4968) acc 87.0192 (89.3115) lr 1.0000e-03 eta 0:04:00
>>> alpha1: 0.178  alpha2: -0.177 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [28/50] batch [5/51] time 0.171 (0.481) data 0.000 (0.302) loss 0.5227 (0.4298) acc 90.0000 (90.7599) lr 9.3721e-04 eta 0:09:22
epoch [28/50] batch [10/51] time 0.165 (0.326) data 0.000 (0.151) loss 0.3950 (0.4209) acc 92.1875 (90.9179) lr 9.3721e-04 eta 0:06:19
epoch [28/50] batch [15/51] time 0.172 (0.276) data 0.001 (0.101) loss 0.2846 (0.4408) acc 96.5000 (90.8077) lr 9.3721e-04 eta 0:05:19
epoch [28/50] batch [20/51] time 0.177 (0.252) data 0.001 (0.076) loss 0.3336 (0.4367) acc 93.5000 (90.4949) lr 9.3721e-04 eta 0:04:50
epoch [28/50] batch [25/51] time 0.172 (0.237) data 0.000 (0.061) loss 0.4264 (0.4390) acc 92.7083 (90.4675) lr 9.3721e-04 eta 0:04:32
epoch [28/50] batch [30/51] time 0.177 (0.228) data 0.000 (0.051) loss 0.4802 (0.4381) acc 86.3208 (90.1708) lr 9.3721e-04 eta 0:04:20
epoch [28/50] batch [35/51] time 0.166 (0.221) data 0.000 (0.043) loss 0.5426 (0.4348) acc 90.6250 (90.4307) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.038) loss 0.5548 (0.4377) acc 83.3333 (90.2472) lr 9.3721e-04 eta 0:04:03
epoch [28/50] batch [45/51] time 0.166 (0.210) data 0.000 (0.034) loss 0.3850 (0.4390) acc 91.8367 (90.3467) lr 9.3721e-04 eta 0:03:57
epoch [28/50] batch [50/51] time 0.185 (0.207) data 0.000 (0.030) loss 0.3173 (0.4407) acc 93.4211 (90.3309) lr 9.3721e-04 eta 0:03:52
>>> alpha1: 0.182  alpha2: -0.185 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [29/50] batch [5/51] time 0.179 (0.443) data 0.000 (0.262) loss 0.2023 (0.3601) acc 97.6852 (92.7764) lr 8.7467e-04 eta 0:08:14
epoch [29/50] batch [10/51] time 0.181 (0.310) data 0.000 (0.131) loss 0.6469 (0.4174) acc 85.2041 (91.2512) lr 8.7467e-04 eta 0:05:45
epoch [29/50] batch [15/51] time 0.179 (0.264) data 0.000 (0.088) loss 0.3450 (0.4023) acc 94.3396 (91.4250) lr 8.7467e-04 eta 0:04:51
epoch [29/50] batch [20/51] time 0.189 (0.242) data 0.000 (0.066) loss 0.2940 (0.4095) acc 96.4286 (91.2418) lr 8.7467e-04 eta 0:04:27
epoch [29/50] batch [25/51] time 0.173 (0.231) data 0.000 (0.053) loss 0.4808 (0.4130) acc 88.2353 (90.8312) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [30/51] time 0.175 (0.222) data 0.000 (0.044) loss 0.3620 (0.4193) acc 89.9038 (90.5759) lr 8.7467e-04 eta 0:04:01
epoch [29/50] batch [35/51] time 0.181 (0.215) data 0.000 (0.038) loss 0.4403 (0.4221) acc 89.8148 (90.5180) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [40/51] time 0.162 (0.210) data 0.000 (0.033) loss 0.7316 (0.4253) acc 81.3830 (90.3556) lr 8.7467e-04 eta 0:03:47
epoch [29/50] batch [45/51] time 0.168 (0.207) data 0.000 (0.029) loss 0.4800 (0.4286) acc 86.4130 (90.2617) lr 8.7467e-04 eta 0:03:42
epoch [29/50] batch [50/51] time 0.178 (0.204) data 0.000 (0.026) loss 0.3414 (0.4228) acc 92.1296 (90.3655) lr 8.7467e-04 eta 0:03:38
>>> alpha1: 0.181  alpha2: -0.192 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [30/50] batch [5/51] time 0.171 (0.467) data 0.000 (0.283) loss 0.4037 (0.3600) acc 91.0000 (91.3927) lr 8.1262e-04 eta 0:08:17
epoch [30/50] batch [10/51] time 0.178 (0.322) data 0.000 (0.142) loss 0.3503 (0.3664) acc 92.3469 (91.8315) lr 8.1262e-04 eta 0:05:41
epoch [30/50] batch [15/51] time 0.169 (0.274) data 0.000 (0.095) loss 0.4470 (0.3810) acc 89.7959 (91.2090) lr 8.1262e-04 eta 0:04:49
epoch [30/50] batch [20/51] time 0.169 (0.250) data 0.001 (0.071) loss 0.3266 (0.3975) acc 92.8571 (91.0278) lr 8.1262e-04 eta 0:04:23
epoch [30/50] batch [25/51] time 0.188 (0.237) data 0.001 (0.057) loss 0.4213 (0.4005) acc 92.4107 (91.2405) lr 8.1262e-04 eta 0:04:07
epoch [30/50] batch [30/51] time 0.169 (0.227) data 0.000 (0.048) loss 0.3193 (0.3998) acc 92.1875 (91.2950) lr 8.1262e-04 eta 0:03:56
epoch [30/50] batch [35/51] time 0.194 (0.221) data 0.000 (0.041) loss 0.3987 (0.4044) acc 90.2778 (91.1253) lr 8.1262e-04 eta 0:03:48
epoch [30/50] batch [40/51] time 0.159 (0.214) data 0.000 (0.036) loss 0.5889 (0.4058) acc 86.9565 (91.1437) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [45/51] time 0.164 (0.209) data 0.000 (0.032) loss 0.3551 (0.4045) acc 91.1458 (91.1424) lr 8.1262e-04 eta 0:03:34
epoch [30/50] batch [50/51] time 0.169 (0.205) data 0.000 (0.029) loss 0.3810 (0.4048) acc 92.0000 (90.9752) lr 8.1262e-04 eta 0:03:28
>>> alpha1: 0.180  alpha2: -0.203 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [31/50] batch [5/51] time 0.169 (0.484) data 0.000 (0.310) loss 0.4724 (0.4248) acc 90.0000 (92.0037) lr 7.5131e-04 eta 0:08:11
epoch [31/50] batch [10/51] time 0.184 (0.330) data 0.000 (0.155) loss 0.2326 (0.4232) acc 94.3396 (90.8771) lr 7.5131e-04 eta 0:05:33
epoch [31/50] batch [15/51] time 0.174 (0.279) data 0.001 (0.103) loss 0.5749 (0.4259) acc 88.4615 (90.9872) lr 7.5131e-04 eta 0:04:40
epoch [31/50] batch [20/51] time 0.178 (0.252) data 0.000 (0.078) loss 0.5045 (0.4390) acc 86.3208 (90.3225) lr 7.5131e-04 eta 0:04:11
epoch [31/50] batch [25/51] time 0.161 (0.236) data 0.000 (0.062) loss 0.4023 (0.4286) acc 89.1304 (90.6384) lr 7.5131e-04 eta 0:03:54
epoch [31/50] batch [30/51] time 0.166 (0.226) data 0.000 (0.052) loss 0.3981 (0.4361) acc 90.9091 (90.3062) lr 7.5131e-04 eta 0:03:44
epoch [31/50] batch [35/51] time 0.173 (0.219) data 0.000 (0.045) loss 0.4416 (0.4417) acc 91.6667 (89.9764) lr 7.5131e-04 eta 0:03:36
epoch [31/50] batch [40/51] time 0.171 (0.214) data 0.000 (0.039) loss 0.4752 (0.4372) acc 84.0000 (90.0063) lr 7.5131e-04 eta 0:03:29
epoch [31/50] batch [45/51] time 0.174 (0.209) data 0.000 (0.035) loss 0.4236 (0.4375) acc 91.3462 (89.8686) lr 7.5131e-04 eta 0:03:23
epoch [31/50] batch [50/51] time 0.183 (0.206) data 0.000 (0.031) loss 0.5797 (0.4355) acc 84.7222 (89.8678) lr 7.5131e-04 eta 0:03:19
>>> alpha1: 0.179  alpha2: -0.198 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [32/50] batch [5/51] time 0.169 (0.449) data 0.001 (0.271) loss 0.3069 (0.3369) acc 93.2292 (93.1686) lr 6.9098e-04 eta 0:07:12
epoch [32/50] batch [10/51] time 0.171 (0.313) data 0.000 (0.136) loss 0.4858 (0.3611) acc 86.7647 (92.0351) lr 6.9098e-04 eta 0:05:00
epoch [32/50] batch [15/51] time 0.173 (0.267) data 0.000 (0.091) loss 0.3327 (0.3406) acc 90.8654 (92.6120) lr 6.9098e-04 eta 0:04:14
epoch [32/50] batch [20/51] time 0.166 (0.243) data 0.000 (0.068) loss 0.4225 (0.3704) acc 91.1458 (92.0852) lr 6.9098e-04 eta 0:03:50
epoch [32/50] batch [25/51] time 0.170 (0.229) data 0.000 (0.054) loss 0.3335 (0.3800) acc 94.1176 (91.7647) lr 6.9098e-04 eta 0:03:35
epoch [32/50] batch [30/51] time 0.169 (0.220) data 0.000 (0.045) loss 0.4422 (0.3861) acc 87.7660 (91.7398) lr 6.9098e-04 eta 0:03:26
epoch [32/50] batch [35/51] time 0.177 (0.213) data 0.000 (0.039) loss 0.3735 (0.3832) acc 93.3673 (91.9080) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [40/51] time 0.163 (0.208) data 0.000 (0.034) loss 0.4412 (0.3949) acc 90.6250 (91.6175) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [45/51] time 0.171 (0.204) data 0.000 (0.030) loss 0.3950 (0.4000) acc 91.8269 (91.4160) lr 6.9098e-04 eta 0:03:08
epoch [32/50] batch [50/51] time 0.165 (0.200) data 0.000 (0.027) loss 0.3027 (0.3995) acc 94.8980 (91.3614) lr 6.9098e-04 eta 0:03:03
>>> alpha1: 0.177  alpha2: -0.199 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [33/50] batch [5/51] time 0.165 (0.486) data 0.000 (0.309) loss 0.6289 (0.4603) acc 83.3333 (88.2919) lr 6.3188e-04 eta 0:07:23
epoch [33/50] batch [10/51] time 0.170 (0.334) data 0.000 (0.154) loss 0.5251 (0.4256) acc 89.0000 (89.9825) lr 6.3188e-04 eta 0:05:03
epoch [33/50] batch [15/51] time 0.174 (0.281) data 0.000 (0.103) loss 0.3970 (0.4301) acc 93.7500 (90.3492) lr 6.3188e-04 eta 0:04:13
epoch [33/50] batch [20/51] time 0.172 (0.257) data 0.000 (0.077) loss 0.3008 (0.4282) acc 92.1569 (90.1302) lr 6.3188e-04 eta 0:03:50
epoch [33/50] batch [25/51] time 0.185 (0.241) data 0.000 (0.062) loss 0.5735 (0.4253) acc 82.8431 (90.1420) lr 6.3188e-04 eta 0:03:35
epoch [33/50] batch [30/51] time 0.172 (0.230) data 0.001 (0.052) loss 0.6420 (0.4322) acc 84.0425 (90.1912) lr 6.3188e-04 eta 0:03:24
epoch [33/50] batch [35/51] time 0.186 (0.223) data 0.000 (0.044) loss 0.5052 (0.4322) acc 88.9423 (90.3794) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [40/51] time 0.164 (0.217) data 0.000 (0.039) loss 0.4584 (0.4268) acc 88.5417 (90.5432) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [45/51] time 0.161 (0.212) data 0.000 (0.035) loss 0.4640 (0.4261) acc 88.2979 (90.3162) lr 6.3188e-04 eta 0:03:05
epoch [33/50] batch [50/51] time 0.187 (0.208) data 0.000 (0.031) loss 0.4093 (0.4266) acc 93.1034 (90.2886) lr 6.3188e-04 eta 0:03:00
>>> alpha1: 0.174  alpha2: -0.189 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [34/50] batch [5/51] time 0.160 (0.494) data 0.000 (0.320) loss 0.6159 (0.4660) acc 84.7826 (89.8232) lr 5.7422e-04 eta 0:07:05
epoch [34/50] batch [10/51] time 0.172 (0.334) data 0.000 (0.160) loss 0.5365 (0.4494) acc 86.7647 (90.0167) lr 5.7422e-04 eta 0:04:46
epoch [34/50] batch [15/51] time 0.173 (0.282) data 0.000 (0.107) loss 0.2537 (0.4167) acc 96.5000 (91.0350) lr 5.7422e-04 eta 0:04:00
epoch [34/50] batch [20/51] time 0.179 (0.257) data 0.000 (0.080) loss 0.3193 (0.4187) acc 94.9074 (90.8111) lr 5.7422e-04 eta 0:03:37
epoch [34/50] batch [25/51] time 0.172 (0.241) data 0.000 (0.064) loss 0.3986 (0.4133) acc 88.2653 (90.5994) lr 5.7422e-04 eta 0:03:23
epoch [34/50] batch [30/51] time 0.179 (0.231) data 0.000 (0.054) loss 0.2634 (0.4070) acc 95.0980 (90.8148) lr 5.7422e-04 eta 0:03:13
epoch [34/50] batch [35/51] time 0.167 (0.223) data 0.000 (0.046) loss 0.4741 (0.4032) acc 90.3061 (90.9775) lr 5.7422e-04 eta 0:03:05
epoch [34/50] batch [40/51] time 0.169 (0.217) data 0.001 (0.040) loss 0.4901 (0.4119) acc 88.0000 (90.6922) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [45/51] time 0.185 (0.212) data 0.000 (0.036) loss 0.2836 (0.4109) acc 92.4528 (90.6632) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [50/51] time 0.179 (0.208) data 0.000 (0.032) loss 0.2898 (0.4064) acc 93.9815 (90.7248) lr 5.7422e-04 eta 0:02:50
>>> alpha1: 0.171  alpha2: -0.183 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [35/50] batch [5/51] time 0.176 (0.467) data 0.000 (0.287) loss 0.3192 (0.3410) acc 90.8654 (90.7160) lr 5.1825e-04 eta 0:06:19
epoch [35/50] batch [10/51] time 0.180 (0.323) data 0.000 (0.144) loss 0.3691 (0.3759) acc 92.3077 (90.9363) lr 5.1825e-04 eta 0:04:19
epoch [35/50] batch [15/51] time 0.199 (0.277) data 0.000 (0.096) loss 0.3391 (0.3853) acc 94.0909 (91.4456) lr 5.1825e-04 eta 0:03:41
epoch [35/50] batch [20/51] time 0.177 (0.252) data 0.000 (0.072) loss 0.3282 (0.3939) acc 93.3962 (91.2297) lr 5.1825e-04 eta 0:03:20
epoch [35/50] batch [25/51] time 0.182 (0.236) data 0.000 (0.058) loss 0.4397 (0.5018) acc 92.9245 (89.7625) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [30/51] time 0.178 (0.225) data 0.001 (0.048) loss 0.6191 (0.5710) acc 87.2642 (89.1060) lr 5.1825e-04 eta 0:02:57
epoch [35/50] batch [35/51] time 0.174 (0.219) data 0.000 (0.041) loss 0.4450 (0.5457) acc 85.5769 (89.3605) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [40/51] time 0.169 (0.213) data 0.000 (0.036) loss 0.4268 (0.5225) acc 90.0000 (89.6488) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [45/51] time 0.184 (0.209) data 0.000 (0.032) loss 0.2399 (0.5032) acc 94.6429 (89.9249) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [50/51] time 0.169 (0.205) data 0.000 (0.029) loss 0.5526 (0.4889) acc 88.0000 (90.2129) lr 5.1825e-04 eta 0:02:37
>>> alpha1: 0.169  alpha2: -0.187 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [36/50] batch [5/51] time 0.194 (0.432) data 0.000 (0.251) loss 0.4948 (0.3922) acc 89.9038 (91.3598) lr 4.6417e-04 eta 0:05:28
epoch [36/50] batch [10/51] time 0.163 (0.305) data 0.000 (0.126) loss 0.4013 (0.3786) acc 89.6739 (91.6284) lr 4.6417e-04 eta 0:03:49
epoch [36/50] batch [15/51] time 0.179 (0.262) data 0.000 (0.084) loss 0.2715 (0.3728) acc 95.3704 (91.8259) lr 4.6417e-04 eta 0:03:16
epoch [36/50] batch [20/51] time 0.170 (0.241) data 0.000 (0.063) loss 0.4553 (0.3656) acc 90.1961 (91.7689) lr 4.6417e-04 eta 0:02:59
epoch [36/50] batch [25/51] time 0.181 (0.228) data 0.000 (0.051) loss 0.2741 (0.3670) acc 93.3673 (91.8687) lr 4.6417e-04 eta 0:02:48
epoch [36/50] batch [30/51] time 0.172 (0.218) data 0.000 (0.042) loss 0.3457 (0.3750) acc 92.8571 (91.5763) lr 4.6417e-04 eta 0:02:40
epoch [36/50] batch [35/51] time 0.171 (0.212) data 0.000 (0.036) loss 0.5625 (0.3919) acc 86.2745 (91.1200) lr 4.6417e-04 eta 0:02:34
epoch [36/50] batch [40/51] time 0.172 (0.207) data 0.000 (0.032) loss 0.4829 (0.3954) acc 87.9808 (90.9335) lr 4.6417e-04 eta 0:02:30
epoch [36/50] batch [45/51] time 0.164 (0.203) data 0.000 (0.028) loss 0.5194 (0.4008) acc 82.8125 (90.7882) lr 4.6417e-04 eta 0:02:26
epoch [36/50] batch [50/51] time 0.169 (0.200) data 0.000 (0.025) loss 0.6028 (0.4039) acc 82.0000 (90.5938) lr 4.6417e-04 eta 0:02:22
>>> alpha1: 0.174  alpha2: -0.194 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.10 <<<
epoch [37/50] batch [5/51] time 0.183 (0.466) data 0.000 (0.284) loss 0.3843 (0.3491) acc 91.8269 (93.3573) lr 4.1221e-04 eta 0:05:30
epoch [37/50] batch [10/51] time 0.177 (0.321) data 0.000 (0.142) loss 0.2322 (0.3388) acc 94.7115 (92.9970) lr 4.1221e-04 eta 0:03:46
epoch [37/50] batch [15/51] time 0.162 (0.274) data 0.000 (0.095) loss 0.4139 (0.3628) acc 90.9574 (92.2379) lr 4.1221e-04 eta 0:03:11
epoch [37/50] batch [20/51] time 0.184 (0.250) data 0.000 (0.071) loss 0.2923 (0.3521) acc 92.8571 (92.4174) lr 4.1221e-04 eta 0:02:53
epoch [37/50] batch [25/51] time 0.177 (0.234) data 0.000 (0.057) loss 0.5385 (0.3490) acc 87.5000 (92.5584) lr 4.1221e-04 eta 0:02:41
epoch [37/50] batch [30/51] time 0.186 (0.225) data 0.000 (0.048) loss 0.3334 (0.3579) acc 93.7500 (92.3534) lr 4.1221e-04 eta 0:02:33
epoch [37/50] batch [35/51] time 0.167 (0.217) data 0.000 (0.041) loss 0.3372 (0.3694) acc 91.8367 (92.0932) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [40/51] time 0.159 (0.212) data 0.000 (0.036) loss 0.4297 (0.3679) acc 90.7609 (92.1438) lr 4.1221e-04 eta 0:02:22
epoch [37/50] batch [45/51] time 0.179 (0.207) data 0.000 (0.032) loss 0.3505 (0.3744) acc 93.8679 (91.8662) lr 4.1221e-04 eta 0:02:18
epoch [37/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.029) loss 0.2494 (0.3730) acc 96.0000 (91.7837) lr 4.1221e-04 eta 0:02:15
>>> alpha1: 0.178  alpha2: -0.197 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.10 <<<
epoch [38/50] batch [5/51] time 0.174 (0.472) data 0.001 (0.284) loss 0.3558 (0.4709) acc 93.5000 (89.3275) lr 3.6258e-04 eta 0:05:10
epoch [38/50] batch [10/51] time 0.161 (0.321) data 0.000 (0.142) loss 0.4015 (0.4249) acc 91.8478 (90.2958) lr 3.6258e-04 eta 0:03:29
epoch [38/50] batch [15/51] time 0.176 (0.273) data 0.001 (0.095) loss 0.3410 (0.4302) acc 92.1569 (90.4716) lr 3.6258e-04 eta 0:02:56
epoch [38/50] batch [20/51] time 0.168 (0.250) data 0.001 (0.071) loss 0.3583 (0.4252) acc 90.8163 (90.1081) lr 3.6258e-04 eta 0:02:40
epoch [38/50] batch [25/51] time 0.183 (0.235) data 0.000 (0.057) loss 0.2858 (0.4072) acc 94.2308 (90.7239) lr 3.6258e-04 eta 0:02:29
epoch [38/50] batch [30/51] time 0.173 (0.225) data 0.000 (0.048) loss 0.4910 (0.4013) acc 90.8654 (91.1005) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.182 (0.217) data 0.000 (0.041) loss 0.3021 (0.3992) acc 92.4528 (91.1263) lr 3.6258e-04 eta 0:02:16
epoch [38/50] batch [40/51] time 0.156 (0.212) data 0.000 (0.036) loss 0.3083 (0.3922) acc 96.0227 (91.4172) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [45/51] time 0.159 (0.207) data 0.000 (0.032) loss 0.4004 (0.3860) acc 92.7778 (91.6034) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [50/51] time 0.174 (0.203) data 0.000 (0.029) loss 0.4869 (0.3824) acc 87.9808 (91.6426) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.172  alpha2: -0.208 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [39/50] batch [5/51] time 0.177 (0.485) data 0.001 (0.303) loss 0.3012 (0.3592) acc 94.1176 (91.6967) lr 3.1545e-04 eta 0:04:54
epoch [39/50] batch [10/51] time 0.181 (0.329) data 0.000 (0.152) loss 0.3207 (0.4001) acc 92.3077 (90.8618) lr 3.1545e-04 eta 0:03:18
epoch [39/50] batch [15/51] time 0.179 (0.278) data 0.000 (0.101) loss 0.4401 (0.4024) acc 90.7407 (90.5677) lr 3.1545e-04 eta 0:02:45
epoch [39/50] batch [20/51] time 0.203 (0.255) data 0.000 (0.076) loss 0.2855 (0.3863) acc 95.5000 (91.2459) lr 3.1545e-04 eta 0:02:31
epoch [39/50] batch [25/51] time 0.189 (0.242) data 0.000 (0.061) loss 0.3534 (0.3798) acc 92.1569 (91.3983) lr 3.1545e-04 eta 0:02:22
epoch [39/50] batch [30/51] time 0.189 (0.232) data 0.000 (0.051) loss 0.2550 (0.3752) acc 93.3962 (91.6202) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [35/51] time 0.188 (0.225) data 0.000 (0.044) loss 0.4648 (0.3837) acc 90.6863 (91.2201) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [40/51] time 0.173 (0.218) data 0.000 (0.038) loss 0.4437 (0.3823) acc 93.2692 (91.3648) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [45/51] time 0.172 (0.213) data 0.000 (0.034) loss 0.5607 (0.3903) acc 83.1731 (91.1197) lr 3.1545e-04 eta 0:02:00
epoch [39/50] batch [50/51] time 0.161 (0.208) data 0.000 (0.031) loss 0.4982 (0.3932) acc 86.1702 (91.1760) lr 3.1545e-04 eta 0:01:57
>>> alpha1: 0.169  alpha2: -0.212 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.10 <<<
epoch [40/50] batch [5/51] time 0.170 (0.435) data 0.000 (0.249) loss 0.3966 (0.4022) acc 89.5833 (90.3083) lr 2.7103e-04 eta 0:04:02
epoch [40/50] batch [10/51] time 0.194 (0.304) data 0.001 (0.125) loss 0.3158 (0.4212) acc 92.1296 (90.1470) lr 2.7103e-04 eta 0:02:47
epoch [40/50] batch [15/51] time 0.186 (0.265) data 0.000 (0.083) loss 0.5916 (0.3971) acc 84.9057 (90.6166) lr 2.7103e-04 eta 0:02:24
epoch [40/50] batch [20/51] time 0.174 (0.243) data 0.000 (0.062) loss 0.3699 (0.3889) acc 87.9808 (90.6975) lr 2.7103e-04 eta 0:02:11
epoch [40/50] batch [25/51] time 0.166 (0.229) data 0.000 (0.050) loss 0.3435 (0.4002) acc 91.3265 (90.3135) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [30/51] time 0.193 (0.221) data 0.000 (0.042) loss 0.4012 (0.3968) acc 89.9038 (90.5089) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [35/51] time 0.186 (0.215) data 0.000 (0.036) loss 0.3959 (0.3960) acc 92.0000 (90.7244) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [40/51] time 0.170 (0.211) data 0.000 (0.031) loss 0.3447 (0.4003) acc 91.5000 (90.5447) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [45/51] time 0.185 (0.207) data 0.000 (0.028) loss 0.3850 (0.3948) acc 92.1053 (90.6801) lr 2.7103e-04 eta 0:01:46
epoch [40/50] batch [50/51] time 0.184 (0.203) data 0.001 (0.025) loss 0.3829 (0.3995) acc 90.6250 (90.7078) lr 2.7103e-04 eta 0:01:43
>>> alpha1: 0.168  alpha2: -0.207 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [41/50] batch [5/51] time 0.164 (0.479) data 0.000 (0.301) loss 0.3854 (0.3714) acc 91.1458 (91.6618) lr 2.2949e-04 eta 0:04:01
epoch [41/50] batch [10/51] time 0.184 (0.328) data 0.000 (0.151) loss 0.2676 (0.3435) acc 95.4082 (92.9090) lr 2.2949e-04 eta 0:02:43
epoch [41/50] batch [15/51] time 0.171 (0.279) data 0.000 (0.101) loss 0.4579 (0.3639) acc 89.3617 (92.6111) lr 2.2949e-04 eta 0:02:18
epoch [41/50] batch [20/51] time 0.174 (0.254) data 0.001 (0.076) loss 0.3150 (0.3728) acc 91.4894 (92.5021) lr 2.2949e-04 eta 0:02:04
epoch [41/50] batch [25/51] time 0.191 (0.240) data 0.001 (0.061) loss 0.2416 (0.3684) acc 95.2830 (92.5152) lr 2.2949e-04 eta 0:01:56
epoch [41/50] batch [30/51] time 0.169 (0.229) data 0.000 (0.051) loss 0.4579 (0.3733) acc 90.5000 (92.2823) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [35/51] time 0.180 (0.221) data 0.000 (0.043) loss 0.3507 (0.3766) acc 90.2778 (92.0976) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.038) loss 0.2712 (0.3739) acc 96.5000 (92.2438) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [45/51] time 0.171 (0.211) data 0.001 (0.034) loss 0.2944 (0.3738) acc 93.5000 (92.1828) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [50/51] time 0.167 (0.207) data 0.000 (0.030) loss 0.2972 (0.3710) acc 92.3469 (92.1814) lr 2.2949e-04 eta 0:01:35
>>> alpha1: 0.162  alpha2: -0.207 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [42/50] batch [5/51] time 0.178 (0.474) data 0.000 (0.291) loss 0.4175 (0.3400) acc 91.3462 (92.5786) lr 1.9098e-04 eta 0:03:35
epoch [42/50] batch [10/51] time 0.174 (0.320) data 0.000 (0.146) loss 0.3554 (0.3815) acc 89.9038 (91.5006) lr 1.9098e-04 eta 0:02:23
epoch [42/50] batch [15/51] time 0.169 (0.270) data 0.000 (0.097) loss 0.2727 (0.3803) acc 94.5000 (91.3654) lr 1.9098e-04 eta 0:01:59
epoch [42/50] batch [20/51] time 0.178 (0.245) data 0.000 (0.073) loss 0.4515 (0.3824) acc 89.1509 (91.5821) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [25/51] time 0.160 (0.230) data 0.000 (0.059) loss 0.4271 (0.3840) acc 90.2174 (91.6575) lr 1.9098e-04 eta 0:01:39
epoch [42/50] batch [30/51] time 0.172 (0.221) data 0.000 (0.049) loss 0.3174 (0.3858) acc 90.3846 (91.5831) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [35/51] time 0.174 (0.215) data 0.000 (0.042) loss 0.4965 (0.3816) acc 86.9792 (91.7292) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [40/51] time 0.186 (0.210) data 0.000 (0.037) loss 0.2620 (0.3717) acc 94.3966 (92.0010) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.169 (0.205) data 0.000 (0.033) loss 0.3706 (0.3728) acc 95.0980 (91.8555) lr 1.9098e-04 eta 0:01:24
epoch [42/50] batch [50/51] time 0.167 (0.201) data 0.000 (0.029) loss 0.4133 (0.3785) acc 88.0000 (91.7257) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.161  alpha2: -0.199 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [43/50] batch [5/51] time 0.172 (0.548) data 0.000 (0.367) loss 0.4943 (0.3436) acc 90.3061 (93.8357) lr 1.5567e-04 eta 0:03:40
epoch [43/50] batch [10/51] time 0.172 (0.364) data 0.000 (0.183) loss 0.3876 (0.3532) acc 87.5000 (92.4652) lr 1.5567e-04 eta 0:02:25
epoch [43/50] batch [15/51] time 0.171 (0.301) data 0.000 (0.122) loss 0.5075 (0.3657) acc 86.5000 (91.6785) lr 1.5567e-04 eta 0:01:58
epoch [43/50] batch [20/51] time 0.171 (0.268) data 0.000 (0.092) loss 0.4533 (0.3687) acc 90.1042 (91.7776) lr 1.5567e-04 eta 0:01:44
epoch [43/50] batch [25/51] time 0.179 (0.250) data 0.001 (0.074) loss 0.3142 (0.3696) acc 92.3077 (91.7808) lr 1.5567e-04 eta 0:01:35
epoch [43/50] batch [30/51] time 0.204 (0.238) data 0.000 (0.061) loss 0.5062 (0.3811) acc 86.0000 (91.5291) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [35/51] time 0.196 (0.231) data 0.000 (0.053) loss 0.4741 (0.3743) acc 93.3673 (91.7818) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [40/51] time 0.187 (0.225) data 0.000 (0.046) loss 0.4371 (0.3828) acc 93.0556 (91.6774) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [45/51] time 0.172 (0.220) data 0.001 (0.041) loss 0.3543 (0.3823) acc 91.1765 (91.6535) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [50/51] time 0.182 (0.215) data 0.000 (0.037) loss 0.3653 (0.3825) acc 92.2727 (91.5737) lr 1.5567e-04 eta 0:01:17
>>> alpha1: 0.161  alpha2: -0.210 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [44/50] batch [5/51] time 0.170 (0.492) data 0.000 (0.309) loss 0.4497 (0.4522) acc 90.8163 (89.5670) lr 1.2369e-04 eta 0:02:53
epoch [44/50] batch [10/51] time 0.168 (0.329) data 0.000 (0.155) loss 0.3263 (0.4325) acc 93.3673 (90.7471) lr 1.2369e-04 eta 0:01:54
epoch [44/50] batch [15/51] time 0.177 (0.279) data 0.000 (0.103) loss 0.4396 (0.4135) acc 86.7647 (90.8532) lr 1.2369e-04 eta 0:01:35
epoch [44/50] batch [20/51] time 0.181 (0.253) data 0.001 (0.077) loss 0.2978 (0.4099) acc 94.1176 (90.9612) lr 1.2369e-04 eta 0:01:25
epoch [44/50] batch [25/51] time 0.178 (0.239) data 0.000 (0.062) loss 0.2227 (0.3927) acc 95.2830 (91.1552) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [30/51] time 0.208 (0.229) data 0.000 (0.052) loss 0.3886 (0.3831) acc 93.1818 (91.4641) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [35/51] time 0.201 (0.223) data 0.001 (0.044) loss 0.4931 (0.3824) acc 89.5000 (91.5912) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.159 (0.216) data 0.000 (0.039) loss 0.4268 (0.3881) acc 90.2174 (91.4310) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.035) loss 0.4064 (0.3867) acc 92.5000 (91.5589) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [50/51] time 0.179 (0.206) data 0.000 (0.031) loss 0.2274 (0.3832) acc 96.7593 (91.7408) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.159  alpha2: -0.212 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [45/50] batch [5/51] time 0.178 (0.486) data 0.000 (0.305) loss 0.2531 (0.4116) acc 95.2830 (90.8599) lr 9.5173e-05 eta 0:02:26
epoch [45/50] batch [10/51] time 0.182 (0.333) data 0.000 (0.153) loss 0.3767 (0.3893) acc 92.3077 (91.6767) lr 9.5173e-05 eta 0:01:38
epoch [45/50] batch [15/51] time 0.178 (0.282) data 0.000 (0.102) loss 0.3096 (0.4064) acc 93.8775 (90.9783) lr 9.5173e-05 eta 0:01:22
epoch [45/50] batch [20/51] time 0.165 (0.255) data 0.000 (0.077) loss 0.4448 (0.4050) acc 90.4255 (91.0331) lr 9.5173e-05 eta 0:01:12
epoch [45/50] batch [25/51] time 0.192 (0.240) data 0.000 (0.061) loss 0.2470 (0.3987) acc 94.6078 (91.1801) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [30/51] time 0.178 (0.229) data 0.000 (0.052) loss 0.4075 (0.3967) acc 90.6250 (91.2419) lr 9.5173e-05 eta 0:01:03
epoch [45/50] batch [35/51] time 0.179 (0.222) data 0.000 (0.044) loss 0.3166 (0.3803) acc 93.1373 (91.6042) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.173 (0.216) data 0.000 (0.039) loss 0.5237 (0.3834) acc 90.3846 (91.6513) lr 9.5173e-05 eta 0:00:57
epoch [45/50] batch [45/51] time 0.183 (0.211) data 0.000 (0.035) loss 0.4675 (0.3780) acc 91.0714 (91.8430) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.171 (0.208) data 0.000 (0.031) loss 0.4713 (0.4189) acc 87.7451 (91.4846) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.156  alpha2: -0.211 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [46/50] batch [5/51] time 0.165 (0.433) data 0.000 (0.257) loss 0.2832 (0.3126) acc 93.6170 (92.3363) lr 7.0224e-05 eta 0:01:48
epoch [46/50] batch [10/51] time 0.157 (0.305) data 0.000 (0.128) loss 0.4257 (0.3453) acc 91.4773 (91.5354) lr 7.0224e-05 eta 0:01:14
epoch [46/50] batch [15/51] time 0.187 (0.264) data 0.000 (0.086) loss 0.5547 (0.3579) acc 90.5660 (91.4445) lr 7.0224e-05 eta 0:01:03
epoch [46/50] batch [20/51] time 0.173 (0.242) data 0.000 (0.064) loss 0.3982 (0.3667) acc 92.3077 (91.4424) lr 7.0224e-05 eta 0:00:56
epoch [46/50] batch [25/51] time 0.183 (0.229) data 0.000 (0.051) loss 0.3153 (0.3634) acc 94.1489 (91.6915) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [30/51] time 0.160 (0.221) data 0.000 (0.043) loss 0.4661 (0.3685) acc 90.7609 (91.5985) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [35/51] time 0.169 (0.213) data 0.000 (0.037) loss 0.3816 (0.3714) acc 91.3265 (91.3635) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [40/51] time 0.165 (0.208) data 0.000 (0.032) loss 0.3682 (0.3737) acc 92.1875 (91.2958) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.171 (0.203) data 0.000 (0.029) loss 0.4447 (0.3764) acc 92.6471 (91.3821) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [50/51] time 0.169 (0.200) data 0.000 (0.026) loss 0.3409 (0.3719) acc 92.0000 (91.5593) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.157  alpha2: -0.221 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [47/50] batch [5/51] time 0.189 (0.483) data 0.000 (0.293) loss 0.4053 (0.2925) acc 92.1875 (94.4104) lr 4.8943e-05 eta 0:01:36
epoch [47/50] batch [10/51] time 0.171 (0.331) data 0.001 (0.147) loss 0.3508 (0.3449) acc 91.5000 (93.1659) lr 4.8943e-05 eta 0:01:04
epoch [47/50] batch [15/51] time 0.171 (0.282) data 0.001 (0.100) loss 0.4300 (0.3452) acc 90.5000 (93.2529) lr 4.8943e-05 eta 0:00:53
epoch [47/50] batch [20/51] time 0.170 (0.255) data 0.001 (0.075) loss 0.3937 (0.3525) acc 90.3409 (92.7971) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.184 (0.240) data 0.000 (0.060) loss 0.5368 (0.3547) acc 89.9038 (92.7274) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [30/51] time 0.176 (0.229) data 0.000 (0.050) loss 0.3423 (0.3539) acc 92.6471 (92.6743) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.176 (0.222) data 0.000 (0.043) loss 0.3421 (0.3994) acc 94.7917 (92.3622) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.173 (0.216) data 0.000 (0.038) loss 0.4136 (0.4009) acc 89.7059 (92.1918) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.172 (0.210) data 0.000 (0.034) loss 0.3097 (0.3991) acc 92.6471 (92.1464) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.162 (0.206) data 0.000 (0.030) loss 0.4141 (0.3937) acc 92.5532 (92.1318) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.157  alpha2: -0.219 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [48/50] batch [5/51] time 0.164 (0.493) data 0.001 (0.321) loss 0.3362 (0.4317) acc 91.8478 (88.8211) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [10/51] time 0.203 (0.337) data 0.000 (0.160) loss 0.2201 (0.3700) acc 96.6346 (90.9649) lr 3.1417e-05 eta 0:00:48
epoch [48/50] batch [15/51] time 0.179 (0.285) data 0.000 (0.107) loss 0.3444 (0.3896) acc 92.7885 (90.7344) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.179 (0.256) data 0.000 (0.080) loss 0.4587 (0.3807) acc 92.4528 (91.4195) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.161 (0.240) data 0.000 (0.064) loss 0.3580 (0.3756) acc 91.8478 (91.5437) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.179 (0.229) data 0.000 (0.054) loss 0.3261 (0.3740) acc 89.7959 (91.6248) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.190 (0.221) data 0.000 (0.046) loss 0.2338 (0.3776) acc 95.7547 (91.6357) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.173 (0.215) data 0.000 (0.040) loss 0.3140 (0.3775) acc 92.5000 (91.5927) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.180 (0.211) data 0.000 (0.036) loss 0.5101 (0.3821) acc 87.9808 (91.6004) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.176 (0.207) data 0.000 (0.032) loss 0.3818 (0.3842) acc 92.3469 (91.4507) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.161  alpha2: -0.227 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [49/50] batch [5/51] time 0.181 (0.472) data 0.000 (0.289) loss 0.3739 (0.3897) acc 90.5000 (90.2399) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.192 (0.329) data 0.001 (0.145) loss 0.2925 (0.3581) acc 94.0000 (91.5800) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.173 (0.276) data 0.000 (0.097) loss 0.4680 (0.3622) acc 87.2549 (91.3412) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.178 (0.251) data 0.000 (0.073) loss 0.3947 (0.3503) acc 89.0000 (91.6075) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.160 (0.235) data 0.000 (0.058) loss 0.2975 (0.3619) acc 95.6522 (91.7040) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.178 (0.225) data 0.000 (0.048) loss 0.4592 (0.3703) acc 90.8654 (91.6638) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.163 (0.218) data 0.000 (0.042) loss 0.4627 (0.3738) acc 92.5532 (91.7114) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.168 (0.213) data 0.000 (0.036) loss 0.4038 (0.3745) acc 90.0000 (91.6061) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.170 (0.207) data 0.000 (0.032) loss 0.3214 (0.3809) acc 92.6471 (91.4629) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.170 (0.203) data 0.000 (0.029) loss 0.3680 (0.3769) acc 93.6274 (91.7046) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.163  alpha2: -0.230 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [50/50] batch [5/51] time 0.167 (0.468) data 0.000 (0.290) loss 0.3264 (0.3643) acc 93.2292 (93.3503) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.161 (0.323) data 0.000 (0.145) loss 0.4597 (0.3678) acc 85.8696 (92.1911) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.184 (0.274) data 0.000 (0.097) loss 0.1989 (0.3461) acc 95.7547 (92.6035) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.180 (0.250) data 0.000 (0.073) loss 0.3708 (0.3590) acc 91.4894 (91.6914) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.165 (0.236) data 0.001 (0.058) loss 0.4491 (0.3664) acc 89.8936 (91.4708) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.178 (0.227) data 0.000 (0.049) loss 0.3244 (0.3670) acc 94.7115 (91.7097) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.159 (0.218) data 0.000 (0.042) loss 0.3577 (0.3570) acc 92.7778 (91.9645) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.174 (0.212) data 0.000 (0.037) loss 0.2819 (0.3520) acc 95.1923 (92.0808) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.161 (0.206) data 0.000 (0.032) loss 0.2840 (0.3580) acc 93.6170 (92.0244) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.159 (0.202) data 0.000 (0.029) loss 0.3301 (0.3572) acc 94.4445 (92.1998) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.04, 0.05, 0.05, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.06, 0.07, 0.06, 0.06, 0.07, 0.06, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]
* matched noise rate: [0.02, 0.03, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.04, 0.04, 0.03, 0.03, 0.04, 0.03, 0.04, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03]
* unmatched noise rate: [0.06, 0.08, 0.09, 0.09, 0.1, 0.11, 0.12, 0.12, 0.12, 0.14, 0.13, 0.13, 0.12, 0.11, 0.12, 0.11, 0.11, 0.11, 0.12, 0.12, 0.11, 0.12, 0.12, 0.12, 0.11, 0.11, 0.1, 0.1, 0.12, 0.1, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.12, 0.11]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:08,  2.87s/it] 12%|█▏        | 3/25 [00:03<00:17,  1.24it/s] 20%|██        | 5/25 [00:03<00:08,  2.28it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.46it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.75it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.05it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.29it/s] 60%|██████    | 15/25 [00:04<00:01,  8.40it/s] 68%|██████▊   | 17/25 [00:04<00:01,  7.45it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.50it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.40it/s] 92%|█████████▏| 23/25 [00:04<00:00, 10.14it/s]100%|██████████| 25/25 [00:05<00:00,  7.51it/s]100%|██████████| 25/25 [00:05<00:00,  4.59it/s]
=> result
* total: 2,463
* correct: 2,201
* accuracy: 89.4%
* error: 10.6%
* macro_f1: 87.4%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 0	acc: 0.0%
* class: 3 (sweet pea)	total: 17	correct: 8	acc: 47.1%
* class: 4 (english marigold)	total: 20	correct: 15	acc: 75.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 13	acc: 92.9%
* class: 10 (snapdragon)	total: 26	correct: 21	acc: 80.8%
* class: 11 (colt's foot)	total: 26	correct: 23	acc: 88.5%
* class: 12 (king protea)	total: 15	correct: 14	acc: 93.3%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 12	acc: 92.3%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 22	acc: 88.0%
* class: 18 (balloon flower)	total: 15	correct: 12	acc: 80.0%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 10	acc: 83.3%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 23	acc: 85.2%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 25	acc: 96.2%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 19	acc: 86.4%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 10	acc: 76.9%
* class: 39 (lenten rose)	total: 20	correct: 20	acc: 100.0%
* class: 40 (barbeton daisy)	total: 38	correct: 19	acc: 50.0%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 2	acc: 5.1%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 18	acc: 85.7%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 26	acc: 92.9%
* class: 50 (petunia)	total: 77	correct: 61	acc: 79.2%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 10	acc: 62.5%
* class: 68 (windflower)	total: 16	correct: 15	acc: 93.8%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 23	acc: 100.0%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 36	acc: 100.0%
* class: 75 (morning glory)	total: 32	correct: 27	acc: 84.4%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 32	acc: 100.0%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 32	acc: 94.1%
* class: 82 (hibiscus)	total: 39	correct: 35	acc: 89.7%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 35	acc: 76.1%
* class: 88 (watercress)	total: 55	correct: 41	acc: 74.5%
* class: 89 (canna lily)	total: 25	correct: 18	acc: 72.0%
* class: 90 (hippeastrum)	total: 23	correct: 18	acc: 78.3%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 13	acc: 92.9%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 35	acc: 92.1%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 19	acc: 76.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 11	acc: 64.7%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 89.4%
Elapsed: 0:28:19
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '0', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_0-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 0
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.259 (1.077) data 0.000 (0.410) loss 4.2678 (4.2753) acc 6.2500 (7.5000) lr 1.0000e-05 eta 0:45:41
epoch [1/50] batch [10/51] time 0.259 (0.669) data 0.000 (0.205) loss 3.8481 (4.1632) acc 15.6250 (9.0625) lr 1.0000e-05 eta 0:28:18
epoch [1/50] batch [15/51] time 0.260 (0.534) data 0.000 (0.137) loss 3.8027 (4.0797) acc 15.6250 (10.2083) lr 1.0000e-05 eta 0:22:33
epoch [1/50] batch [20/51] time 0.271 (0.466) data 0.000 (0.103) loss 3.5606 (3.9777) acc 25.0000 (12.5000) lr 1.0000e-05 eta 0:19:39
epoch [1/50] batch [25/51] time 0.258 (0.425) data 0.000 (0.082) loss 3.0292 (3.8913) acc 40.6250 (15.3750) lr 1.0000e-05 eta 0:17:52
epoch [1/50] batch [30/51] time 0.273 (0.398) data 0.000 (0.069) loss 2.6189 (3.7293) acc 37.5000 (19.4792) lr 1.0000e-05 eta 0:16:42
epoch [1/50] batch [35/51] time 0.271 (0.379) data 0.000 (0.059) loss 2.2355 (3.5735) acc 40.6250 (22.5893) lr 1.0000e-05 eta 0:15:53
epoch [1/50] batch [40/51] time 0.257 (0.364) data 0.000 (0.051) loss 2.0038 (3.4229) acc 56.2500 (25.7031) lr 1.0000e-05 eta 0:15:14
epoch [1/50] batch [45/51] time 0.257 (0.352) data 0.000 (0.046) loss 2.0285 (3.3008) acc 59.3750 (28.6806) lr 1.0000e-05 eta 0:14:42
epoch [1/50] batch [50/51] time 0.257 (0.343) data 0.000 (0.041) loss 2.1473 (3.1904) acc 46.8750 (30.8125) lr 1.0000e-05 eta 0:14:16
epoch [2/50] batch [5/51] time 0.268 (0.566) data 0.000 (0.284) loss 1.3081 (1.9899) acc 71.8750 (56.2500) lr 2.0000e-03 eta 0:23:31
epoch [2/50] batch [10/51] time 0.270 (0.416) data 0.000 (0.142) loss 2.2403 (1.9819) acc 40.6250 (56.2500) lr 2.0000e-03 eta 0:17:15
epoch [2/50] batch [15/51] time 0.260 (0.364) data 0.000 (0.095) loss 1.6949 (1.8307) acc 62.5000 (59.7917) lr 2.0000e-03 eta 0:15:04
epoch [2/50] batch [20/51] time 0.265 (0.340) data 0.000 (0.071) loss 1.2850 (1.7874) acc 68.7500 (60.3125) lr 2.0000e-03 eta 0:14:01
epoch [2/50] batch [25/51] time 0.260 (0.324) data 0.000 (0.057) loss 1.3011 (1.7283) acc 68.7500 (61.2500) lr 2.0000e-03 eta 0:13:22
epoch [2/50] batch [30/51] time 0.259 (0.314) data 0.000 (0.047) loss 1.5162 (1.7489) acc 65.6250 (60.6250) lr 2.0000e-03 eta 0:12:56
epoch [2/50] batch [35/51] time 0.258 (0.307) data 0.000 (0.041) loss 1.5627 (1.7299) acc 68.7500 (61.5179) lr 2.0000e-03 eta 0:12:36
epoch [2/50] batch [40/51] time 0.257 (0.301) data 0.000 (0.036) loss 1.5650 (1.7223) acc 62.5000 (61.5625) lr 2.0000e-03 eta 0:12:20
epoch [2/50] batch [45/51] time 0.257 (0.296) data 0.000 (0.032) loss 1.8307 (1.7091) acc 56.2500 (61.5972) lr 2.0000e-03 eta 0:12:06
epoch [2/50] batch [50/51] time 0.256 (0.292) data 0.000 (0.029) loss 1.3969 (1.6828) acc 75.0000 (61.8750) lr 2.0000e-03 eta 0:11:55
epoch [3/50] batch [5/51] time 0.263 (0.577) data 0.000 (0.297) loss 1.5139 (1.4398) acc 62.5000 (61.8750) lr 1.9980e-03 eta 0:23:28
epoch [3/50] batch [10/51] time 0.266 (0.422) data 0.000 (0.149) loss 1.3722 (1.4049) acc 59.3750 (63.7500) lr 1.9980e-03 eta 0:17:08
epoch [3/50] batch [15/51] time 0.259 (0.369) data 0.000 (0.099) loss 1.3724 (1.4020) acc 65.6250 (63.5417) lr 1.9980e-03 eta 0:14:57
epoch [3/50] batch [20/51] time 0.259 (0.343) data 0.000 (0.075) loss 1.4858 (1.4404) acc 65.6250 (63.1250) lr 1.9980e-03 eta 0:13:53
epoch [3/50] batch [25/51] time 0.261 (0.327) data 0.000 (0.060) loss 0.9633 (1.3868) acc 71.8750 (64.8750) lr 1.9980e-03 eta 0:13:13
epoch [3/50] batch [30/51] time 0.274 (0.317) data 0.000 (0.050) loss 1.6627 (1.4181) acc 50.0000 (64.1667) lr 1.9980e-03 eta 0:12:47
epoch [3/50] batch [35/51] time 0.268 (0.311) data 0.000 (0.043) loss 0.9961 (1.3970) acc 84.3750 (65.7143) lr 1.9980e-03 eta 0:12:29
epoch [3/50] batch [40/51] time 0.258 (0.304) data 0.000 (0.037) loss 1.1375 (1.3701) acc 71.8750 (66.0938) lr 1.9980e-03 eta 0:12:12
epoch [3/50] batch [45/51] time 0.260 (0.299) data 0.000 (0.033) loss 1.3345 (1.3496) acc 65.6250 (66.5972) lr 1.9980e-03 eta 0:11:59
epoch [3/50] batch [50/51] time 0.261 (0.295) data 0.000 (0.030) loss 1.5490 (1.3481) acc 65.6250 (67.0000) lr 1.9980e-03 eta 0:11:48
epoch [4/50] batch [5/51] time 0.260 (0.575) data 0.000 (0.291) loss 0.9635 (0.9818) acc 81.2500 (74.3750) lr 1.9921e-03 eta 0:22:55
epoch [4/50] batch [10/51] time 0.263 (0.420) data 0.000 (0.146) loss 1.5902 (1.0702) acc 78.1250 (75.0000) lr 1.9921e-03 eta 0:16:41
epoch [4/50] batch [15/51] time 0.267 (0.369) data 0.000 (0.097) loss 1.0102 (1.0997) acc 75.0000 (74.3750) lr 1.9921e-03 eta 0:14:39
epoch [4/50] batch [20/51] time 0.261 (0.345) data 0.000 (0.073) loss 1.3017 (1.1188) acc 75.0000 (74.3750) lr 1.9921e-03 eta 0:13:38
epoch [4/50] batch [25/51] time 0.275 (0.329) data 0.000 (0.058) loss 1.0863 (1.1356) acc 75.0000 (73.0000) lr 1.9921e-03 eta 0:13:01
epoch [4/50] batch [30/51] time 0.265 (0.319) data 0.000 (0.049) loss 0.9230 (1.1248) acc 81.2500 (73.7500) lr 1.9921e-03 eta 0:12:35
epoch [4/50] batch [35/51] time 0.260 (0.311) data 0.000 (0.042) loss 0.9166 (1.0953) acc 81.2500 (74.8214) lr 1.9921e-03 eta 0:12:14
epoch [4/50] batch [40/51] time 0.257 (0.305) data 0.000 (0.037) loss 1.1691 (1.1032) acc 78.1250 (74.5312) lr 1.9921e-03 eta 0:11:58
epoch [4/50] batch [45/51] time 0.258 (0.300) data 0.000 (0.033) loss 1.0767 (1.1106) acc 62.5000 (73.6806) lr 1.9921e-03 eta 0:11:44
epoch [4/50] batch [50/51] time 0.263 (0.296) data 0.000 (0.029) loss 1.0205 (1.0995) acc 78.1250 (73.8750) lr 1.9921e-03 eta 0:11:34
epoch [5/50] batch [5/51] time 0.285 (0.580) data 0.000 (0.284) loss 0.5430 (1.0401) acc 84.3750 (73.7500) lr 1.9823e-03 eta 0:22:36
epoch [5/50] batch [10/51] time 0.260 (0.423) data 0.000 (0.142) loss 1.0880 (0.9779) acc 68.7500 (74.6875) lr 1.9823e-03 eta 0:16:27
epoch [5/50] batch [15/51] time 0.265 (0.370) data 0.000 (0.095) loss 1.2724 (1.0003) acc 71.8750 (75.4167) lr 1.9823e-03 eta 0:14:23
epoch [5/50] batch [20/51] time 0.262 (0.344) data 0.000 (0.071) loss 0.9054 (0.9709) acc 78.1250 (75.6250) lr 1.9823e-03 eta 0:13:20
epoch [5/50] batch [25/51] time 0.259 (0.329) data 0.000 (0.057) loss 1.1136 (0.9661) acc 71.8750 (76.2500) lr 1.9823e-03 eta 0:12:42
epoch [5/50] batch [30/51] time 0.281 (0.318) data 0.000 (0.047) loss 0.6972 (0.9376) acc 81.2500 (77.5000) lr 1.9823e-03 eta 0:12:17
epoch [5/50] batch [35/51] time 0.261 (0.311) data 0.000 (0.041) loss 0.6515 (0.9268) acc 87.5000 (77.7679) lr 1.9823e-03 eta 0:11:58
epoch [5/50] batch [40/51] time 0.258 (0.305) data 0.000 (0.036) loss 0.6771 (0.9075) acc 84.3750 (78.2031) lr 1.9823e-03 eta 0:11:43
epoch [5/50] batch [45/51] time 0.258 (0.300) data 0.000 (0.032) loss 0.7240 (0.9053) acc 87.5000 (78.4722) lr 1.9823e-03 eta 0:11:29
epoch [5/50] batch [50/51] time 0.257 (0.295) data 0.000 (0.029) loss 0.8244 (0.8984) acc 81.2500 (78.7500) lr 1.9823e-03 eta 0:11:18
epoch [6/50] batch [5/51] time 0.263 (0.580) data 0.000 (0.301) loss 0.4830 (0.6724) acc 90.6250 (85.6250) lr 1.9686e-03 eta 0:22:08
epoch [6/50] batch [10/51] time 0.260 (0.421) data 0.000 (0.151) loss 0.6482 (0.7571) acc 87.5000 (82.8125) lr 1.9686e-03 eta 0:16:02
epoch [6/50] batch [15/51] time 0.270 (0.370) data 0.000 (0.101) loss 0.6061 (0.7276) acc 93.7500 (83.7500) lr 1.9686e-03 eta 0:14:02
epoch [6/50] batch [20/51] time 0.260 (0.343) data 0.000 (0.075) loss 0.5628 (0.7364) acc 93.7500 (83.4375) lr 1.9686e-03 eta 0:13:00
epoch [6/50] batch [25/51] time 0.282 (0.328) data 0.000 (0.060) loss 1.0889 (0.7679) acc 71.8750 (82.1250) lr 1.9686e-03 eta 0:12:23
epoch [6/50] batch [30/51] time 0.275 (0.317) data 0.000 (0.050) loss 0.5571 (0.7683) acc 96.8750 (82.0833) lr 1.9686e-03 eta 0:11:59
epoch [6/50] batch [35/51] time 0.266 (0.310) data 0.000 (0.043) loss 0.5444 (0.7520) acc 90.6250 (82.5893) lr 1.9686e-03 eta 0:11:40
epoch [6/50] batch [40/51] time 0.257 (0.304) data 0.000 (0.038) loss 0.9530 (0.7576) acc 78.1250 (82.1094) lr 1.9686e-03 eta 0:11:24
epoch [6/50] batch [45/51] time 0.260 (0.299) data 0.000 (0.034) loss 0.5629 (0.7532) acc 87.5000 (82.3611) lr 1.9686e-03 eta 0:11:11
epoch [6/50] batch [50/51] time 0.258 (0.295) data 0.000 (0.030) loss 0.6234 (0.7532) acc 84.3750 (82.1250) lr 1.9686e-03 eta 0:11:01
epoch [7/50] batch [5/51] time 0.261 (0.567) data 0.000 (0.302) loss 0.9015 (0.7125) acc 84.3750 (83.7500) lr 1.9511e-03 eta 0:21:08
epoch [7/50] batch [10/51] time 0.268 (0.418) data 0.000 (0.151) loss 0.5685 (0.6582) acc 90.6250 (85.9375) lr 1.9511e-03 eta 0:15:34
epoch [7/50] batch [15/51] time 0.259 (0.366) data 0.000 (0.101) loss 0.5942 (0.6754) acc 87.5000 (84.3750) lr 1.9511e-03 eta 0:13:36
epoch [7/50] batch [20/51] time 0.259 (0.340) data 0.000 (0.076) loss 0.7901 (0.6752) acc 81.2500 (83.5938) lr 1.9511e-03 eta 0:12:37
epoch [7/50] batch [25/51] time 0.263 (0.325) data 0.000 (0.060) loss 0.6860 (0.6695) acc 84.3750 (84.2500) lr 1.9511e-03 eta 0:12:02
epoch [7/50] batch [30/51] time 0.258 (0.315) data 0.000 (0.050) loss 0.6579 (0.6645) acc 81.2500 (84.6875) lr 1.9511e-03 eta 0:11:36
epoch [7/50] batch [35/51] time 0.260 (0.308) data 0.000 (0.043) loss 0.4807 (0.6642) acc 90.6250 (85.2679) lr 1.9511e-03 eta 0:11:20
epoch [7/50] batch [40/51] time 0.257 (0.302) data 0.000 (0.038) loss 0.4532 (0.6471) acc 87.5000 (85.3125) lr 1.9511e-03 eta 0:11:05
epoch [7/50] batch [45/51] time 0.259 (0.297) data 0.000 (0.034) loss 1.1936 (0.6458) acc 71.8750 (85.5556) lr 1.9511e-03 eta 0:10:52
epoch [7/50] batch [50/51] time 0.257 (0.293) data 0.000 (0.030) loss 0.4904 (0.6385) acc 90.6250 (85.5625) lr 1.9511e-03 eta 0:10:42
epoch [8/50] batch [5/51] time 0.269 (0.565) data 0.000 (0.278) loss 0.7342 (0.6248) acc 78.1250 (82.5000) lr 1.9298e-03 eta 0:20:35
epoch [8/50] batch [10/51] time 0.259 (0.414) data 0.000 (0.139) loss 0.3226 (0.5469) acc 93.7500 (86.5625) lr 1.9298e-03 eta 0:15:03
epoch [8/50] batch [15/51] time 0.266 (0.364) data 0.000 (0.093) loss 0.8080 (0.5794) acc 84.3750 (87.2917) lr 1.9298e-03 eta 0:13:12
epoch [8/50] batch [20/51] time 0.269 (0.340) data 0.000 (0.070) loss 0.4021 (0.5957) acc 96.8750 (87.5000) lr 1.9298e-03 eta 0:12:19
epoch [8/50] batch [25/51] time 0.272 (0.326) data 0.000 (0.056) loss 0.5976 (0.5923) acc 90.6250 (87.5000) lr 1.9298e-03 eta 0:11:46
epoch [8/50] batch [30/51] time 0.262 (0.316) data 0.000 (0.047) loss 0.5277 (0.5835) acc 90.6250 (87.8125) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [35/51] time 0.267 (0.308) data 0.000 (0.040) loss 0.6483 (0.5745) acc 90.6250 (88.2143) lr 1.9298e-03 eta 0:11:05
epoch [8/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.035) loss 0.6094 (0.5703) acc 81.2500 (87.9688) lr 1.9298e-03 eta 0:10:50
epoch [8/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.031) loss 0.9444 (0.5702) acc 84.3750 (88.0556) lr 1.9298e-03 eta 0:10:38
epoch [8/50] batch [50/51] time 0.257 (0.293) data 0.000 (0.028) loss 0.3644 (0.5599) acc 90.6250 (88.4375) lr 1.9298e-03 eta 0:10:28
epoch [9/50] batch [5/51] time 0.271 (0.544) data 0.000 (0.273) loss 0.3587 (0.4482) acc 96.8750 (92.5000) lr 1.9048e-03 eta 0:19:21
epoch [9/50] batch [10/51] time 0.277 (0.404) data 0.000 (0.137) loss 0.5523 (0.4486) acc 87.5000 (90.6250) lr 1.9048e-03 eta 0:14:20
epoch [9/50] batch [15/51] time 0.259 (0.356) data 0.000 (0.091) loss 0.5087 (0.4659) acc 84.3750 (90.4167) lr 1.9048e-03 eta 0:12:38
epoch [9/50] batch [20/51] time 0.259 (0.333) data 0.000 (0.068) loss 0.2525 (0.4760) acc 96.8750 (89.8438) lr 1.9048e-03 eta 0:11:46
epoch [9/50] batch [25/51] time 0.275 (0.322) data 0.000 (0.055) loss 0.4147 (0.4770) acc 96.8750 (90.2500) lr 1.9048e-03 eta 0:11:20
epoch [9/50] batch [30/51] time 0.265 (0.313) data 0.000 (0.046) loss 0.3860 (0.4869) acc 96.8750 (89.8958) lr 1.9048e-03 eta 0:11:01
epoch [9/50] batch [35/51] time 0.264 (0.307) data 0.000 (0.039) loss 0.3576 (0.4762) acc 93.7500 (90.0000) lr 1.9048e-03 eta 0:10:47
epoch [9/50] batch [40/51] time 0.256 (0.302) data 0.000 (0.034) loss 0.5699 (0.4793) acc 90.6250 (89.9219) lr 1.9048e-03 eta 0:10:34
epoch [9/50] batch [45/51] time 0.256 (0.297) data 0.000 (0.031) loss 0.3496 (0.4890) acc 96.8750 (89.7222) lr 1.9048e-03 eta 0:10:22
epoch [9/50] batch [50/51] time 0.256 (0.293) data 0.000 (0.027) loss 0.4142 (0.4822) acc 96.8750 (90.0625) lr 1.9048e-03 eta 0:10:12
epoch [10/50] batch [5/51] time 0.283 (0.598) data 0.000 (0.300) loss 0.3219 (0.4359) acc 90.6250 (93.7500) lr 1.8763e-03 eta 0:20:46
epoch [10/50] batch [10/51] time 0.277 (0.439) data 0.000 (0.150) loss 0.5867 (0.4257) acc 87.5000 (92.1875) lr 1.8763e-03 eta 0:15:14
epoch [10/50] batch [15/51] time 0.265 (0.382) data 0.000 (0.100) loss 0.4672 (0.4548) acc 87.5000 (91.0417) lr 1.8763e-03 eta 0:13:13
epoch [10/50] batch [20/51] time 0.262 (0.353) data 0.000 (0.075) loss 0.5254 (0.4337) acc 87.5000 (91.7188) lr 1.8763e-03 eta 0:12:11
epoch [10/50] batch [25/51] time 0.260 (0.336) data 0.000 (0.060) loss 0.2881 (0.4237) acc 100.0000 (92.5000) lr 1.8763e-03 eta 0:11:34
epoch [10/50] batch [30/51] time 0.261 (0.324) data 0.000 (0.050) loss 0.6674 (0.4331) acc 84.3750 (91.9792) lr 1.8763e-03 eta 0:11:07
epoch [10/50] batch [35/51] time 0.263 (0.316) data 0.000 (0.043) loss 0.3887 (0.4301) acc 93.7500 (91.9643) lr 1.8763e-03 eta 0:10:50
epoch [10/50] batch [40/51] time 0.259 (0.310) data 0.000 (0.038) loss 0.3817 (0.4240) acc 90.6250 (92.1094) lr 1.8763e-03 eta 0:10:35
epoch [10/50] batch [45/51] time 0.259 (0.304) data 0.000 (0.034) loss 0.3452 (0.4266) acc 96.8750 (92.0139) lr 1.8763e-03 eta 0:10:21
epoch [10/50] batch [50/51] time 0.258 (0.299) data 0.000 (0.030) loss 0.4387 (0.4366) acc 90.6250 (91.8125) lr 1.8763e-03 eta 0:10:11
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.250  alpha2: -0.123 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [11/50] batch [5/51] time 0.163 (0.930) data 0.000 (0.275) loss 0.6179 (0.7133) acc 86.9792 (83.1708) lr 1.8443e-03 eta 0:31:31
epoch [11/50] batch [10/51] time 0.159 (0.672) data 0.000 (0.138) loss 0.7513 (0.6764) acc 80.4348 (84.0765) lr 1.8443e-03 eta 0:22:44
epoch [11/50] batch [15/51] time 0.166 (0.588) data 0.000 (0.092) loss 0.6047 (0.7068) acc 85.2041 (83.7902) lr 1.8443e-03 eta 0:19:50
epoch [11/50] batch [20/51] time 0.179 (0.484) data 0.000 (0.069) loss 0.7117 (0.6775) acc 79.3478 (84.4493) lr 1.8443e-03 eta 0:16:17
epoch [11/50] batch [25/51] time 0.158 (0.450) data 0.000 (0.055) loss 0.7658 (0.6893) acc 82.7778 (84.4587) lr 1.8443e-03 eta 0:15:05
epoch [11/50] batch [30/51] time 0.179 (0.403) data 0.000 (0.046) loss 0.5681 (0.7126) acc 85.0000 (83.9658) lr 1.8443e-03 eta 0:13:30
epoch [11/50] batch [35/51] time 0.167 (0.390) data 0.000 (0.039) loss 0.9439 (0.7353) acc 80.1020 (83.4903) lr 1.8443e-03 eta 0:13:00
epoch [11/50] batch [40/51] time 0.163 (0.393) data 0.000 (0.035) loss 0.8590 (0.7250) acc 79.6875 (83.5809) lr 1.8443e-03 eta 0:13:06
epoch [11/50] batch [45/51] time 0.170 (0.383) data 0.000 (0.031) loss 0.4921 (0.7257) acc 90.1961 (83.5997) lr 1.8443e-03 eta 0:12:44
epoch [11/50] batch [50/51] time 0.150 (0.361) data 0.000 (0.028) loss 0.7902 (0.7302) acc 80.3571 (83.6029) lr 1.8443e-03 eta 0:11:58
>>> alpha1: 0.270  alpha2: -0.076 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [12/50] batch [5/51] time 0.176 (0.429) data 0.000 (0.255) loss 0.4525 (0.6350) acc 90.0000 (86.6890) lr 1.8090e-03 eta 0:14:11
epoch [12/50] batch [10/51] time 0.176 (0.301) data 0.000 (0.127) loss 0.5234 (0.6583) acc 89.1509 (86.6037) lr 1.8090e-03 eta 0:09:55
epoch [12/50] batch [15/51] time 0.174 (0.258) data 0.000 (0.085) loss 0.6812 (0.6562) acc 85.5769 (85.8527) lr 1.8090e-03 eta 0:08:28
epoch [12/50] batch [20/51] time 0.167 (0.237) data 0.000 (0.064) loss 0.5205 (0.6429) acc 88.5417 (85.4166) lr 1.8090e-03 eta 0:07:46
epoch [12/50] batch [25/51] time 0.172 (0.224) data 0.000 (0.051) loss 0.6313 (0.6555) acc 84.5000 (84.9200) lr 1.8090e-03 eta 0:07:19
epoch [12/50] batch [30/51] time 0.175 (0.216) data 0.000 (0.043) loss 0.6324 (0.6612) acc 85.6383 (84.7685) lr 1.8090e-03 eta 0:07:02
epoch [12/50] batch [35/51] time 0.158 (0.209) data 0.000 (0.037) loss 0.4327 (0.6402) acc 90.0000 (85.1387) lr 1.8090e-03 eta 0:06:49
epoch [12/50] batch [40/51] time 0.169 (0.205) data 0.000 (0.032) loss 0.5401 (0.6307) acc 86.5000 (85.2688) lr 1.8090e-03 eta 0:06:39
epoch [12/50] batch [45/51] time 0.180 (0.217) data 0.001 (0.029) loss 0.4580 (0.6284) acc 89.3519 (85.2514) lr 1.8090e-03 eta 0:07:01
epoch [12/50] batch [50/51] time 0.166 (0.212) data 0.000 (0.026) loss 0.7879 (0.6372) acc 82.1429 (85.1659) lr 1.8090e-03 eta 0:06:50
>>> alpha1: 0.256  alpha2: -0.102 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [13/50] batch [5/51] time 0.196 (0.443) data 0.001 (0.258) loss 0.4394 (0.5687) acc 90.3061 (85.1383) lr 1.7705e-03 eta 0:14:15
epoch [13/50] batch [10/51] time 0.180 (0.312) data 0.000 (0.129) loss 0.6099 (0.5937) acc 89.0000 (85.5018) lr 1.7705e-03 eta 0:10:02
epoch [13/50] batch [15/51] time 0.195 (0.270) data 0.000 (0.086) loss 0.7807 (0.5904) acc 84.0909 (85.9081) lr 1.7705e-03 eta 0:08:38
epoch [13/50] batch [20/51] time 0.162 (0.246) data 0.000 (0.065) loss 0.6177 (0.5779) acc 79.7872 (86.1910) lr 1.7705e-03 eta 0:07:51
epoch [13/50] batch [25/51] time 0.162 (0.262) data 0.000 (0.052) loss 0.6877 (0.5878) acc 85.1064 (86.2919) lr 1.7705e-03 eta 0:08:20
epoch [13/50] batch [30/51] time 0.192 (0.248) data 0.000 (0.043) loss 0.6965 (0.5957) acc 78.4314 (85.8470) lr 1.7705e-03 eta 0:07:53
epoch [13/50] batch [35/51] time 0.182 (0.238) data 0.000 (0.037) loss 0.4902 (0.5997) acc 83.1818 (85.8203) lr 1.7705e-03 eta 0:07:33
epoch [13/50] batch [40/51] time 0.163 (0.230) data 0.000 (0.032) loss 0.6970 (0.6118) acc 81.2500 (85.5886) lr 1.7705e-03 eta 0:07:16
epoch [13/50] batch [45/51] time 0.165 (0.223) data 0.000 (0.029) loss 0.5706 (0.6102) acc 84.3750 (85.5592) lr 1.7705e-03 eta 0:07:02
epoch [13/50] batch [50/51] time 0.178 (0.218) data 0.000 (0.026) loss 0.5489 (0.5998) acc 84.8039 (85.8321) lr 1.7705e-03 eta 0:06:51
>>> alpha1: 0.252  alpha2: -0.096 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [14/50] batch [5/51] time 0.950 (0.616) data 0.000 (0.279) loss 0.4320 (0.5602) acc 90.5172 (86.2104) lr 1.7290e-03 eta 0:19:19
epoch [14/50] batch [10/51] time 0.167 (0.466) data 0.000 (0.140) loss 0.5851 (0.5477) acc 86.2245 (87.4583) lr 1.7290e-03 eta 0:14:34
epoch [14/50] batch [15/51] time 0.173 (0.369) data 0.001 (0.093) loss 0.6035 (0.5793) acc 85.0962 (86.2807) lr 1.7290e-03 eta 0:11:31
epoch [14/50] batch [20/51] time 0.169 (0.358) data 0.000 (0.070) loss 0.8359 (0.6115) acc 78.0000 (85.5969) lr 1.7290e-03 eta 0:11:08
epoch [14/50] batch [25/51] time 0.185 (0.322) data 0.000 (0.056) loss 0.6707 (0.6823) acc 86.5385 (85.0967) lr 1.7290e-03 eta 0:09:59
epoch [14/50] batch [30/51] time 0.198 (0.298) data 0.001 (0.047) loss 0.6155 (0.6708) acc 88.4615 (85.2065) lr 1.7290e-03 eta 0:09:12
epoch [14/50] batch [35/51] time 0.172 (0.280) data 0.000 (0.040) loss 0.6314 (0.6578) acc 79.4118 (85.2386) lr 1.7290e-03 eta 0:08:37
epoch [14/50] batch [40/51] time 0.157 (0.267) data 0.000 (0.035) loss 0.7358 (0.6401) acc 82.3864 (85.5698) lr 1.7290e-03 eta 0:08:12
epoch [14/50] batch [45/51] time 0.169 (0.256) data 0.000 (0.031) loss 0.6029 (0.6445) acc 85.0000 (85.5802) lr 1.7290e-03 eta 0:07:52
epoch [14/50] batch [50/51] time 0.173 (0.247) data 0.000 (0.028) loss 0.6387 (0.6399) acc 80.7692 (85.5624) lr 1.7290e-03 eta 0:07:34
>>> alpha1: 0.246  alpha2: -0.097 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [15/50] batch [5/51] time 0.173 (0.487) data 0.001 (0.301) loss 0.6974 (0.5524) acc 87.2340 (87.4126) lr 1.6845e-03 eta 0:14:51
epoch [15/50] batch [10/51] time 0.170 (0.330) data 0.000 (0.150) loss 0.8074 (0.5914) acc 78.4314 (86.3840) lr 1.6845e-03 eta 0:10:02
epoch [15/50] batch [15/51] time 0.185 (0.280) data 0.000 (0.100) loss 0.4881 (0.5732) acc 89.5455 (87.2876) lr 1.6845e-03 eta 0:08:29
epoch [15/50] batch [20/51] time 0.179 (0.253) data 0.000 (0.075) loss 0.6126 (0.5835) acc 87.5000 (86.8803) lr 1.6845e-03 eta 0:07:38
epoch [15/50] batch [25/51] time 0.168 (0.238) data 0.000 (0.060) loss 0.5364 (0.5656) acc 89.5000 (87.3738) lr 1.6845e-03 eta 0:07:10
epoch [15/50] batch [30/51] time 0.177 (0.227) data 0.000 (0.050) loss 0.6375 (0.5606) acc 84.5000 (87.3898) lr 1.6845e-03 eta 0:06:50
epoch [15/50] batch [35/51] time 0.179 (0.220) data 0.000 (0.043) loss 0.8150 (0.5568) acc 76.8519 (87.4604) lr 1.6845e-03 eta 0:06:35
epoch [15/50] batch [40/51] time 0.165 (0.214) data 0.000 (0.038) loss 0.7696 (0.5621) acc 85.2041 (87.2399) lr 1.6845e-03 eta 0:06:23
epoch [15/50] batch [45/51] time 0.160 (0.209) data 0.000 (0.034) loss 0.6367 (0.5629) acc 86.1702 (87.2175) lr 1.6845e-03 eta 0:06:13
epoch [15/50] batch [50/51] time 0.169 (0.205) data 0.000 (0.030) loss 0.6661 (0.5635) acc 85.7843 (87.3115) lr 1.6845e-03 eta 0:06:06
>>> alpha1: 0.228  alpha2: -0.101 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [16/50] batch [5/51] time 0.181 (0.445) data 0.000 (0.267) loss 0.3985 (0.4997) acc 92.5926 (87.2366) lr 1.6374e-03 eta 0:13:12
epoch [16/50] batch [10/51] time 0.177 (0.310) data 0.000 (0.133) loss 0.6247 (0.5603) acc 86.7347 (86.4795) lr 1.6374e-03 eta 0:09:10
epoch [16/50] batch [15/51] time 0.170 (0.265) data 0.000 (0.089) loss 0.6601 (0.5578) acc 88.2353 (86.7212) lr 1.6374e-03 eta 0:07:49
epoch [16/50] batch [20/51] time 0.184 (0.243) data 0.000 (0.067) loss 0.5733 (0.5467) acc 86.7924 (87.3562) lr 1.6374e-03 eta 0:07:09
epoch [16/50] batch [25/51] time 0.174 (0.229) data 0.000 (0.054) loss 0.8542 (0.5712) acc 82.4468 (87.1061) lr 1.6374e-03 eta 0:06:43
epoch [16/50] batch [30/51] time 0.170 (0.221) data 0.000 (0.045) loss 0.4763 (0.5681) acc 89.2157 (87.0977) lr 1.6374e-03 eta 0:06:27
epoch [16/50] batch [35/51] time 0.175 (0.214) data 0.000 (0.038) loss 0.4964 (0.5650) acc 86.5385 (87.1490) lr 1.6374e-03 eta 0:06:14
epoch [16/50] batch [40/51] time 0.175 (0.209) data 0.000 (0.034) loss 0.5108 (0.5698) acc 87.0192 (86.8885) lr 1.6374e-03 eta 0:06:04
epoch [16/50] batch [45/51] time 0.171 (0.205) data 0.001 (0.030) loss 0.5309 (0.5618) acc 89.7059 (87.1922) lr 1.6374e-03 eta 0:05:56
epoch [16/50] batch [50/51] time 0.171 (0.201) data 0.000 (0.027) loss 0.3953 (0.5604) acc 89.7059 (87.1304) lr 1.6374e-03 eta 0:05:49
>>> alpha1: 0.221  alpha2: -0.135 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [17/50] batch [5/51] time 0.180 (0.482) data 0.000 (0.295) loss 0.4188 (0.4551) acc 92.7885 (90.0691) lr 1.5878e-03 eta 0:13:54
epoch [17/50] batch [10/51] time 0.181 (0.331) data 0.000 (0.148) loss 0.2747 (0.4857) acc 93.7500 (89.1947) lr 1.5878e-03 eta 0:09:31
epoch [17/50] batch [15/51] time 0.184 (0.279) data 0.000 (0.099) loss 0.5472 (0.5039) acc 85.1852 (88.5486) lr 1.5878e-03 eta 0:07:59
epoch [17/50] batch [20/51] time 0.180 (0.253) data 0.000 (0.075) loss 0.4683 (0.5087) acc 92.1296 (88.6683) lr 1.5878e-03 eta 0:07:13
epoch [17/50] batch [25/51] time 0.162 (0.237) data 0.000 (0.060) loss 0.4212 (0.5069) acc 89.3617 (88.4942) lr 1.5878e-03 eta 0:06:44
epoch [17/50] batch [30/51] time 0.170 (0.227) data 0.000 (0.050) loss 0.4529 (0.5192) acc 88.7255 (88.0091) lr 1.5878e-03 eta 0:06:26
epoch [17/50] batch [35/51] time 0.175 (0.219) data 0.000 (0.043) loss 0.4567 (0.5165) acc 91.8269 (88.3471) lr 1.5878e-03 eta 0:06:12
epoch [17/50] batch [40/51] time 0.164 (0.213) data 0.000 (0.037) loss 0.5408 (0.5167) acc 88.0208 (88.4179) lr 1.5878e-03 eta 0:06:01
epoch [17/50] batch [45/51] time 0.177 (0.209) data 0.000 (0.033) loss 0.3870 (0.5198) acc 89.6226 (88.2526) lr 1.5878e-03 eta 0:05:52
epoch [17/50] batch [50/51] time 0.174 (0.205) data 0.000 (0.030) loss 0.6398 (0.5275) acc 86.0577 (87.9144) lr 1.5878e-03 eta 0:05:44
>>> alpha1: 0.210  alpha2: -0.139 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [18/50] batch [5/51] time 0.186 (0.462) data 0.000 (0.278) loss 0.4096 (0.5113) acc 87.7193 (86.9390) lr 1.5358e-03 eta 0:12:54
epoch [18/50] batch [10/51] time 0.166 (0.315) data 0.000 (0.139) loss 0.8056 (0.5720) acc 73.4694 (85.2445) lr 1.5358e-03 eta 0:08:47
epoch [18/50] batch [15/51] time 0.176 (0.269) data 0.000 (0.093) loss 0.5840 (0.5345) acc 86.3208 (86.9282) lr 1.5358e-03 eta 0:07:29
epoch [18/50] batch [20/51] time 0.162 (0.245) data 0.000 (0.070) loss 0.6325 (0.5454) acc 87.7660 (86.9979) lr 1.5358e-03 eta 0:06:46
epoch [18/50] batch [25/51] time 0.162 (0.229) data 0.000 (0.056) loss 0.4552 (0.5257) acc 88.2979 (87.7746) lr 1.5358e-03 eta 0:06:20
epoch [18/50] batch [30/51] time 0.179 (0.221) data 0.000 (0.046) loss 0.7013 (0.5258) acc 82.8431 (87.8811) lr 1.5358e-03 eta 0:06:04
epoch [18/50] batch [35/51] time 0.184 (0.214) data 0.000 (0.040) loss 0.4666 (0.5184) acc 87.5000 (87.8730) lr 1.5358e-03 eta 0:05:53
epoch [18/50] batch [40/51] time 0.181 (0.210) data 0.000 (0.035) loss 0.5519 (0.5242) acc 88.6364 (87.7635) lr 1.5358e-03 eta 0:05:44
epoch [18/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.031) loss 0.6082 (0.5207) acc 87.0000 (87.9049) lr 1.5358e-03 eta 0:05:36
epoch [18/50] batch [50/51] time 0.168 (0.202) data 0.000 (0.028) loss 0.4809 (0.5272) acc 86.0000 (87.8313) lr 1.5358e-03 eta 0:05:29
>>> alpha1: 0.213  alpha2: -0.141 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [19/50] batch [5/51] time 0.179 (0.433) data 0.000 (0.252) loss 0.4582 (0.4612) acc 91.6667 (90.7091) lr 1.4818e-03 eta 0:11:45
epoch [19/50] batch [10/51] time 0.179 (0.306) data 0.000 (0.126) loss 0.6199 (0.5166) acc 84.2593 (89.7959) lr 1.4818e-03 eta 0:08:15
epoch [19/50] batch [15/51] time 0.168 (0.262) data 0.000 (0.084) loss 2.9090 (0.6526) acc 59.0000 (88.5286) lr 1.4818e-03 eta 0:07:03
epoch [19/50] batch [20/51] time 0.172 (0.241) data 0.000 (0.063) loss 0.7278 (0.6953) acc 82.1429 (87.4928) lr 1.4818e-03 eta 0:06:28
epoch [19/50] batch [25/51] time 0.185 (0.228) data 0.000 (0.051) loss 0.6138 (0.6558) acc 85.0000 (87.8058) lr 1.4818e-03 eta 0:06:06
epoch [19/50] batch [30/51] time 0.169 (0.219) data 0.000 (0.042) loss 0.4897 (0.6258) acc 86.5000 (88.1581) lr 1.4818e-03 eta 0:05:50
epoch [19/50] batch [35/51] time 0.184 (0.212) data 0.000 (0.036) loss 0.7204 (0.6086) acc 81.1321 (88.0782) lr 1.4818e-03 eta 0:05:38
epoch [19/50] batch [40/51] time 0.158 (0.207) data 0.000 (0.032) loss 0.5305 (0.5986) acc 91.3043 (88.2143) lr 1.4818e-03 eta 0:05:29
epoch [19/50] batch [45/51] time 0.168 (0.202) data 0.000 (0.028) loss 0.4747 (0.5871) acc 87.2549 (88.2746) lr 1.4818e-03 eta 0:05:21
epoch [19/50] batch [50/51] time 0.175 (0.200) data 0.000 (0.025) loss 0.3768 (0.5727) acc 91.0377 (88.3546) lr 1.4818e-03 eta 0:05:15
>>> alpha1: 0.206  alpha2: -0.137 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [20/50] batch [5/51] time 0.179 (0.470) data 0.000 (0.300) loss 0.5517 (0.5402) acc 87.9630 (88.5870) lr 1.4258e-03 eta 0:12:21
epoch [20/50] batch [10/51] time 0.199 (0.324) data 0.000 (0.150) loss 0.3211 (0.5270) acc 92.7273 (88.6392) lr 1.4258e-03 eta 0:08:28
epoch [20/50] batch [15/51] time 0.170 (0.275) data 0.000 (0.100) loss 0.4343 (0.5347) acc 93.7500 (88.7022) lr 1.4258e-03 eta 0:07:10
epoch [20/50] batch [20/51] time 0.174 (0.251) data 0.000 (0.075) loss 0.5585 (0.5351) acc 82.6531 (87.8889) lr 1.4258e-03 eta 0:06:31
epoch [20/50] batch [25/51] time 0.170 (0.236) data 0.000 (0.060) loss 0.5347 (0.5392) acc 88.7255 (87.9040) lr 1.4258e-03 eta 0:06:07
epoch [20/50] batch [30/51] time 0.166 (0.225) data 0.000 (0.050) loss 0.4212 (0.5431) acc 89.7959 (87.6121) lr 1.4258e-03 eta 0:05:49
epoch [20/50] batch [35/51] time 0.181 (0.218) data 0.000 (0.043) loss 0.4931 (0.5320) acc 90.0000 (87.9432) lr 1.4258e-03 eta 0:05:36
epoch [20/50] batch [40/51] time 0.163 (0.212) data 0.000 (0.038) loss 0.7252 (0.5294) acc 79.6875 (87.9277) lr 1.4258e-03 eta 0:05:26
epoch [20/50] batch [45/51] time 0.169 (0.208) data 0.000 (0.034) loss 0.3149 (0.5237) acc 93.6274 (88.0039) lr 1.4258e-03 eta 0:05:18
epoch [20/50] batch [50/51] time 0.172 (0.204) data 0.000 (0.030) loss 0.4075 (0.5246) acc 92.7885 (88.0969) lr 1.4258e-03 eta 0:05:12
>>> alpha1: 0.206  alpha2: -0.136 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [21/50] batch [5/51] time 0.182 (0.490) data 0.000 (0.313) loss 0.5578 (0.4725) acc 84.4340 (88.8561) lr 1.3681e-03 eta 0:12:27
epoch [21/50] batch [10/51] time 0.189 (0.332) data 0.000 (0.157) loss 0.4590 (0.4588) acc 90.0943 (88.5250) lr 1.3681e-03 eta 0:08:24
epoch [21/50] batch [15/51] time 0.188 (0.280) data 0.000 (0.104) loss 0.6161 (0.4765) acc 85.5769 (88.3072) lr 1.3681e-03 eta 0:07:04
epoch [21/50] batch [20/51] time 0.186 (0.254) data 0.000 (0.078) loss 0.7151 (0.4839) acc 86.1111 (88.4897) lr 1.3681e-03 eta 0:06:24
epoch [21/50] batch [25/51] time 0.171 (0.238) data 0.000 (0.063) loss 0.5357 (0.4874) acc 86.7647 (88.6923) lr 1.3681e-03 eta 0:05:58
epoch [21/50] batch [30/51] time 0.158 (0.227) data 0.000 (0.052) loss 0.4990 (0.4817) acc 88.3333 (88.9893) lr 1.3681e-03 eta 0:05:40
epoch [21/50] batch [35/51] time 0.178 (0.220) data 0.000 (0.045) loss 0.4539 (0.4753) acc 89.6226 (89.1909) lr 1.3681e-03 eta 0:05:28
epoch [21/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.039) loss 0.4178 (0.4733) acc 92.6471 (89.1593) lr 1.3681e-03 eta 0:05:19
epoch [21/50] batch [45/51] time 0.167 (0.209) data 0.000 (0.035) loss 0.4950 (0.4718) acc 87.0000 (89.1997) lr 1.3681e-03 eta 0:05:10
epoch [21/50] batch [50/51] time 0.172 (0.205) data 0.000 (0.031) loss 0.3073 (0.4789) acc 91.3462 (89.0200) lr 1.3681e-03 eta 0:05:03
>>> alpha1: 0.197  alpha2: -0.139 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.12 <<<
epoch [22/50] batch [5/51] time 0.191 (0.480) data 0.000 (0.296) loss 0.3083 (0.3633) acc 95.3390 (91.7936) lr 1.3090e-03 eta 0:11:46
epoch [22/50] batch [10/51] time 0.166 (0.326) data 0.000 (0.148) loss 0.4809 (0.4346) acc 89.6739 (89.8204) lr 1.3090e-03 eta 0:07:58
epoch [22/50] batch [15/51] time 0.178 (0.275) data 0.000 (0.099) loss 0.4356 (0.4234) acc 90.8163 (90.4065) lr 1.3090e-03 eta 0:06:43
epoch [22/50] batch [20/51] time 0.187 (0.252) data 0.000 (0.074) loss 0.4195 (0.4205) acc 90.0000 (90.4328) lr 1.3090e-03 eta 0:06:07
epoch [22/50] batch [25/51] time 0.169 (0.237) data 0.000 (0.059) loss 0.4715 (0.4477) acc 90.0000 (89.8115) lr 1.3090e-03 eta 0:05:44
epoch [22/50] batch [30/51] time 0.163 (0.228) data 0.000 (0.049) loss 0.6138 (0.4533) acc 83.1522 (89.7730) lr 1.3090e-03 eta 0:05:30
epoch [22/50] batch [35/51] time 0.180 (0.221) data 0.013 (0.043) loss 0.5678 (0.4629) acc 86.9318 (89.3314) lr 1.3090e-03 eta 0:05:19
epoch [22/50] batch [40/51] time 0.166 (0.216) data 0.000 (0.037) loss 0.5273 (0.4757) acc 83.6735 (89.1362) lr 1.3090e-03 eta 0:05:10
epoch [22/50] batch [45/51] time 0.176 (0.210) data 0.000 (0.033) loss 0.3422 (0.4783) acc 91.5094 (89.0725) lr 1.3090e-03 eta 0:05:01
epoch [22/50] batch [50/51] time 0.176 (0.206) data 0.000 (0.030) loss 0.4738 (0.4770) acc 89.6226 (89.1662) lr 1.3090e-03 eta 0:04:54
>>> alpha1: 0.197  alpha2: -0.142 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [23/50] batch [5/51] time 0.177 (0.475) data 0.001 (0.281) loss 0.3779 (0.4111) acc 90.6863 (91.6753) lr 1.2487e-03 eta 0:11:15
epoch [23/50] batch [10/51] time 0.171 (0.325) data 0.000 (0.141) loss 0.4509 (0.4231) acc 87.7660 (91.0454) lr 1.2487e-03 eta 0:07:41
epoch [23/50] batch [15/51] time 0.169 (0.276) data 0.000 (0.094) loss 0.4779 (0.5495) acc 90.3061 (88.8812) lr 1.2487e-03 eta 0:06:29
epoch [23/50] batch [20/51] time 0.165 (0.251) data 0.000 (0.070) loss 0.7305 (0.5539) acc 80.7292 (88.1367) lr 1.2487e-03 eta 0:05:53
epoch [23/50] batch [25/51] time 0.179 (0.237) data 0.000 (0.056) loss 0.4347 (0.5303) acc 90.2778 (88.6057) lr 1.2487e-03 eta 0:05:32
epoch [23/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.047) loss 0.4397 (0.5198) acc 89.1509 (88.5985) lr 1.2487e-03 eta 0:05:16
epoch [23/50] batch [35/51] time 0.174 (0.220) data 0.000 (0.040) loss 0.5772 (0.5096) acc 85.5000 (88.8693) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [40/51] time 0.172 (0.214) data 0.000 (0.035) loss 0.5149 (0.5034) acc 86.7647 (88.8135) lr 1.2487e-03 eta 0:04:57
epoch [23/50] batch [45/51] time 0.175 (0.209) data 0.000 (0.031) loss 0.4593 (0.5045) acc 90.0943 (88.7600) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.161 (0.205) data 0.000 (0.028) loss 0.6676 (0.5040) acc 85.1064 (88.8358) lr 1.2487e-03 eta 0:04:41
>>> alpha1: 0.198  alpha2: -0.149 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [24/50] batch [5/51] time 0.170 (0.492) data 0.001 (0.316) loss 0.5798 (0.5029) acc 85.5000 (89.9662) lr 1.1874e-03 eta 0:11:15
epoch [24/50] batch [10/51] time 0.194 (0.334) data 0.000 (0.158) loss 0.3177 (0.4599) acc 91.6667 (90.9252) lr 1.1874e-03 eta 0:07:36
epoch [24/50] batch [15/51] time 0.186 (0.279) data 0.000 (0.106) loss 0.4552 (0.4632) acc 90.5660 (90.6101) lr 1.1874e-03 eta 0:06:20
epoch [24/50] batch [20/51] time 0.186 (0.254) data 0.000 (0.079) loss 0.3457 (0.4761) acc 93.1373 (90.3127) lr 1.1874e-03 eta 0:05:44
epoch [24/50] batch [25/51] time 0.167 (0.238) data 0.000 (0.063) loss 0.5317 (0.4767) acc 88.2653 (90.1012) lr 1.1874e-03 eta 0:05:21
epoch [24/50] batch [30/51] time 0.169 (0.228) data 0.000 (0.053) loss 0.6516 (0.4735) acc 86.5000 (90.2505) lr 1.1874e-03 eta 0:05:07
epoch [24/50] batch [35/51] time 0.176 (0.220) data 0.000 (0.045) loss 0.4642 (0.4654) acc 91.1765 (90.4129) lr 1.1874e-03 eta 0:04:55
epoch [24/50] batch [40/51] time 0.179 (0.214) data 0.000 (0.040) loss 0.3209 (0.4721) acc 92.2727 (90.1248) lr 1.1874e-03 eta 0:04:46
epoch [24/50] batch [45/51] time 0.184 (0.211) data 0.000 (0.035) loss 0.3537 (0.4572) acc 91.6667 (90.2828) lr 1.1874e-03 eta 0:04:40
epoch [24/50] batch [50/51] time 0.165 (0.206) data 0.001 (0.032) loss 0.3946 (0.4533) acc 91.6667 (90.2857) lr 1.1874e-03 eta 0:04:33
>>> alpha1: 0.200  alpha2: -0.157 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [25/50] batch [5/51] time 0.169 (0.442) data 0.000 (0.258) loss 0.4767 (0.4413) acc 89.3617 (90.8530) lr 1.1253e-03 eta 0:09:43
epoch [25/50] batch [10/51] time 0.220 (0.315) data 0.001 (0.129) loss 0.6048 (0.4538) acc 83.6364 (89.4300) lr 1.1253e-03 eta 0:06:55
epoch [25/50] batch [15/51] time 0.183 (0.267) data 0.000 (0.086) loss 0.4527 (0.4603) acc 89.7059 (89.3801) lr 1.1253e-03 eta 0:05:49
epoch [25/50] batch [20/51] time 0.184 (0.244) data 0.000 (0.065) loss 0.4224 (0.4638) acc 91.9643 (89.5066) lr 1.1253e-03 eta 0:05:19
epoch [25/50] batch [25/51] time 0.178 (0.230) data 0.000 (0.052) loss 0.7303 (0.4775) acc 82.5000 (89.2032) lr 1.1253e-03 eta 0:04:59
epoch [25/50] batch [30/51] time 0.184 (0.221) data 0.000 (0.043) loss 0.3757 (0.4693) acc 91.0714 (89.3984) lr 1.1253e-03 eta 0:04:45
epoch [25/50] batch [35/51] time 0.183 (0.213) data 0.000 (0.037) loss 0.3896 (0.4718) acc 88.8393 (89.0411) lr 1.1253e-03 eta 0:04:35
epoch [25/50] batch [40/51] time 0.177 (0.209) data 0.000 (0.033) loss 0.3269 (0.5012) acc 93.5185 (88.9254) lr 1.1253e-03 eta 0:04:28
epoch [25/50] batch [45/51] time 0.165 (0.204) data 0.000 (0.029) loss 0.5852 (0.4894) acc 86.2245 (89.0605) lr 1.1253e-03 eta 0:04:21
epoch [25/50] batch [50/51] time 0.166 (0.201) data 0.000 (0.026) loss 0.3571 (0.4848) acc 91.0000 (89.1567) lr 1.1253e-03 eta 0:04:16
>>> alpha1: 0.199  alpha2: -0.156 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [26/50] batch [5/51] time 0.192 (0.461) data 0.000 (0.280) loss 0.3981 (0.3618) acc 91.6667 (92.5513) lr 1.0628e-03 eta 0:09:45
epoch [26/50] batch [10/51] time 0.187 (0.320) data 0.000 (0.140) loss 0.4645 (0.3856) acc 91.3265 (92.0526) lr 1.0628e-03 eta 0:06:44
epoch [26/50] batch [15/51] time 0.170 (0.271) data 0.000 (0.094) loss 0.6255 (0.4066) acc 87.0000 (91.7502) lr 1.0628e-03 eta 0:05:41
epoch [26/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.070) loss 0.3702 (0.4169) acc 92.1569 (91.5886) lr 1.0628e-03 eta 0:05:09
epoch [26/50] batch [25/51] time 0.162 (0.232) data 0.000 (0.056) loss 0.4704 (0.4316) acc 87.7660 (90.8918) lr 1.0628e-03 eta 0:04:49
epoch [26/50] batch [30/51] time 0.169 (0.222) data 0.000 (0.047) loss 0.5050 (0.4260) acc 85.0000 (90.8354) lr 1.0628e-03 eta 0:04:36
epoch [26/50] batch [35/51] time 0.160 (0.215) data 0.000 (0.040) loss 0.4920 (0.4191) acc 88.8889 (91.0361) lr 1.0628e-03 eta 0:04:26
epoch [26/50] batch [40/51] time 0.181 (0.210) data 0.000 (0.035) loss 0.4861 (0.4209) acc 89.0909 (91.0829) lr 1.0628e-03 eta 0:04:19
epoch [26/50] batch [45/51] time 0.167 (0.206) data 0.000 (0.031) loss 0.3331 (0.4215) acc 93.3673 (91.0379) lr 1.0628e-03 eta 0:04:13
epoch [26/50] batch [50/51] time 0.171 (0.203) data 0.000 (0.028) loss 0.4423 (0.4264) acc 85.7843 (90.8099) lr 1.0628e-03 eta 0:04:08
>>> alpha1: 0.201  alpha2: -0.166 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [27/50] batch [5/51] time 0.176 (0.456) data 0.000 (0.271) loss 0.4348 (0.5676) acc 91.3462 (90.8497) lr 1.0000e-03 eta 0:09:16
epoch [27/50] batch [10/51] time 0.162 (0.315) data 0.000 (0.136) loss 0.4557 (0.5053) acc 91.3043 (90.5522) lr 1.0000e-03 eta 0:06:22
epoch [27/50] batch [15/51] time 0.175 (0.269) data 0.000 (0.091) loss 0.3130 (0.4878) acc 94.2308 (90.4737) lr 1.0000e-03 eta 0:05:25
epoch [27/50] batch [20/51] time 0.185 (0.246) data 0.002 (0.068) loss 0.3969 (0.4647) acc 90.0943 (90.6966) lr 1.0000e-03 eta 0:04:56
epoch [27/50] batch [25/51] time 0.193 (0.232) data 0.001 (0.055) loss 0.4252 (0.4693) acc 88.4259 (90.5276) lr 1.0000e-03 eta 0:04:38
epoch [27/50] batch [30/51] time 0.167 (0.223) data 0.000 (0.046) loss 0.4508 (0.4752) acc 89.8936 (90.2347) lr 1.0000e-03 eta 0:04:25
epoch [27/50] batch [35/51] time 0.176 (0.216) data 0.000 (0.039) loss 0.5470 (0.4704) acc 88.2653 (90.0556) lr 1.0000e-03 eta 0:04:17
epoch [27/50] batch [40/51] time 0.177 (0.211) data 0.000 (0.034) loss 0.4192 (0.4640) acc 93.3962 (90.2366) lr 1.0000e-03 eta 0:04:10
epoch [27/50] batch [45/51] time 0.175 (0.207) data 0.000 (0.030) loss 0.4093 (0.4610) acc 87.7358 (90.1844) lr 1.0000e-03 eta 0:04:04
epoch [27/50] batch [50/51] time 0.162 (0.203) data 0.000 (0.027) loss 0.6455 (0.4620) acc 85.6383 (90.1210) lr 1.0000e-03 eta 0:03:58
>>> alpha1: 0.202  alpha2: -0.171 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [28/50] batch [5/51] time 0.170 (0.451) data 0.000 (0.273) loss 0.7050 (0.4456) acc 81.3726 (89.9066) lr 9.3721e-04 eta 0:08:47
epoch [28/50] batch [10/51] time 0.181 (0.313) data 0.000 (0.136) loss 0.4329 (0.4091) acc 88.8889 (90.8664) lr 9.3721e-04 eta 0:06:04
epoch [28/50] batch [15/51] time 0.191 (0.266) data 0.000 (0.091) loss 0.3031 (0.3970) acc 93.8679 (91.0464) lr 9.3721e-04 eta 0:05:07
epoch [28/50] batch [20/51] time 0.171 (0.242) data 0.000 (0.068) loss 0.5232 (0.4058) acc 91.1765 (91.1227) lr 9.3721e-04 eta 0:04:38
epoch [28/50] batch [25/51] time 0.167 (0.228) data 0.000 (0.055) loss 0.4613 (0.4161) acc 88.5417 (90.6789) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [30/51] time 0.174 (0.219) data 0.000 (0.046) loss 0.2632 (0.4113) acc 94.7115 (90.8100) lr 9.3721e-04 eta 0:04:10
epoch [28/50] batch [35/51] time 0.171 (0.213) data 0.000 (0.039) loss 0.4085 (0.4213) acc 89.8936 (90.4847) lr 9.3721e-04 eta 0:04:02
epoch [28/50] batch [40/51] time 0.174 (0.208) data 0.000 (0.034) loss 0.3707 (0.4295) acc 93.2692 (90.2359) lr 9.3721e-04 eta 0:03:55
epoch [28/50] batch [45/51] time 0.179 (0.204) data 0.000 (0.030) loss 0.4340 (0.4214) acc 87.9630 (90.3370) lr 9.3721e-04 eta 0:03:50
epoch [28/50] batch [50/51] time 0.187 (0.201) data 0.000 (0.027) loss 0.4917 (0.4184) acc 89.2241 (90.4862) lr 9.3721e-04 eta 0:03:46
>>> alpha1: 0.198  alpha2: -0.179 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [29/50] batch [5/51] time 0.171 (0.440) data 0.000 (0.265) loss 0.2983 (0.3993) acc 92.6471 (91.3750) lr 8.7467e-04 eta 0:08:11
epoch [29/50] batch [10/51] time 0.169 (0.305) data 0.000 (0.132) loss 0.5645 (0.4336) acc 87.5000 (90.3536) lr 8.7467e-04 eta 0:05:39
epoch [29/50] batch [15/51] time 0.186 (0.262) data 0.000 (0.088) loss 0.3167 (0.4448) acc 92.7273 (90.1082) lr 8.7467e-04 eta 0:04:49
epoch [29/50] batch [20/51] time 0.177 (0.240) data 0.000 (0.066) loss 0.5253 (0.4229) acc 86.7647 (90.4261) lr 8.7467e-04 eta 0:04:24
epoch [29/50] batch [25/51] time 0.180 (0.227) data 0.000 (0.053) loss 0.4864 (0.4331) acc 86.5385 (89.9948) lr 8.7467e-04 eta 0:04:08
epoch [29/50] batch [30/51] time 0.188 (0.220) data 0.000 (0.044) loss 0.4119 (0.4350) acc 91.0714 (89.7785) lr 8.7467e-04 eta 0:03:59
epoch [29/50] batch [35/51] time 0.175 (0.213) data 0.000 (0.038) loss 0.3853 (0.4312) acc 89.8936 (89.8044) lr 8.7467e-04 eta 0:03:51
epoch [29/50] batch [40/51] time 0.160 (0.208) data 0.000 (0.033) loss 0.4254 (0.4267) acc 89.8936 (90.0611) lr 8.7467e-04 eta 0:03:45
epoch [29/50] batch [45/51] time 0.164 (0.204) data 0.000 (0.030) loss 0.3823 (0.4307) acc 89.2857 (90.0299) lr 8.7467e-04 eta 0:03:39
epoch [29/50] batch [50/51] time 0.169 (0.201) data 0.000 (0.027) loss 0.4581 (0.4366) acc 93.5000 (90.0596) lr 8.7467e-04 eta 0:03:34
>>> alpha1: 0.194  alpha2: -0.184 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [30/50] batch [5/51] time 0.171 (0.476) data 0.000 (0.297) loss 0.4172 (0.3656) acc 90.1961 (93.3580) lr 8.1262e-04 eta 0:08:27
epoch [30/50] batch [10/51] time 0.167 (0.326) data 0.000 (0.149) loss 0.4367 (0.3995) acc 92.1875 (92.4163) lr 8.1262e-04 eta 0:05:46
epoch [30/50] batch [15/51] time 0.181 (0.278) data 0.000 (0.099) loss 0.2453 (0.3874) acc 94.0909 (91.9642) lr 8.1262e-04 eta 0:04:53
epoch [30/50] batch [20/51] time 0.184 (0.253) data 0.000 (0.074) loss 0.2706 (0.3748) acc 96.8750 (92.2447) lr 8.1262e-04 eta 0:04:25
epoch [30/50] batch [25/51] time 0.174 (0.237) data 0.001 (0.060) loss 0.5236 (0.3959) acc 90.3061 (91.4562) lr 8.1262e-04 eta 0:04:07
epoch [30/50] batch [30/51] time 0.175 (0.227) data 0.000 (0.050) loss 0.4238 (0.3976) acc 92.7885 (91.5625) lr 8.1262e-04 eta 0:03:55
epoch [30/50] batch [35/51] time 0.202 (0.221) data 0.000 (0.043) loss 0.3202 (0.3961) acc 91.8367 (91.5725) lr 8.1262e-04 eta 0:03:49
epoch [30/50] batch [40/51] time 0.178 (0.215) data 0.000 (0.037) loss 0.5095 (0.4090) acc 89.3519 (90.9224) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [45/51] time 0.180 (0.210) data 0.000 (0.033) loss 0.5469 (0.4149) acc 85.6481 (90.6710) lr 8.1262e-04 eta 0:03:35
epoch [30/50] batch [50/51] time 0.167 (0.206) data 0.000 (0.030) loss 0.4359 (0.4090) acc 93.3673 (90.9137) lr 8.1262e-04 eta 0:03:30
>>> alpha1: 0.193  alpha2: -0.197 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [31/50] batch [5/51] time 0.179 (0.481) data 0.000 (0.289) loss 0.4098 (0.3986) acc 89.7059 (89.7596) lr 7.5131e-04 eta 0:08:07
epoch [31/50] batch [10/51] time 0.205 (0.335) data 0.000 (0.144) loss 0.4410 (0.4281) acc 90.3846 (89.6349) lr 7.5131e-04 eta 0:05:38
epoch [31/50] batch [15/51] time 0.176 (0.282) data 0.000 (0.096) loss 0.4546 (0.4121) acc 88.7755 (90.4130) lr 7.5131e-04 eta 0:04:42
epoch [31/50] batch [20/51] time 0.177 (0.256) data 0.000 (0.072) loss 0.2710 (0.4050) acc 95.7447 (90.8525) lr 7.5131e-04 eta 0:04:15
epoch [31/50] batch [25/51] time 0.174 (0.239) data 0.000 (0.058) loss 0.6438 (0.4109) acc 84.8039 (90.6830) lr 7.5131e-04 eta 0:03:57
epoch [31/50] batch [30/51] time 0.174 (0.228) data 0.000 (0.048) loss 0.4157 (0.4123) acc 92.7885 (90.8136) lr 7.5131e-04 eta 0:03:46
epoch [31/50] batch [35/51] time 0.178 (0.221) data 0.000 (0.041) loss 0.4623 (0.4206) acc 91.5094 (90.7672) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [40/51] time 0.170 (0.215) data 0.000 (0.036) loss 0.4569 (0.4265) acc 88.2353 (90.6380) lr 7.5131e-04 eta 0:03:30
epoch [31/50] batch [45/51] time 0.176 (0.210) data 0.000 (0.032) loss 0.4111 (0.4214) acc 88.6792 (90.7096) lr 7.5131e-04 eta 0:03:24
epoch [31/50] batch [50/51] time 0.170 (0.206) data 0.000 (0.029) loss 0.4078 (0.4260) acc 92.1569 (90.7834) lr 7.5131e-04 eta 0:03:20
>>> alpha1: 0.190  alpha2: -0.205 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [32/50] batch [5/51] time 0.169 (0.470) data 0.000 (0.281) loss 0.4039 (0.3549) acc 92.0000 (91.7743) lr 6.9098e-04 eta 0:07:33
epoch [32/50] batch [10/51] time 0.171 (0.326) data 0.000 (0.141) loss 0.4307 (0.4033) acc 89.2157 (91.1516) lr 6.9098e-04 eta 0:05:12
epoch [32/50] batch [15/51] time 0.171 (0.276) data 0.000 (0.094) loss 0.2622 (0.4030) acc 95.0980 (91.4826) lr 6.9098e-04 eta 0:04:23
epoch [32/50] batch [20/51] time 0.172 (0.252) data 0.000 (0.071) loss 0.6913 (0.4126) acc 83.8542 (91.1170) lr 6.9098e-04 eta 0:03:59
epoch [32/50] batch [25/51] time 0.172 (0.237) data 0.000 (0.056) loss 0.3803 (0.4220) acc 91.1458 (90.8311) lr 6.9098e-04 eta 0:03:43
epoch [32/50] batch [30/51] time 0.174 (0.227) data 0.000 (0.047) loss 0.2791 (0.4259) acc 96.1538 (90.7897) lr 6.9098e-04 eta 0:03:32
epoch [32/50] batch [35/51] time 0.177 (0.220) data 0.000 (0.040) loss 0.2826 (0.4161) acc 93.8775 (90.9822) lr 6.9098e-04 eta 0:03:25
epoch [32/50] batch [40/51] time 0.171 (0.214) data 0.000 (0.035) loss 0.5147 (0.4113) acc 91.1765 (91.1319) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [45/51] time 0.162 (0.209) data 0.000 (0.031) loss 0.3684 (0.4157) acc 93.0851 (91.1678) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [50/51] time 0.167 (0.206) data 0.000 (0.028) loss 0.3140 (0.4117) acc 91.3265 (91.2903) lr 6.9098e-04 eta 0:03:09
>>> alpha1: 0.186  alpha2: -0.208 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [33/50] batch [5/51] time 0.174 (0.492) data 0.000 (0.310) loss 0.5431 (0.4286) acc 86.0577 (90.6165) lr 6.3188e-04 eta 0:07:29
epoch [33/50] batch [10/51] time 0.162 (0.340) data 0.000 (0.155) loss 0.5010 (0.4635) acc 86.9565 (89.9018) lr 6.3188e-04 eta 0:05:08
epoch [33/50] batch [15/51] time 0.183 (0.289) data 0.000 (0.104) loss 0.4128 (0.4422) acc 91.5094 (90.4081) lr 6.3188e-04 eta 0:04:20
epoch [33/50] batch [20/51] time 0.194 (0.264) data 0.000 (0.078) loss 0.4171 (0.4335) acc 90.3846 (90.0962) lr 6.3188e-04 eta 0:03:57
epoch [33/50] batch [25/51] time 0.187 (0.248) data 0.000 (0.062) loss 0.4304 (0.4250) acc 91.6667 (90.4642) lr 6.3188e-04 eta 0:03:41
epoch [33/50] batch [30/51] time 0.180 (0.236) data 0.000 (0.052) loss 0.2946 (0.4265) acc 96.2963 (90.5777) lr 6.3188e-04 eta 0:03:29
epoch [33/50] batch [35/51] time 0.173 (0.227) data 0.000 (0.045) loss 0.4421 (0.4336) acc 91.6667 (90.6295) lr 6.3188e-04 eta 0:03:20
epoch [33/50] batch [40/51] time 0.193 (0.221) data 0.001 (0.039) loss 0.4615 (0.4295) acc 88.1818 (90.7580) lr 6.3188e-04 eta 0:03:13
epoch [33/50] batch [45/51] time 0.187 (0.216) data 0.000 (0.035) loss 0.6429 (0.4307) acc 82.5893 (90.7363) lr 6.3188e-04 eta 0:03:08
epoch [33/50] batch [50/51] time 0.172 (0.212) data 0.000 (0.031) loss 0.3177 (0.4337) acc 94.1176 (90.5693) lr 6.3188e-04 eta 0:03:03
>>> alpha1: 0.185  alpha2: -0.210 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [34/50] batch [5/51] time 0.180 (0.472) data 0.000 (0.291) loss 0.3329 (0.3804) acc 93.9815 (91.5476) lr 5.7422e-04 eta 0:06:47
epoch [34/50] batch [10/51] time 0.167 (0.324) data 0.000 (0.145) loss 0.3968 (0.3696) acc 92.8571 (92.4230) lr 5.7422e-04 eta 0:04:37
epoch [34/50] batch [15/51] time 0.194 (0.275) data 0.000 (0.097) loss 0.5882 (0.3994) acc 86.3208 (91.3248) lr 5.7422e-04 eta 0:03:54
epoch [34/50] batch [20/51] time 0.184 (0.251) data 0.000 (0.073) loss 0.4358 (0.3933) acc 93.8775 (91.6961) lr 5.7422e-04 eta 0:03:32
epoch [34/50] batch [25/51] time 0.170 (0.234) data 0.000 (0.058) loss 0.3412 (0.3903) acc 90.5000 (91.3239) lr 5.7422e-04 eta 0:03:17
epoch [34/50] batch [30/51] time 0.171 (0.226) data 0.000 (0.049) loss 0.3562 (0.3933) acc 92.1569 (91.1961) lr 5.7422e-04 eta 0:03:08
epoch [34/50] batch [35/51] time 0.176 (0.219) data 0.000 (0.042) loss 0.6135 (0.4048) acc 83.4906 (90.9282) lr 5.7422e-04 eta 0:03:01
epoch [34/50] batch [40/51] time 0.173 (0.213) data 0.000 (0.037) loss 0.2824 (0.3996) acc 94.7115 (91.1027) lr 5.7422e-04 eta 0:02:55
epoch [34/50] batch [45/51] time 0.161 (0.208) data 0.000 (0.033) loss 0.5590 (0.4075) acc 87.7660 (90.9702) lr 5.7422e-04 eta 0:02:50
epoch [34/50] batch [50/51] time 0.157 (0.204) data 0.000 (0.029) loss 0.5160 (0.4057) acc 87.7778 (90.9161) lr 5.7422e-04 eta 0:02:46
>>> alpha1: 0.186  alpha2: -0.212 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [35/50] batch [5/51] time 0.177 (0.465) data 0.000 (0.277) loss 0.3996 (0.3519) acc 92.9245 (91.9943) lr 5.1825e-04 eta 0:06:16
epoch [35/50] batch [10/51] time 0.189 (0.321) data 0.000 (0.139) loss 0.2663 (0.3522) acc 93.6274 (92.2069) lr 5.1825e-04 eta 0:04:19
epoch [35/50] batch [15/51] time 0.183 (0.275) data 0.000 (0.092) loss 0.3605 (0.3797) acc 94.4444 (91.6720) lr 5.1825e-04 eta 0:03:40
epoch [35/50] batch [20/51] time 0.184 (0.249) data 0.000 (0.069) loss 0.3843 (0.3782) acc 93.7500 (91.7387) lr 5.1825e-04 eta 0:03:18
epoch [35/50] batch [25/51] time 0.177 (0.234) data 0.000 (0.056) loss 0.3026 (0.3836) acc 94.8113 (91.5601) lr 5.1825e-04 eta 0:03:05
epoch [35/50] batch [30/51] time 0.181 (0.224) data 0.000 (0.046) loss 0.3457 (0.3892) acc 94.1176 (91.6255) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.173 (0.218) data 0.000 (0.040) loss 0.2294 (0.3833) acc 96.1538 (91.8550) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [40/51] time 0.171 (0.213) data 0.000 (0.035) loss 0.4687 (0.3874) acc 87.2549 (91.6940) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [45/51] time 0.170 (0.208) data 0.000 (0.031) loss 0.4772 (0.3885) acc 88.2353 (91.6081) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [50/51] time 0.161 (0.204) data 0.000 (0.028) loss 0.3547 (0.3931) acc 92.0213 (91.4057) lr 5.1825e-04 eta 0:02:36
>>> alpha1: 0.181  alpha2: -0.206 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [36/50] batch [5/51] time 0.177 (0.572) data 0.000 (0.276) loss 0.4049 (0.3696) acc 90.5660 (91.6690) lr 4.6417e-04 eta 0:07:14
epoch [36/50] batch [10/51] time 0.190 (0.375) data 0.000 (0.138) loss 0.2781 (0.4019) acc 96.2264 (91.7768) lr 4.6417e-04 eta 0:04:42
epoch [36/50] batch [15/51] time 0.165 (0.311) data 0.000 (0.092) loss 0.4260 (0.3838) acc 92.1875 (92.4576) lr 4.6417e-04 eta 0:03:53
epoch [36/50] batch [20/51] time 0.163 (0.278) data 0.000 (0.069) loss 0.4881 (0.3857) acc 89.3617 (91.8837) lr 4.6417e-04 eta 0:03:26
epoch [36/50] batch [25/51] time 0.173 (0.258) data 0.000 (0.056) loss 0.4359 (0.3895) acc 89.7059 (91.5285) lr 4.6417e-04 eta 0:03:10
epoch [36/50] batch [30/51] time 0.167 (0.243) data 0.000 (0.046) loss 0.2860 (0.3928) acc 92.8571 (91.4766) lr 4.6417e-04 eta 0:02:58
epoch [36/50] batch [35/51] time 0.173 (0.234) data 0.000 (0.040) loss 0.2467 (0.3836) acc 96.1538 (91.7487) lr 4.6417e-04 eta 0:02:50
epoch [36/50] batch [40/51] time 0.177 (0.227) data 0.000 (0.035) loss 0.2603 (0.3767) acc 93.0556 (91.7092) lr 4.6417e-04 eta 0:02:44
epoch [36/50] batch [45/51] time 0.177 (0.221) data 0.000 (0.031) loss 0.4262 (0.3838) acc 89.6226 (91.5863) lr 4.6417e-04 eta 0:02:38
epoch [36/50] batch [50/51] time 0.168 (0.216) data 0.000 (0.028) loss 0.3502 (0.3886) acc 94.5000 (91.3950) lr 4.6417e-04 eta 0:02:34
>>> alpha1: 0.180  alpha2: -0.201 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [37/50] batch [5/51] time 0.169 (0.464) data 0.000 (0.285) loss 0.3410 (0.3872) acc 92.0000 (93.3698) lr 4.1221e-04 eta 0:05:29
epoch [37/50] batch [10/51] time 0.166 (0.320) data 0.000 (0.142) loss 0.4450 (0.3643) acc 90.8163 (93.5499) lr 4.1221e-04 eta 0:03:45
epoch [37/50] batch [15/51] time 0.170 (0.270) data 0.000 (0.095) loss 0.2579 (0.3690) acc 96.5686 (92.7744) lr 4.1221e-04 eta 0:03:09
epoch [37/50] batch [20/51] time 0.189 (0.247) data 0.001 (0.071) loss 0.3483 (0.3790) acc 93.4211 (92.2920) lr 4.1221e-04 eta 0:02:51
epoch [37/50] batch [25/51] time 0.180 (0.232) data 0.000 (0.057) loss 0.4109 (0.3783) acc 90.6863 (92.2015) lr 4.1221e-04 eta 0:02:40
epoch [37/50] batch [30/51] time 0.178 (0.223) data 0.000 (0.048) loss 0.3503 (0.3771) acc 93.3673 (92.0131) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [35/51] time 0.178 (0.217) data 0.001 (0.041) loss 0.4802 (0.3844) acc 87.2642 (91.8390) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [40/51] time 0.159 (0.211) data 0.000 (0.036) loss 0.4302 (0.3893) acc 91.6667 (91.6493) lr 4.1221e-04 eta 0:02:22
epoch [37/50] batch [45/51] time 0.178 (0.207) data 0.000 (0.032) loss 0.3777 (0.3872) acc 90.2778 (91.5586) lr 4.1221e-04 eta 0:02:18
epoch [37/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.029) loss 0.4309 (0.3900) acc 91.5000 (91.5102) lr 4.1221e-04 eta 0:02:15
>>> alpha1: 0.181  alpha2: -0.188 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [38/50] batch [5/51] time 0.219 (0.470) data 0.000 (0.283) loss 0.2773 (0.3140) acc 93.0000 (92.8978) lr 3.6258e-04 eta 0:05:09
epoch [38/50] batch [10/51] time 0.181 (0.324) data 0.000 (0.141) loss 0.3499 (0.3520) acc 94.3396 (91.8129) lr 3.6258e-04 eta 0:03:31
epoch [38/50] batch [15/51] time 0.191 (0.274) data 0.000 (0.094) loss 0.4215 (0.3756) acc 90.0943 (91.2350) lr 3.6258e-04 eta 0:02:57
epoch [38/50] batch [20/51] time 0.185 (0.249) data 0.000 (0.071) loss 0.3833 (0.4086) acc 91.3636 (90.7384) lr 3.6258e-04 eta 0:02:40
epoch [38/50] batch [25/51] time 0.186 (0.236) data 0.000 (0.057) loss 0.4216 (0.4039) acc 88.9423 (90.8222) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [30/51] time 0.183 (0.226) data 0.000 (0.047) loss 0.3430 (0.4079) acc 92.7885 (90.6994) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.167 (0.219) data 0.000 (0.041) loss 0.3197 (0.4062) acc 94.3878 (90.8641) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [40/51] time 0.174 (0.214) data 0.000 (0.036) loss 0.4075 (0.4081) acc 91.3462 (90.9073) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [45/51] time 0.172 (0.209) data 0.000 (0.032) loss 0.2718 (0.4077) acc 94.2308 (90.9325) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [50/51] time 0.172 (0.205) data 0.000 (0.028) loss 0.3157 (0.4092) acc 91.8269 (90.9261) lr 3.6258e-04 eta 0:02:05
>>> alpha1: 0.179  alpha2: -0.191 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.11 <<<
epoch [39/50] batch [5/51] time 0.169 (0.459) data 0.000 (0.278) loss 0.3619 (0.3404) acc 93.0000 (92.7608) lr 3.1545e-04 eta 0:04:38
epoch [39/50] batch [10/51] time 0.179 (0.314) data 0.000 (0.139) loss 0.4148 (0.3804) acc 92.1296 (91.6919) lr 3.1545e-04 eta 0:03:09
epoch [39/50] batch [15/51] time 0.172 (0.271) data 0.000 (0.093) loss 0.4358 (0.3908) acc 87.7551 (90.9515) lr 3.1545e-04 eta 0:02:41
epoch [39/50] batch [20/51] time 0.170 (0.248) data 0.000 (0.070) loss 0.3410 (0.3848) acc 91.0000 (91.2050) lr 3.1545e-04 eta 0:02:26
epoch [39/50] batch [25/51] time 0.180 (0.234) data 0.000 (0.056) loss 0.4614 (0.3871) acc 88.9423 (91.3112) lr 3.1545e-04 eta 0:02:17
epoch [39/50] batch [30/51] time 0.165 (0.224) data 0.000 (0.047) loss 0.1927 (0.4031) acc 97.7273 (91.4196) lr 3.1545e-04 eta 0:02:10
epoch [39/50] batch [35/51] time 0.175 (0.217) data 0.000 (0.040) loss 0.3358 (0.3979) acc 91.8269 (91.4143) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [40/51] time 0.176 (0.212) data 0.000 (0.035) loss 0.5052 (0.4047) acc 89.1509 (91.2124) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [45/51] time 0.174 (0.207) data 0.000 (0.031) loss 0.3628 (0.4012) acc 92.3077 (91.3124) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [50/51] time 0.167 (0.204) data 0.001 (0.028) loss 0.4302 (0.4041) acc 91.3265 (91.2474) lr 3.1545e-04 eta 0:01:54
>>> alpha1: 0.181  alpha2: -0.193 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [40/50] batch [5/51] time 0.170 (0.456) data 0.000 (0.269) loss 0.3774 (0.3508) acc 93.0000 (92.9735) lr 2.7103e-04 eta 0:04:13
epoch [40/50] batch [10/51] time 0.173 (0.317) data 0.000 (0.135) loss 0.4075 (0.3759) acc 91.0000 (91.8018) lr 2.7103e-04 eta 0:02:54
epoch [40/50] batch [15/51] time 0.176 (0.269) data 0.000 (0.090) loss 0.4584 (0.3894) acc 88.7255 (91.2675) lr 2.7103e-04 eta 0:02:26
epoch [40/50] batch [20/51] time 0.198 (0.247) data 0.000 (0.067) loss 0.2710 (0.3879) acc 91.9643 (91.1103) lr 2.7103e-04 eta 0:02:13
epoch [40/50] batch [25/51] time 0.171 (0.232) data 0.000 (0.054) loss 0.5038 (0.3900) acc 87.7451 (91.1695) lr 2.7103e-04 eta 0:02:04
epoch [40/50] batch [30/51] time 0.179 (0.223) data 0.000 (0.046) loss 0.2569 (0.3839) acc 93.5185 (91.3369) lr 2.7103e-04 eta 0:01:58
epoch [40/50] batch [35/51] time 0.177 (0.216) data 0.000 (0.039) loss 0.5373 (0.3908) acc 86.9792 (91.3990) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [40/51] time 0.169 (0.211) data 0.000 (0.034) loss 0.4192 (0.3901) acc 87.7551 (91.2904) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [45/51] time 0.178 (0.206) data 0.000 (0.030) loss 0.2409 (0.3851) acc 96.2963 (91.4333) lr 2.7103e-04 eta 0:01:46
epoch [40/50] batch [50/51] time 0.162 (0.203) data 0.000 (0.027) loss 0.2628 (0.3815) acc 96.2766 (91.5318) lr 2.7103e-04 eta 0:01:43
>>> alpha1: 0.180  alpha2: -0.198 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.10 <<<
epoch [41/50] batch [5/51] time 0.168 (0.456) data 0.000 (0.278) loss 0.2675 (0.2607) acc 92.5000 (95.2226) lr 2.2949e-04 eta 0:03:50
epoch [41/50] batch [10/51] time 0.177 (0.316) data 0.000 (0.139) loss 0.4207 (0.3572) acc 90.5660 (93.4201) lr 2.2949e-04 eta 0:02:37
epoch [41/50] batch [15/51] time 0.159 (0.268) data 0.000 (0.093) loss 0.4095 (0.3669) acc 92.2222 (93.4282) lr 2.2949e-04 eta 0:02:12
epoch [41/50] batch [20/51] time 0.163 (0.247) data 0.000 (0.070) loss 0.3992 (0.3525) acc 91.4894 (93.5028) lr 2.2949e-04 eta 0:02:00
epoch [41/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.056) loss 0.2665 (0.3514) acc 95.6731 (93.4121) lr 2.2949e-04 eta 0:01:52
epoch [41/50] batch [30/51] time 0.184 (0.224) data 0.000 (0.047) loss 0.3792 (0.3628) acc 93.3036 (92.9487) lr 2.2949e-04 eta 0:01:47
epoch [41/50] batch [35/51] time 0.178 (0.217) data 0.000 (0.040) loss 0.5925 (0.3789) acc 87.5000 (92.5322) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [40/51] time 0.170 (0.212) data 0.000 (0.035) loss 0.3051 (0.3836) acc 93.6274 (92.2689) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [45/51] time 0.178 (0.207) data 0.000 (0.031) loss 0.4826 (0.3885) acc 89.1509 (92.2058) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [50/51] time 0.185 (0.204) data 0.001 (0.028) loss 0.4775 (0.3995) acc 90.7407 (92.0122) lr 2.2949e-04 eta 0:01:33
>>> alpha1: 0.180  alpha2: -0.191 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.12 <<<
epoch [42/50] batch [5/51] time 0.173 (0.454) data 0.000 (0.276) loss 0.4248 (0.3978) acc 90.6863 (91.3664) lr 1.9098e-04 eta 0:03:25
epoch [42/50] batch [10/51] time 0.174 (0.316) data 0.000 (0.138) loss 0.2155 (0.3552) acc 98.5000 (92.2410) lr 1.9098e-04 eta 0:02:22
epoch [42/50] batch [15/51] time 0.167 (0.270) data 0.001 (0.092) loss 0.4330 (0.3609) acc 91.6667 (92.3029) lr 1.9098e-04 eta 0:01:59
epoch [42/50] batch [20/51] time 0.181 (0.247) data 0.000 (0.069) loss 0.1995 (0.3578) acc 96.8182 (92.2308) lr 1.9098e-04 eta 0:01:48
epoch [42/50] batch [25/51] time 0.172 (0.232) data 0.000 (0.055) loss 0.3760 (0.3717) acc 90.6250 (91.4163) lr 1.9098e-04 eta 0:01:40
epoch [42/50] batch [30/51] time 0.169 (0.222) data 0.000 (0.046) loss 0.4636 (0.3814) acc 88.0000 (91.3501) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [35/51] time 0.192 (0.216) data 0.000 (0.040) loss 0.3314 (0.3704) acc 94.8113 (91.7275) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.160 (0.211) data 0.000 (0.035) loss 0.2955 (0.3710) acc 94.1489 (91.6954) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.173 (0.207) data 0.000 (0.031) loss 0.2916 (0.3769) acc 93.2692 (91.6105) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.160 (0.202) data 0.000 (0.028) loss 0.4928 (0.3797) acc 90.4255 (91.5626) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.177  alpha2: -0.194 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.11 <<<
epoch [43/50] batch [5/51] time 0.163 (0.460) data 0.000 (0.278) loss 0.4314 (0.3838) acc 89.8936 (91.4224) lr 1.5567e-04 eta 0:03:05
epoch [43/50] batch [10/51] time 0.182 (0.318) data 0.000 (0.139) loss 0.2367 (0.3275) acc 95.0893 (92.8520) lr 1.5567e-04 eta 0:02:06
epoch [43/50] batch [15/51] time 0.169 (0.270) data 0.000 (0.093) loss 0.2627 (0.3437) acc 94.0000 (92.5943) lr 1.5567e-04 eta 0:01:46
epoch [43/50] batch [20/51] time 0.165 (0.247) data 0.001 (0.070) loss 0.6173 (0.3581) acc 87.5000 (92.3588) lr 1.5567e-04 eta 0:01:35
epoch [43/50] batch [25/51] time 0.161 (0.233) data 0.000 (0.056) loss 0.4509 (0.3635) acc 90.7609 (92.0693) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [30/51] time 0.166 (0.224) data 0.000 (0.047) loss 0.2991 (0.3791) acc 93.3673 (91.6200) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [35/51] time 0.172 (0.217) data 0.000 (0.040) loss 0.3826 (0.3763) acc 91.0000 (91.6717) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [40/51] time 0.179 (0.212) data 0.000 (0.035) loss 0.4932 (0.3816) acc 90.9091 (91.4422) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.031) loss 0.3621 (0.3794) acc 92.8571 (91.3649) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [50/51] time 0.189 (0.205) data 0.000 (0.028) loss 0.2213 (0.3765) acc 97.7679 (91.5099) lr 1.5567e-04 eta 0:01:13
>>> alpha1: 0.176  alpha2: -0.193 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [44/50] batch [5/51] time 0.172 (0.463) data 0.000 (0.281) loss 0.2228 (0.3506) acc 97.0588 (91.6113) lr 1.2369e-04 eta 0:02:42
epoch [44/50] batch [10/51] time 0.172 (0.329) data 0.000 (0.141) loss 0.3461 (0.3723) acc 92.6471 (91.4721) lr 1.2369e-04 eta 0:01:54
epoch [44/50] batch [15/51] time 0.198 (0.281) data 0.000 (0.094) loss 0.3973 (0.3693) acc 87.9630 (91.7465) lr 1.2369e-04 eta 0:01:36
epoch [44/50] batch [20/51] time 0.192 (0.255) data 0.000 (0.070) loss 0.4004 (0.3718) acc 89.7321 (92.0197) lr 1.2369e-04 eta 0:01:25
epoch [44/50] batch [25/51] time 0.187 (0.240) data 0.000 (0.056) loss 0.2683 (0.3747) acc 92.1296 (91.8211) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [30/51] time 0.174 (0.229) data 0.000 (0.047) loss 0.3055 (0.3761) acc 93.7500 (91.9035) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [35/51] time 0.173 (0.221) data 0.000 (0.040) loss 0.3725 (0.3852) acc 92.7885 (91.6755) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.163 (0.215) data 0.000 (0.035) loss 0.3375 (0.3804) acc 91.6667 (91.8529) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.165 (0.209) data 0.000 (0.031) loss 0.5074 (0.3896) acc 86.9792 (91.4903) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [50/51] time 0.166 (0.205) data 0.000 (0.028) loss 0.2104 (0.3818) acc 94.3878 (91.6080) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.174  alpha2: -0.197 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.11 <<<
epoch [45/50] batch [5/51] time 0.178 (0.466) data 0.001 (0.287) loss 0.4399 (0.4253) acc 94.7115 (90.9097) lr 9.5173e-05 eta 0:02:20
epoch [45/50] batch [10/51] time 0.173 (0.324) data 0.000 (0.144) loss 0.3566 (0.4088) acc 91.3462 (90.8076) lr 9.5173e-05 eta 0:01:35
epoch [45/50] batch [15/51] time 0.182 (0.274) data 0.000 (0.096) loss 0.5105 (0.4340) acc 91.1765 (90.1676) lr 9.5173e-05 eta 0:01:19
epoch [45/50] batch [20/51] time 0.194 (0.250) data 0.000 (0.072) loss 0.3742 (0.4153) acc 89.4231 (90.6805) lr 9.5173e-05 eta 0:01:11
epoch [45/50] batch [25/51] time 0.168 (0.235) data 0.000 (0.058) loss 0.4220 (0.3996) acc 93.7500 (91.0303) lr 9.5173e-05 eta 0:01:06
epoch [45/50] batch [30/51] time 0.165 (0.225) data 0.001 (0.048) loss 0.6804 (0.3972) acc 85.4167 (91.0914) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [35/51] time 0.182 (0.218) data 0.000 (0.041) loss 0.5041 (0.4037) acc 87.5000 (90.9546) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [40/51] time 0.182 (0.213) data 0.000 (0.036) loss 0.5142 (0.4047) acc 89.8148 (91.0346) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.032) loss 0.3475 (0.4088) acc 91.8367 (90.9816) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.029) loss 0.3046 (0.3935) acc 93.6274 (91.3588) lr 9.5173e-05 eta 0:00:52
>>> alpha1: 0.180  alpha2: -0.205 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [46/50] batch [5/51] time 0.174 (0.428) data 0.000 (0.245) loss 0.3479 (0.3564) acc 91.8269 (91.7469) lr 7.0224e-05 eta 0:01:46
epoch [46/50] batch [10/51] time 0.172 (0.301) data 0.000 (0.123) loss 0.4180 (0.3733) acc 90.8163 (91.1322) lr 7.0224e-05 eta 0:01:13
epoch [46/50] batch [15/51] time 0.158 (0.258) data 0.000 (0.082) loss 0.3592 (0.3494) acc 88.3333 (91.7727) lr 7.0224e-05 eta 0:01:02
epoch [46/50] batch [20/51] time 0.174 (0.239) data 0.001 (0.061) loss 0.3954 (0.3598) acc 92.6471 (91.4097) lr 7.0224e-05 eta 0:00:56
epoch [46/50] batch [25/51] time 0.171 (0.227) data 0.000 (0.049) loss 0.3713 (0.3678) acc 90.6863 (91.2323) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [30/51] time 0.181 (0.220) data 0.000 (0.041) loss 0.2814 (0.3680) acc 94.0909 (91.3940) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [35/51] time 0.181 (0.214) data 0.000 (0.035) loss 0.3177 (0.3701) acc 90.5660 (91.4271) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [40/51] time 0.166 (0.208) data 0.000 (0.031) loss 0.3111 (0.3659) acc 94.3878 (91.5617) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.166 (0.204) data 0.000 (0.027) loss 0.5220 (0.3754) acc 91.3265 (91.5465) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [50/51] time 0.169 (0.201) data 0.000 (0.025) loss 0.2971 (0.3720) acc 92.6471 (91.6803) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.181  alpha2: -0.204 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [47/50] batch [5/51] time 0.174 (0.456) data 0.000 (0.271) loss 0.3591 (0.3915) acc 92.7885 (91.8320) lr 4.8943e-05 eta 0:01:30
epoch [47/50] batch [10/51] time 0.169 (0.316) data 0.000 (0.136) loss 0.4718 (0.3605) acc 90.0000 (93.0219) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [15/51] time 0.185 (0.269) data 0.000 (0.091) loss 0.4048 (0.3498) acc 91.0377 (92.5874) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [20/51] time 0.190 (0.246) data 0.000 (0.068) loss 0.6681 (0.3679) acc 83.9623 (92.0793) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [25/51] time 0.182 (0.233) data 0.000 (0.054) loss 0.2315 (0.3679) acc 95.2830 (92.2891) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.173 (0.222) data 0.000 (0.045) loss 0.3618 (0.3654) acc 91.6667 (92.2433) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [35/51] time 0.160 (0.216) data 0.000 (0.039) loss 0.3283 (0.3652) acc 93.4783 (92.3899) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.172 (0.210) data 0.000 (0.034) loss 0.4388 (0.3673) acc 91.8269 (92.3525) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.162 (0.206) data 0.000 (0.030) loss 0.4959 (0.3665) acc 90.4255 (92.3498) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.181 (0.203) data 0.000 (0.027) loss 0.3545 (0.3689) acc 90.9091 (92.2789) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.181  alpha2: -0.207 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.10 <<<
epoch [48/50] batch [5/51] time 0.177 (0.455) data 0.000 (0.271) loss 0.2241 (0.3529) acc 96.0784 (94.0802) lr 3.1417e-05 eta 0:01:07
epoch [48/50] batch [10/51] time 0.174 (0.315) data 0.000 (0.136) loss 0.3925 (0.3822) acc 91.8269 (92.6043) lr 3.1417e-05 eta 0:00:45
epoch [48/50] batch [15/51] time 0.171 (0.269) data 0.000 (0.091) loss 0.3317 (0.3535) acc 94.6078 (93.2309) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [20/51] time 0.177 (0.246) data 0.000 (0.068) loss 0.3968 (0.3703) acc 91.9811 (92.6554) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [25/51] time 0.178 (0.232) data 0.000 (0.054) loss 0.3960 (0.3706) acc 93.0556 (92.6902) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [30/51] time 0.203 (0.223) data 0.000 (0.045) loss 0.3876 (0.3697) acc 90.6250 (92.5724) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.166 (0.216) data 0.000 (0.039) loss 0.3666 (0.3736) acc 91.3265 (92.3094) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.167 (0.211) data 0.000 (0.034) loss 0.3744 (0.3756) acc 92.5000 (92.1309) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [45/51] time 0.165 (0.206) data 0.000 (0.030) loss 0.5162 (0.3794) acc 90.8163 (92.0570) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.187 (0.202) data 0.000 (0.027) loss 0.2610 (0.3695) acc 93.5345 (92.2370) lr 3.1417e-05 eta 0:00:20
>>> alpha1: 0.182  alpha2: -0.210 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [49/50] batch [5/51] time 0.185 (0.439) data 0.001 (0.259) loss 0.2979 (0.3595) acc 94.1964 (92.8922) lr 1.7713e-05 eta 0:00:42
epoch [49/50] batch [10/51] time 0.177 (0.309) data 0.000 (0.130) loss 0.2880 (0.3170) acc 93.0000 (93.7115) lr 1.7713e-05 eta 0:00:28
epoch [49/50] batch [15/51] time 0.172 (0.264) data 0.000 (0.086) loss 0.3070 (0.3327) acc 90.6863 (93.5117) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [20/51] time 0.172 (0.242) data 0.000 (0.065) loss 0.4039 (0.3377) acc 90.1961 (93.5741) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [25/51] time 0.151 (0.229) data 0.000 (0.052) loss 0.4343 (0.3384) acc 89.8810 (93.3220) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.167 (0.221) data 0.000 (0.043) loss 0.4492 (0.3495) acc 90.3061 (93.0125) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [35/51] time 0.161 (0.214) data 0.000 (0.037) loss 0.4163 (0.3565) acc 91.8478 (92.6342) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.163 (0.208) data 0.000 (0.033) loss 0.3986 (0.3619) acc 91.4894 (92.4568) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [45/51] time 0.177 (0.204) data 0.000 (0.029) loss 0.3210 (0.3650) acc 94.8113 (92.5322) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.181 (0.201) data 0.000 (0.026) loss 0.3135 (0.3610) acc 95.0000 (92.6844) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.181  alpha2: -0.213 <<<
>>> noisy rate: 0.00 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.11 <<<
epoch [50/50] batch [5/51] time 0.192 (0.458) data 0.000 (0.273) loss 0.5098 (0.3729) acc 87.7451 (91.8713) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.200 (0.321) data 0.000 (0.137) loss 0.4018 (0.3536) acc 89.9038 (92.2448) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.172 (0.276) data 0.000 (0.091) loss 0.6260 (0.3701) acc 84.8958 (91.8008) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.177 (0.253) data 0.000 (0.068) loss 0.2838 (0.3641) acc 92.9245 (92.0089) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.164 (0.237) data 0.000 (0.055) loss 0.5356 (0.3680) acc 88.5417 (92.1646) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.170 (0.227) data 0.000 (0.046) loss 0.5716 (0.3674) acc 83.3333 (91.9506) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.181 (0.220) data 0.000 (0.039) loss 0.3659 (0.3632) acc 90.7407 (91.9692) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.034) loss 0.4908 (0.3699) acc 88.2353 (91.8197) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.031) loss 0.5495 (0.3716) acc 88.7755 (91.8106) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.169 (0.205) data 0.000 (0.027) loss 0.3213 (0.3680) acc 93.5000 (91.9079) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_0FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]
* matched noise rate: [0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.04, 0.05, 0.05, 0.05, 0.04, 0.05, 0.04, 0.04, 0.04, 0.04, 0.04]
* unmatched noise rate: [0.08, 0.1, 0.1, 0.11, 0.12, 0.11, 0.11, 0.11, 0.12, 0.12, 0.11, 0.12, 0.12, 0.11, 0.12, 0.11, 0.11, 0.12, 0.11, 0.13, 0.12, 0.12, 0.13, 0.13, 0.11, 0.11, 0.12, 0.11, 0.11, 0.11, 0.1, 0.12, 0.11, 0.11, 0.11, 0.11, 0.11, 0.1, 0.12, 0.11]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<00:59,  2.47s/it] 12%|█▏        | 3/25 [00:02<00:15,  1.42it/s] 20%|██        | 5/25 [00:02<00:07,  2.55it/s] 24%|██▍       | 6/25 [00:02<00:05,  3.17it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.67it/s] 40%|████      | 10/25 [00:03<00:02,  6.13it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.45it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.58it/s] 64%|██████▍   | 16/25 [00:03<00:00,  9.21it/s] 72%|███████▏  | 18/25 [00:04<00:00,  8.38it/s] 80%|████████  | 20/25 [00:04<00:00,  9.29it/s] 88%|████████▊ | 22/25 [00:04<00:00, 10.05it/s] 96%|█████████▌| 24/25 [00:04<00:00, 10.63it/s]100%|██████████| 25/25 [00:05<00:00,  4.89it/s]
=> result
* total: 2,463
* correct: 2,185
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 88.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 3	acc: 25.0%
* class: 3 (sweet pea)	total: 17	correct: 10	acc: 58.8%
* class: 4 (english marigold)	total: 20	correct: 16	acc: 80.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 12	acc: 85.7%
* class: 10 (snapdragon)	total: 26	correct: 24	acc: 92.3%
* class: 11 (colt's foot)	total: 26	correct: 24	acc: 92.3%
* class: 12 (king protea)	total: 15	correct: 14	acc: 93.3%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 12	acc: 92.3%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 21	acc: 84.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 24	acc: 88.9%
* class: 23 (red ginger)	total: 13	correct: 12	acc: 92.3%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 22	acc: 84.6%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 11	acc: 91.7%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 31	acc: 96.9%
* class: 37 (great masterwort)	total: 17	correct: 16	acc: 94.1%
* class: 38 (siam tulip)	total: 13	correct: 4	acc: 30.8%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 22	acc: 57.9%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 1	acc: 2.6%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 9	acc: 75.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 26	acc: 92.9%
* class: 50 (petunia)	total: 77	correct: 41	acc: 53.2%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 14	acc: 87.5%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 12	acc: 92.3%
* class: 67 (bearded iris)	total: 16	correct: 12	acc: 75.0%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 23	acc: 79.3%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 49	acc: 96.1%
* class: 74 (thorn apple)	total: 36	correct: 31	acc: 86.1%
* class: 75 (morning glory)	total: 32	correct: 30	acc: 93.8%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 40	acc: 95.2%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 32	acc: 94.1%
* class: 82 (hibiscus)	total: 39	correct: 36	acc: 92.3%
* class: 83 (columbine)	total: 26	correct: 23	acc: 88.5%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 31	acc: 67.4%
* class: 88 (watercress)	total: 55	correct: 42	acc: 76.4%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 18	acc: 78.3%
* class: 91 (bee balm)	total: 20	correct: 18	acc: 90.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 32	acc: 84.2%
* class: 95 (camellia)	total: 27	correct: 21	acc: 77.8%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 21	acc: 84.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 15	acc: 88.2%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 89.8%
Elapsed: 0:27:57
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_2-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.269 (1.037) data 0.000 (0.345) loss 4.3319 (4.2958) acc 6.2500 (13.1250) lr 1.0000e-05 eta 0:43:58
epoch [1/50] batch [10/51] time 0.263 (0.650) data 0.000 (0.173) loss 3.8441 (4.1517) acc 25.0000 (15.3125) lr 1.0000e-05 eta 0:27:31
epoch [1/50] batch [15/51] time 0.259 (0.520) data 0.000 (0.115) loss 4.0911 (4.0747) acc 25.0000 (16.6667) lr 1.0000e-05 eta 0:21:58
epoch [1/50] batch [20/51] time 0.258 (0.456) data 0.000 (0.086) loss 3.3296 (3.9200) acc 28.1250 (19.6875) lr 1.0000e-05 eta 0:19:14
epoch [1/50] batch [25/51] time 0.258 (0.418) data 0.000 (0.069) loss 2.6518 (3.7085) acc 46.8750 (24.0000) lr 1.0000e-05 eta 0:17:34
epoch [1/50] batch [30/51] time 0.258 (0.392) data 0.000 (0.058) loss 2.1389 (3.5626) acc 59.3750 (26.9792) lr 1.0000e-05 eta 0:16:26
epoch [1/50] batch [35/51] time 0.259 (0.373) data 0.000 (0.049) loss 2.6553 (3.4577) acc 56.2500 (30.0000) lr 1.0000e-05 eta 0:15:38
epoch [1/50] batch [40/51] time 0.257 (0.359) data 0.000 (0.043) loss 2.6985 (3.4032) acc 56.2500 (31.6406) lr 1.0000e-05 eta 0:15:00
epoch [1/50] batch [45/51] time 0.258 (0.347) data 0.000 (0.039) loss 3.1009 (3.3186) acc 40.6250 (33.4722) lr 1.0000e-05 eta 0:14:30
epoch [1/50] batch [50/51] time 0.259 (0.339) data 0.000 (0.035) loss 2.8638 (3.2535) acc 50.0000 (35.1250) lr 1.0000e-05 eta 0:14:06
epoch [2/50] batch [5/51] time 0.270 (0.515) data 0.000 (0.238) loss 2.2771 (2.7196) acc 53.1250 (42.5000) lr 2.0000e-03 eta 0:21:25
epoch [2/50] batch [10/51] time 0.264 (0.391) data 0.000 (0.119) loss 2.2230 (2.4943) acc 65.6250 (49.6875) lr 2.0000e-03 eta 0:16:13
epoch [2/50] batch [15/51] time 0.314 (0.354) data 0.000 (0.080) loss 2.4552 (2.4883) acc 56.2500 (51.6667) lr 2.0000e-03 eta 0:14:39
epoch [2/50] batch [20/51] time 0.266 (0.332) data 0.000 (0.060) loss 2.1497 (2.3999) acc 46.8750 (52.6562) lr 2.0000e-03 eta 0:13:43
epoch [2/50] batch [25/51] time 0.262 (0.319) data 0.000 (0.048) loss 2.3543 (2.4740) acc 53.1250 (51.7500) lr 2.0000e-03 eta 0:13:08
epoch [2/50] batch [30/51] time 0.260 (0.310) data 0.000 (0.040) loss 2.3893 (2.4674) acc 65.6250 (52.0833) lr 2.0000e-03 eta 0:12:45
epoch [2/50] batch [35/51] time 0.274 (0.304) data 0.000 (0.034) loss 1.8357 (2.4804) acc 78.1250 (52.3214) lr 2.0000e-03 eta 0:12:29
epoch [2/50] batch [40/51] time 0.260 (0.299) data 0.000 (0.030) loss 2.0399 (2.4669) acc 62.5000 (53.0469) lr 2.0000e-03 eta 0:12:15
epoch [2/50] batch [45/51] time 0.260 (0.295) data 0.000 (0.027) loss 2.6477 (2.4520) acc 37.5000 (53.6111) lr 2.0000e-03 eta 0:12:03
epoch [2/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.024) loss 2.6910 (2.4226) acc 50.0000 (54.3750) lr 2.0000e-03 eta 0:11:53
epoch [3/50] batch [5/51] time 0.267 (0.552) data 0.000 (0.270) loss 1.6748 (2.2012) acc 59.3750 (60.6250) lr 1.9980e-03 eta 0:22:28
epoch [3/50] batch [10/51] time 0.264 (0.410) data 0.000 (0.135) loss 2.4920 (2.2735) acc 46.8750 (58.1250) lr 1.9980e-03 eta 0:16:38
epoch [3/50] batch [15/51] time 0.258 (0.362) data 0.000 (0.090) loss 1.9256 (2.1947) acc 59.3750 (59.3750) lr 1.9980e-03 eta 0:14:39
epoch [3/50] batch [20/51] time 0.260 (0.337) data 0.001 (0.068) loss 1.3571 (2.1869) acc 75.0000 (58.9062) lr 1.9980e-03 eta 0:13:38
epoch [3/50] batch [25/51] time 0.270 (0.324) data 0.000 (0.054) loss 1.6516 (2.1798) acc 68.7500 (59.5000) lr 1.9980e-03 eta 0:13:04
epoch [3/50] batch [30/51] time 0.261 (0.315) data 0.000 (0.045) loss 2.8977 (2.1286) acc 56.2500 (60.4167) lr 1.9980e-03 eta 0:12:41
epoch [3/50] batch [35/51] time 0.263 (0.308) data 0.000 (0.039) loss 2.2191 (2.1799) acc 62.5000 (60.4464) lr 1.9980e-03 eta 0:12:22
epoch [3/50] batch [40/51] time 0.256 (0.301) data 0.000 (0.034) loss 2.4863 (2.1999) acc 50.0000 (60.3906) lr 1.9980e-03 eta 0:12:05
epoch [3/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.030) loss 2.7937 (2.2173) acc 50.0000 (60.2083) lr 1.9980e-03 eta 0:11:52
epoch [3/50] batch [50/51] time 0.261 (0.293) data 0.000 (0.027) loss 3.0685 (2.1886) acc 43.7500 (60.9375) lr 1.9980e-03 eta 0:11:42
epoch [4/50] batch [5/51] time 0.282 (0.521) data 0.000 (0.240) loss 2.1879 (1.9173) acc 56.2500 (66.8750) lr 1.9921e-03 eta 0:20:45
epoch [4/50] batch [10/51] time 0.261 (0.392) data 0.000 (0.120) loss 2.0437 (2.0784) acc 62.5000 (62.5000) lr 1.9921e-03 eta 0:15:36
epoch [4/50] batch [15/51] time 0.259 (0.349) data 0.000 (0.080) loss 1.8771 (1.9855) acc 71.8750 (66.0417) lr 1.9921e-03 eta 0:13:50
epoch [4/50] batch [20/51] time 0.260 (0.329) data 0.000 (0.060) loss 1.3342 (1.9617) acc 75.0000 (65.7812) lr 1.9921e-03 eta 0:13:01
epoch [4/50] batch [25/51] time 0.265 (0.316) data 0.000 (0.048) loss 2.0597 (1.9843) acc 65.6250 (66.2500) lr 1.9921e-03 eta 0:12:28
epoch [4/50] batch [30/51] time 0.268 (0.307) data 0.000 (0.040) loss 2.6656 (2.0533) acc 56.2500 (65.2083) lr 1.9921e-03 eta 0:12:06
epoch [4/50] batch [35/51] time 0.269 (0.301) data 0.000 (0.035) loss 1.8747 (2.0373) acc 68.7500 (65.6250) lr 1.9921e-03 eta 0:11:50
epoch [4/50] batch [40/51] time 0.257 (0.296) data 0.000 (0.030) loss 1.5874 (2.0252) acc 65.6250 (65.7812) lr 1.9921e-03 eta 0:11:36
epoch [4/50] batch [45/51] time 0.256 (0.291) data 0.000 (0.027) loss 2.3724 (2.0197) acc 68.7500 (66.0417) lr 1.9921e-03 eta 0:11:24
epoch [4/50] batch [50/51] time 0.256 (0.288) data 0.000 (0.024) loss 2.6920 (2.0433) acc 62.5000 (65.8750) lr 1.9921e-03 eta 0:11:15
epoch [5/50] batch [5/51] time 0.278 (0.560) data 0.016 (0.272) loss 1.3968 (1.9352) acc 59.3750 (67.5000) lr 1.9823e-03 eta 0:21:51
epoch [5/50] batch [10/51] time 0.272 (0.412) data 0.000 (0.136) loss 1.6809 (2.0680) acc 68.7500 (65.9375) lr 1.9823e-03 eta 0:16:01
epoch [5/50] batch [15/51] time 0.263 (0.362) data 0.000 (0.091) loss 2.0044 (1.9952) acc 68.7500 (67.9167) lr 1.9823e-03 eta 0:14:04
epoch [5/50] batch [20/51] time 0.258 (0.337) data 0.000 (0.068) loss 1.9781 (2.0616) acc 68.7500 (66.5625) lr 1.9823e-03 eta 0:13:04
epoch [5/50] batch [25/51] time 0.265 (0.322) data 0.000 (0.055) loss 1.9804 (1.9495) acc 62.5000 (68.6250) lr 1.9823e-03 eta 0:12:28
epoch [5/50] batch [30/51] time 0.259 (0.312) data 0.000 (0.046) loss 1.5916 (1.8869) acc 65.6250 (68.7500) lr 1.9823e-03 eta 0:12:03
epoch [5/50] batch [35/51] time 0.260 (0.305) data 0.000 (0.039) loss 1.4511 (1.8539) acc 68.7500 (68.5714) lr 1.9823e-03 eta 0:11:45
epoch [5/50] batch [40/51] time 0.257 (0.299) data 0.000 (0.034) loss 3.2959 (1.8711) acc 65.6250 (68.1250) lr 1.9823e-03 eta 0:11:30
epoch [5/50] batch [45/51] time 0.258 (0.295) data 0.000 (0.030) loss 3.3251 (1.9125) acc 46.8750 (67.5000) lr 1.9823e-03 eta 0:11:18
epoch [5/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.027) loss 1.6836 (1.9235) acc 65.6250 (67.0625) lr 1.9823e-03 eta 0:11:08
epoch [6/50] batch [5/51] time 0.266 (0.549) data 0.000 (0.268) loss 2.3192 (1.8287) acc 59.3750 (71.2500) lr 1.9686e-03 eta 0:20:56
epoch [6/50] batch [10/51] time 0.259 (0.410) data 0.000 (0.134) loss 1.8573 (1.8199) acc 65.6250 (70.6250) lr 1.9686e-03 eta 0:15:36
epoch [6/50] batch [15/51] time 0.259 (0.361) data 0.000 (0.090) loss 1.9575 (1.8039) acc 65.6250 (70.4167) lr 1.9686e-03 eta 0:13:42
epoch [6/50] batch [20/51] time 0.258 (0.337) data 0.000 (0.067) loss 2.0706 (1.7623) acc 50.0000 (71.0938) lr 1.9686e-03 eta 0:12:46
epoch [6/50] batch [25/51] time 0.259 (0.323) data 0.000 (0.054) loss 1.8506 (1.8386) acc 71.8750 (70.1250) lr 1.9686e-03 eta 0:12:12
epoch [6/50] batch [30/51] time 0.259 (0.313) data 0.000 (0.045) loss 2.5136 (1.8674) acc 65.6250 (69.8958) lr 1.9686e-03 eta 0:11:49
epoch [6/50] batch [35/51] time 0.267 (0.306) data 0.000 (0.038) loss 2.3808 (1.8780) acc 59.3750 (68.8393) lr 1.9686e-03 eta 0:11:31
epoch [6/50] batch [40/51] time 0.258 (0.300) data 0.000 (0.034) loss 1.2709 (1.8493) acc 75.0000 (69.1406) lr 1.9686e-03 eta 0:11:16
epoch [6/50] batch [45/51] time 0.257 (0.295) data 0.000 (0.030) loss 1.6529 (1.8531) acc 81.2500 (69.7222) lr 1.9686e-03 eta 0:11:04
epoch [6/50] batch [50/51] time 0.256 (0.292) data 0.000 (0.027) loss 1.7681 (1.8329) acc 65.6250 (69.6875) lr 1.9686e-03 eta 0:10:54
epoch [7/50] batch [5/51] time 0.260 (0.550) data 0.000 (0.279) loss 2.4565 (2.0796) acc 65.6250 (69.3750) lr 1.9511e-03 eta 0:20:32
epoch [7/50] batch [10/51] time 0.259 (0.411) data 0.000 (0.139) loss 2.2693 (2.0169) acc 62.5000 (68.4375) lr 1.9511e-03 eta 0:15:19
epoch [7/50] batch [15/51] time 0.259 (0.362) data 0.000 (0.093) loss 1.1890 (1.7851) acc 78.1250 (70.4167) lr 1.9511e-03 eta 0:13:26
epoch [7/50] batch [20/51] time 0.259 (0.338) data 0.000 (0.070) loss 1.5681 (1.8114) acc 78.1250 (70.4688) lr 1.9511e-03 eta 0:12:30
epoch [7/50] batch [25/51] time 0.266 (0.323) data 0.000 (0.056) loss 1.6677 (1.7575) acc 75.0000 (71.3750) lr 1.9511e-03 eta 0:11:56
epoch [7/50] batch [30/51] time 0.270 (0.314) data 0.000 (0.047) loss 1.6118 (1.7676) acc 65.6250 (71.4583) lr 1.9511e-03 eta 0:11:34
epoch [7/50] batch [35/51] time 0.270 (0.307) data 0.000 (0.040) loss 1.5896 (1.8031) acc 68.7500 (70.8929) lr 1.9511e-03 eta 0:11:17
epoch [7/50] batch [40/51] time 0.257 (0.301) data 0.000 (0.035) loss 1.7160 (1.7956) acc 75.0000 (71.3281) lr 1.9511e-03 eta 0:11:02
epoch [7/50] batch [45/51] time 0.258 (0.296) data 0.000 (0.031) loss 1.9868 (1.7837) acc 75.0000 (71.5278) lr 1.9511e-03 eta 0:10:50
epoch [7/50] batch [50/51] time 0.261 (0.292) data 0.000 (0.028) loss 1.7391 (1.7685) acc 71.8750 (71.6875) lr 1.9511e-03 eta 0:10:40
epoch [8/50] batch [5/51] time 0.276 (0.561) data 0.000 (0.271) loss 1.0717 (1.6747) acc 75.0000 (73.7500) lr 1.9298e-03 eta 0:20:28
epoch [8/50] batch [10/51] time 0.270 (0.414) data 0.000 (0.136) loss 1.2963 (1.5123) acc 78.1250 (77.1875) lr 1.9298e-03 eta 0:15:03
epoch [8/50] batch [15/51] time 0.272 (0.365) data 0.000 (0.091) loss 0.8543 (1.5328) acc 81.2500 (76.6667) lr 1.9298e-03 eta 0:13:15
epoch [8/50] batch [20/51] time 0.265 (0.342) data 0.000 (0.068) loss 1.9429 (1.5743) acc 71.8750 (75.7812) lr 1.9298e-03 eta 0:12:22
epoch [8/50] batch [25/51] time 0.259 (0.327) data 0.000 (0.054) loss 1.9283 (1.6268) acc 75.0000 (74.6250) lr 1.9298e-03 eta 0:11:48
epoch [8/50] batch [30/51] time 0.259 (0.316) data 0.000 (0.045) loss 1.9366 (1.5983) acc 78.1250 (75.6250) lr 1.9298e-03 eta 0:11:24
epoch [8/50] batch [35/51] time 0.277 (0.309) data 0.000 (0.039) loss 1.2453 (1.6514) acc 87.5000 (75.1786) lr 1.9298e-03 eta 0:11:07
epoch [8/50] batch [40/51] time 0.260 (0.303) data 0.000 (0.034) loss 2.0377 (1.7003) acc 65.6250 (74.4531) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [45/51] time 0.258 (0.299) data 0.000 (0.030) loss 1.2044 (1.6684) acc 87.5000 (75.0694) lr 1.9298e-03 eta 0:10:41
epoch [8/50] batch [50/51] time 0.257 (0.294) data 0.000 (0.027) loss 1.6072 (1.6844) acc 78.1250 (74.6250) lr 1.9298e-03 eta 0:10:31
epoch [9/50] batch [5/51] time 0.272 (0.556) data 0.000 (0.284) loss 2.3079 (1.5810) acc 75.0000 (78.7500) lr 1.9048e-03 eta 0:19:47
epoch [9/50] batch [10/51] time 0.270 (0.411) data 0.000 (0.142) loss 1.4966 (1.4382) acc 75.0000 (80.9375) lr 1.9048e-03 eta 0:14:35
epoch [9/50] batch [15/51] time 0.270 (0.362) data 0.000 (0.095) loss 1.0625 (1.5183) acc 78.1250 (78.9583) lr 1.9048e-03 eta 0:12:49
epoch [9/50] batch [20/51] time 0.259 (0.338) data 0.000 (0.071) loss 1.3908 (1.5858) acc 84.3750 (78.2812) lr 1.9048e-03 eta 0:11:56
epoch [9/50] batch [25/51] time 0.270 (0.323) data 0.000 (0.057) loss 1.7541 (1.6275) acc 78.1250 (77.1250) lr 1.9048e-03 eta 0:11:23
epoch [9/50] batch [30/51] time 0.259 (0.313) data 0.000 (0.048) loss 1.4267 (1.6160) acc 78.1250 (76.2500) lr 1.9048e-03 eta 0:11:01
epoch [9/50] batch [35/51] time 0.259 (0.306) data 0.000 (0.041) loss 1.8053 (1.5951) acc 75.0000 (76.3393) lr 1.9048e-03 eta 0:10:43
epoch [9/50] batch [40/51] time 0.257 (0.300) data 0.000 (0.036) loss 2.1986 (1.6045) acc 75.0000 (76.5625) lr 1.9048e-03 eta 0:10:29
epoch [9/50] batch [45/51] time 0.258 (0.295) data 0.000 (0.032) loss 1.4606 (1.6011) acc 75.0000 (76.4583) lr 1.9048e-03 eta 0:10:18
epoch [9/50] batch [50/51] time 0.256 (0.291) data 0.000 (0.029) loss 1.9977 (1.6118) acc 75.0000 (76.0625) lr 1.9048e-03 eta 0:10:09
epoch [10/50] batch [5/51] time 0.260 (0.504) data 0.000 (0.234) loss 1.1494 (1.4682) acc 81.2500 (79.3750) lr 1.8763e-03 eta 0:17:32
epoch [10/50] batch [10/51] time 0.259 (0.383) data 0.000 (0.117) loss 1.8575 (1.6191) acc 68.7500 (76.2500) lr 1.8763e-03 eta 0:13:16
epoch [10/50] batch [15/51] time 0.262 (0.345) data 0.000 (0.078) loss 1.2316 (1.5954) acc 84.3750 (76.4583) lr 1.8763e-03 eta 0:11:55
epoch [10/50] batch [20/51] time 0.272 (0.325) data 0.000 (0.059) loss 0.8956 (1.6342) acc 81.2500 (75.9375) lr 1.8763e-03 eta 0:11:12
epoch [10/50] batch [25/51] time 0.259 (0.312) data 0.000 (0.047) loss 1.1959 (1.5982) acc 75.0000 (75.3750) lr 1.8763e-03 eta 0:10:44
epoch [10/50] batch [30/51] time 0.275 (0.304) data 0.000 (0.039) loss 1.1173 (1.5853) acc 84.3750 (75.7292) lr 1.8763e-03 eta 0:10:26
epoch [10/50] batch [35/51] time 0.259 (0.298) data 0.000 (0.034) loss 1.4436 (1.6412) acc 75.0000 (74.8214) lr 1.8763e-03 eta 0:10:12
epoch [10/50] batch [40/51] time 0.257 (0.293) data 0.000 (0.030) loss 1.8424 (1.6007) acc 71.8750 (74.7656) lr 1.8763e-03 eta 0:10:01
epoch [10/50] batch [45/51] time 0.257 (0.289) data 0.000 (0.026) loss 1.3332 (1.6018) acc 78.1250 (75.2083) lr 1.8763e-03 eta 0:09:51
epoch [10/50] batch [50/51] time 0.256 (0.286) data 0.000 (0.024) loss 1.0225 (1.6094) acc 84.3750 (75.5000) lr 1.8763e-03 eta 0:09:43
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.314  alpha2: -0.037 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.05 <<<
epoch [11/50] batch [5/51] time 0.175 (0.998) data 0.000 (0.306) loss 1.0100 (0.9877) acc 75.5000 (77.9093) lr 1.8443e-03 eta 0:33:51
epoch [11/50] batch [10/51] time 0.824 (0.711) data 0.000 (0.153) loss 0.5946 (0.9336) acc 89.0625 (79.0398) lr 1.8443e-03 eta 0:24:03
epoch [11/50] batch [15/51] time 0.162 (0.609) data 0.000 (0.102) loss 1.0196 (0.9101) acc 77.2222 (79.8513) lr 1.8443e-03 eta 0:20:33
epoch [11/50] batch [20/51] time 0.164 (0.498) data 0.000 (0.077) loss 0.7819 (0.9051) acc 84.8958 (79.7868) lr 1.8443e-03 eta 0:16:45
epoch [11/50] batch [25/51] time 0.162 (0.452) data 0.000 (0.061) loss 0.6476 (0.9023) acc 84.8958 (79.8066) lr 1.8443e-03 eta 0:15:10
epoch [11/50] batch [30/51] time 0.168 (0.404) data 0.000 (0.051) loss 0.8039 (0.8920) acc 77.6042 (79.9080) lr 1.8443e-03 eta 0:13:32
epoch [11/50] batch [35/51] time 0.160 (0.385) data 0.000 (0.044) loss 0.9698 (0.8766) acc 78.2609 (80.3124) lr 1.8443e-03 eta 0:12:52
epoch [11/50] batch [40/51] time 0.806 (0.374) data 0.000 (0.039) loss 0.8307 (0.8635) acc 82.2727 (80.6823) lr 1.8443e-03 eta 0:12:27
epoch [11/50] batch [45/51] time 0.168 (0.363) data 0.000 (0.034) loss 0.5984 (0.8572) acc 85.2041 (80.7217) lr 1.8443e-03 eta 0:12:04
epoch [11/50] batch [50/51] time 0.153 (0.343) data 0.000 (0.031) loss 0.8425 (0.8570) acc 76.7442 (80.7685) lr 1.8443e-03 eta 0:11:23
>>> alpha1: 0.257  alpha2: -0.087 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [12/50] batch [5/51] time 0.178 (0.726) data 0.000 (0.305) loss 0.6417 (0.7851) acc 81.2500 (80.0240) lr 1.8090e-03 eta 0:23:59
epoch [12/50] batch [10/51] time 0.162 (0.507) data 0.000 (0.153) loss 0.7650 (0.7443) acc 79.7872 (82.3119) lr 1.8090e-03 eta 0:16:42
epoch [12/50] batch [15/51] time 0.184 (0.394) data 0.000 (0.102) loss 0.5464 (0.7348) acc 85.8491 (83.1355) lr 1.8090e-03 eta 0:12:57
epoch [12/50] batch [20/51] time 0.168 (0.338) data 0.000 (0.077) loss 0.7576 (0.7497) acc 79.6875 (82.2924) lr 1.8090e-03 eta 0:11:05
epoch [12/50] batch [25/51] time 0.186 (0.306) data 0.000 (0.061) loss 0.5997 (0.7499) acc 86.7647 (82.2561) lr 1.8090e-03 eta 0:10:00
epoch [12/50] batch [30/51] time 0.190 (0.285) data 0.000 (0.051) loss 0.5457 (0.7401) acc 88.7755 (82.4939) lr 1.8090e-03 eta 0:09:17
epoch [12/50] batch [35/51] time 0.177 (0.269) data 0.000 (0.044) loss 0.8992 (0.7453) acc 80.7292 (82.2606) lr 1.8090e-03 eta 0:08:45
epoch [12/50] batch [40/51] time 0.183 (0.272) data 0.000 (0.038) loss 0.6205 (0.7301) acc 88.8889 (82.6071) lr 1.8090e-03 eta 0:08:50
epoch [12/50] batch [45/51] time 0.168 (0.261) data 0.000 (0.034) loss 0.8079 (0.7139) acc 79.5000 (83.1206) lr 1.8090e-03 eta 0:08:26
epoch [12/50] batch [50/51] time 0.167 (0.251) data 0.000 (0.031) loss 0.8424 (0.7212) acc 79.5000 (82.8095) lr 1.8090e-03 eta 0:08:06
>>> alpha1: 0.234  alpha2: -0.099 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [13/50] batch [5/51] time 0.202 (0.445) data 0.000 (0.263) loss 0.6781 (0.7024) acc 84.0909 (83.8395) lr 1.7705e-03 eta 0:14:20
epoch [13/50] batch [10/51] time 0.193 (0.312) data 0.000 (0.132) loss 0.6122 (0.7011) acc 83.0189 (82.5042) lr 1.7705e-03 eta 0:10:00
epoch [13/50] batch [15/51] time 0.161 (0.265) data 0.000 (0.088) loss 0.4863 (0.6920) acc 89.6739 (82.7691) lr 1.7705e-03 eta 0:08:28
epoch [13/50] batch [20/51] time 0.174 (0.242) data 0.000 (0.066) loss 0.6558 (0.6630) acc 83.8235 (83.8844) lr 1.7705e-03 eta 0:07:43
epoch [13/50] batch [25/51] time 0.167 (0.229) data 0.000 (0.053) loss 0.7837 (0.6569) acc 82.1429 (84.1020) lr 1.7705e-03 eta 0:07:17
epoch [13/50] batch [30/51] time 0.174 (0.220) data 0.000 (0.044) loss 0.5517 (0.6513) acc 86.7347 (84.3644) lr 1.7705e-03 eta 0:06:58
epoch [13/50] batch [35/51] time 0.176 (0.214) data 0.000 (0.038) loss 0.7700 (0.6516) acc 86.2245 (84.3905) lr 1.7705e-03 eta 0:06:47
epoch [13/50] batch [40/51] time 0.177 (0.209) data 0.000 (0.033) loss 0.6923 (0.6580) acc 84.3137 (84.3380) lr 1.7705e-03 eta 0:06:36
epoch [13/50] batch [45/51] time 0.157 (0.204) data 0.000 (0.029) loss 0.8036 (0.6644) acc 77.2222 (83.9339) lr 1.7705e-03 eta 0:06:26
epoch [13/50] batch [50/51] time 0.162 (0.201) data 0.000 (0.027) loss 0.6074 (0.6627) acc 85.4167 (84.0539) lr 1.7705e-03 eta 0:06:18
>>> alpha1: 0.217  alpha2: -0.107 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [14/50] batch [5/51] time 0.197 (0.461) data 0.000 (0.266) loss 0.7423 (0.6086) acc 86.0000 (85.6445) lr 1.7290e-03 eta 0:14:27
epoch [14/50] batch [10/51] time 0.186 (0.321) data 0.000 (0.133) loss 0.4173 (0.6340) acc 90.2778 (84.6958) lr 1.7290e-03 eta 0:10:02
epoch [14/50] batch [15/51] time 0.171 (0.275) data 0.000 (0.089) loss 0.6977 (0.6187) acc 82.5000 (85.5392) lr 1.7290e-03 eta 0:08:34
epoch [14/50] batch [20/51] time 0.166 (0.250) data 0.000 (0.067) loss 0.4850 (0.6064) acc 87.7551 (85.4608) lr 1.7290e-03 eta 0:07:47
epoch [14/50] batch [25/51] time 0.166 (0.235) data 0.000 (0.053) loss 0.7906 (0.6112) acc 83.6735 (85.6053) lr 1.7290e-03 eta 0:07:17
epoch [14/50] batch [30/51] time 0.180 (0.225) data 0.000 (0.045) loss 0.7447 (0.6132) acc 78.7037 (85.5243) lr 1.7290e-03 eta 0:06:57
epoch [14/50] batch [35/51] time 0.186 (0.218) data 0.000 (0.038) loss 0.6834 (0.6090) acc 83.8235 (85.7444) lr 1.7290e-03 eta 0:06:44
epoch [14/50] batch [40/51] time 0.157 (0.212) data 0.000 (0.034) loss 0.6387 (0.6170) acc 86.1111 (85.5648) lr 1.7290e-03 eta 0:06:30
epoch [14/50] batch [45/51] time 0.159 (0.206) data 0.000 (0.030) loss 0.5211 (0.6171) acc 90.2174 (85.6829) lr 1.7290e-03 eta 0:06:20
epoch [14/50] batch [50/51] time 0.166 (0.202) data 0.000 (0.027) loss 0.7005 (0.6208) acc 79.5918 (85.5662) lr 1.7290e-03 eta 0:06:11
>>> alpha1: 0.210  alpha2: -0.111 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [15/50] batch [5/51] time 0.167 (0.545) data 0.001 (0.364) loss 0.5591 (0.5065) acc 83.8542 (88.0368) lr 1.6845e-03 eta 0:16:37
epoch [15/50] batch [10/51] time 0.187 (0.366) data 0.000 (0.185) loss 0.5520 (0.5479) acc 91.6667 (87.1436) lr 1.6845e-03 eta 0:11:07
epoch [15/50] batch [15/51] time 0.173 (0.301) data 0.000 (0.123) loss 0.3537 (0.5383) acc 91.8367 (87.7462) lr 1.6845e-03 eta 0:09:08
epoch [15/50] batch [20/51] time 0.167 (0.269) data 0.001 (0.093) loss 0.4567 (0.5493) acc 90.3061 (88.0144) lr 1.6845e-03 eta 0:08:08
epoch [15/50] batch [25/51] time 0.172 (0.252) data 0.001 (0.075) loss 0.5744 (0.5708) acc 86.7021 (87.0462) lr 1.6845e-03 eta 0:07:36
epoch [15/50] batch [30/51] time 0.177 (0.240) data 0.000 (0.062) loss 0.6937 (0.5857) acc 81.3830 (86.2761) lr 1.6845e-03 eta 0:07:13
epoch [15/50] batch [35/51] time 0.193 (0.232) data 0.000 (0.054) loss 0.6335 (0.5962) acc 86.2745 (86.1032) lr 1.6845e-03 eta 0:06:57
epoch [15/50] batch [40/51] time 0.165 (0.226) data 0.001 (0.047) loss 0.7284 (0.5950) acc 79.1667 (86.2293) lr 1.6845e-03 eta 0:06:45
epoch [15/50] batch [45/51] time 0.155 (0.219) data 0.000 (0.042) loss 0.5169 (0.5967) acc 88.6364 (86.2018) lr 1.6845e-03 eta 0:06:32
epoch [15/50] batch [50/51] time 0.171 (0.214) data 0.000 (0.038) loss 0.5687 (0.5907) acc 86.2745 (86.3893) lr 1.6845e-03 eta 0:06:22
>>> alpha1: 0.189  alpha2: -0.128 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [16/50] batch [5/51] time 0.173 (0.437) data 0.000 (0.260) loss 0.7647 (0.6607) acc 81.0000 (85.6245) lr 1.6374e-03 eta 0:12:58
epoch [16/50] batch [10/51] time 0.166 (0.305) data 0.000 (0.131) loss 0.5711 (0.6586) acc 88.2653 (86.0394) lr 1.6374e-03 eta 0:09:02
epoch [16/50] batch [15/51] time 0.180 (0.264) data 0.000 (0.088) loss 0.5566 (0.6139) acc 85.5000 (86.3426) lr 1.6374e-03 eta 0:07:46
epoch [16/50] batch [20/51] time 0.176 (0.242) data 0.000 (0.066) loss 0.6240 (0.6018) acc 88.0208 (86.6146) lr 1.6374e-03 eta 0:07:06
epoch [16/50] batch [25/51] time 0.169 (0.228) data 0.000 (0.053) loss 0.5590 (0.5945) acc 89.5833 (86.3702) lr 1.6374e-03 eta 0:06:41
epoch [16/50] batch [30/51] time 0.174 (0.239) data 0.000 (0.044) loss 0.4535 (0.5914) acc 87.9808 (86.4270) lr 1.6374e-03 eta 0:06:59
epoch [16/50] batch [35/51] time 0.178 (0.229) data 0.000 (0.038) loss 0.5593 (0.5964) acc 90.3846 (86.2249) lr 1.6374e-03 eta 0:06:40
epoch [16/50] batch [40/51] time 0.173 (0.222) data 0.000 (0.033) loss 0.6147 (0.5877) acc 87.0192 (86.4561) lr 1.6374e-03 eta 0:06:27
epoch [16/50] batch [45/51] time 0.173 (0.216) data 0.000 (0.029) loss 0.4453 (0.5813) acc 90.3846 (86.5924) lr 1.6374e-03 eta 0:06:16
epoch [16/50] batch [50/51] time 0.172 (0.212) data 0.000 (0.026) loss 0.5585 (0.5813) acc 86.0577 (86.5598) lr 1.6374e-03 eta 0:06:07
>>> alpha1: 0.182  alpha2: -0.130 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.06 <<<
epoch [17/50] batch [5/51] time 0.173 (0.472) data 0.000 (0.293) loss 0.8540 (0.6178) acc 77.9412 (84.6466) lr 1.5878e-03 eta 0:13:35
epoch [17/50] batch [10/51] time 0.174 (0.324) data 0.000 (0.147) loss 0.5615 (0.6070) acc 85.2941 (84.8958) lr 1.5878e-03 eta 0:09:19
epoch [17/50] batch [15/51] time 0.179 (0.273) data 0.000 (0.098) loss 0.7455 (0.6203) acc 86.7021 (85.2678) lr 1.5878e-03 eta 0:07:49
epoch [17/50] batch [20/51] time 0.170 (0.249) data 0.000 (0.074) loss 0.6807 (0.6060) acc 80.0000 (85.5532) lr 1.5878e-03 eta 0:07:06
epoch [17/50] batch [25/51] time 0.161 (0.234) data 0.000 (0.059) loss 0.5612 (0.5939) acc 86.4130 (85.7995) lr 1.5878e-03 eta 0:06:39
epoch [17/50] batch [30/51] time 0.167 (0.225) data 0.000 (0.049) loss 0.5691 (0.5804) acc 87.7551 (86.0766) lr 1.5878e-03 eta 0:06:23
epoch [17/50] batch [35/51] time 0.177 (0.218) data 0.000 (0.042) loss 0.3730 (0.5623) acc 90.8163 (86.5284) lr 1.5878e-03 eta 0:06:09
epoch [17/50] batch [40/51] time 0.172 (0.212) data 0.000 (0.037) loss 0.6366 (0.5580) acc 85.5769 (86.6298) lr 1.5878e-03 eta 0:05:59
epoch [17/50] batch [45/51] time 0.184 (0.207) data 0.000 (0.033) loss 0.5338 (0.5597) acc 85.5263 (86.7429) lr 1.5878e-03 eta 0:05:50
epoch [17/50] batch [50/51] time 0.169 (0.204) data 0.000 (0.030) loss 0.6527 (0.5641) acc 86.2245 (86.6297) lr 1.5878e-03 eta 0:05:42
>>> alpha1: 0.176  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [18/50] batch [5/51] time 0.173 (0.485) data 0.001 (0.297) loss 0.4796 (0.4405) acc 91.6667 (91.3492) lr 1.5358e-03 eta 0:13:33
epoch [18/50] batch [10/51] time 0.169 (0.329) data 0.001 (0.149) loss 0.4237 (0.4546) acc 89.0000 (90.5081) lr 1.5358e-03 eta 0:09:10
epoch [18/50] batch [15/51] time 0.192 (0.278) data 0.001 (0.099) loss 0.5488 (0.5075) acc 86.3208 (88.8891) lr 1.5358e-03 eta 0:07:43
epoch [18/50] batch [20/51] time 0.183 (0.254) data 0.000 (0.075) loss 0.5380 (0.5141) acc 88.7255 (88.5959) lr 1.5358e-03 eta 0:07:01
epoch [18/50] batch [25/51] time 0.181 (0.238) data 0.000 (0.060) loss 0.3930 (0.5173) acc 92.4528 (88.6688) lr 1.5358e-03 eta 0:06:34
epoch [18/50] batch [30/51] time 0.189 (0.227) data 0.000 (0.050) loss 0.6343 (0.5351) acc 83.9623 (88.1352) lr 1.5358e-03 eta 0:06:15
epoch [18/50] batch [35/51] time 0.186 (0.220) data 0.012 (0.043) loss 0.5165 (0.5334) acc 88.7255 (88.0567) lr 1.5358e-03 eta 0:06:01
epoch [18/50] batch [40/51] time 0.182 (0.214) data 0.000 (0.038) loss 0.3869 (0.5303) acc 92.4107 (88.2438) lr 1.5358e-03 eta 0:05:51
epoch [18/50] batch [45/51] time 0.165 (0.209) data 0.000 (0.034) loss 0.5312 (0.5260) acc 88.2653 (88.3422) lr 1.5358e-03 eta 0:05:42
epoch [18/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.030) loss 0.5465 (0.5259) acc 88.0435 (88.2467) lr 1.5358e-03 eta 0:05:34
>>> alpha1: 0.174  alpha2: -0.132 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [19/50] batch [5/51] time 0.195 (0.478) data 0.000 (0.294) loss 0.6954 (0.5774) acc 82.8704 (84.2959) lr 1.4818e-03 eta 0:12:57
epoch [19/50] batch [10/51] time 0.174 (0.330) data 0.000 (0.149) loss 0.4374 (0.5886) acc 92.3077 (85.8111) lr 1.4818e-03 eta 0:08:55
epoch [19/50] batch [15/51] time 0.188 (0.282) data 0.000 (0.099) loss 0.6041 (0.5802) acc 83.5000 (86.0341) lr 1.4818e-03 eta 0:07:36
epoch [19/50] batch [20/51] time 0.189 (0.258) data 0.000 (0.075) loss 0.6495 (0.5584) acc 84.6154 (86.6993) lr 1.4818e-03 eta 0:06:56
epoch [19/50] batch [25/51] time 0.187 (0.242) data 0.000 (0.060) loss 0.4278 (0.5393) acc 92.4528 (87.2925) lr 1.4818e-03 eta 0:06:29
epoch [19/50] batch [30/51] time 0.169 (0.232) data 0.000 (0.050) loss 0.5037 (0.5466) acc 87.0000 (86.7875) lr 1.4818e-03 eta 0:06:12
epoch [19/50] batch [35/51] time 0.180 (0.223) data 0.000 (0.043) loss 0.4710 (0.5447) acc 88.7255 (86.7007) lr 1.4818e-03 eta 0:05:56
epoch [19/50] batch [40/51] time 0.158 (0.218) data 0.000 (0.037) loss 0.7458 (0.5544) acc 84.4445 (86.5599) lr 1.4818e-03 eta 0:05:46
epoch [19/50] batch [45/51] time 0.158 (0.212) data 0.001 (0.033) loss 0.6222 (0.5510) acc 88.0682 (86.8744) lr 1.4818e-03 eta 0:05:36
epoch [19/50] batch [50/51] time 0.166 (0.208) data 0.000 (0.030) loss 0.4639 (0.5458) acc 86.7347 (87.0512) lr 1.4818e-03 eta 0:05:28
>>> alpha1: 0.170  alpha2: -0.138 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.08 <<<
epoch [20/50] batch [5/51] time 0.158 (0.447) data 0.000 (0.276) loss 0.3852 (0.4733) acc 92.2222 (90.8632) lr 1.4258e-03 eta 0:11:45
epoch [20/50] batch [10/51] time 0.170 (0.309) data 0.000 (0.138) loss 0.4682 (0.4551) acc 90.5000 (90.4944) lr 1.4258e-03 eta 0:08:05
epoch [20/50] batch [15/51] time 0.181 (0.265) data 0.000 (0.092) loss 0.2779 (0.4674) acc 92.5926 (89.5623) lr 1.4258e-03 eta 0:06:54
epoch [20/50] batch [20/51] time 0.173 (0.244) data 0.000 (0.069) loss 0.4840 (0.4792) acc 91.3462 (89.3745) lr 1.4258e-03 eta 0:06:20
epoch [20/50] batch [25/51] time 0.163 (0.258) data 0.000 (0.055) loss 0.7501 (0.4968) acc 75.5208 (88.5158) lr 1.4258e-03 eta 0:06:41
epoch [20/50] batch [30/51] time 0.170 (0.245) data 0.000 (0.046) loss 0.5575 (0.5210) acc 87.7551 (87.8553) lr 1.4258e-03 eta 0:06:19
epoch [20/50] batch [35/51] time 0.173 (0.235) data 0.000 (0.040) loss 0.4992 (0.5106) acc 83.5000 (87.9544) lr 1.4258e-03 eta 0:06:03
epoch [20/50] batch [40/51] time 0.167 (0.228) data 0.000 (0.035) loss 0.5562 (0.5127) acc 83.5000 (87.7440) lr 1.4258e-03 eta 0:05:50
epoch [20/50] batch [45/51] time 0.177 (0.221) data 0.000 (0.031) loss 0.5208 (0.5155) acc 92.1296 (87.7254) lr 1.4258e-03 eta 0:05:39
epoch [20/50] batch [50/51] time 0.181 (0.216) data 0.000 (0.028) loss 0.2397 (0.5105) acc 95.0893 (88.0089) lr 1.4258e-03 eta 0:05:30
>>> alpha1: 0.167  alpha2: -0.145 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [21/50] batch [5/51] time 0.176 (0.496) data 0.000 (0.314) loss 0.7080 (0.5437) acc 79.5000 (87.0887) lr 1.3681e-03 eta 0:12:36
epoch [21/50] batch [10/51] time 0.184 (0.342) data 0.001 (0.157) loss 0.4228 (0.5012) acc 89.2157 (88.5720) lr 1.3681e-03 eta 0:08:40
epoch [21/50] batch [15/51] time 0.181 (0.293) data 0.000 (0.105) loss 0.5633 (0.5031) acc 88.8889 (88.8147) lr 1.3681e-03 eta 0:07:23
epoch [21/50] batch [20/51] time 0.176 (0.264) data 0.000 (0.079) loss 0.5905 (0.5092) acc 84.4340 (88.3597) lr 1.3681e-03 eta 0:06:38
epoch [21/50] batch [25/51] time 0.165 (0.246) data 0.000 (0.063) loss 0.5658 (0.5147) acc 88.0208 (88.2917) lr 1.3681e-03 eta 0:06:10
epoch [21/50] batch [30/51] time 0.166 (0.235) data 0.000 (0.053) loss 0.6504 (0.5073) acc 87.2340 (88.5627) lr 1.3681e-03 eta 0:05:51
epoch [21/50] batch [35/51] time 0.153 (0.226) data 0.000 (0.045) loss 0.6923 (0.5105) acc 84.8837 (88.4397) lr 1.3681e-03 eta 0:05:37
epoch [21/50] batch [40/51] time 0.161 (0.219) data 0.000 (0.040) loss 0.4790 (0.5044) acc 87.2340 (88.4644) lr 1.3681e-03 eta 0:05:25
epoch [21/50] batch [45/51] time 0.165 (0.214) data 0.000 (0.035) loss 0.7172 (0.5097) acc 83.3333 (88.2707) lr 1.3681e-03 eta 0:05:17
epoch [21/50] batch [50/51] time 0.158 (0.209) data 0.001 (0.032) loss 0.5146 (0.5063) acc 88.8889 (88.2675) lr 1.3681e-03 eta 0:05:08
>>> alpha1: 0.163  alpha2: -0.138 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [22/50] batch [5/51] time 0.186 (0.473) data 0.000 (0.295) loss 0.3550 (0.4106) acc 91.8367 (91.4129) lr 1.3090e-03 eta 0:11:37
epoch [22/50] batch [10/51] time 0.183 (0.325) data 0.000 (0.148) loss 0.6350 (0.4962) acc 85.8491 (89.5097) lr 1.3090e-03 eta 0:07:57
epoch [22/50] batch [15/51] time 0.167 (0.275) data 0.001 (0.099) loss 0.3417 (0.4721) acc 88.7755 (89.9919) lr 1.3090e-03 eta 0:06:42
epoch [22/50] batch [20/51] time 0.184 (0.250) data 0.000 (0.074) loss 0.6505 (0.4759) acc 80.0000 (89.5620) lr 1.3090e-03 eta 0:06:04
epoch [22/50] batch [25/51] time 0.170 (0.235) data 0.000 (0.059) loss 0.6155 (0.4799) acc 87.0000 (89.7276) lr 1.3090e-03 eta 0:05:41
epoch [22/50] batch [30/51] time 0.165 (0.225) data 0.000 (0.049) loss 0.4215 (0.4654) acc 88.0208 (89.7854) lr 1.3090e-03 eta 0:05:26
epoch [22/50] batch [35/51] time 0.170 (0.218) data 0.000 (0.042) loss 0.4346 (0.4703) acc 91.3265 (89.7521) lr 1.3090e-03 eta 0:05:14
epoch [22/50] batch [40/51] time 0.169 (0.212) data 0.000 (0.037) loss 0.6315 (0.4752) acc 83.8235 (89.7116) lr 1.3090e-03 eta 0:05:05
epoch [22/50] batch [45/51] time 0.160 (0.208) data 0.000 (0.033) loss 0.6178 (0.4860) acc 85.7955 (89.3900) lr 1.3090e-03 eta 0:04:58
epoch [22/50] batch [50/51] time 0.181 (0.205) data 0.000 (0.030) loss 0.6696 (0.4820) acc 81.0185 (89.3663) lr 1.3090e-03 eta 0:04:53
>>> alpha1: 0.160  alpha2: -0.141 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.08 <<<
epoch [23/50] batch [5/51] time 0.168 (0.466) data 0.000 (0.286) loss 0.3775 (0.8858) acc 88.7755 (83.3307) lr 1.2487e-03 eta 0:11:02
epoch [23/50] batch [10/51] time 0.173 (0.320) data 0.000 (0.143) loss 0.4126 (0.6474) acc 88.7255 (87.0239) lr 1.2487e-03 eta 0:07:34
epoch [23/50] batch [15/51] time 0.181 (0.273) data 0.000 (0.096) loss 0.4589 (0.5898) acc 90.3846 (87.4789) lr 1.2487e-03 eta 0:06:25
epoch [23/50] batch [20/51] time 0.178 (0.251) data 0.000 (0.072) loss 0.5494 (0.5742) acc 90.2778 (87.6382) lr 1.2487e-03 eta 0:05:53
epoch [23/50] batch [25/51] time 0.176 (0.235) data 0.000 (0.057) loss 0.5362 (0.5516) acc 87.5000 (87.8004) lr 1.2487e-03 eta 0:05:29
epoch [23/50] batch [30/51] time 0.174 (0.225) data 0.000 (0.048) loss 0.5786 (0.5401) acc 89.7059 (88.2051) lr 1.2487e-03 eta 0:05:14
epoch [23/50] batch [35/51] time 0.184 (0.218) data 0.000 (0.041) loss 0.4611 (0.5468) acc 89.5000 (87.9091) lr 1.2487e-03 eta 0:05:03
epoch [23/50] batch [40/51] time 0.166 (0.213) data 0.000 (0.036) loss 0.3285 (0.5189) acc 91.8367 (88.6550) lr 1.2487e-03 eta 0:04:55
epoch [23/50] batch [45/51] time 0.164 (0.208) data 0.000 (0.032) loss 0.5023 (0.5112) acc 86.9792 (88.6520) lr 1.2487e-03 eta 0:04:47
epoch [23/50] batch [50/51] time 0.167 (0.204) data 0.000 (0.029) loss 0.6200 (0.5183) acc 82.6531 (88.3427) lr 1.2487e-03 eta 0:04:40
>>> alpha1: 0.157  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [24/50] batch [5/51] time 0.173 (0.451) data 0.001 (0.274) loss 0.5212 (0.5752) acc 84.5000 (88.9490) lr 1.1874e-03 eta 0:10:18
epoch [24/50] batch [10/51] time 0.170 (0.313) data 0.000 (0.137) loss 0.5229 (0.5182) acc 89.0000 (89.5966) lr 1.1874e-03 eta 0:07:07
epoch [24/50] batch [15/51] time 0.173 (0.268) data 0.000 (0.092) loss 0.4763 (0.4809) acc 91.3462 (90.3734) lr 1.1874e-03 eta 0:06:05
epoch [24/50] batch [20/51] time 0.176 (0.246) data 0.000 (0.069) loss 0.4930 (0.4783) acc 89.5833 (89.9773) lr 1.1874e-03 eta 0:05:33
epoch [24/50] batch [25/51] time 0.183 (0.233) data 0.000 (0.055) loss 0.3190 (0.4663) acc 92.8571 (90.1726) lr 1.1874e-03 eta 0:05:14
epoch [24/50] batch [30/51] time 0.181 (0.222) data 0.000 (0.046) loss 0.3591 (0.4672) acc 94.8113 (90.0452) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [35/51] time 0.163 (0.216) data 0.000 (0.039) loss 0.4857 (0.4626) acc 88.5870 (90.0995) lr 1.1874e-03 eta 0:04:49
epoch [24/50] batch [40/51] time 0.172 (0.211) data 0.000 (0.035) loss 0.5475 (0.4912) acc 88.4615 (89.4302) lr 1.1874e-03 eta 0:04:41
epoch [24/50] batch [45/51] time 0.161 (0.205) data 0.000 (0.031) loss 0.4773 (0.4861) acc 88.8298 (89.4559) lr 1.1874e-03 eta 0:04:33
epoch [24/50] batch [50/51] time 0.167 (0.201) data 0.000 (0.028) loss 0.3888 (0.4821) acc 92.0000 (89.4650) lr 1.1874e-03 eta 0:04:27
>>> alpha1: 0.152  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [25/50] batch [5/51] time 0.167 (0.425) data 0.000 (0.241) loss 0.5356 (0.4636) acc 88.0208 (87.9700) lr 1.1253e-03 eta 0:09:20
epoch [25/50] batch [10/51] time 0.171 (0.304) data 0.000 (0.123) loss 0.5219 (0.4944) acc 90.5000 (88.1483) lr 1.1253e-03 eta 0:06:39
epoch [25/50] batch [15/51] time 0.178 (0.263) data 0.000 (0.082) loss 0.5238 (0.4924) acc 87.5000 (88.6004) lr 1.1253e-03 eta 0:05:45
epoch [25/50] batch [20/51] time 0.173 (0.241) data 0.001 (0.062) loss 0.4310 (0.4599) acc 88.4615 (89.3912) lr 1.1253e-03 eta 0:05:14
epoch [25/50] batch [25/51] time 0.174 (0.227) data 0.000 (0.049) loss 0.3977 (0.4587) acc 88.9423 (89.4263) lr 1.1253e-03 eta 0:04:55
epoch [25/50] batch [30/51] time 0.167 (0.218) data 0.000 (0.041) loss 0.4796 (0.4505) acc 86.2245 (89.3409) lr 1.1253e-03 eta 0:04:41
epoch [25/50] batch [35/51] time 0.185 (0.213) data 0.000 (0.035) loss 0.4657 (0.4534) acc 91.0377 (89.5211) lr 1.1253e-03 eta 0:04:34
epoch [25/50] batch [40/51] time 0.182 (0.209) data 0.000 (0.031) loss 0.3539 (0.4549) acc 88.2353 (89.5097) lr 1.1253e-03 eta 0:04:29
epoch [25/50] batch [45/51] time 0.171 (0.206) data 0.000 (0.028) loss 0.5592 (0.4538) acc 85.6383 (89.5342) lr 1.1253e-03 eta 0:04:23
epoch [25/50] batch [50/51] time 0.176 (0.202) data 0.000 (0.025) loss 0.5265 (0.4569) acc 90.0000 (89.5343) lr 1.1253e-03 eta 0:04:18
>>> alpha1: 0.149  alpha2: -0.122 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.03 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [26/50] batch [5/51] time 0.172 (0.418) data 0.000 (0.234) loss 0.4861 (0.4312) acc 90.1961 (89.7261) lr 1.0628e-03 eta 0:08:51
epoch [26/50] batch [10/51] time 0.171 (0.297) data 0.001 (0.117) loss 0.3613 (0.4801) acc 93.1373 (89.6375) lr 1.0628e-03 eta 0:06:16
epoch [26/50] batch [15/51] time 0.158 (0.256) data 0.000 (0.078) loss 0.3869 (0.4446) acc 91.6667 (90.7709) lr 1.0628e-03 eta 0:05:22
epoch [26/50] batch [20/51] time 0.173 (0.236) data 0.000 (0.059) loss 0.4294 (0.4347) acc 90.8654 (91.1276) lr 1.0628e-03 eta 0:04:56
epoch [26/50] batch [25/51] time 0.182 (0.223) data 0.000 (0.047) loss 0.4932 (0.5239) acc 91.9811 (89.9842) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [30/51] time 0.168 (0.213) data 0.000 (0.039) loss 0.3061 (0.5127) acc 96.0000 (89.9942) lr 1.0628e-03 eta 0:04:25
epoch [26/50] batch [35/51] time 0.158 (0.208) data 0.000 (0.034) loss 0.4873 (0.4983) acc 89.7727 (90.1020) lr 1.0628e-03 eta 0:04:18
epoch [26/50] batch [40/51] time 0.163 (0.204) data 0.000 (0.029) loss 0.4473 (0.4993) acc 88.8298 (89.9397) lr 1.0628e-03 eta 0:04:11
epoch [26/50] batch [45/51] time 0.169 (0.200) data 0.000 (0.026) loss 0.4788 (0.4946) acc 87.5000 (89.9184) lr 1.0628e-03 eta 0:04:06
epoch [26/50] batch [50/51] time 0.171 (0.197) data 0.000 (0.024) loss 0.3819 (0.5180) acc 92.0000 (89.6166) lr 1.0628e-03 eta 0:04:01
>>> alpha1: 0.148  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [27/50] batch [5/51] time 0.171 (0.466) data 0.000 (0.285) loss 0.3246 (0.3496) acc 92.3469 (92.3581) lr 1.0000e-03 eta 0:09:27
epoch [27/50] batch [10/51] time 0.180 (0.319) data 0.000 (0.144) loss 0.2764 (0.3793) acc 97.1698 (92.0482) lr 1.0000e-03 eta 0:06:27
epoch [27/50] batch [15/51] time 0.187 (0.274) data 0.001 (0.096) loss 0.4775 (0.3949) acc 91.0000 (91.8705) lr 1.0000e-03 eta 0:05:30
epoch [27/50] batch [20/51] time 0.176 (0.250) data 0.000 (0.072) loss 0.3937 (0.4005) acc 89.7059 (91.5369) lr 1.0000e-03 eta 0:05:00
epoch [27/50] batch [25/51] time 0.187 (0.236) data 0.000 (0.058) loss 0.3818 (0.4046) acc 92.0000 (91.3017) lr 1.0000e-03 eta 0:04:42
epoch [27/50] batch [30/51] time 0.191 (0.227) data 0.000 (0.049) loss 0.4103 (0.4079) acc 92.9245 (91.1873) lr 1.0000e-03 eta 0:04:30
epoch [27/50] batch [35/51] time 0.180 (0.220) data 0.000 (0.042) loss 0.5205 (0.4154) acc 88.2979 (90.9808) lr 1.0000e-03 eta 0:04:21
epoch [27/50] batch [40/51] time 0.171 (0.215) data 0.000 (0.037) loss 0.4475 (0.4132) acc 90.1961 (91.1003) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [45/51] time 0.172 (0.210) data 0.000 (0.033) loss 0.3881 (0.4239) acc 88.2353 (90.8609) lr 1.0000e-03 eta 0:04:07
epoch [27/50] batch [50/51] time 0.179 (0.207) data 0.000 (0.029) loss 0.3799 (0.4207) acc 91.5094 (90.9496) lr 1.0000e-03 eta 0:04:02
>>> alpha1: 0.147  alpha2: -0.121 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [28/50] batch [5/51] time 0.168 (0.476) data 0.000 (0.295) loss 0.5818 (0.4864) acc 85.7143 (88.4498) lr 9.3721e-04 eta 0:09:16
epoch [28/50] batch [10/51] time 0.186 (0.326) data 0.000 (0.148) loss 0.4245 (0.4434) acc 92.1569 (89.5190) lr 9.3721e-04 eta 0:06:18
epoch [28/50] batch [15/51] time 0.173 (0.274) data 0.000 (0.099) loss 0.3712 (0.4421) acc 91.8269 (89.6152) lr 9.3721e-04 eta 0:05:16
epoch [28/50] batch [20/51] time 0.184 (0.249) data 0.000 (0.074) loss 0.2930 (0.4363) acc 95.3704 (89.7334) lr 9.3721e-04 eta 0:04:47
epoch [28/50] batch [25/51] time 0.166 (0.234) data 0.000 (0.059) loss 0.4376 (0.4382) acc 88.5000 (89.5575) lr 9.3721e-04 eta 0:04:28
epoch [28/50] batch [30/51] time 0.171 (0.223) data 0.000 (0.049) loss 0.4376 (0.4406) acc 90.3061 (89.6137) lr 9.3721e-04 eta 0:04:15
epoch [28/50] batch [35/51] time 0.165 (0.216) data 0.000 (0.042) loss 0.4749 (0.4411) acc 86.4583 (89.6317) lr 9.3721e-04 eta 0:04:05
epoch [28/50] batch [40/51] time 0.178 (0.210) data 0.000 (0.037) loss 0.3878 (0.4472) acc 89.8148 (89.4907) lr 9.3721e-04 eta 0:03:58
epoch [28/50] batch [45/51] time 0.161 (0.206) data 0.000 (0.033) loss 0.3293 (0.4408) acc 93.0851 (89.6945) lr 9.3721e-04 eta 0:03:51
epoch [28/50] batch [50/51] time 0.182 (0.202) data 0.000 (0.030) loss 0.4720 (0.4417) acc 87.9464 (89.5891) lr 9.3721e-04 eta 0:03:47
>>> alpha1: 0.146  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [29/50] batch [5/51] time 0.165 (0.419) data 0.000 (0.242) loss 0.4067 (0.4589) acc 89.5833 (89.0915) lr 8.7467e-04 eta 0:07:48
epoch [29/50] batch [10/51] time 0.171 (0.299) data 0.000 (0.121) loss 0.7238 (0.4979) acc 82.6087 (88.2322) lr 8.7467e-04 eta 0:05:32
epoch [29/50] batch [15/51] time 0.177 (0.259) data 0.000 (0.081) loss 0.3607 (0.4697) acc 93.3962 (89.4146) lr 8.7467e-04 eta 0:04:46
epoch [29/50] batch [20/51] time 0.231 (0.241) data 0.000 (0.061) loss 0.4817 (0.4916) acc 89.2157 (88.7792) lr 8.7467e-04 eta 0:04:25
epoch [29/50] batch [25/51] time 0.195 (0.230) data 0.001 (0.049) loss 0.3184 (0.4817) acc 94.2308 (89.1982) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [30/51] time 0.179 (0.223) data 0.001 (0.041) loss 0.4943 (0.4751) acc 91.1765 (89.4523) lr 8.7467e-04 eta 0:04:03
epoch [29/50] batch [35/51] time 0.174 (0.217) data 0.000 (0.035) loss 0.3810 (0.4571) acc 93.0000 (90.0525) lr 8.7467e-04 eta 0:03:55
epoch [29/50] batch [40/51] time 0.159 (0.210) data 0.000 (0.031) loss 0.6010 (0.4553) acc 80.9783 (89.9451) lr 8.7467e-04 eta 0:03:47
epoch [29/50] batch [45/51] time 0.177 (0.206) data 0.000 (0.027) loss 0.4274 (0.4531) acc 88.8889 (89.8413) lr 8.7467e-04 eta 0:03:41
epoch [29/50] batch [50/51] time 0.164 (0.202) data 0.000 (0.024) loss 0.3184 (0.4503) acc 92.8571 (89.8095) lr 8.7467e-04 eta 0:03:36
>>> alpha1: 0.146  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [30/50] batch [5/51] time 0.159 (0.457) data 0.000 (0.277) loss 0.4458 (0.4335) acc 86.9318 (89.1345) lr 8.1262e-04 eta 0:08:06
epoch [30/50] batch [10/51] time 0.175 (0.318) data 0.000 (0.138) loss 0.5020 (0.4430) acc 91.2791 (89.2458) lr 8.1262e-04 eta 0:05:37
epoch [30/50] batch [15/51] time 0.180 (0.272) data 0.000 (0.092) loss 0.3695 (0.4480) acc 89.6226 (89.1335) lr 8.1262e-04 eta 0:04:47
epoch [30/50] batch [20/51] time 0.176 (0.247) data 0.000 (0.069) loss 0.2056 (0.4287) acc 95.7547 (89.5825) lr 8.1262e-04 eta 0:04:19
epoch [30/50] batch [25/51] time 0.190 (0.234) data 0.000 (0.056) loss 0.2693 (0.4206) acc 93.6364 (90.0326) lr 8.1262e-04 eta 0:04:04
epoch [30/50] batch [30/51] time 0.194 (0.224) data 0.000 (0.046) loss 0.4084 (0.4204) acc 89.8148 (90.1133) lr 8.1262e-04 eta 0:03:53
epoch [30/50] batch [35/51] time 0.166 (0.218) data 0.000 (0.040) loss 0.3822 (0.4356) acc 91.6667 (89.8174) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [40/51] time 0.171 (0.212) data 0.000 (0.035) loss 0.3019 (0.4289) acc 91.6667 (89.9138) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [45/51] time 0.166 (0.207) data 0.000 (0.031) loss 0.4861 (0.4341) acc 90.3061 (89.9466) lr 8.1262e-04 eta 0:03:32
epoch [30/50] batch [50/51] time 0.169 (0.203) data 0.000 (0.028) loss 0.4477 (0.4304) acc 89.7059 (90.1777) lr 8.1262e-04 eta 0:03:27
>>> alpha1: 0.143  alpha2: -0.132 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [31/50] batch [5/51] time 0.169 (0.450) data 0.000 (0.278) loss 0.4771 (0.3859) acc 90.0000 (93.0048) lr 7.5131e-04 eta 0:07:36
epoch [31/50] batch [10/51] time 0.164 (0.312) data 0.000 (0.139) loss 0.4209 (0.3902) acc 87.5000 (91.9036) lr 7.5131e-04 eta 0:05:14
epoch [31/50] batch [15/51] time 0.174 (0.264) data 0.000 (0.093) loss 0.4222 (0.4192) acc 87.9808 (91.0936) lr 7.5131e-04 eta 0:04:25
epoch [31/50] batch [20/51] time 0.159 (0.241) data 0.000 (0.070) loss 0.3539 (0.3989) acc 94.0217 (91.6620) lr 7.5131e-04 eta 0:04:01
epoch [31/50] batch [25/51] time 0.176 (0.229) data 0.000 (0.056) loss 0.2405 (0.3884) acc 94.7115 (91.8282) lr 7.5131e-04 eta 0:03:47
epoch [31/50] batch [30/51] time 0.173 (0.220) data 0.000 (0.047) loss 0.4011 (0.3951) acc 91.6667 (91.7135) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [35/51] time 0.178 (0.216) data 0.000 (0.040) loss 0.2787 (0.3986) acc 95.5000 (91.6158) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [40/51] time 0.166 (0.211) data 0.000 (0.035) loss 0.5083 (0.4014) acc 86.2245 (91.4211) lr 7.5131e-04 eta 0:03:26
epoch [31/50] batch [45/51] time 0.172 (0.206) data 0.000 (0.031) loss 0.3796 (0.4031) acc 92.7885 (91.3710) lr 7.5131e-04 eta 0:03:21
epoch [31/50] batch [50/51] time 0.163 (0.203) data 0.000 (0.028) loss 0.4817 (0.4117) acc 90.0000 (91.1331) lr 7.5131e-04 eta 0:03:16
>>> alpha1: 0.142  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [32/50] batch [5/51] time 0.187 (0.441) data 0.012 (0.260) loss 0.2548 (0.3125) acc 96.6346 (95.0513) lr 6.9098e-04 eta 0:07:04
epoch [32/50] batch [10/51] time 0.161 (0.309) data 0.000 (0.130) loss 0.4268 (0.3772) acc 90.4255 (92.8304) lr 6.9098e-04 eta 0:04:56
epoch [32/50] batch [15/51] time 0.167 (0.264) data 0.000 (0.087) loss 0.3641 (0.3779) acc 92.3469 (92.6786) lr 6.9098e-04 eta 0:04:11
epoch [32/50] batch [20/51] time 0.174 (0.242) data 0.000 (0.065) loss 0.3125 (0.3743) acc 93.2292 (92.3908) lr 6.9098e-04 eta 0:03:49
epoch [32/50] batch [25/51] time 0.176 (0.228) data 0.000 (0.052) loss 0.2472 (0.3876) acc 95.2830 (92.0976) lr 6.9098e-04 eta 0:03:35
epoch [32/50] batch [30/51] time 0.174 (0.219) data 0.000 (0.043) loss 0.5614 (0.3854) acc 88.7255 (92.1744) lr 6.9098e-04 eta 0:03:25
epoch [32/50] batch [35/51] time 0.183 (0.213) data 0.001 (0.037) loss 0.5626 (0.3906) acc 88.5417 (91.9941) lr 6.9098e-04 eta 0:03:18
epoch [32/50] batch [40/51] time 0.165 (0.209) data 0.000 (0.033) loss 0.4069 (0.3868) acc 92.3469 (92.0642) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [45/51] time 0.168 (0.204) data 0.000 (0.029) loss 0.4736 (0.3953) acc 88.0000 (91.7049) lr 6.9098e-04 eta 0:03:08
epoch [32/50] batch [50/51] time 0.164 (0.201) data 0.000 (0.026) loss 0.4276 (0.3936) acc 90.1042 (91.7212) lr 6.9098e-04 eta 0:03:04
>>> alpha1: 0.143  alpha2: -0.107 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [33/50] batch [5/51] time 0.176 (0.460) data 0.000 (0.277) loss 0.4034 (0.3744) acc 91.4894 (91.9925) lr 6.3188e-04 eta 0:07:00
epoch [33/50] batch [10/51] time 0.177 (0.316) data 0.000 (0.139) loss 0.3255 (0.3848) acc 91.0377 (92.2157) lr 6.3188e-04 eta 0:04:46
epoch [33/50] batch [15/51] time 0.168 (0.270) data 0.000 (0.092) loss 0.3243 (0.3798) acc 93.6170 (92.2677) lr 6.3188e-04 eta 0:04:03
epoch [33/50] batch [20/51] time 0.162 (0.280) data 0.000 (0.069) loss 0.4143 (0.3761) acc 90.4255 (91.9885) lr 6.3188e-04 eta 0:04:11
epoch [33/50] batch [25/51] time 0.206 (0.261) data 0.000 (0.056) loss 0.4806 (0.3933) acc 90.6863 (91.5139) lr 6.3188e-04 eta 0:03:52
epoch [33/50] batch [30/51] time 0.171 (0.246) data 0.000 (0.046) loss 0.3165 (0.3992) acc 93.6274 (91.4120) lr 6.3188e-04 eta 0:03:38
epoch [33/50] batch [35/51] time 0.174 (0.236) data 0.000 (0.040) loss 0.3714 (0.3994) acc 87.9808 (91.2116) lr 6.3188e-04 eta 0:03:28
epoch [33/50] batch [40/51] time 0.168 (0.228) data 0.000 (0.035) loss 0.3470 (0.4060) acc 91.0000 (91.0282) lr 6.3188e-04 eta 0:03:20
epoch [33/50] batch [45/51] time 0.167 (0.221) data 0.000 (0.031) loss 0.4156 (0.3997) acc 88.5000 (91.0020) lr 6.3188e-04 eta 0:03:13
epoch [33/50] batch [50/51] time 0.177 (0.216) data 0.000 (0.028) loss 0.6133 (0.4027) acc 87.5000 (90.8952) lr 6.3188e-04 eta 0:03:07
>>> alpha1: 0.141  alpha2: -0.104 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [34/50] batch [5/51] time 0.188 (0.437) data 0.000 (0.257) loss 0.2459 (0.3598) acc 93.3673 (91.1368) lr 5.7422e-04 eta 0:06:16
epoch [34/50] batch [10/51] time 0.177 (0.309) data 0.000 (0.129) loss 0.5116 (0.4051) acc 91.5094 (90.4746) lr 5.7422e-04 eta 0:04:25
epoch [34/50] batch [15/51] time 0.174 (0.266) data 0.000 (0.086) loss 0.4132 (0.3827) acc 90.6863 (91.2772) lr 5.7422e-04 eta 0:03:46
epoch [34/50] batch [20/51] time 0.178 (0.244) data 0.000 (0.064) loss 0.4501 (0.3847) acc 90.8163 (91.4169) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [25/51] time 0.169 (0.230) data 0.000 (0.052) loss 0.3285 (0.3797) acc 93.6170 (91.6652) lr 5.7422e-04 eta 0:03:13
epoch [34/50] batch [30/51] time 0.187 (0.220) data 0.000 (0.043) loss 0.3098 (0.3945) acc 93.6364 (91.3036) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [35/51] time 0.185 (0.216) data 0.000 (0.037) loss 0.3975 (0.3904) acc 90.1961 (91.4189) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [40/51] time 0.168 (0.210) data 0.000 (0.032) loss 0.3240 (0.3835) acc 94.0000 (91.6457) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.029) loss 0.4447 (0.3868) acc 90.5000 (91.5943) lr 5.7422e-04 eta 0:02:48
epoch [34/50] batch [50/51] time 0.176 (0.201) data 0.000 (0.026) loss 0.3412 (0.3924) acc 91.9811 (91.3562) lr 5.7422e-04 eta 0:02:44
>>> alpha1: 0.139  alpha2: -0.098 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [35/50] batch [5/51] time 0.176 (0.443) data 0.000 (0.253) loss 0.3582 (0.3218) acc 94.7115 (92.9639) lr 5.1825e-04 eta 0:05:59
epoch [35/50] batch [10/51] time 0.177 (0.308) data 0.000 (0.126) loss 0.2608 (0.3725) acc 96.6981 (92.8491) lr 5.1825e-04 eta 0:04:08
epoch [35/50] batch [15/51] time 0.174 (0.264) data 0.000 (0.084) loss 0.3215 (0.3642) acc 93.3673 (92.7071) lr 5.1825e-04 eta 0:03:31
epoch [35/50] batch [20/51] time 0.192 (0.240) data 0.000 (0.063) loss 0.3491 (0.3850) acc 93.5185 (92.0410) lr 5.1825e-04 eta 0:03:10
epoch [35/50] batch [25/51] time 0.174 (0.226) data 0.000 (0.051) loss 0.3692 (0.4320) acc 93.5000 (91.5530) lr 5.1825e-04 eta 0:02:58
epoch [35/50] batch [30/51] time 0.180 (0.218) data 0.000 (0.042) loss 0.3988 (0.4359) acc 90.5000 (91.2966) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [35/51] time 0.163 (0.211) data 0.000 (0.036) loss 0.4941 (0.4397) acc 87.7660 (91.1870) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [40/51] time 0.166 (0.206) data 0.000 (0.032) loss 0.2677 (0.4274) acc 95.3125 (91.3894) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [45/51] time 0.169 (0.202) data 0.000 (0.028) loss 0.2899 (0.4221) acc 94.5000 (91.4450) lr 5.1825e-04 eta 0:02:35
epoch [35/50] batch [50/51] time 0.177 (0.199) data 0.000 (0.026) loss 0.3891 (0.4166) acc 87.2642 (91.2887) lr 5.1825e-04 eta 0:02:32
>>> alpha1: 0.140  alpha2: -0.092 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [36/50] batch [5/51] time 0.174 (0.420) data 0.000 (0.247) loss 0.4297 (0.3950) acc 92.3913 (92.1719) lr 4.6417e-04 eta 0:05:19
epoch [36/50] batch [10/51] time 0.172 (0.297) data 0.000 (0.124) loss 0.4151 (0.3864) acc 92.6471 (92.1850) lr 4.6417e-04 eta 0:03:44
epoch [36/50] batch [15/51] time 0.179 (0.258) data 0.000 (0.083) loss 0.3656 (0.4060) acc 91.6667 (91.2916) lr 4.6417e-04 eta 0:03:13
epoch [36/50] batch [20/51] time 0.171 (0.238) data 0.000 (0.062) loss 0.4801 (0.4021) acc 90.6863 (91.2808) lr 4.6417e-04 eta 0:02:57
epoch [36/50] batch [25/51] time 0.167 (0.226) data 0.000 (0.050) loss 0.3904 (0.3990) acc 93.3673 (91.3099) lr 4.6417e-04 eta 0:02:47
epoch [36/50] batch [30/51] time 0.189 (0.219) data 0.000 (0.042) loss 0.2814 (0.3967) acc 96.3542 (91.1341) lr 4.6417e-04 eta 0:02:40
epoch [36/50] batch [35/51] time 0.181 (0.213) data 0.000 (0.036) loss 0.4021 (0.3976) acc 90.9091 (91.0548) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [40/51] time 0.171 (0.208) data 0.000 (0.031) loss 0.4890 (0.4047) acc 87.2549 (90.8153) lr 4.6417e-04 eta 0:02:30
epoch [36/50] batch [45/51] time 0.158 (0.204) data 0.000 (0.028) loss 0.4406 (0.4064) acc 90.0000 (90.9600) lr 4.6417e-04 eta 0:02:26
epoch [36/50] batch [50/51] time 0.173 (0.201) data 0.000 (0.025) loss 0.4789 (0.4076) acc 85.0962 (90.8227) lr 4.6417e-04 eta 0:02:23
>>> alpha1: 0.140  alpha2: -0.094 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.08 <<<
epoch [37/50] batch [5/51] time 0.170 (0.466) data 0.000 (0.287) loss 0.3692 (0.4518) acc 89.5833 (89.5989) lr 4.1221e-04 eta 0:05:30
epoch [37/50] batch [10/51] time 0.175 (0.322) data 0.000 (0.144) loss 0.3446 (0.3928) acc 94.6078 (91.4947) lr 4.1221e-04 eta 0:03:46
epoch [37/50] batch [15/51] time 0.176 (0.272) data 0.000 (0.096) loss 0.4251 (0.4431) acc 90.8163 (90.8405) lr 4.1221e-04 eta 0:03:10
epoch [37/50] batch [20/51] time 0.179 (0.248) data 0.000 (0.072) loss 0.4949 (0.4544) acc 87.0192 (90.1827) lr 4.1221e-04 eta 0:02:51
epoch [37/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.058) loss 0.3706 (0.4298) acc 89.3617 (90.4650) lr 4.1221e-04 eta 0:02:40
epoch [37/50] batch [30/51] time 0.172 (0.224) data 0.000 (0.048) loss 0.2013 (0.4129) acc 95.9184 (90.8752) lr 4.1221e-04 eta 0:02:33
epoch [37/50] batch [35/51] time 0.177 (0.217) data 0.000 (0.041) loss 0.3940 (0.4070) acc 88.0000 (91.0428) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [40/51] time 0.183 (0.229) data 0.000 (0.036) loss 0.3211 (0.3960) acc 91.6667 (91.2545) lr 4.1221e-04 eta 0:02:34
epoch [37/50] batch [45/51] time 0.159 (0.222) data 0.000 (0.032) loss 0.2939 (0.3909) acc 93.4783 (91.4555) lr 4.1221e-04 eta 0:02:28
epoch [37/50] batch [50/51] time 0.172 (0.217) data 0.000 (0.029) loss 0.2124 (0.3876) acc 96.1538 (91.4718) lr 4.1221e-04 eta 0:02:24
>>> alpha1: 0.139  alpha2: -0.104 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [38/50] batch [5/51] time 0.171 (0.433) data 0.000 (0.264) loss 0.3597 (0.3806) acc 91.6667 (91.4439) lr 3.6258e-04 eta 0:04:44
epoch [38/50] batch [10/51] time 0.178 (0.305) data 0.000 (0.132) loss 0.3448 (0.3684) acc 90.7407 (91.5406) lr 3.6258e-04 eta 0:03:19
epoch [38/50] batch [15/51] time 0.179 (0.261) data 0.000 (0.088) loss 0.3908 (0.3826) acc 91.8367 (91.6700) lr 3.6258e-04 eta 0:02:48
epoch [38/50] batch [20/51] time 0.171 (0.240) data 0.000 (0.066) loss 0.5199 (0.3826) acc 87.7551 (91.6770) lr 3.6258e-04 eta 0:02:34
epoch [38/50] batch [25/51] time 0.192 (0.228) data 0.000 (0.053) loss 0.3957 (0.3788) acc 90.9091 (91.5653) lr 3.6258e-04 eta 0:02:25
epoch [38/50] batch [30/51] time 0.181 (0.219) data 0.000 (0.044) loss 0.3734 (0.3750) acc 91.6667 (91.8168) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [35/51] time 0.174 (0.212) data 0.000 (0.038) loss 0.3253 (0.3756) acc 92.1875 (91.8531) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [40/51] time 0.172 (0.207) data 0.000 (0.033) loss 0.3764 (0.3773) acc 94.2308 (91.8501) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [45/51] time 0.172 (0.204) data 0.000 (0.030) loss 0.4106 (0.3776) acc 87.9808 (91.7139) lr 3.6258e-04 eta 0:02:05
epoch [38/50] batch [50/51] time 0.164 (0.200) data 0.000 (0.027) loss 0.3112 (0.3807) acc 94.2708 (91.8265) lr 3.6258e-04 eta 0:02:02
>>> alpha1: 0.139  alpha2: -0.108 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [39/50] batch [5/51] time 0.170 (0.461) data 0.000 (0.281) loss 0.5171 (0.5132) acc 88.5417 (89.2250) lr 3.1545e-04 eta 0:04:39
epoch [39/50] batch [10/51] time 0.167 (0.319) data 0.000 (0.141) loss 0.2830 (0.4248) acc 93.8775 (91.3253) lr 3.1545e-04 eta 0:03:11
epoch [39/50] batch [15/51] time 0.165 (0.268) data 0.000 (0.094) loss 0.2425 (0.4095) acc 94.7917 (91.2641) lr 3.1545e-04 eta 0:02:39
epoch [39/50] batch [20/51] time 0.186 (0.243) data 0.001 (0.070) loss 0.4507 (0.4060) acc 88.2353 (91.0982) lr 3.1545e-04 eta 0:02:24
epoch [39/50] batch [25/51] time 0.182 (0.230) data 0.011 (0.057) loss 0.4164 (0.4091) acc 90.1961 (91.1364) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [30/51] time 0.181 (0.220) data 0.000 (0.047) loss 0.4676 (0.4146) acc 86.0577 (91.1580) lr 3.1545e-04 eta 0:02:07
epoch [39/50] batch [35/51] time 0.185 (0.213) data 0.000 (0.041) loss 0.5130 (0.4166) acc 87.7193 (91.0043) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [40/51] time 0.163 (0.208) data 0.000 (0.036) loss 0.3408 (0.4075) acc 93.2292 (91.1427) lr 3.1545e-04 eta 0:01:58
epoch [39/50] batch [45/51] time 0.167 (0.203) data 0.000 (0.032) loss 0.2502 (0.3981) acc 95.5000 (91.3897) lr 3.1545e-04 eta 0:01:55
epoch [39/50] batch [50/51] time 0.186 (0.201) data 0.000 (0.028) loss 0.2630 (0.3954) acc 95.1923 (91.5010) lr 3.1545e-04 eta 0:01:52
>>> alpha1: 0.137  alpha2: -0.107 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [40/50] batch [5/51] time 0.202 (0.436) data 0.023 (0.258) loss 0.4121 (0.3954) acc 90.8163 (90.9563) lr 2.7103e-04 eta 0:04:02
epoch [40/50] batch [10/51] time 0.183 (0.305) data 0.000 (0.129) loss 0.4515 (0.4152) acc 89.7059 (90.1359) lr 2.7103e-04 eta 0:02:48
epoch [40/50] batch [15/51] time 0.176 (0.261) data 0.000 (0.086) loss 0.4199 (0.3991) acc 89.1509 (90.9824) lr 2.7103e-04 eta 0:02:22
epoch [40/50] batch [20/51] time 0.170 (0.238) data 0.000 (0.065) loss 0.3424 (0.3751) acc 91.1765 (91.4356) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [25/51] time 0.170 (0.224) data 0.000 (0.052) loss 0.4323 (0.3773) acc 88.2353 (91.2246) lr 2.7103e-04 eta 0:02:00
epoch [40/50] batch [30/51] time 0.199 (0.216) data 0.000 (0.043) loss 0.2842 (0.3767) acc 96.1538 (91.4767) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [35/51] time 0.170 (0.209) data 0.000 (0.037) loss 0.3373 (0.3749) acc 93.1373 (91.5626) lr 2.7103e-04 eta 0:01:50
epoch [40/50] batch [40/51] time 0.169 (0.206) data 0.000 (0.032) loss 0.3022 (0.3680) acc 92.7083 (91.8012) lr 2.7103e-04 eta 0:01:47
epoch [40/50] batch [45/51] time 0.170 (0.202) data 0.000 (0.029) loss 0.3843 (0.3711) acc 91.1765 (91.7845) lr 2.7103e-04 eta 0:01:44
epoch [40/50] batch [50/51] time 0.170 (0.199) data 0.000 (0.026) loss 0.3880 (0.3734) acc 93.1373 (91.8107) lr 2.7103e-04 eta 0:01:41
>>> alpha1: 0.136  alpha2: -0.107 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [41/50] batch [5/51] time 0.182 (0.434) data 0.000 (0.258) loss 0.4147 (0.3958) acc 91.8182 (91.2091) lr 2.2949e-04 eta 0:03:39
epoch [41/50] batch [10/51] time 0.177 (0.306) data 0.001 (0.129) loss 0.4096 (0.4028) acc 88.2353 (90.5368) lr 2.2949e-04 eta 0:02:33
epoch [41/50] batch [15/51] time 0.169 (0.261) data 0.000 (0.086) loss 0.3800 (0.3765) acc 92.1875 (91.4885) lr 2.2949e-04 eta 0:02:09
epoch [41/50] batch [20/51] time 0.172 (0.239) data 0.000 (0.065) loss 0.2324 (0.3507) acc 93.2692 (92.2932) lr 2.2949e-04 eta 0:01:57
epoch [41/50] batch [25/51] time 0.167 (0.226) data 0.000 (0.052) loss 0.3639 (0.3624) acc 92.0213 (91.9563) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [30/51] time 0.169 (0.217) data 0.000 (0.043) loss 0.2268 (0.3669) acc 96.0000 (91.9755) lr 2.2949e-04 eta 0:01:44
epoch [41/50] batch [35/51] time 0.167 (0.211) data 0.000 (0.037) loss 0.2835 (0.3665) acc 95.9184 (91.9668) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [40/51] time 0.179 (0.207) data 0.000 (0.032) loss 0.4378 (0.3655) acc 88.8889 (92.0023) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [45/51] time 0.174 (0.203) data 0.000 (0.029) loss 0.4066 (0.3708) acc 91.9811 (91.9843) lr 2.2949e-04 eta 0:01:34
epoch [41/50] batch [50/51] time 0.164 (0.199) data 0.000 (0.026) loss 0.5063 (0.3718) acc 88.2653 (91.9526) lr 2.2949e-04 eta 0:01:31
>>> alpha1: 0.135  alpha2: -0.099 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [42/50] batch [5/51] time 0.177 (0.507) data 0.001 (0.330) loss 0.3488 (0.2956) acc 91.8269 (94.7226) lr 1.9098e-04 eta 0:03:50
epoch [42/50] batch [10/51] time 0.179 (0.343) data 0.000 (0.165) loss 0.3368 (0.3386) acc 93.5185 (94.0978) lr 1.9098e-04 eta 0:02:34
epoch [42/50] batch [15/51] time 0.205 (0.290) data 0.000 (0.110) loss 0.4230 (0.3681) acc 89.7959 (93.5335) lr 1.9098e-04 eta 0:02:08
epoch [42/50] batch [20/51] time 0.181 (0.261) data 0.000 (0.083) loss 0.3404 (0.3708) acc 92.2727 (93.0665) lr 1.9098e-04 eta 0:01:54
epoch [42/50] batch [25/51] time 0.181 (0.243) data 0.000 (0.066) loss 0.2371 (0.3734) acc 96.8182 (92.8271) lr 1.9098e-04 eta 0:01:45
epoch [42/50] batch [30/51] time 0.182 (0.232) data 0.001 (0.055) loss 0.5084 (0.3825) acc 87.0192 (92.4708) lr 1.9098e-04 eta 0:01:39
epoch [42/50] batch [35/51] time 0.183 (0.226) data 0.000 (0.047) loss 0.3244 (0.3795) acc 94.2308 (92.4811) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [40/51] time 0.164 (0.220) data 0.000 (0.042) loss 0.3417 (0.3751) acc 93.0851 (92.4178) lr 1.9098e-04 eta 0:01:32
epoch [42/50] batch [45/51] time 0.165 (0.214) data 0.000 (0.037) loss 0.4244 (0.3759) acc 91.1458 (92.4014) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [50/51] time 0.179 (0.210) data 0.004 (0.033) loss 0.7370 (0.3815) acc 85.0962 (92.2513) lr 1.9098e-04 eta 0:01:25
>>> alpha1: 0.134  alpha2: -0.094 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [43/50] batch [5/51] time 0.181 (0.465) data 0.000 (0.273) loss 0.2529 (0.2746) acc 93.8679 (95.2099) lr 1.5567e-04 eta 0:03:07
epoch [43/50] batch [10/51] time 0.162 (0.318) data 0.000 (0.137) loss 0.5272 (0.3449) acc 90.7609 (93.4427) lr 1.5567e-04 eta 0:02:06
epoch [43/50] batch [15/51] time 0.182 (0.270) data 0.000 (0.091) loss 0.1845 (0.3460) acc 97.2222 (93.1148) lr 1.5567e-04 eta 0:01:46
epoch [43/50] batch [20/51] time 0.173 (0.247) data 0.000 (0.069) loss 0.2556 (0.3484) acc 95.1923 (92.5848) lr 1.5567e-04 eta 0:01:35
epoch [43/50] batch [25/51] time 0.182 (0.232) data 0.000 (0.055) loss 0.4728 (0.3688) acc 91.3636 (92.1297) lr 1.5567e-04 eta 0:01:28
epoch [43/50] batch [30/51] time 0.174 (0.222) data 0.000 (0.046) loss 0.2063 (0.3611) acc 97.0588 (92.3797) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [35/51] time 0.165 (0.215) data 0.000 (0.039) loss 0.4414 (0.3636) acc 87.7660 (92.4364) lr 1.5567e-04 eta 0:01:20
epoch [43/50] batch [40/51] time 0.164 (0.209) data 0.000 (0.034) loss 0.2738 (0.3595) acc 93.7500 (92.4823) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.031) loss 0.3269 (0.3601) acc 92.3469 (92.3321) lr 1.5567e-04 eta 0:01:14
epoch [43/50] batch [50/51] time 0.170 (0.202) data 0.000 (0.028) loss 0.3115 (0.3560) acc 92.5000 (92.3875) lr 1.5567e-04 eta 0:01:12
>>> alpha1: 0.133  alpha2: -0.089 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [44/50] batch [5/51] time 0.165 (0.445) data 0.001 (0.270) loss 0.3475 (0.4041) acc 91.6667 (90.2776) lr 1.2369e-04 eta 0:02:36
epoch [44/50] batch [10/51] time 0.170 (0.312) data 0.000 (0.135) loss 0.5130 (0.3931) acc 88.2653 (91.0081) lr 1.2369e-04 eta 0:01:48
epoch [44/50] batch [15/51] time 0.182 (0.268) data 0.000 (0.090) loss 0.3619 (0.3875) acc 91.8367 (91.3008) lr 1.2369e-04 eta 0:01:31
epoch [44/50] batch [20/51] time 0.176 (0.245) data 0.000 (0.068) loss 0.2737 (0.3861) acc 94.8113 (91.3726) lr 1.2369e-04 eta 0:01:22
epoch [44/50] batch [25/51] time 0.169 (0.229) data 0.000 (0.054) loss 0.2920 (0.3767) acc 94.6808 (91.6774) lr 1.2369e-04 eta 0:01:16
epoch [44/50] batch [30/51] time 0.194 (0.221) data 0.000 (0.045) loss 0.2602 (0.3636) acc 96.8182 (92.0812) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [35/51] time 0.166 (0.215) data 0.000 (0.039) loss 0.3007 (0.3577) acc 94.3878 (92.2880) lr 1.2369e-04 eta 0:01:09
epoch [44/50] batch [40/51] time 0.170 (0.210) data 0.000 (0.034) loss 0.3054 (0.3552) acc 94.1176 (92.4479) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.030) loss 0.2613 (0.3539) acc 96.4286 (92.5872) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.170 (0.202) data 0.001 (0.027) loss 0.4417 (0.3589) acc 91.5000 (92.5063) lr 1.2369e-04 eta 0:01:02
>>> alpha1: 0.133  alpha2: -0.083 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [45/50] batch [5/51] time 0.182 (0.475) data 0.000 (0.296) loss 0.3321 (0.3610) acc 93.9815 (92.7233) lr 9.5173e-05 eta 0:02:22
epoch [45/50] batch [10/51] time 0.170 (0.324) data 0.000 (0.148) loss 0.3953 (0.3771) acc 91.1765 (92.3041) lr 9.5173e-05 eta 0:01:35
epoch [45/50] batch [15/51] time 0.156 (0.271) data 0.000 (0.099) loss 0.5040 (0.3876) acc 85.2273 (91.5885) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [20/51] time 0.170 (0.248) data 0.001 (0.075) loss 0.4094 (0.3837) acc 89.7959 (91.2738) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [25/51] time 0.182 (0.235) data 0.000 (0.060) loss 0.4117 (0.3759) acc 90.8163 (91.5995) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [30/51] time 0.188 (0.226) data 0.000 (0.050) loss 0.3470 (0.3729) acc 91.5094 (91.7014) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [35/51] time 0.181 (0.220) data 0.000 (0.043) loss 0.4702 (0.3691) acc 91.3462 (91.8743) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [40/51] time 0.167 (0.214) data 0.000 (0.038) loss 0.2714 (0.3655) acc 94.8980 (91.9986) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [45/51] time 0.173 (0.209) data 0.000 (0.034) loss 0.1962 (0.3651) acc 95.6731 (91.9473) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.030) loss 0.6192 (0.3660) acc 83.8235 (91.9183) lr 9.5173e-05 eta 0:00:52
>>> alpha1: 0.134  alpha2: -0.082 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [46/50] batch [5/51] time 0.175 (0.463) data 0.000 (0.281) loss 0.3453 (0.4312) acc 94.3878 (92.8504) lr 7.0224e-05 eta 0:01:55
epoch [46/50] batch [10/51] time 0.172 (0.317) data 0.000 (0.142) loss 0.2693 (0.4200) acc 94.5000 (91.9531) lr 7.0224e-05 eta 0:01:17
epoch [46/50] batch [15/51] time 0.184 (0.272) data 0.016 (0.096) loss 0.3790 (0.4076) acc 87.5000 (91.4595) lr 7.0224e-05 eta 0:01:05
epoch [46/50] batch [20/51] time 0.198 (0.250) data 0.000 (0.072) loss 0.3796 (0.3900) acc 88.6364 (91.5173) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [25/51] time 0.183 (0.237) data 0.000 (0.058) loss 0.3273 (0.3817) acc 95.1923 (91.8058) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [30/51] time 0.167 (0.227) data 0.000 (0.048) loss 0.2596 (0.3752) acc 95.3125 (91.8574) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.199 (0.221) data 0.000 (0.041) loss 0.3001 (0.3742) acc 94.7368 (91.8562) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.036) loss 0.3806 (0.3676) acc 93.0000 (92.0760) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.032) loss 0.4957 (0.3614) acc 89.0000 (92.2797) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [50/51] time 0.161 (0.205) data 0.000 (0.029) loss 0.3892 (0.3675) acc 90.9574 (92.0545) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.136  alpha2: -0.080 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [47/50] batch [5/51] time 0.191 (0.431) data 0.000 (0.250) loss 0.2388 (0.3514) acc 95.7627 (92.3883) lr 4.8943e-05 eta 0:01:25
epoch [47/50] batch [10/51] time 0.164 (0.302) data 0.000 (0.125) loss 0.2649 (0.3605) acc 95.2128 (92.2189) lr 4.8943e-05 eta 0:00:58
epoch [47/50] batch [15/51] time 0.169 (0.260) data 0.000 (0.084) loss 0.2795 (0.3501) acc 93.3673 (92.6507) lr 4.8943e-05 eta 0:00:49
epoch [47/50] batch [20/51] time 0.193 (0.239) data 0.000 (0.063) loss 0.2849 (0.3610) acc 95.7547 (92.5006) lr 4.8943e-05 eta 0:00:44
epoch [47/50] batch [25/51] time 0.170 (0.226) data 0.000 (0.050) loss 0.2948 (0.3658) acc 94.8980 (92.3035) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [30/51] time 0.171 (0.217) data 0.000 (0.042) loss 0.2180 (0.3720) acc 95.5882 (92.0132) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [35/51] time 0.171 (0.212) data 0.000 (0.036) loss 0.3596 (0.3803) acc 89.2157 (91.6363) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [40/51] time 0.172 (0.207) data 0.000 (0.032) loss 0.2993 (0.3764) acc 93.7500 (91.7851) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [45/51] time 0.165 (0.203) data 0.000 (0.028) loss 0.2453 (0.3757) acc 94.3878 (91.8143) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.163 (0.199) data 0.000 (0.025) loss 0.3410 (0.3691) acc 92.1875 (91.9634) lr 4.8943e-05 eta 0:00:30
>>> alpha1: 0.136  alpha2: -0.087 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.07 <<<
epoch [48/50] batch [5/51] time 0.173 (0.465) data 0.000 (0.283) loss 0.4582 (0.3805) acc 87.7451 (91.3702) lr 3.1417e-05 eta 0:01:08
epoch [48/50] batch [10/51] time 0.173 (0.322) data 0.000 (0.141) loss 0.4404 (0.3843) acc 89.7059 (92.1725) lr 3.1417e-05 eta 0:00:46
epoch [48/50] batch [15/51] time 0.168 (0.272) data 0.000 (0.094) loss 0.3314 (0.3558) acc 92.8571 (92.4867) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [20/51] time 0.167 (0.248) data 0.000 (0.071) loss 0.3392 (0.3575) acc 93.3673 (92.1548) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.189 (0.235) data 0.000 (0.057) loss 0.3533 (0.3652) acc 95.4546 (92.1693) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.198 (0.226) data 0.000 (0.047) loss 0.4417 (0.3589) acc 88.2075 (92.2728) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.178 (0.219) data 0.000 (0.041) loss 0.3891 (0.3602) acc 90.1042 (92.2213) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.169 (0.214) data 0.000 (0.036) loss 0.4664 (0.3655) acc 89.7059 (92.0918) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.163 (0.208) data 0.000 (0.032) loss 0.3910 (0.3744) acc 91.6667 (91.8476) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.187 (0.205) data 0.000 (0.028) loss 0.3163 (0.3701) acc 95.2586 (91.9843) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.133  alpha2: -0.096 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [49/50] batch [5/51] time 0.179 (0.518) data 0.000 (0.333) loss 0.3054 (0.3954) acc 94.2308 (91.5527) lr 1.7713e-05 eta 0:00:50
epoch [49/50] batch [10/51] time 0.170 (0.348) data 0.000 (0.166) loss 0.4796 (0.3690) acc 89.7059 (91.3144) lr 1.7713e-05 eta 0:00:31
epoch [49/50] batch [15/51] time 0.183 (0.292) data 0.000 (0.111) loss 0.3308 (0.3694) acc 93.8679 (91.1943) lr 1.7713e-05 eta 0:00:25
epoch [49/50] batch [20/51] time 0.177 (0.264) data 0.000 (0.083) loss 0.5862 (0.3848) acc 89.5833 (91.3367) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [25/51] time 0.168 (0.247) data 0.000 (0.067) loss 0.3294 (0.3780) acc 93.3673 (91.6937) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [30/51] time 0.197 (0.237) data 0.000 (0.056) loss 0.5431 (0.3925) acc 87.7660 (91.3414) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [35/51] time 0.186 (0.229) data 0.000 (0.048) loss 0.4474 (0.3844) acc 88.9423 (91.5558) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [40/51] time 0.172 (0.223) data 0.000 (0.042) loss 0.3973 (0.3973) acc 91.1765 (91.3934) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.173 (0.217) data 0.000 (0.037) loss 0.3578 (0.3915) acc 90.1961 (91.4987) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.185 (0.213) data 0.000 (0.034) loss 0.3147 (0.3805) acc 94.2982 (91.7667) lr 1.7713e-05 eta 0:00:11
>>> alpha1: 0.132  alpha2: -0.101 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [50/50] batch [5/51] time 0.175 (0.457) data 0.000 (0.273) loss 0.3213 (0.3631) acc 95.1923 (92.0501) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.170 (0.315) data 0.000 (0.137) loss 0.3421 (0.3659) acc 93.6274 (91.9887) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [15/51] time 0.173 (0.268) data 0.000 (0.091) loss 0.3591 (0.3594) acc 91.8269 (92.2697) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.164 (0.245) data 0.000 (0.069) loss 0.4199 (0.3539) acc 88.0208 (92.6626) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.163 (0.230) data 0.000 (0.055) loss 0.3390 (0.3599) acc 93.0851 (92.4103) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [30/51] time 0.161 (0.220) data 0.000 (0.046) loss 0.5262 (0.3725) acc 87.5000 (92.1183) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.187 (0.214) data 0.000 (0.039) loss 0.3153 (0.3631) acc 91.5000 (92.2629) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.171 (0.210) data 0.000 (0.034) loss 0.3781 (0.3680) acc 92.1569 (92.1318) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.176 (0.206) data 0.000 (0.031) loss 0.3121 (0.3631) acc 93.8679 (92.3296) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.170 (0.202) data 0.000 (0.028) loss 0.2896 (0.3602) acc 94.1176 (92.4142) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.03, 0.04, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]
* matched noise rate: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.01, 0.01, 0.02, 0.02]
* unmatched noise rate: [0.05, 0.07, 0.07, 0.07, 0.07, 0.07, 0.06, 0.07, 0.07, 0.08, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<00:55,  2.30s/it] 12%|█▏        | 3/25 [00:02<00:14,  1.52it/s] 20%|██        | 5/25 [00:02<00:07,  2.77it/s] 28%|██▊       | 7/25 [00:02<00:04,  4.09it/s] 36%|███▌      | 9/25 [00:02<00:02,  5.46it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.78it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.98it/s] 60%|██████    | 15/25 [00:03<00:01,  9.02it/s] 68%|██████▊   | 17/25 [00:03<00:01,  6.42it/s] 76%|███████▌  | 19/25 [00:04<00:00,  7.50it/s] 84%|████████▍ | 21/25 [00:04<00:00,  8.55it/s] 92%|█████████▏| 23/25 [00:04<00:00,  9.46it/s]100%|██████████| 25/25 [00:04<00:00,  7.17it/s]100%|██████████| 25/25 [00:05<00:00,  4.94it/s]
=> result
* total: 2,463
* correct: 2,241
* accuracy: 91.0%
* error: 9.0%
* macro_f1: 90.2%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 11	acc: 91.7%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 5	acc: 41.7%
* class: 3 (sweet pea)	total: 17	correct: 15	acc: 88.2%
* class: 4 (english marigold)	total: 20	correct: 15	acc: 75.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 25	acc: 96.2%
* class: 11 (colt's foot)	total: 26	correct: 23	acc: 88.5%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 12	acc: 92.3%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 22	acc: 88.0%
* class: 18 (balloon flower)	total: 15	correct: 12	acc: 80.0%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 11	acc: 91.7%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 25	acc: 92.6%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 24	acc: 92.3%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 13	acc: 92.9%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 11	acc: 84.6%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 17	acc: 44.7%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 2	acc: 5.1%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 8	acc: 66.7%
* class: 45 (wallflower)	total: 59	correct: 59	acc: 100.0%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 27	acc: 96.4%
* class: 50 (petunia)	total: 77	correct: 74	acc: 96.1%
* class: 51 (wild pansy)	total: 26	correct: 25	acc: 96.2%
* class: 52 (primula)	total: 28	correct: 25	acc: 89.3%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 32	acc: 97.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 30	acc: 96.8%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 14	acc: 87.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 34	acc: 94.4%
* class: 75 (morning glory)	total: 32	correct: 31	acc: 96.9%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 38	acc: 90.5%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 30	acc: 88.2%
* class: 82 (hibiscus)	total: 39	correct: 36	acc: 92.3%
* class: 83 (columbine)	total: 26	correct: 22	acc: 84.6%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 16	acc: 88.9%
* class: 87 (cyclamen)	total: 46	correct: 30	acc: 65.2%
* class: 88 (watercress)	total: 55	correct: 43	acc: 78.2%
* class: 89 (canna lily)	total: 25	correct: 21	acc: 84.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 11	acc: 78.6%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 35	acc: 92.1%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 15	acc: 75.0%
* class: 97 (mexican petunia)	total: 25	correct: 23	acc: 92.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 13	acc: 76.5%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 91.6%
Elapsed: 0:27:44
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_2-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.268 (1.143) data 0.000 (0.295) loss 3.9772 (4.1803) acc 18.7500 (7.5000) lr 1.0000e-05 eta 0:48:27
epoch [1/50] batch [10/51] time 0.276 (0.708) data 0.000 (0.148) loss 3.9184 (4.1511) acc 12.5000 (11.2500) lr 1.0000e-05 eta 0:29:57
epoch [1/50] batch [15/51] time 0.288 (0.562) data 0.000 (0.099) loss 3.5708 (4.0556) acc 28.1250 (13.3333) lr 1.0000e-05 eta 0:23:45
epoch [1/50] batch [20/51] time 0.272 (0.489) data 0.000 (0.074) loss 3.3526 (3.9746) acc 28.1250 (14.6875) lr 1.0000e-05 eta 0:20:36
epoch [1/50] batch [25/51] time 0.275 (0.445) data 0.000 (0.059) loss 3.2735 (3.8191) acc 34.3750 (18.5000) lr 1.0000e-05 eta 0:18:43
epoch [1/50] batch [30/51] time 0.267 (0.415) data 0.000 (0.049) loss 3.1984 (3.7353) acc 37.5000 (20.8333) lr 1.0000e-05 eta 0:17:27
epoch [1/50] batch [35/51] time 0.269 (0.395) data 0.000 (0.042) loss 3.1488 (3.6947) acc 37.5000 (22.6786) lr 1.0000e-05 eta 0:16:32
epoch [1/50] batch [40/51] time 0.261 (0.378) data 0.000 (0.037) loss 2.7572 (3.5896) acc 40.6250 (25.0000) lr 1.0000e-05 eta 0:15:48
epoch [1/50] batch [45/51] time 0.260 (0.365) data 0.000 (0.033) loss 2.7853 (3.5106) acc 53.1250 (27.0833) lr 1.0000e-05 eta 0:15:14
epoch [1/50] batch [50/51] time 0.261 (0.355) data 0.000 (0.030) loss 3.7827 (3.4762) acc 34.3750 (28.5625) lr 1.0000e-05 eta 0:14:46
epoch [2/50] batch [5/51] time 0.271 (0.511) data 0.000 (0.230) loss 1.9779 (2.8748) acc 71.8750 (48.7500) lr 2.0000e-03 eta 0:21:13
epoch [2/50] batch [10/51] time 0.265 (0.389) data 0.000 (0.115) loss 2.9602 (2.6141) acc 43.7500 (52.5000) lr 2.0000e-03 eta 0:16:09
epoch [2/50] batch [15/51] time 0.272 (0.349) data 0.000 (0.077) loss 2.4254 (2.6136) acc 62.5000 (52.5000) lr 2.0000e-03 eta 0:14:27
epoch [2/50] batch [20/51] time 0.269 (0.330) data 0.000 (0.058) loss 3.4043 (2.6136) acc 46.8750 (52.6562) lr 2.0000e-03 eta 0:13:37
epoch [2/50] batch [25/51] time 0.273 (0.318) data 0.000 (0.046) loss 1.9821 (2.6342) acc 59.3750 (51.6250) lr 2.0000e-03 eta 0:13:06
epoch [2/50] batch [30/51] time 0.267 (0.310) data 0.000 (0.039) loss 2.7333 (2.6100) acc 50.0000 (52.0833) lr 2.0000e-03 eta 0:12:45
epoch [2/50] batch [35/51] time 0.265 (0.305) data 0.000 (0.033) loss 1.9561 (2.6018) acc 65.6250 (52.6786) lr 2.0000e-03 eta 0:12:30
epoch [2/50] batch [40/51] time 0.263 (0.300) data 0.000 (0.029) loss 2.1699 (2.5577) acc 56.2500 (53.5156) lr 2.0000e-03 eta 0:12:17
epoch [2/50] batch [45/51] time 0.262 (0.296) data 0.000 (0.026) loss 2.1055 (2.5257) acc 62.5000 (53.8194) lr 2.0000e-03 eta 0:12:05
epoch [2/50] batch [50/51] time 0.262 (0.292) data 0.000 (0.023) loss 2.1771 (2.4984) acc 59.3750 (54.8125) lr 2.0000e-03 eta 0:11:55
epoch [3/50] batch [5/51] time 0.273 (0.552) data 0.000 (0.269) loss 1.7299 (2.0767) acc 68.7500 (60.6250) lr 1.9980e-03 eta 0:22:29
epoch [3/50] batch [10/51] time 0.263 (0.409) data 0.000 (0.135) loss 2.7111 (2.3115) acc 56.2500 (57.1875) lr 1.9980e-03 eta 0:16:36
epoch [3/50] batch [15/51] time 0.264 (0.362) data 0.000 (0.090) loss 2.1038 (2.3280) acc 65.6250 (57.2917) lr 1.9980e-03 eta 0:14:40
epoch [3/50] batch [20/51] time 0.267 (0.338) data 0.000 (0.067) loss 2.4268 (2.3033) acc 59.3750 (58.1250) lr 1.9980e-03 eta 0:13:40
epoch [3/50] batch [25/51] time 0.267 (0.324) data 0.000 (0.054) loss 1.7847 (2.2600) acc 59.3750 (57.8750) lr 1.9980e-03 eta 0:13:05
epoch [3/50] batch [30/51] time 0.274 (0.315) data 0.000 (0.045) loss 1.9399 (2.2116) acc 62.5000 (59.5833) lr 1.9980e-03 eta 0:12:40
epoch [3/50] batch [35/51] time 0.264 (0.308) data 0.000 (0.039) loss 2.3217 (2.1796) acc 68.7500 (60.3571) lr 1.9980e-03 eta 0:12:22
epoch [3/50] batch [40/51] time 0.267 (0.303) data 0.000 (0.034) loss 2.1131 (2.2149) acc 56.2500 (60.3906) lr 1.9980e-03 eta 0:12:08
epoch [3/50] batch [45/51] time 0.268 (0.299) data 0.000 (0.030) loss 2.2626 (2.2197) acc 65.6250 (60.2778) lr 1.9980e-03 eta 0:11:57
epoch [3/50] batch [50/51] time 0.263 (0.295) data 0.000 (0.027) loss 1.8202 (2.2402) acc 68.7500 (60.0000) lr 1.9980e-03 eta 0:11:47
epoch [4/50] batch [5/51] time 0.275 (0.523) data 0.000 (0.242) loss 2.0651 (2.0840) acc 71.8750 (68.1250) lr 1.9921e-03 eta 0:20:50
epoch [4/50] batch [10/51] time 0.275 (0.395) data 0.000 (0.121) loss 1.9106 (2.0500) acc 68.7500 (67.5000) lr 1.9921e-03 eta 0:15:43
epoch [4/50] batch [15/51] time 0.263 (0.352) data 0.000 (0.081) loss 1.6498 (2.1498) acc 62.5000 (63.5417) lr 1.9921e-03 eta 0:13:57
epoch [4/50] batch [20/51] time 0.263 (0.330) data 0.000 (0.061) loss 1.8193 (2.1611) acc 65.6250 (63.2812) lr 1.9921e-03 eta 0:13:05
epoch [4/50] batch [25/51] time 0.264 (0.318) data 0.000 (0.049) loss 1.7383 (2.0923) acc 59.3750 (63.7500) lr 1.9921e-03 eta 0:12:34
epoch [4/50] batch [30/51] time 0.270 (0.310) data 0.000 (0.040) loss 1.9154 (2.1007) acc 62.5000 (63.7500) lr 1.9921e-03 eta 0:12:14
epoch [4/50] batch [35/51] time 0.273 (0.305) data 0.000 (0.035) loss 2.2405 (2.1139) acc 59.3750 (63.5714) lr 1.9921e-03 eta 0:11:59
epoch [4/50] batch [40/51] time 0.262 (0.299) data 0.000 (0.030) loss 1.0759 (2.0879) acc 84.3750 (64.2969) lr 1.9921e-03 eta 0:11:45
epoch [4/50] batch [45/51] time 0.261 (0.295) data 0.000 (0.027) loss 1.9958 (2.0837) acc 62.5000 (64.0972) lr 1.9921e-03 eta 0:11:34
epoch [4/50] batch [50/51] time 0.261 (0.292) data 0.000 (0.024) loss 2.0702 (2.0661) acc 68.7500 (64.1875) lr 1.9921e-03 eta 0:11:24
epoch [5/50] batch [5/51] time 0.288 (0.556) data 0.000 (0.266) loss 2.0673 (1.9363) acc 68.7500 (63.7500) lr 1.9823e-03 eta 0:21:42
epoch [5/50] batch [10/51] time 0.282 (0.416) data 0.000 (0.133) loss 1.4727 (1.9121) acc 75.0000 (66.8750) lr 1.9823e-03 eta 0:16:12
epoch [5/50] batch [15/51] time 0.264 (0.367) data 0.000 (0.089) loss 2.0390 (1.8321) acc 78.1250 (68.7500) lr 1.9823e-03 eta 0:14:15
epoch [5/50] batch [20/51] time 0.266 (0.342) data 0.000 (0.067) loss 1.4600 (1.8888) acc 75.0000 (67.3438) lr 1.9823e-03 eta 0:13:16
epoch [5/50] batch [25/51] time 0.279 (0.330) data 0.000 (0.053) loss 1.5417 (1.9166) acc 68.7500 (66.8750) lr 1.9823e-03 eta 0:12:45
epoch [5/50] batch [30/51] time 0.266 (0.320) data 0.000 (0.045) loss 2.2667 (1.9304) acc 62.5000 (66.4583) lr 1.9823e-03 eta 0:12:21
epoch [5/50] batch [35/51] time 0.279 (0.314) data 0.001 (0.038) loss 1.1105 (1.8644) acc 87.5000 (68.0357) lr 1.9823e-03 eta 0:12:06
epoch [5/50] batch [40/51] time 0.268 (0.309) data 0.000 (0.033) loss 1.7957 (1.9026) acc 65.6250 (68.2031) lr 1.9823e-03 eta 0:11:51
epoch [5/50] batch [45/51] time 0.266 (0.304) data 0.000 (0.030) loss 1.7409 (1.9334) acc 62.5000 (67.7778) lr 1.9823e-03 eta 0:11:39
epoch [5/50] batch [50/51] time 0.264 (0.300) data 0.000 (0.027) loss 1.8242 (1.9352) acc 71.8750 (67.8125) lr 1.9823e-03 eta 0:11:28
epoch [6/50] batch [5/51] time 0.269 (0.539) data 0.000 (0.258) loss 1.9045 (2.1238) acc 71.8750 (64.3750) lr 1.9686e-03 eta 0:20:35
epoch [6/50] batch [10/51] time 0.265 (0.403) data 0.000 (0.129) loss 2.2547 (2.0704) acc 71.8750 (67.1875) lr 1.9686e-03 eta 0:15:21
epoch [6/50] batch [15/51] time 0.278 (0.360) data 0.000 (0.086) loss 2.0914 (1.9911) acc 65.6250 (67.0833) lr 1.9686e-03 eta 0:13:39
epoch [6/50] batch [20/51] time 0.281 (0.338) data 0.000 (0.065) loss 1.5375 (1.9611) acc 71.8750 (67.1875) lr 1.9686e-03 eta 0:12:48
epoch [6/50] batch [25/51] time 0.267 (0.324) data 0.000 (0.052) loss 2.3603 (2.0090) acc 59.3750 (66.5000) lr 1.9686e-03 eta 0:12:14
epoch [6/50] batch [30/51] time 0.268 (0.315) data 0.000 (0.043) loss 2.0991 (2.0236) acc 68.7500 (66.5625) lr 1.9686e-03 eta 0:11:52
epoch [6/50] batch [35/51] time 0.269 (0.309) data 0.000 (0.037) loss 1.8750 (2.0109) acc 75.0000 (66.8750) lr 1.9686e-03 eta 0:11:37
epoch [6/50] batch [40/51] time 0.264 (0.303) data 0.000 (0.033) loss 1.1935 (1.9627) acc 71.8750 (66.5625) lr 1.9686e-03 eta 0:11:23
epoch [6/50] batch [45/51] time 0.262 (0.299) data 0.000 (0.029) loss 0.7808 (1.9317) acc 90.6250 (67.9167) lr 1.9686e-03 eta 0:11:11
epoch [6/50] batch [50/51] time 0.261 (0.295) data 0.000 (0.026) loss 1.1019 (1.8841) acc 75.0000 (68.7500) lr 1.9686e-03 eta 0:11:02
epoch [7/50] batch [5/51] time 0.265 (0.539) data 0.000 (0.254) loss 1.3501 (2.0767) acc 78.1250 (71.2500) lr 1.9511e-03 eta 0:20:06
epoch [7/50] batch [10/51] time 0.264 (0.406) data 0.000 (0.127) loss 1.5692 (1.8626) acc 71.8750 (68.7500) lr 1.9511e-03 eta 0:15:06
epoch [7/50] batch [15/51] time 0.264 (0.361) data 0.000 (0.085) loss 2.2089 (1.7814) acc 71.8750 (71.2500) lr 1.9511e-03 eta 0:13:25
epoch [7/50] batch [20/51] time 0.266 (0.340) data 0.000 (0.064) loss 1.7669 (1.8419) acc 68.7500 (70.9375) lr 1.9511e-03 eta 0:12:36
epoch [7/50] batch [25/51] time 0.266 (0.327) data 0.000 (0.051) loss 1.4217 (1.8055) acc 68.7500 (71.1250) lr 1.9511e-03 eta 0:12:05
epoch [7/50] batch [30/51] time 0.268 (0.317) data 0.000 (0.043) loss 1.5932 (1.7665) acc 68.7500 (71.2500) lr 1.9511e-03 eta 0:11:42
epoch [7/50] batch [35/51] time 0.276 (0.311) data 0.000 (0.037) loss 1.8071 (1.8001) acc 68.7500 (70.7143) lr 1.9511e-03 eta 0:11:27
epoch [7/50] batch [40/51] time 0.266 (0.305) data 0.000 (0.032) loss 1.2101 (1.7817) acc 87.5000 (71.1719) lr 1.9511e-03 eta 0:11:12
epoch [7/50] batch [45/51] time 0.266 (0.301) data 0.000 (0.028) loss 2.6291 (1.7693) acc 59.3750 (71.3194) lr 1.9511e-03 eta 0:11:01
epoch [7/50] batch [50/51] time 0.264 (0.297) data 0.000 (0.026) loss 1.0020 (1.8040) acc 81.2500 (71.4375) lr 1.9511e-03 eta 0:10:52
epoch [8/50] batch [5/51] time 0.270 (0.540) data 0.000 (0.263) loss 0.8669 (1.6103) acc 75.0000 (73.7500) lr 1.9298e-03 eta 0:19:41
epoch [8/50] batch [10/51] time 0.266 (0.406) data 0.001 (0.132) loss 1.7002 (1.5981) acc 75.0000 (75.0000) lr 1.9298e-03 eta 0:14:46
epoch [8/50] batch [15/51] time 0.284 (0.363) data 0.016 (0.089) loss 1.9656 (1.6254) acc 68.7500 (74.7917) lr 1.9298e-03 eta 0:13:10
epoch [8/50] batch [20/51] time 0.279 (0.340) data 0.000 (0.067) loss 1.8311 (1.6789) acc 75.0000 (73.9062) lr 1.9298e-03 eta 0:12:18
epoch [8/50] batch [25/51] time 0.264 (0.326) data 0.000 (0.054) loss 1.5670 (1.6784) acc 68.7500 (73.7500) lr 1.9298e-03 eta 0:11:45
epoch [8/50] batch [30/51] time 0.273 (0.316) data 0.000 (0.045) loss 1.6738 (1.6855) acc 81.2500 (73.7500) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [35/51] time 0.279 (0.310) data 0.000 (0.038) loss 2.4736 (1.6982) acc 59.3750 (73.3036) lr 1.9298e-03 eta 0:11:09
epoch [8/50] batch [40/51] time 0.263 (0.305) data 0.000 (0.034) loss 1.6379 (1.7096) acc 78.1250 (73.5938) lr 1.9298e-03 eta 0:10:55
epoch [8/50] batch [45/51] time 0.262 (0.300) data 0.000 (0.030) loss 1.6292 (1.6999) acc 65.6250 (73.5417) lr 1.9298e-03 eta 0:10:44
epoch [8/50] batch [50/51] time 0.262 (0.296) data 0.000 (0.027) loss 1.2040 (1.6772) acc 81.2500 (73.8125) lr 1.9298e-03 eta 0:10:34
epoch [9/50] batch [5/51] time 0.279 (0.533) data 0.000 (0.252) loss 1.6792 (1.4879) acc 78.1250 (77.5000) lr 1.9048e-03 eta 0:18:58
epoch [9/50] batch [10/51] time 0.263 (0.398) data 0.000 (0.126) loss 2.0934 (1.5499) acc 68.7500 (75.6250) lr 1.9048e-03 eta 0:14:09
epoch [9/50] batch [15/51] time 0.263 (0.354) data 0.000 (0.084) loss 2.3880 (1.7174) acc 71.8750 (74.1667) lr 1.9048e-03 eta 0:12:33
epoch [9/50] batch [20/51] time 0.272 (0.332) data 0.000 (0.063) loss 2.2770 (1.7439) acc 71.8750 (74.2188) lr 1.9048e-03 eta 0:11:44
epoch [9/50] batch [25/51] time 0.263 (0.319) data 0.000 (0.051) loss 1.0812 (1.6749) acc 81.2500 (74.8750) lr 1.9048e-03 eta 0:11:14
epoch [9/50] batch [30/51] time 0.267 (0.311) data 0.000 (0.042) loss 0.7631 (1.6628) acc 87.5000 (75.2083) lr 1.9048e-03 eta 0:10:56
epoch [9/50] batch [35/51] time 0.265 (0.306) data 0.000 (0.036) loss 1.6764 (1.6781) acc 84.3750 (75.2679) lr 1.9048e-03 eta 0:10:44
epoch [9/50] batch [40/51] time 0.262 (0.301) data 0.000 (0.032) loss 1.5657 (1.6686) acc 78.1250 (74.6875) lr 1.9048e-03 eta 0:10:32
epoch [9/50] batch [45/51] time 0.262 (0.296) data 0.000 (0.028) loss 1.7419 (1.6667) acc 75.0000 (74.7222) lr 1.9048e-03 eta 0:10:21
epoch [9/50] batch [50/51] time 0.264 (0.293) data 0.000 (0.025) loss 1.8600 (1.6550) acc 71.8750 (74.8125) lr 1.9048e-03 eta 0:10:12
epoch [10/50] batch [5/51] time 0.276 (0.582) data 0.000 (0.278) loss 1.5890 (1.7747) acc 68.7500 (71.2500) lr 1.8763e-03 eta 0:20:13
epoch [10/50] batch [10/51] time 0.267 (0.425) data 0.000 (0.139) loss 0.7801 (1.7158) acc 90.6250 (74.0625) lr 1.8763e-03 eta 0:14:44
epoch [10/50] batch [15/51] time 0.265 (0.372) data 0.000 (0.093) loss 1.6220 (1.6275) acc 81.2500 (74.5833) lr 1.8763e-03 eta 0:12:52
epoch [10/50] batch [20/51] time 0.264 (0.346) data 0.000 (0.070) loss 2.0733 (1.5654) acc 71.8750 (75.6250) lr 1.8763e-03 eta 0:11:56
epoch [10/50] batch [25/51] time 0.264 (0.330) data 0.000 (0.056) loss 1.0559 (1.5546) acc 81.2500 (75.8750) lr 1.8763e-03 eta 0:11:22
epoch [10/50] batch [30/51] time 0.285 (0.322) data 0.000 (0.047) loss 2.0119 (1.5969) acc 75.0000 (75.9375) lr 1.8763e-03 eta 0:11:03
epoch [10/50] batch [35/51] time 0.270 (0.314) data 0.000 (0.040) loss 1.9483 (1.6010) acc 68.7500 (75.5357) lr 1.8763e-03 eta 0:10:45
epoch [10/50] batch [40/51] time 0.262 (0.308) data 0.000 (0.035) loss 0.8173 (1.5902) acc 81.2500 (75.3125) lr 1.8763e-03 eta 0:10:30
epoch [10/50] batch [45/51] time 0.262 (0.303) data 0.000 (0.031) loss 2.0623 (1.6049) acc 68.7500 (75.4167) lr 1.8763e-03 eta 0:10:18
epoch [10/50] batch [50/51] time 0.266 (0.299) data 0.000 (0.028) loss 1.9679 (1.6197) acc 59.3750 (74.8750) lr 1.8763e-03 eta 0:10:09
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.385  alpha2: -0.048 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [11/50] batch [5/51] time 0.829 (1.083) data 0.000 (0.327) loss 0.6808 (0.9605) acc 87.2549 (79.5522) lr 1.8443e-03 eta 0:36:44
epoch [11/50] batch [10/51] time 0.181 (0.631) data 0.000 (0.163) loss 1.0961 (0.8967) acc 70.4545 (79.9353) lr 1.8443e-03 eta 0:21:21
epoch [11/50] batch [15/51] time 0.955 (0.532) data 0.000 (0.109) loss 0.7591 (0.9196) acc 84.0517 (79.0587) lr 1.8443e-03 eta 0:17:56
epoch [11/50] batch [20/51] time 0.188 (0.553) data 0.000 (0.082) loss 0.6984 (0.9050) acc 87.9310 (79.5188) lr 1.8443e-03 eta 0:18:36
epoch [11/50] batch [25/51] time 0.174 (0.477) data 0.000 (0.065) loss 0.7435 (0.9032) acc 80.7692 (79.4184) lr 1.8443e-03 eta 0:16:01
epoch [11/50] batch [30/51] time 0.210 (0.430) data 0.000 (0.055) loss 0.6631 (0.8909) acc 85.1852 (79.6357) lr 1.8443e-03 eta 0:14:23
epoch [11/50] batch [35/51] time 0.183 (0.416) data 0.000 (0.047) loss 0.5561 (0.8797) acc 93.7500 (79.9852) lr 1.8443e-03 eta 0:13:54
epoch [11/50] batch [40/51] time 0.176 (0.402) data 0.000 (0.041) loss 0.9172 (0.8833) acc 77.8302 (79.9035) lr 1.8443e-03 eta 0:13:24
epoch [11/50] batch [45/51] time 0.168 (0.377) data 0.000 (0.036) loss 0.9173 (0.8642) acc 88.0000 (80.4377) lr 1.8443e-03 eta 0:12:31
epoch [11/50] batch [50/51] time 0.177 (0.357) data 0.000 (0.033) loss 0.9563 (0.8636) acc 75.9259 (80.3940) lr 1.8443e-03 eta 0:11:50
>>> alpha1: 0.300  alpha2: -0.082 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [12/50] batch [5/51] time 0.176 (0.441) data 0.001 (0.258) loss 0.9938 (0.8326) acc 76.5000 (79.7591) lr 1.8090e-03 eta 0:14:35
epoch [12/50] batch [10/51] time 0.175 (0.376) data 0.000 (0.129) loss 0.4248 (0.7481) acc 93.0000 (81.8948) lr 1.8090e-03 eta 0:12:23
epoch [12/50] batch [15/51] time 0.177 (0.351) data 0.000 (0.086) loss 0.7324 (0.7296) acc 82.3529 (82.2719) lr 1.8090e-03 eta 0:11:32
epoch [12/50] batch [20/51] time 0.160 (0.307) data 0.000 (0.065) loss 0.9157 (0.7271) acc 75.0000 (82.2108) lr 1.8090e-03 eta 0:10:04
epoch [12/50] batch [25/51] time 0.171 (0.281) data 0.000 (0.052) loss 0.5257 (0.7034) acc 87.7451 (82.7196) lr 1.8090e-03 eta 0:09:12
epoch [12/50] batch [30/51] time 0.185 (0.283) data 0.000 (0.043) loss 0.8353 (0.7192) acc 79.9020 (82.3285) lr 1.8090e-03 eta 0:09:13
epoch [12/50] batch [35/51] time 0.172 (0.285) data 0.000 (0.037) loss 0.6065 (0.7726) acc 84.8039 (81.7519) lr 1.8090e-03 eta 0:09:16
epoch [12/50] batch [40/51] time 0.172 (0.271) data 0.000 (0.033) loss 0.4995 (0.7460) acc 86.5385 (82.4059) lr 1.8090e-03 eta 0:08:48
epoch [12/50] batch [45/51] time 0.168 (0.260) data 0.000 (0.029) loss 1.0917 (0.7488) acc 78.0000 (82.4535) lr 1.8090e-03 eta 0:08:25
epoch [12/50] batch [50/51] time 0.179 (0.251) data 0.001 (0.026) loss 0.6215 (0.7393) acc 87.5000 (82.8964) lr 1.8090e-03 eta 0:08:06
>>> alpha1: 0.264  alpha2: -0.104 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [13/50] batch [5/51] time 0.178 (0.437) data 0.000 (0.253) loss 0.6390 (0.6556) acc 84.4340 (84.6810) lr 1.7705e-03 eta 0:14:03
epoch [13/50] batch [10/51] time 0.159 (0.304) data 0.000 (0.127) loss 0.6532 (0.6897) acc 84.2391 (83.7530) lr 1.7705e-03 eta 0:09:46
epoch [13/50] batch [15/51] time 0.190 (0.263) data 0.000 (0.085) loss 0.6073 (0.6671) acc 83.3333 (84.1598) lr 1.7705e-03 eta 0:08:24
epoch [13/50] batch [20/51] time 0.183 (0.241) data 0.000 (0.064) loss 0.7341 (0.6554) acc 81.5000 (84.4441) lr 1.7705e-03 eta 0:07:41
epoch [13/50] batch [25/51] time 0.166 (0.226) data 0.000 (0.051) loss 0.7798 (0.6622) acc 83.6956 (83.8714) lr 1.7705e-03 eta 0:07:13
epoch [13/50] batch [30/51] time 0.194 (0.219) data 0.000 (0.043) loss 0.5897 (0.6413) acc 87.0192 (84.4840) lr 1.7705e-03 eta 0:06:57
epoch [13/50] batch [35/51] time 0.169 (0.211) data 0.000 (0.037) loss 0.6445 (0.6521) acc 86.5000 (84.2397) lr 1.7705e-03 eta 0:06:42
epoch [13/50] batch [40/51] time 0.157 (0.206) data 0.001 (0.032) loss 0.9009 (0.6654) acc 81.6667 (84.2969) lr 1.7705e-03 eta 0:06:31
epoch [13/50] batch [45/51] time 0.178 (0.203) data 0.000 (0.029) loss 0.8793 (0.6634) acc 78.2407 (84.3085) lr 1.7705e-03 eta 0:06:23
epoch [13/50] batch [50/51] time 0.168 (0.199) data 0.000 (0.026) loss 0.6072 (0.6608) acc 87.0000 (84.2797) lr 1.7705e-03 eta 0:06:16
>>> alpha1: 0.245  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [14/50] batch [5/51] time 0.180 (0.406) data 0.000 (0.223) loss 0.5623 (0.6421) acc 86.7924 (85.2259) lr 1.7290e-03 eta 0:12:44
epoch [14/50] batch [10/51] time 0.179 (0.292) data 0.000 (0.112) loss 0.7153 (0.6653) acc 83.3333 (84.4502) lr 1.7290e-03 eta 0:09:08
epoch [14/50] batch [15/51] time 0.183 (0.253) data 0.000 (0.075) loss 0.6481 (0.6624) acc 83.5106 (84.3495) lr 1.7290e-03 eta 0:07:53
epoch [14/50] batch [20/51] time 0.175 (0.263) data 0.000 (0.056) loss 0.5659 (0.6300) acc 86.2245 (85.1794) lr 1.7290e-03 eta 0:08:10
epoch [14/50] batch [25/51] time 0.186 (0.247) data 0.000 (0.045) loss 0.4551 (0.6279) acc 90.5000 (85.3117) lr 1.7290e-03 eta 0:07:39
epoch [14/50] batch [30/51] time 0.176 (0.235) data 0.000 (0.038) loss 0.6026 (0.6199) acc 88.7255 (85.6093) lr 1.7290e-03 eta 0:07:17
epoch [14/50] batch [35/51] time 0.178 (0.228) data 0.000 (0.032) loss 0.6802 (0.6193) acc 81.7708 (85.6806) lr 1.7290e-03 eta 0:07:01
epoch [14/50] batch [40/51] time 0.153 (0.220) data 0.000 (0.028) loss 0.8521 (0.6156) acc 75.0000 (85.6761) lr 1.7290e-03 eta 0:06:46
epoch [14/50] batch [45/51] time 0.168 (0.214) data 0.000 (0.025) loss 0.5202 (0.6129) acc 86.5000 (85.6834) lr 1.7290e-03 eta 0:06:33
epoch [14/50] batch [50/51] time 0.168 (0.209) data 0.000 (0.023) loss 0.5965 (0.6157) acc 86.4583 (85.6023) lr 1.7290e-03 eta 0:06:24
>>> alpha1: 0.233  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [15/50] batch [5/51] time 0.163 (0.454) data 0.000 (0.272) loss 0.5792 (0.5329) acc 89.3617 (87.1042) lr 1.6845e-03 eta 0:13:50
epoch [15/50] batch [10/51] time 0.172 (0.313) data 0.000 (0.136) loss 0.5341 (0.6017) acc 86.7647 (85.8082) lr 1.6845e-03 eta 0:09:31
epoch [15/50] batch [15/51] time 0.784 (0.307) data 0.001 (0.091) loss 0.6913 (0.6064) acc 83.5227 (85.8165) lr 1.6845e-03 eta 0:09:18
epoch [15/50] batch [20/51] time 0.171 (0.274) data 0.001 (0.068) loss 0.5933 (0.6056) acc 85.7143 (85.3917) lr 1.6845e-03 eta 0:08:17
epoch [15/50] batch [25/51] time 0.165 (0.253) data 0.001 (0.055) loss 0.7318 (0.5941) acc 79.6875 (85.7929) lr 1.6845e-03 eta 0:07:38
epoch [15/50] batch [30/51] time 0.170 (0.258) data 0.000 (0.046) loss 0.5531 (0.6002) acc 89.2157 (85.3784) lr 1.6845e-03 eta 0:07:46
epoch [15/50] batch [35/51] time 0.198 (0.247) data 0.000 (0.039) loss 0.7369 (0.6047) acc 83.1818 (85.3839) lr 1.6845e-03 eta 0:07:23
epoch [15/50] batch [40/51] time 0.169 (0.237) data 0.000 (0.034) loss 0.5534 (0.6101) acc 86.2745 (85.3937) lr 1.6845e-03 eta 0:07:04
epoch [15/50] batch [45/51] time 0.157 (0.229) data 0.000 (0.031) loss 0.6783 (0.6052) acc 81.6667 (85.5522) lr 1.6845e-03 eta 0:06:49
epoch [15/50] batch [50/51] time 0.169 (0.222) data 0.000 (0.027) loss 0.7180 (0.6032) acc 82.3529 (85.5516) lr 1.6845e-03 eta 0:06:37
>>> alpha1: 0.197  alpha2: -0.146 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [16/50] batch [5/51] time 0.189 (0.502) data 0.000 (0.313) loss 0.4984 (0.4544) acc 88.6364 (90.6340) lr 1.6374e-03 eta 0:14:53
epoch [16/50] batch [10/51] time 0.167 (0.341) data 0.001 (0.157) loss 0.4399 (0.4897) acc 93.8775 (89.4072) lr 1.6374e-03 eta 0:10:05
epoch [16/50] batch [15/51] time 0.177 (0.286) data 0.001 (0.105) loss 0.5726 (0.4974) acc 88.7255 (89.1177) lr 1.6374e-03 eta 0:08:26
epoch [16/50] batch [20/51] time 0.160 (0.259) data 0.001 (0.078) loss 0.6657 (0.5219) acc 82.0652 (88.2257) lr 1.6374e-03 eta 0:07:36
epoch [16/50] batch [25/51] time 0.165 (0.243) data 0.000 (0.063) loss 0.4411 (0.5152) acc 95.3125 (88.3683) lr 1.6374e-03 eta 0:07:08
epoch [16/50] batch [30/51] time 0.172 (0.232) data 0.001 (0.052) loss 0.5917 (0.5221) acc 85.8696 (88.2244) lr 1.6374e-03 eta 0:06:47
epoch [16/50] batch [35/51] time 0.191 (0.224) data 0.000 (0.045) loss 0.3960 (0.5236) acc 91.0714 (88.4069) lr 1.6374e-03 eta 0:06:32
epoch [16/50] batch [40/51] time 0.159 (0.217) data 0.000 (0.039) loss 0.7018 (0.5377) acc 80.9783 (88.0525) lr 1.6374e-03 eta 0:06:19
epoch [16/50] batch [45/51] time 0.163 (0.212) data 0.000 (0.035) loss 0.5314 (0.5410) acc 89.5833 (87.8348) lr 1.6374e-03 eta 0:06:08
epoch [16/50] batch [50/51] time 0.170 (0.207) data 0.000 (0.032) loss 0.4833 (0.5461) acc 88.7255 (87.7056) lr 1.6374e-03 eta 0:05:59
>>> alpha1: 0.187  alpha2: -0.151 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [17/50] batch [5/51] time 0.176 (0.411) data 0.000 (0.227) loss 0.5829 (0.5141) acc 87.2449 (88.1151) lr 1.5878e-03 eta 0:11:50
epoch [17/50] batch [10/51] time 0.183 (0.294) data 0.000 (0.114) loss 0.4418 (0.5316) acc 90.2778 (88.4259) lr 1.5878e-03 eta 0:08:27
epoch [17/50] batch [15/51] time 0.170 (0.255) data 0.000 (0.076) loss 0.5863 (0.5278) acc 86.7347 (88.2779) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [20/51] time 0.168 (0.234) data 0.001 (0.057) loss 0.4350 (0.5247) acc 93.8775 (88.5238) lr 1.5878e-03 eta 0:06:40
epoch [17/50] batch [25/51] time 0.169 (0.221) data 0.001 (0.046) loss 0.3994 (0.5325) acc 91.0000 (88.3324) lr 1.5878e-03 eta 0:06:18
epoch [17/50] batch [30/51] time 0.160 (0.213) data 0.001 (0.038) loss 0.6603 (0.5600) acc 83.3333 (87.2395) lr 1.5878e-03 eta 0:06:02
epoch [17/50] batch [35/51] time 0.185 (0.208) data 0.001 (0.033) loss 0.5329 (0.5426) acc 87.7451 (87.6655) lr 1.5878e-03 eta 0:05:53
epoch [17/50] batch [40/51] time 0.165 (0.203) data 0.000 (0.029) loss 0.4765 (0.5520) acc 88.5417 (87.3797) lr 1.5878e-03 eta 0:05:43
epoch [17/50] batch [45/51] time 0.170 (0.199) data 0.000 (0.026) loss 0.6474 (0.5483) acc 88.7255 (87.4532) lr 1.5878e-03 eta 0:05:36
epoch [17/50] batch [50/51] time 0.170 (0.196) data 0.000 (0.023) loss 0.7869 (0.5530) acc 85.2941 (87.3762) lr 1.5878e-03 eta 0:05:29
>>> alpha1: 0.181  alpha2: -0.148 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.10 <<<
epoch [18/50] batch [5/51] time 0.170 (0.450) data 0.000 (0.271) loss 0.4144 (0.5300) acc 90.3061 (87.4475) lr 1.5358e-03 eta 0:12:35
epoch [18/50] batch [10/51] time 0.196 (0.321) data 0.000 (0.136) loss 0.5085 (0.5038) acc 87.0370 (88.3596) lr 1.5358e-03 eta 0:08:56
epoch [18/50] batch [15/51] time 0.173 (0.274) data 0.001 (0.091) loss 0.6330 (0.5002) acc 86.9792 (88.5754) lr 1.5358e-03 eta 0:07:37
epoch [18/50] batch [20/51] time 0.195 (0.250) data 0.000 (0.068) loss 0.6197 (0.5240) acc 83.7719 (87.8859) lr 1.5358e-03 eta 0:06:56
epoch [18/50] batch [25/51] time 0.183 (0.236) data 0.000 (0.054) loss 0.3305 (0.5228) acc 88.7255 (87.9697) lr 1.5358e-03 eta 0:06:32
epoch [18/50] batch [30/51] time 0.195 (0.228) data 0.000 (0.045) loss 0.3634 (0.5188) acc 92.1296 (87.8144) lr 1.5358e-03 eta 0:06:16
epoch [18/50] batch [35/51] time 0.169 (0.221) data 0.000 (0.039) loss 0.5584 (0.5299) acc 84.8837 (87.4646) lr 1.5358e-03 eta 0:06:04
epoch [18/50] batch [40/51] time 0.179 (0.216) data 0.000 (0.034) loss 0.7166 (0.5378) acc 83.4906 (87.4354) lr 1.5358e-03 eta 0:05:55
epoch [18/50] batch [45/51] time 0.169 (0.211) data 0.000 (0.030) loss 0.4458 (0.5410) acc 89.5000 (87.2983) lr 1.5358e-03 eta 0:05:46
epoch [18/50] batch [50/51] time 0.165 (0.207) data 0.000 (0.027) loss 0.5956 (0.5444) acc 85.2041 (87.1170) lr 1.5358e-03 eta 0:05:37
>>> alpha1: 0.177  alpha2: -0.152 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.11 <<<
epoch [19/50] batch [5/51] time 0.169 (0.448) data 0.000 (0.271) loss 0.6086 (0.5041) acc 81.0000 (87.8693) lr 1.4818e-03 eta 0:12:08
epoch [19/50] batch [10/51] time 0.176 (0.310) data 0.000 (0.135) loss 0.5464 (0.5472) acc 87.5000 (87.4808) lr 1.4818e-03 eta 0:08:23
epoch [19/50] batch [15/51] time 0.166 (0.265) data 0.000 (0.090) loss 0.4658 (0.5312) acc 86.2245 (87.5863) lr 1.4818e-03 eta 0:07:08
epoch [19/50] batch [20/51] time 0.180 (0.243) data 0.000 (0.068) loss 0.5371 (0.5262) acc 90.1961 (87.4822) lr 1.4818e-03 eta 0:06:32
epoch [19/50] batch [25/51] time 0.181 (0.230) data 0.000 (0.054) loss 0.3620 (0.5381) acc 91.8269 (87.3685) lr 1.4818e-03 eta 0:06:10
epoch [19/50] batch [30/51] time 0.179 (0.223) data 0.000 (0.045) loss 0.4606 (0.5286) acc 91.6667 (87.6537) lr 1.4818e-03 eta 0:05:57
epoch [19/50] batch [35/51] time 0.189 (0.217) data 0.000 (0.039) loss 0.4305 (0.5129) acc 90.7407 (88.1591) lr 1.4818e-03 eta 0:05:46
epoch [19/50] batch [40/51] time 0.166 (0.212) data 0.000 (0.034) loss 0.5037 (0.5253) acc 86.2245 (87.8170) lr 1.4818e-03 eta 0:05:37
epoch [19/50] batch [45/51] time 0.168 (0.207) data 0.000 (0.030) loss 0.4355 (0.5168) acc 89.0000 (87.9996) lr 1.4818e-03 eta 0:05:28
epoch [19/50] batch [50/51] time 0.159 (0.203) data 0.000 (0.027) loss 0.4201 (0.5234) acc 90.7609 (87.6917) lr 1.4818e-03 eta 0:05:20
>>> alpha1: 0.173  alpha2: -0.145 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.11 <<<
epoch [20/50] batch [5/51] time 0.180 (0.428) data 0.000 (0.248) loss 0.4668 (0.5374) acc 87.9630 (87.1193) lr 1.4258e-03 eta 0:11:14
epoch [20/50] batch [10/51] time 0.174 (0.303) data 0.000 (0.124) loss 0.4420 (0.5274) acc 89.4231 (86.8996) lr 1.4258e-03 eta 0:07:55
epoch [20/50] batch [15/51] time 0.178 (0.260) data 0.001 (0.083) loss 0.4749 (0.5009) acc 89.1509 (88.0314) lr 1.4258e-03 eta 0:06:46
epoch [20/50] batch [20/51] time 0.166 (0.238) data 0.000 (0.062) loss 0.7524 (0.5194) acc 87.7551 (87.7807) lr 1.4258e-03 eta 0:06:11
epoch [20/50] batch [25/51] time 0.164 (0.247) data 0.000 (0.050) loss 0.7483 (0.5291) acc 81.2500 (87.5449) lr 1.4258e-03 eta 0:06:24
epoch [20/50] batch [30/51] time 0.159 (0.235) data 0.000 (0.042) loss 0.6518 (0.5277) acc 81.1111 (87.4071) lr 1.4258e-03 eta 0:06:04
epoch [20/50] batch [35/51] time 0.182 (0.227) data 0.000 (0.036) loss 0.3864 (0.5221) acc 91.8182 (87.7589) lr 1.4258e-03 eta 0:05:50
epoch [20/50] batch [40/51] time 0.160 (0.220) data 0.000 (0.031) loss 0.5366 (0.5210) acc 87.7660 (87.8171) lr 1.4258e-03 eta 0:05:38
epoch [20/50] batch [45/51] time 0.171 (0.215) data 0.000 (0.028) loss 0.4917 (0.5166) acc 87.2449 (87.9262) lr 1.4258e-03 eta 0:05:29
epoch [20/50] batch [50/51] time 0.182 (0.211) data 0.000 (0.025) loss 0.4560 (0.5178) acc 90.5660 (87.9197) lr 1.4258e-03 eta 0:05:23
>>> alpha1: 0.169  alpha2: -0.137 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.11 <<<
epoch [21/50] batch [5/51] time 0.176 (0.482) data 0.001 (0.295) loss 0.3486 (0.5327) acc 91.3462 (87.4634) lr 1.3681e-03 eta 0:12:14
epoch [21/50] batch [10/51] time 0.191 (0.329) data 0.000 (0.148) loss 0.3657 (0.5331) acc 90.5660 (87.2128) lr 1.3681e-03 eta 0:08:20
epoch [21/50] batch [15/51] time 0.181 (0.278) data 0.000 (0.098) loss 0.3961 (0.5172) acc 90.4546 (88.1447) lr 1.3681e-03 eta 0:07:01
epoch [21/50] batch [20/51] time 0.156 (0.252) data 0.000 (0.074) loss 0.5195 (0.5156) acc 90.3409 (88.3775) lr 1.3681e-03 eta 0:06:20
epoch [21/50] batch [25/51] time 0.172 (0.237) data 0.001 (0.059) loss 0.7187 (0.5332) acc 80.8824 (87.6758) lr 1.3681e-03 eta 0:05:57
epoch [21/50] batch [30/51] time 0.181 (0.228) data 0.000 (0.049) loss 0.4631 (0.5117) acc 88.6792 (88.2096) lr 1.3681e-03 eta 0:05:42
epoch [21/50] batch [35/51] time 0.172 (0.220) data 0.000 (0.042) loss 0.4236 (0.5195) acc 91.0000 (88.1077) lr 1.3681e-03 eta 0:05:29
epoch [21/50] batch [40/51] time 0.164 (0.215) data 0.000 (0.037) loss 0.5685 (0.5112) acc 88.5417 (88.3502) lr 1.3681e-03 eta 0:05:19
epoch [21/50] batch [45/51] time 0.166 (0.210) data 0.000 (0.033) loss 0.4622 (0.5081) acc 89.2857 (88.5641) lr 1.3681e-03 eta 0:05:11
epoch [21/50] batch [50/51] time 0.172 (0.205) data 0.000 (0.030) loss 0.5973 (0.5024) acc 86.0577 (88.6860) lr 1.3681e-03 eta 0:05:03
>>> alpha1: 0.167  alpha2: -0.140 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.11 <<<
epoch [22/50] batch [5/51] time 0.172 (0.456) data 0.000 (0.276) loss 0.4170 (0.4618) acc 90.6863 (89.4030) lr 1.3090e-03 eta 0:11:12
epoch [22/50] batch [10/51] time 0.166 (0.315) data 0.000 (0.138) loss 0.4487 (0.4937) acc 88.0208 (88.2374) lr 1.3090e-03 eta 0:07:42
epoch [22/50] batch [15/51] time 0.174 (0.267) data 0.000 (0.092) loss 0.4658 (0.4971) acc 88.7755 (88.1721) lr 1.3090e-03 eta 0:06:31
epoch [22/50] batch [20/51] time 0.169 (0.244) data 0.000 (0.069) loss 0.3609 (0.4976) acc 93.0000 (88.4838) lr 1.3090e-03 eta 0:05:55
epoch [22/50] batch [25/51] time 0.192 (0.231) data 0.000 (0.055) loss 0.5464 (0.4882) acc 86.1111 (88.6558) lr 1.3090e-03 eta 0:05:36
epoch [22/50] batch [30/51] time 0.169 (0.222) data 0.000 (0.046) loss 0.3150 (0.4779) acc 95.9184 (89.2734) lr 1.3090e-03 eta 0:05:22
epoch [22/50] batch [35/51] time 0.179 (0.216) data 0.000 (0.040) loss 0.4700 (0.4888) acc 87.7551 (88.7668) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [40/51] time 0.171 (0.210) data 0.000 (0.035) loss 0.2719 (0.4892) acc 94.2308 (88.7643) lr 1.3090e-03 eta 0:05:02
epoch [22/50] batch [45/51] time 0.160 (0.205) data 0.000 (0.031) loss 0.3345 (0.4885) acc 91.8478 (88.6924) lr 1.3090e-03 eta 0:04:54
epoch [22/50] batch [50/51] time 0.178 (0.201) data 0.000 (0.028) loss 0.2956 (0.4795) acc 94.9074 (89.0156) lr 1.3090e-03 eta 0:04:47
>>> alpha1: 0.163  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.10 <<<
epoch [23/50] batch [5/51] time 0.186 (0.466) data 0.000 (0.287) loss 0.5425 (0.4473) acc 86.8421 (89.7813) lr 1.2487e-03 eta 0:11:03
epoch [23/50] batch [10/51] time 0.171 (0.321) data 0.000 (0.144) loss 0.4003 (0.5190) acc 90.0000 (88.6416) lr 1.2487e-03 eta 0:07:35
epoch [23/50] batch [15/51] time 0.170 (0.274) data 0.000 (0.096) loss 0.5674 (0.6288) acc 86.5000 (87.3749) lr 1.2487e-03 eta 0:06:27
epoch [23/50] batch [20/51] time 0.176 (0.250) data 0.000 (0.072) loss 0.6095 (0.5972) acc 84.0000 (86.8964) lr 1.2487e-03 eta 0:05:51
epoch [23/50] batch [25/51] time 0.179 (0.235) data 0.000 (0.058) loss 0.4466 (0.5584) acc 91.3462 (87.8647) lr 1.2487e-03 eta 0:05:30
epoch [23/50] batch [30/51] time 0.191 (0.227) data 0.015 (0.049) loss 0.5753 (0.5395) acc 85.2041 (88.2566) lr 1.2487e-03 eta 0:05:17
epoch [23/50] batch [35/51] time 0.180 (0.220) data 0.000 (0.042) loss 0.3947 (0.5258) acc 91.1765 (88.4907) lr 1.2487e-03 eta 0:05:07
epoch [23/50] batch [40/51] time 0.180 (0.215) data 0.000 (0.037) loss 0.3853 (0.5167) acc 91.3462 (88.6970) lr 1.2487e-03 eta 0:04:58
epoch [23/50] batch [45/51] time 0.159 (0.210) data 0.000 (0.033) loss 0.3732 (0.5122) acc 92.9348 (88.7806) lr 1.2487e-03 eta 0:04:50
epoch [23/50] batch [50/51] time 0.160 (0.206) data 0.000 (0.029) loss 0.3591 (0.5095) acc 89.8936 (88.7592) lr 1.2487e-03 eta 0:04:43
>>> alpha1: 0.160  alpha2: -0.130 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [24/50] batch [5/51] time 0.180 (0.471) data 0.000 (0.292) loss 0.2114 (0.4235) acc 96.7593 (90.4052) lr 1.1874e-03 eta 0:10:46
epoch [24/50] batch [10/51] time 0.169 (0.325) data 0.000 (0.146) loss 0.5077 (0.4579) acc 89.0000 (89.8826) lr 1.1874e-03 eta 0:07:24
epoch [24/50] batch [15/51] time 0.180 (0.274) data 0.000 (0.097) loss 0.3169 (0.4384) acc 93.2292 (90.1557) lr 1.1874e-03 eta 0:06:13
epoch [24/50] batch [20/51] time 0.173 (0.250) data 0.000 (0.073) loss 0.3601 (0.4312) acc 90.8654 (90.4539) lr 1.1874e-03 eta 0:05:39
epoch [24/50] batch [25/51] time 0.168 (0.235) data 0.000 (0.059) loss 0.2740 (0.4203) acc 95.0000 (90.6108) lr 1.1874e-03 eta 0:05:17
epoch [24/50] batch [30/51] time 0.173 (0.225) data 0.000 (0.049) loss 0.4603 (0.4255) acc 88.4615 (90.5255) lr 1.1874e-03 eta 0:05:02
epoch [24/50] batch [35/51] time 0.168 (0.218) data 0.000 (0.042) loss 0.4688 (0.4341) acc 90.5000 (90.3506) lr 1.1874e-03 eta 0:04:51
epoch [24/50] batch [40/51] time 0.172 (0.212) data 0.000 (0.037) loss 0.4258 (0.4400) acc 90.3846 (90.2589) lr 1.1874e-03 eta 0:04:43
epoch [24/50] batch [45/51] time 0.158 (0.207) data 0.000 (0.033) loss 0.5619 (0.4525) acc 85.8696 (89.9975) lr 1.1874e-03 eta 0:04:36
epoch [24/50] batch [50/51] time 0.166 (0.204) data 0.000 (0.029) loss 0.5786 (0.4606) acc 85.7143 (89.6548) lr 1.1874e-03 eta 0:04:30
>>> alpha1: 0.156  alpha2: -0.129 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [25/50] batch [5/51] time 0.189 (0.441) data 0.017 (0.262) loss 0.4674 (0.4164) acc 94.5000 (92.6307) lr 1.1253e-03 eta 0:09:42
epoch [25/50] batch [10/51] time 0.194 (0.314) data 0.000 (0.131) loss 0.6861 (0.4509) acc 82.7273 (90.8163) lr 1.1253e-03 eta 0:06:53
epoch [25/50] batch [15/51] time 0.179 (0.269) data 0.000 (0.088) loss 0.3977 (0.4600) acc 89.6226 (89.9589) lr 1.1253e-03 eta 0:05:52
epoch [25/50] batch [20/51] time 0.170 (0.247) data 0.000 (0.066) loss 0.3650 (0.4544) acc 92.0000 (89.8591) lr 1.1253e-03 eta 0:05:22
epoch [25/50] batch [25/51] time 0.183 (0.232) data 0.000 (0.053) loss 0.4850 (0.4610) acc 87.7451 (89.7274) lr 1.1253e-03 eta 0:05:01
epoch [25/50] batch [30/51] time 0.190 (0.223) data 0.000 (0.044) loss 0.6159 (0.4735) acc 88.8298 (89.5406) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [35/51] time 0.168 (0.217) data 0.000 (0.038) loss 0.4178 (0.4811) acc 90.6250 (89.1919) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [40/51] time 0.166 (0.211) data 0.000 (0.033) loss 0.4401 (0.4752) acc 89.2857 (89.0660) lr 1.1253e-03 eta 0:04:31
epoch [25/50] batch [45/51] time 0.180 (0.207) data 0.000 (0.029) loss 0.4758 (0.4801) acc 86.3636 (88.7561) lr 1.1253e-03 eta 0:04:25
epoch [25/50] batch [50/51] time 0.161 (0.218) data 0.000 (0.026) loss 0.3149 (0.4694) acc 95.2128 (89.0959) lr 1.1253e-03 eta 0:04:37
>>> alpha1: 0.153  alpha2: -0.131 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [26/50] batch [5/51] time 0.168 (0.482) data 0.000 (0.306) loss 0.5363 (0.4268) acc 89.7959 (92.6310) lr 1.0628e-03 eta 0:10:11
epoch [26/50] batch [10/51] time 0.188 (0.331) data 0.000 (0.153) loss 0.3788 (0.4301) acc 90.4546 (91.9439) lr 1.0628e-03 eta 0:06:58
epoch [26/50] batch [15/51] time 0.200 (0.280) data 0.000 (0.102) loss 0.3175 (0.4444) acc 89.2241 (91.0492) lr 1.0628e-03 eta 0:05:53
epoch [26/50] batch [20/51] time 0.170 (0.254) data 0.000 (0.077) loss 0.3825 (0.4397) acc 89.2157 (90.8598) lr 1.0628e-03 eta 0:05:18
epoch [26/50] batch [25/51] time 0.185 (0.238) data 0.000 (0.061) loss 0.3856 (0.4531) acc 87.7451 (90.2209) lr 1.0628e-03 eta 0:04:57
epoch [26/50] batch [30/51] time 0.178 (0.229) data 0.000 (0.051) loss 0.6242 (0.4589) acc 86.0577 (89.9539) lr 1.0628e-03 eta 0:04:45
epoch [26/50] batch [35/51] time 0.178 (0.222) data 0.000 (0.044) loss 0.4314 (0.4645) acc 91.9811 (89.8591) lr 1.0628e-03 eta 0:04:35
epoch [26/50] batch [40/51] time 0.176 (0.216) data 0.000 (0.038) loss 0.3454 (0.4691) acc 96.2264 (89.6600) lr 1.0628e-03 eta 0:04:27
epoch [26/50] batch [45/51] time 0.163 (0.211) data 0.000 (0.034) loss 0.4025 (0.4601) acc 86.9792 (89.7549) lr 1.0628e-03 eta 0:04:19
epoch [26/50] batch [50/51] time 0.182 (0.207) data 0.000 (0.031) loss 0.4129 (0.4611) acc 92.6471 (89.7998) lr 1.0628e-03 eta 0:04:14
>>> alpha1: 0.148  alpha2: -0.123 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.11 <<<
epoch [27/50] batch [5/51] time 0.177 (0.442) data 0.000 (0.262) loss 0.4723 (0.8356) acc 87.9808 (84.4435) lr 1.0000e-03 eta 0:08:58
epoch [27/50] batch [10/51] time 0.183 (0.311) data 0.000 (0.131) loss 0.3044 (0.6332) acc 93.0000 (86.7851) lr 1.0000e-03 eta 0:06:17
epoch [27/50] batch [15/51] time 0.183 (0.264) data 0.000 (0.087) loss 0.4063 (0.5620) acc 87.7273 (87.4284) lr 1.0000e-03 eta 0:05:19
epoch [27/50] batch [20/51] time 0.170 (0.243) data 0.000 (0.066) loss 0.1795 (0.5176) acc 98.0392 (88.4940) lr 1.0000e-03 eta 0:04:52
epoch [27/50] batch [25/51] time 0.180 (0.228) data 0.000 (0.053) loss 0.3357 (0.5053) acc 91.3636 (88.6252) lr 1.0000e-03 eta 0:04:33
epoch [27/50] batch [30/51] time 0.171 (0.219) data 0.000 (0.044) loss 0.4767 (0.4972) acc 89.8936 (88.9007) lr 1.0000e-03 eta 0:04:21
epoch [27/50] batch [35/51] time 0.179 (0.214) data 0.000 (0.038) loss 0.5152 (0.4929) acc 89.6226 (89.0549) lr 1.0000e-03 eta 0:04:13
epoch [27/50] batch [40/51] time 0.176 (0.209) data 0.000 (0.033) loss 0.2737 (0.4836) acc 95.7547 (89.3766) lr 1.0000e-03 eta 0:04:06
epoch [27/50] batch [45/51] time 0.161 (0.204) data 0.000 (0.029) loss 0.3251 (0.4741) acc 94.6808 (89.5244) lr 1.0000e-03 eta 0:04:00
epoch [27/50] batch [50/51] time 0.169 (0.201) data 0.000 (0.026) loss 0.5783 (0.4724) acc 86.2745 (89.4800) lr 1.0000e-03 eta 0:03:55
>>> alpha1: 0.146  alpha2: -0.126 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [28/50] batch [5/51] time 0.175 (0.450) data 0.000 (0.268) loss 0.3819 (0.3623) acc 91.5000 (92.5748) lr 9.3721e-04 eta 0:08:45
epoch [28/50] batch [10/51] time 0.189 (0.314) data 0.000 (0.134) loss 0.4063 (0.3822) acc 93.2692 (91.9994) lr 9.3721e-04 eta 0:06:05
epoch [28/50] batch [15/51] time 0.177 (0.268) data 0.000 (0.090) loss 0.2316 (0.3816) acc 95.5000 (91.9861) lr 9.3721e-04 eta 0:05:10
epoch [28/50] batch [20/51] time 0.166 (0.245) data 0.000 (0.067) loss 0.2060 (0.3879) acc 95.9184 (91.6543) lr 9.3721e-04 eta 0:04:41
epoch [28/50] batch [25/51] time 0.169 (0.230) data 0.000 (0.054) loss 0.4694 (0.3963) acc 90.0000 (91.5583) lr 9.3721e-04 eta 0:04:23
epoch [28/50] batch [30/51] time 0.172 (0.220) data 0.000 (0.045) loss 0.5231 (0.4130) acc 85.5000 (91.1013) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [35/51] time 0.171 (0.214) data 0.000 (0.039) loss 0.5748 (0.4119) acc 90.1042 (91.2943) lr 9.3721e-04 eta 0:04:03
epoch [28/50] batch [40/51] time 0.168 (0.208) data 0.000 (0.034) loss 0.4357 (0.4193) acc 87.2549 (90.8500) lr 9.3721e-04 eta 0:03:55
epoch [28/50] batch [45/51] time 0.171 (0.203) data 0.000 (0.030) loss 0.3498 (0.4223) acc 93.2692 (90.7985) lr 9.3721e-04 eta 0:03:49
epoch [28/50] batch [50/51] time 0.177 (0.200) data 0.000 (0.027) loss 0.4873 (0.4337) acc 87.5000 (90.3708) lr 9.3721e-04 eta 0:03:44
>>> alpha1: 0.144  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [29/50] batch [5/51] time 0.175 (0.433) data 0.000 (0.254) loss 0.4199 (0.4468) acc 91.5000 (91.5353) lr 8.7467e-04 eta 0:08:03
epoch [29/50] batch [10/51] time 0.198 (0.305) data 0.000 (0.127) loss 0.5069 (0.4298) acc 86.7347 (91.3030) lr 8.7467e-04 eta 0:05:38
epoch [29/50] batch [15/51] time 0.190 (0.264) data 0.000 (0.086) loss 0.4672 (0.4182) acc 91.0377 (91.1598) lr 8.7467e-04 eta 0:04:52
epoch [29/50] batch [20/51] time 0.167 (0.243) data 0.000 (0.064) loss 0.2900 (0.4089) acc 93.8775 (91.2668) lr 8.7467e-04 eta 0:04:27
epoch [29/50] batch [25/51] time 0.169 (0.230) data 0.000 (0.051) loss 0.4370 (0.4089) acc 95.0000 (91.5534) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [30/51] time 0.175 (0.222) data 0.000 (0.043) loss 0.3893 (0.4232) acc 90.3846 (91.2448) lr 8.7467e-04 eta 0:04:02
epoch [29/50] batch [35/51] time 0.170 (0.215) data 0.000 (0.037) loss 0.4425 (0.4173) acc 89.0000 (91.3034) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [40/51] time 0.175 (0.211) data 0.000 (0.032) loss 0.4593 (0.4208) acc 85.9375 (91.1998) lr 8.7467e-04 eta 0:03:47
epoch [29/50] batch [45/51] time 0.167 (0.207) data 0.000 (0.029) loss 0.3931 (0.4203) acc 89.6739 (91.2788) lr 8.7467e-04 eta 0:03:42
epoch [29/50] batch [50/51] time 0.183 (0.204) data 0.000 (0.026) loss 0.4455 (0.4112) acc 88.1818 (91.2783) lr 8.7467e-04 eta 0:03:38
>>> alpha1: 0.144  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [30/50] batch [5/51] time 0.166 (0.471) data 0.000 (0.281) loss 0.4751 (0.4420) acc 91.8367 (90.3209) lr 8.1262e-04 eta 0:08:21
epoch [30/50] batch [10/51] time 0.193 (0.325) data 0.000 (0.140) loss 0.4133 (0.4198) acc 92.3469 (90.9521) lr 8.1262e-04 eta 0:05:44
epoch [30/50] batch [15/51] time 0.172 (0.275) data 0.000 (0.094) loss 0.4920 (0.4161) acc 88.2653 (90.7302) lr 8.1262e-04 eta 0:04:50
epoch [30/50] batch [20/51] time 0.165 (0.249) data 0.000 (0.070) loss 0.3870 (0.4107) acc 90.4255 (90.7902) lr 8.1262e-04 eta 0:04:21
epoch [30/50] batch [25/51] time 0.186 (0.234) data 0.000 (0.056) loss 0.5188 (0.4100) acc 87.0192 (90.4857) lr 8.1262e-04 eta 0:04:05
epoch [30/50] batch [30/51] time 0.175 (0.224) data 0.000 (0.047) loss 0.3865 (0.4126) acc 88.2979 (90.3195) lr 8.1262e-04 eta 0:03:53
epoch [30/50] batch [35/51] time 0.186 (0.218) data 0.000 (0.040) loss 0.3868 (0.4119) acc 93.6364 (90.5606) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [40/51] time 0.152 (0.212) data 0.000 (0.035) loss 0.5656 (0.4108) acc 89.5349 (90.7822) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [45/51] time 0.167 (0.206) data 0.000 (0.031) loss 0.2369 (0.4087) acc 98.4694 (90.8822) lr 8.1262e-04 eta 0:03:31
epoch [30/50] batch [50/51] time 0.169 (0.202) data 0.000 (0.028) loss 0.4144 (0.4065) acc 90.0000 (90.8482) lr 8.1262e-04 eta 0:03:26
>>> alpha1: 0.141  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [31/50] batch [5/51] time 0.189 (0.428) data 0.000 (0.232) loss 0.5025 (0.4272) acc 86.5741 (91.1521) lr 7.5131e-04 eta 0:07:14
epoch [31/50] batch [10/51] time 0.177 (0.301) data 0.000 (0.116) loss 0.2238 (0.4245) acc 96.6981 (91.1738) lr 7.5131e-04 eta 0:05:04
epoch [31/50] batch [15/51] time 0.189 (0.261) data 0.000 (0.078) loss 0.5510 (0.4268) acc 90.0000 (91.0249) lr 7.5131e-04 eta 0:04:22
epoch [31/50] batch [20/51] time 0.181 (0.239) data 0.000 (0.058) loss 0.4249 (0.4225) acc 91.8182 (91.0878) lr 7.5131e-04 eta 0:03:59
epoch [31/50] batch [25/51] time 0.172 (0.227) data 0.000 (0.047) loss 0.4426 (0.4096) acc 88.0208 (91.5135) lr 7.5131e-04 eta 0:03:46
epoch [31/50] batch [30/51] time 0.159 (0.218) data 0.001 (0.039) loss 0.4290 (0.4242) acc 93.3333 (91.0991) lr 7.5131e-04 eta 0:03:35
epoch [31/50] batch [35/51] time 0.187 (0.212) data 0.000 (0.033) loss 0.4264 (0.4268) acc 93.2692 (91.0767) lr 7.5131e-04 eta 0:03:28
epoch [31/50] batch [40/51] time 0.183 (0.209) data 0.000 (0.030) loss 0.4811 (0.4241) acc 87.2549 (91.1949) lr 7.5131e-04 eta 0:03:24
epoch [31/50] batch [45/51] time 0.192 (0.206) data 0.000 (0.027) loss 0.2130 (0.4136) acc 96.3636 (91.4224) lr 7.5131e-04 eta 0:03:20
epoch [31/50] batch [50/51] time 0.186 (0.203) data 0.000 (0.024) loss 0.2831 (0.4061) acc 93.1818 (91.5135) lr 7.5131e-04 eta 0:03:17
>>> alpha1: 0.139  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [32/50] batch [5/51] time 0.175 (0.446) data 0.001 (0.260) loss 0.3942 (0.3628) acc 90.6863 (91.4918) lr 6.9098e-04 eta 0:07:09
epoch [32/50] batch [10/51] time 0.170 (0.314) data 0.000 (0.130) loss 0.3613 (0.3459) acc 93.5000 (92.5857) lr 6.9098e-04 eta 0:05:01
epoch [32/50] batch [15/51] time 0.177 (0.268) data 0.000 (0.087) loss 0.3848 (0.3467) acc 92.4528 (92.4787) lr 6.9098e-04 eta 0:04:15
epoch [32/50] batch [20/51] time 0.162 (0.244) data 0.000 (0.065) loss 0.4010 (0.3813) acc 92.0213 (91.6711) lr 6.9098e-04 eta 0:03:51
epoch [32/50] batch [25/51] time 0.195 (0.231) data 0.000 (0.052) loss 0.4204 (0.3841) acc 91.6667 (91.5007) lr 6.9098e-04 eta 0:03:38
epoch [32/50] batch [30/51] time 0.176 (0.221) data 0.000 (0.044) loss 0.5137 (0.3876) acc 89.1304 (91.4672) lr 6.9098e-04 eta 0:03:27
epoch [32/50] batch [35/51] time 0.168 (0.214) data 0.000 (0.037) loss 0.4169 (0.3849) acc 91.8367 (91.6077) lr 6.9098e-04 eta 0:03:20
epoch [32/50] batch [40/51] time 0.159 (0.209) data 0.000 (0.033) loss 0.5169 (0.3917) acc 88.0435 (91.2685) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [45/51] time 0.166 (0.205) data 0.000 (0.029) loss 0.3584 (0.3917) acc 92.3469 (91.2310) lr 6.9098e-04 eta 0:03:09
epoch [32/50] batch [50/51] time 0.165 (0.201) data 0.000 (0.026) loss 0.2415 (0.3947) acc 97.4490 (91.2050) lr 6.9098e-04 eta 0:03:04
>>> alpha1: 0.138  alpha2: -0.116 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [33/50] batch [5/51] time 0.168 (0.486) data 0.000 (0.304) loss 0.5963 (0.4657) acc 86.4130 (89.9553) lr 6.3188e-04 eta 0:07:23
epoch [33/50] batch [10/51] time 0.166 (0.334) data 0.000 (0.152) loss 0.4327 (0.4389) acc 89.0625 (89.8054) lr 6.3188e-04 eta 0:05:03
epoch [33/50] batch [15/51] time 0.165 (0.280) data 0.000 (0.102) loss 0.4419 (0.4371) acc 91.1458 (90.6372) lr 6.3188e-04 eta 0:04:13
epoch [33/50] batch [20/51] time 0.172 (0.254) data 0.001 (0.076) loss 0.2113 (0.4348) acc 94.1176 (90.5749) lr 6.3188e-04 eta 0:03:48
epoch [33/50] batch [25/51] time 0.189 (0.240) data 0.000 (0.061) loss 0.4196 (0.4228) acc 91.3462 (90.9166) lr 6.3188e-04 eta 0:03:34
epoch [33/50] batch [30/51] time 0.172 (0.229) data 0.000 (0.051) loss 0.5882 (0.4390) acc 86.2245 (90.6151) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [35/51] time 0.183 (0.222) data 0.000 (0.044) loss 0.4712 (0.4403) acc 89.1509 (90.6260) lr 6.3188e-04 eta 0:03:15
epoch [33/50] batch [40/51] time 0.161 (0.216) data 0.000 (0.038) loss 0.4852 (0.4312) acc 86.1702 (90.7431) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.165 (0.210) data 0.000 (0.034) loss 0.5588 (0.4279) acc 86.4583 (90.8071) lr 6.3188e-04 eta 0:03:03
epoch [33/50] batch [50/51] time 0.187 (0.207) data 0.000 (0.031) loss 0.3782 (0.4279) acc 90.0862 (90.7715) lr 6.3188e-04 eta 0:02:59
>>> alpha1: 0.137  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [34/50] batch [5/51] time 0.183 (0.473) data 0.000 (0.295) loss 0.7218 (0.4638) acc 81.3830 (89.0599) lr 5.7422e-04 eta 0:06:47
epoch [34/50] batch [10/51] time 0.163 (0.325) data 0.000 (0.149) loss 0.4076 (0.4319) acc 90.9574 (90.0008) lr 5.7422e-04 eta 0:04:38
epoch [34/50] batch [15/51] time 0.169 (0.274) data 0.000 (0.100) loss 0.3160 (0.4215) acc 93.5000 (90.4020) lr 5.7422e-04 eta 0:03:53
epoch [34/50] batch [20/51] time 0.176 (0.251) data 0.000 (0.075) loss 0.2963 (0.4130) acc 91.5094 (90.7602) lr 5.7422e-04 eta 0:03:32
epoch [34/50] batch [25/51] time 0.174 (0.236) data 0.000 (0.060) loss 0.3663 (0.4035) acc 90.5000 (90.7024) lr 5.7422e-04 eta 0:03:18
epoch [34/50] batch [30/51] time 0.186 (0.227) data 0.000 (0.050) loss 0.2696 (0.3966) acc 93.3962 (90.8940) lr 5.7422e-04 eta 0:03:09
epoch [34/50] batch [35/51] time 0.182 (0.221) data 0.000 (0.043) loss 0.3194 (0.3861) acc 95.8333 (91.3779) lr 5.7422e-04 eta 0:03:03
epoch [34/50] batch [40/51] time 0.174 (0.214) data 0.000 (0.038) loss 0.3874 (0.3937) acc 91.8269 (91.1146) lr 5.7422e-04 eta 0:02:57
epoch [34/50] batch [45/51] time 0.180 (0.210) data 0.001 (0.033) loss 0.4153 (0.3973) acc 91.2037 (90.9791) lr 5.7422e-04 eta 0:02:52
epoch [34/50] batch [50/51] time 0.173 (0.206) data 0.000 (0.030) loss 0.4736 (0.3994) acc 89.9038 (91.0041) lr 5.7422e-04 eta 0:02:48
>>> alpha1: 0.136  alpha2: -0.111 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [35/50] batch [5/51] time 0.173 (0.431) data 0.000 (0.246) loss 0.3554 (0.3478) acc 90.1961 (93.5904) lr 5.1825e-04 eta 0:05:49
epoch [35/50] batch [10/51] time 0.184 (0.304) data 0.000 (0.123) loss 0.3143 (0.3722) acc 93.6274 (92.2941) lr 5.1825e-04 eta 0:04:04
epoch [35/50] batch [15/51] time 0.187 (0.262) data 0.001 (0.082) loss 0.4761 (0.3829) acc 87.7273 (91.5137) lr 5.1825e-04 eta 0:03:29
epoch [35/50] batch [20/51] time 0.178 (0.241) data 0.000 (0.062) loss 0.4028 (0.3905) acc 91.1765 (91.4594) lr 5.1825e-04 eta 0:03:12
epoch [35/50] batch [25/51] time 0.192 (0.228) data 0.000 (0.049) loss 0.4624 (0.4833) acc 90.5660 (90.2727) lr 5.1825e-04 eta 0:03:00
epoch [35/50] batch [30/51] time 0.180 (0.220) data 0.000 (0.041) loss 0.4950 (0.5166) acc 86.8182 (90.1414) lr 5.1825e-04 eta 0:02:52
epoch [35/50] batch [35/51] time 0.174 (0.214) data 0.000 (0.035) loss 0.3589 (0.5036) acc 93.2292 (90.2310) lr 5.1825e-04 eta 0:02:47
epoch [35/50] batch [40/51] time 0.178 (0.209) data 0.000 (0.031) loss 0.4963 (0.4874) acc 91.2037 (90.4328) lr 5.1825e-04 eta 0:02:42
epoch [35/50] batch [45/51] time 0.176 (0.204) data 0.000 (0.028) loss 0.2674 (0.4775) acc 95.8333 (90.5590) lr 5.1825e-04 eta 0:02:37
epoch [35/50] batch [50/51] time 0.157 (0.201) data 0.000 (0.025) loss 0.4735 (0.4728) acc 90.5556 (90.5816) lr 5.1825e-04 eta 0:02:33
>>> alpha1: 0.136  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [36/50] batch [5/51] time 0.203 (0.435) data 0.000 (0.241) loss 0.3953 (0.3492) acc 93.5185 (92.3138) lr 4.6417e-04 eta 0:05:30
epoch [36/50] batch [10/51] time 0.166 (0.308) data 0.000 (0.121) loss 0.4505 (0.3606) acc 90.9574 (92.5806) lr 4.6417e-04 eta 0:03:52
epoch [36/50] batch [15/51] time 0.192 (0.264) data 0.000 (0.081) loss 0.3065 (0.3709) acc 93.0556 (92.2702) lr 4.6417e-04 eta 0:03:17
epoch [36/50] batch [20/51] time 0.171 (0.242) data 0.000 (0.061) loss 0.4069 (0.3517) acc 89.2157 (92.3688) lr 4.6417e-04 eta 0:03:00
epoch [36/50] batch [25/51] time 0.176 (0.229) data 0.000 (0.049) loss 0.2714 (0.3609) acc 94.2708 (92.2883) lr 4.6417e-04 eta 0:02:49
epoch [36/50] batch [30/51] time 0.165 (0.220) data 0.000 (0.040) loss 0.1968 (0.3671) acc 98.4375 (92.1025) lr 4.6417e-04 eta 0:02:41
epoch [36/50] batch [35/51] time 0.203 (0.214) data 0.000 (0.035) loss 0.5164 (0.3803) acc 85.0000 (91.6331) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [40/51] time 0.172 (0.209) data 0.000 (0.030) loss 0.3992 (0.3822) acc 90.3846 (91.5268) lr 4.6417e-04 eta 0:02:31
epoch [36/50] batch [45/51] time 0.165 (0.205) data 0.000 (0.027) loss 0.5288 (0.3898) acc 84.8958 (91.2659) lr 4.6417e-04 eta 0:02:27
epoch [36/50] batch [50/51] time 0.163 (0.202) data 0.000 (0.024) loss 0.4881 (0.3878) acc 89.5833 (91.3197) lr 4.6417e-04 eta 0:02:24
>>> alpha1: 0.136  alpha2: -0.104 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [37/50] batch [5/51] time 0.162 (0.462) data 0.000 (0.292) loss 0.3353 (0.3699) acc 92.0213 (92.4317) lr 4.1221e-04 eta 0:05:27
epoch [37/50] batch [10/51] time 0.185 (0.319) data 0.000 (0.146) loss 0.2136 (0.3501) acc 96.2264 (92.8673) lr 4.1221e-04 eta 0:03:44
epoch [37/50] batch [15/51] time 0.162 (0.269) data 0.000 (0.097) loss 0.5280 (0.3673) acc 89.8936 (92.6625) lr 4.1221e-04 eta 0:03:08
epoch [37/50] batch [20/51] time 0.186 (0.247) data 0.000 (0.074) loss 0.3407 (0.3527) acc 93.8596 (92.9020) lr 4.1221e-04 eta 0:02:51
epoch [37/50] batch [25/51] time 0.169 (0.232) data 0.000 (0.059) loss 0.3457 (0.3509) acc 91.5000 (92.7439) lr 4.1221e-04 eta 0:02:39
epoch [37/50] batch [30/51] time 0.170 (0.222) data 0.000 (0.049) loss 0.3338 (0.3620) acc 94.0000 (92.3566) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [35/51] time 0.163 (0.215) data 0.000 (0.042) loss 0.3662 (0.3686) acc 93.0851 (92.0068) lr 4.1221e-04 eta 0:02:26
epoch [37/50] batch [40/51] time 0.165 (0.210) data 0.000 (0.037) loss 0.5111 (0.3726) acc 86.9565 (91.9456) lr 4.1221e-04 eta 0:02:21
epoch [37/50] batch [45/51] time 0.187 (0.205) data 0.000 (0.033) loss 0.2977 (0.3841) acc 94.9074 (91.7377) lr 4.1221e-04 eta 0:02:17
epoch [37/50] batch [50/51] time 0.164 (0.201) data 0.000 (0.030) loss 0.2566 (0.3812) acc 96.7391 (91.9222) lr 4.1221e-04 eta 0:02:13
>>> alpha1: 0.134  alpha2: -0.105 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [38/50] batch [5/51] time 0.170 (0.487) data 0.000 (0.303) loss 0.5365 (0.4343) acc 88.7255 (89.7265) lr 3.6258e-04 eta 0:05:20
epoch [38/50] batch [10/51] time 0.168 (0.331) data 0.000 (0.151) loss 0.3937 (0.4112) acc 91.0000 (91.2386) lr 3.6258e-04 eta 0:03:36
epoch [38/50] batch [15/51] time 0.183 (0.277) data 0.000 (0.101) loss 0.4378 (0.4138) acc 91.3462 (91.1733) lr 3.6258e-04 eta 0:02:59
epoch [38/50] batch [20/51] time 0.164 (0.251) data 0.000 (0.076) loss 0.4166 (0.4150) acc 89.0625 (91.0671) lr 3.6258e-04 eta 0:02:41
epoch [38/50] batch [25/51] time 0.182 (0.236) data 0.000 (0.061) loss 0.2602 (0.3924) acc 95.0980 (91.6707) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [30/51] time 0.176 (0.226) data 0.000 (0.051) loss 0.4382 (0.3885) acc 87.0192 (91.6754) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.200 (0.218) data 0.000 (0.043) loss 0.2607 (0.3937) acc 95.1923 (91.3937) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [40/51] time 0.157 (0.212) data 0.000 (0.038) loss 0.2846 (0.3857) acc 95.6522 (91.6848) lr 3.6258e-04 eta 0:02:12
epoch [38/50] batch [45/51] time 0.157 (0.207) data 0.000 (0.034) loss 0.4585 (0.3788) acc 90.7609 (91.9441) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [50/51] time 0.166 (0.203) data 0.000 (0.030) loss 0.3713 (0.3718) acc 92.5000 (92.1321) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.133  alpha2: -0.103 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [39/50] batch [5/51] time 0.179 (0.447) data 0.000 (0.269) loss 0.2770 (0.3378) acc 94.3878 (92.2479) lr 3.1545e-04 eta 0:04:31
epoch [39/50] batch [10/51] time 0.169 (0.310) data 0.000 (0.135) loss 0.3156 (0.3585) acc 93.0000 (92.1637) lr 3.1545e-04 eta 0:03:06
epoch [39/50] batch [15/51] time 0.182 (0.267) data 0.000 (0.090) loss 0.2958 (0.3547) acc 94.0909 (92.3491) lr 3.1545e-04 eta 0:02:39
epoch [39/50] batch [20/51] time 0.181 (0.246) data 0.000 (0.067) loss 0.5036 (0.3631) acc 83.5000 (92.0282) lr 3.1545e-04 eta 0:02:25
epoch [39/50] batch [25/51] time 0.178 (0.232) data 0.000 (0.054) loss 0.3371 (0.3622) acc 92.0000 (92.0964) lr 3.1545e-04 eta 0:02:16
epoch [39/50] batch [30/51] time 0.181 (0.222) data 0.000 (0.045) loss 0.4472 (0.3659) acc 89.8148 (92.1214) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [35/51] time 0.207 (0.217) data 0.000 (0.039) loss 0.5383 (0.3730) acc 90.1961 (91.9061) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [40/51] time 0.184 (0.212) data 0.000 (0.034) loss 0.2671 (0.3646) acc 91.9643 (92.0703) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [45/51] time 0.181 (0.208) data 0.000 (0.030) loss 0.3754 (0.3697) acc 93.0556 (91.9771) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [50/51] time 0.160 (0.204) data 0.000 (0.027) loss 0.4832 (0.3757) acc 89.3617 (91.8395) lr 3.1545e-04 eta 0:01:54
>>> alpha1: 0.133  alpha2: -0.104 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [40/50] batch [5/51] time 0.174 (0.432) data 0.000 (0.253) loss 0.4273 (0.4287) acc 88.5417 (89.9072) lr 2.7103e-04 eta 0:04:00
epoch [40/50] batch [10/51] time 0.180 (0.303) data 0.000 (0.127) loss 0.3453 (0.4197) acc 90.2778 (90.6577) lr 2.7103e-04 eta 0:02:47
epoch [40/50] batch [15/51] time 0.177 (0.264) data 0.000 (0.085) loss 0.4319 (0.3943) acc 90.8654 (91.6656) lr 2.7103e-04 eta 0:02:24
epoch [40/50] batch [20/51] time 0.181 (0.241) data 0.000 (0.064) loss 0.2964 (0.3800) acc 95.0000 (92.0270) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [25/51] time 0.173 (0.230) data 0.000 (0.051) loss 0.4151 (0.3843) acc 88.7255 (91.5444) lr 2.7103e-04 eta 0:02:03
epoch [40/50] batch [30/51] time 0.190 (0.222) data 0.001 (0.043) loss 0.4209 (0.3856) acc 92.3077 (91.3823) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [35/51] time 0.194 (0.216) data 0.000 (0.037) loss 0.3193 (0.3782) acc 92.4528 (91.5654) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [40/51] time 0.167 (0.211) data 0.000 (0.032) loss 0.3518 (0.3817) acc 94.8980 (91.4783) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [45/51] time 0.186 (0.207) data 0.000 (0.029) loss 0.3566 (0.3755) acc 89.9123 (91.6586) lr 2.7103e-04 eta 0:01:46
epoch [40/50] batch [50/51] time 0.190 (0.204) data 0.000 (0.026) loss 0.4440 (0.3871) acc 89.4068 (91.3004) lr 2.7103e-04 eta 0:01:44
>>> alpha1: 0.132  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [41/50] batch [5/51] time 0.167 (0.478) data 0.000 (0.290) loss 0.4130 (0.3465) acc 88.8298 (92.1730) lr 2.2949e-04 eta 0:04:01
epoch [41/50] batch [10/51] time 0.167 (0.323) data 0.000 (0.145) loss 0.2963 (0.3645) acc 94.1489 (91.9745) lr 2.2949e-04 eta 0:02:41
epoch [41/50] batch [15/51] time 0.159 (0.272) data 0.000 (0.097) loss 0.4155 (0.4000) acc 87.2222 (90.8753) lr 2.2949e-04 eta 0:02:14
epoch [41/50] batch [20/51] time 0.170 (0.248) data 0.000 (0.073) loss 0.3168 (0.3938) acc 95.7447 (91.4006) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [25/51] time 0.173 (0.234) data 0.000 (0.058) loss 0.2437 (0.3962) acc 96.1538 (91.4821) lr 2.2949e-04 eta 0:01:53
epoch [41/50] batch [30/51] time 0.179 (0.226) data 0.000 (0.049) loss 0.4226 (0.3969) acc 89.2157 (91.3162) lr 2.2949e-04 eta 0:01:48
epoch [41/50] batch [35/51] time 0.185 (0.218) data 0.001 (0.042) loss 0.4375 (0.3985) acc 87.0536 (91.1545) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [40/51] time 0.161 (0.211) data 0.000 (0.037) loss 0.4426 (0.3979) acc 89.8936 (91.0946) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [45/51] time 0.167 (0.207) data 0.000 (0.033) loss 0.3562 (0.3968) acc 91.4894 (91.0509) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [50/51] time 0.163 (0.204) data 0.000 (0.029) loss 0.2221 (0.3852) acc 96.2766 (91.4586) lr 2.2949e-04 eta 0:01:33
>>> alpha1: 0.132  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [42/50] batch [5/51] time 0.190 (0.466) data 0.000 (0.276) loss 0.3138 (0.3128) acc 91.3462 (93.8442) lr 1.9098e-04 eta 0:03:31
epoch [42/50] batch [10/51] time 0.170 (0.322) data 0.000 (0.138) loss 0.3152 (0.3471) acc 93.5000 (92.9139) lr 1.9098e-04 eta 0:02:24
epoch [42/50] batch [15/51] time 0.178 (0.271) data 0.000 (0.092) loss 0.2065 (0.3403) acc 96.0000 (92.8818) lr 1.9098e-04 eta 0:02:00
epoch [42/50] batch [20/51] time 0.182 (0.247) data 0.000 (0.069) loss 0.4537 (0.3464) acc 90.5660 (92.7664) lr 1.9098e-04 eta 0:01:48
epoch [42/50] batch [25/51] time 0.163 (0.235) data 0.000 (0.055) loss 0.4561 (0.3531) acc 89.3617 (92.5591) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [30/51] time 0.172 (0.225) data 0.001 (0.046) loss 0.2990 (0.3546) acc 95.5000 (92.4623) lr 1.9098e-04 eta 0:01:36
epoch [42/50] batch [35/51] time 0.183 (0.220) data 0.000 (0.040) loss 0.5332 (0.3610) acc 86.7021 (92.2827) lr 1.9098e-04 eta 0:01:33
epoch [42/50] batch [40/51] time 0.191 (0.215) data 0.000 (0.035) loss 0.4447 (0.3589) acc 91.2281 (92.4125) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [45/51] time 0.168 (0.210) data 0.000 (0.031) loss 0.2985 (0.3609) acc 96.5000 (92.3726) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [50/51] time 0.176 (0.206) data 0.000 (0.028) loss 0.4128 (0.3702) acc 90.5660 (92.1452) lr 1.9098e-04 eta 0:01:24
>>> alpha1: 0.132  alpha2: -0.116 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [43/50] batch [5/51] time 0.173 (0.476) data 0.000 (0.299) loss 0.5015 (0.3516) acc 90.3061 (92.9725) lr 1.5567e-04 eta 0:03:11
epoch [43/50] batch [10/51] time 0.178 (0.326) data 0.000 (0.150) loss 0.3257 (0.3527) acc 91.5094 (92.4273) lr 1.5567e-04 eta 0:02:09
epoch [43/50] batch [15/51] time 0.171 (0.275) data 0.000 (0.100) loss 0.4723 (0.3726) acc 88.2353 (91.8804) lr 1.5567e-04 eta 0:01:48
epoch [43/50] batch [20/51] time 0.173 (0.249) data 0.000 (0.075) loss 0.4901 (0.3714) acc 85.5000 (91.8587) lr 1.5567e-04 eta 0:01:36
epoch [43/50] batch [25/51] time 0.195 (0.236) data 0.000 (0.060) loss 0.3234 (0.3789) acc 91.8269 (91.4808) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.182 (0.226) data 0.000 (0.050) loss 0.3223 (0.3715) acc 93.5000 (91.7841) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.197 (0.220) data 0.000 (0.043) loss 0.4732 (0.3756) acc 92.6471 (91.8654) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [40/51] time 0.181 (0.215) data 0.000 (0.038) loss 0.4647 (0.3813) acc 89.9038 (91.7955) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.173 (0.210) data 0.000 (0.034) loss 0.4502 (0.3767) acc 89.9038 (91.8602) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.176 (0.206) data 0.000 (0.030) loss 0.3403 (0.3752) acc 89.1509 (91.7481) lr 1.5567e-04 eta 0:01:13
>>> alpha1: 0.133  alpha2: -0.117 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [44/50] batch [5/51] time 0.175 (0.491) data 0.000 (0.310) loss 0.4739 (0.4196) acc 89.1304 (90.6964) lr 1.2369e-04 eta 0:02:52
epoch [44/50] batch [10/51] time 0.189 (0.332) data 0.000 (0.155) loss 0.4342 (0.3943) acc 89.0000 (91.5812) lr 1.2369e-04 eta 0:01:55
epoch [44/50] batch [15/51] time 0.183 (0.281) data 0.000 (0.104) loss 0.5012 (0.3830) acc 86.7647 (91.8355) lr 1.2369e-04 eta 0:01:35
epoch [44/50] batch [20/51] time 0.182 (0.255) data 0.000 (0.078) loss 0.3761 (0.3952) acc 91.9811 (91.5678) lr 1.2369e-04 eta 0:01:25
epoch [44/50] batch [25/51] time 0.173 (0.240) data 0.000 (0.062) loss 0.4218 (0.4039) acc 89.9038 (91.2637) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [30/51] time 0.217 (0.229) data 0.000 (0.052) loss 0.2504 (0.3914) acc 93.3036 (91.1925) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [35/51] time 0.184 (0.223) data 0.000 (0.045) loss 0.4744 (0.3839) acc 88.7255 (91.4400) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.176 (0.218) data 0.000 (0.039) loss 0.4710 (0.3879) acc 90.5556 (91.3823) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.172 (0.212) data 0.001 (0.035) loss 0.3562 (0.3907) acc 92.7083 (91.3394) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [50/51] time 0.179 (0.208) data 0.000 (0.032) loss 0.2310 (0.3867) acc 96.3636 (91.5295) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.130  alpha2: -0.124 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [45/50] batch [5/51] time 0.180 (0.471) data 0.000 (0.289) loss 0.2152 (0.4289) acc 96.6981 (90.7184) lr 9.5173e-05 eta 0:02:21
epoch [45/50] batch [10/51] time 0.178 (0.325) data 0.000 (0.145) loss 0.3383 (0.3872) acc 93.0000 (92.1632) lr 9.5173e-05 eta 0:01:36
epoch [45/50] batch [15/51] time 0.170 (0.274) data 0.000 (0.096) loss 0.1985 (0.3873) acc 98.0000 (91.9764) lr 9.5173e-05 eta 0:01:19
epoch [45/50] batch [20/51] time 0.183 (0.250) data 0.001 (0.072) loss 0.5120 (0.3818) acc 86.4130 (92.0027) lr 9.5173e-05 eta 0:01:11
epoch [45/50] batch [25/51] time 0.170 (0.234) data 0.000 (0.058) loss 0.2572 (0.3876) acc 96.0784 (91.9047) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [30/51] time 0.166 (0.224) data 0.000 (0.048) loss 0.3376 (0.3754) acc 92.1875 (92.0828) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [35/51] time 0.164 (0.216) data 0.000 (0.042) loss 0.4756 (0.3667) acc 89.5833 (92.3368) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [40/51] time 0.169 (0.222) data 0.000 (0.036) loss 0.3396 (0.3660) acc 91.6667 (92.1983) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [45/51] time 0.178 (0.216) data 0.001 (0.032) loss 0.4522 (0.3676) acc 91.2037 (92.1007) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.177 (0.212) data 0.000 (0.029) loss 0.3839 (0.4087) acc 91.0377 (91.8001) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.128  alpha2: -0.122 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [46/50] batch [5/51] time 0.166 (0.490) data 0.000 (0.303) loss 0.4687 (0.3681) acc 90.2174 (91.6012) lr 7.0224e-05 eta 0:02:02
epoch [46/50] batch [10/51] time 0.178 (0.338) data 0.000 (0.152) loss 0.2781 (0.3436) acc 91.4894 (92.2879) lr 7.0224e-05 eta 0:01:22
epoch [46/50] batch [15/51] time 0.180 (0.285) data 0.000 (0.101) loss 0.5290 (0.3611) acc 87.5000 (92.0065) lr 7.0224e-05 eta 0:01:08
epoch [46/50] batch [20/51] time 0.180 (0.258) data 0.000 (0.076) loss 0.3026 (0.3604) acc 92.1296 (91.9977) lr 7.0224e-05 eta 0:01:00
epoch [46/50] batch [25/51] time 0.182 (0.242) data 0.000 (0.061) loss 0.3314 (0.3595) acc 92.6471 (91.7974) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.175 (0.231) data 0.000 (0.051) loss 0.4051 (0.3704) acc 92.0213 (91.5724) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.186 (0.223) data 0.000 (0.044) loss 0.3833 (0.3641) acc 95.0000 (91.7162) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [40/51] time 0.169 (0.217) data 0.000 (0.038) loss 0.4350 (0.3674) acc 89.0000 (91.6928) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.165 (0.211) data 0.000 (0.034) loss 0.3900 (0.3726) acc 94.7917 (91.5709) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.165 (0.207) data 0.000 (0.031) loss 0.2956 (0.3674) acc 95.7447 (91.7788) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.128  alpha2: -0.115 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [47/50] batch [5/51] time 0.168 (0.614) data 0.000 (0.265) loss 0.3205 (0.3100) acc 96.3542 (93.4270) lr 4.8943e-05 eta 0:02:02
epoch [47/50] batch [10/51] time 0.182 (0.396) data 0.000 (0.133) loss 0.4577 (0.3378) acc 88.0208 (92.7812) lr 4.8943e-05 eta 0:01:16
epoch [47/50] batch [15/51] time 0.182 (0.325) data 0.000 (0.089) loss 0.3716 (0.3390) acc 91.1458 (92.5932) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [20/51] time 0.157 (0.287) data 0.000 (0.066) loss 0.3551 (0.3433) acc 96.5909 (92.4736) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [25/51] time 0.182 (0.264) data 0.000 (0.053) loss 0.5204 (0.3464) acc 90.3061 (92.5933) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [30/51] time 0.193 (0.250) data 0.000 (0.044) loss 0.3176 (0.3464) acc 96.0784 (92.5678) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [35/51] time 0.169 (0.240) data 0.001 (0.038) loss 0.2927 (0.3873) acc 93.8775 (92.2833) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [40/51] time 0.177 (0.232) data 0.000 (0.033) loss 0.4130 (0.3940) acc 89.1509 (91.8888) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [45/51] time 0.173 (0.224) data 0.001 (0.030) loss 0.3999 (0.3931) acc 89.2157 (91.8773) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [50/51] time 0.158 (0.219) data 0.000 (0.027) loss 0.4403 (0.3931) acc 89.4445 (91.7868) lr 4.8943e-05 eta 0:00:33
>>> alpha1: 0.128  alpha2: -0.117 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [48/50] batch [5/51] time 0.163 (0.510) data 0.000 (0.334) loss 0.3490 (0.3894) acc 94.6808 (90.4243) lr 3.1417e-05 eta 0:01:15
epoch [48/50] batch [10/51] time 0.189 (0.347) data 0.000 (0.167) loss 0.1757 (0.3468) acc 97.0000 (92.0762) lr 3.1417e-05 eta 0:00:49
epoch [48/50] batch [15/51] time 0.181 (0.290) data 0.000 (0.112) loss 0.4783 (0.3760) acc 91.3462 (91.4839) lr 3.1417e-05 eta 0:00:40
epoch [48/50] batch [20/51] time 0.178 (0.262) data 0.001 (0.084) loss 0.5275 (0.3791) acc 85.8491 (91.0207) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.173 (0.246) data 0.000 (0.067) loss 0.3119 (0.3647) acc 94.3878 (91.6616) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.185 (0.235) data 0.001 (0.056) loss 0.3042 (0.3670) acc 96.1538 (91.7324) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.188 (0.229) data 0.000 (0.049) loss 0.2648 (0.3691) acc 95.1923 (91.7676) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.173 (0.223) data 0.000 (0.043) loss 0.3113 (0.3782) acc 92.1569 (91.5623) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [45/51] time 0.179 (0.217) data 0.000 (0.038) loss 0.5012 (0.3808) acc 86.7647 (91.4058) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.172 (0.213) data 0.000 (0.034) loss 0.3625 (0.3903) acc 94.6078 (91.2690) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.128  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [49/50] batch [5/51] time 0.195 (0.468) data 0.000 (0.279) loss 0.3438 (0.3876) acc 92.1296 (90.8797) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.181 (0.327) data 0.000 (0.139) loss 0.3755 (0.3724) acc 91.5094 (91.3861) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.177 (0.275) data 0.000 (0.093) loss 0.4022 (0.3770) acc 92.6471 (91.3681) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.172 (0.250) data 0.000 (0.070) loss 0.3694 (0.3642) acc 89.2157 (91.6623) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.167 (0.235) data 0.000 (0.056) loss 0.4096 (0.3614) acc 93.2292 (91.9126) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.188 (0.226) data 0.000 (0.047) loss 0.4009 (0.3620) acc 90.3846 (91.8857) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.169 (0.218) data 0.000 (0.040) loss 0.4243 (0.3585) acc 92.0000 (92.0689) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.173 (0.213) data 0.000 (0.035) loss 0.4693 (0.3618) acc 91.8269 (92.0955) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.173 (0.208) data 0.000 (0.031) loss 0.3669 (0.3620) acc 93.7500 (92.1878) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.180 (0.204) data 0.000 (0.028) loss 0.4785 (0.3668) acc 86.5741 (92.0358) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.128  alpha2: -0.113 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [50/50] batch [5/51] time 0.176 (0.458) data 0.000 (0.281) loss 0.3657 (0.3324) acc 94.2308 (93.5302) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.172 (0.319) data 0.001 (0.141) loss 0.3951 (0.3576) acc 91.6667 (92.6389) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.181 (0.275) data 0.001 (0.094) loss 0.2277 (0.3431) acc 97.5962 (92.9386) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.190 (0.258) data 0.000 (0.071) loss 0.4426 (0.3519) acc 90.7609 (92.8070) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.174 (0.245) data 0.000 (0.057) loss 0.3298 (0.3559) acc 94.3878 (92.5450) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.184 (0.234) data 0.000 (0.047) loss 0.4518 (0.3717) acc 87.7358 (91.9561) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.181 (0.226) data 0.001 (0.041) loss 0.3082 (0.3679) acc 92.3469 (92.0510) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.168 (0.219) data 0.000 (0.036) loss 0.2844 (0.3703) acc 94.0000 (91.9786) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.184 (0.214) data 0.000 (0.032) loss 0.2254 (0.3728) acc 95.1923 (91.9702) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.163 (0.209) data 0.000 (0.028) loss 0.3391 (0.3695) acc 93.0851 (92.1161) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.05, 0.05, 0.05, 0.04, 0.04, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
* matched noise rate: [0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]
* unmatched noise rate: [0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.11, 0.11, 0.11, 0.11, 0.1, 0.1, 0.09, 0.1, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09, 0.1, 0.09, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:03,  2.65s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.34it/s] 16%|█▌        | 4/25 [00:02<00:10,  1.91it/s] 20%|██        | 5/25 [00:03<00:07,  2.56it/s] 24%|██▍       | 6/25 [00:03<00:05,  3.34it/s] 32%|███▏      | 8/25 [00:03<00:03,  5.12it/s] 40%|████      | 10/25 [00:03<00:02,  6.72it/s] 48%|████▊     | 12/25 [00:03<00:01,  8.08it/s] 56%|█████▌    | 14/25 [00:03<00:01,  9.18it/s] 64%|██████▍   | 16/25 [00:03<00:00, 10.02it/s] 72%|███████▏  | 18/25 [00:04<00:00,  7.10it/s] 80%|████████  | 20/25 [00:04<00:00,  8.20it/s] 88%|████████▊ | 22/25 [00:04<00:00,  9.16it/s] 96%|█████████▌| 24/25 [00:04<00:00,  9.86it/s]100%|██████████| 25/25 [00:05<00:00,  4.60it/s]
=> result
* total: 2,463
* correct: 2,233
* accuracy: 90.7%
* error: 9.3%
* macro_f1: 89.3%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 5	acc: 41.7%
* class: 3 (sweet pea)	total: 17	correct: 12	acc: 70.6%
* class: 4 (english marigold)	total: 20	correct: 12	acc: 60.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 24	acc: 92.3%
* class: 11 (colt's foot)	total: 26	correct: 22	acc: 84.6%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 13	acc: 100.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 20	acc: 80.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 16	acc: 94.1%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 18	acc: 90.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 22	acc: 84.6%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 13	acc: 92.9%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 11	acc: 91.7%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 19	acc: 86.4%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 2	acc: 15.4%
* class: 39 (lenten rose)	total: 20	correct: 20	acc: 100.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 17	acc: 43.6%
* class: 43 (poinsettia)	total: 28	correct: 27	acc: 96.4%
* class: 44 (bolero deep blue)	total: 12	correct: 3	acc: 25.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 65	acc: 84.4%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 18	acc: 94.7%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 19	acc: 95.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 15	acc: 93.8%
* class: 64 (californian poppy)	total: 31	correct: 30	acc: 96.8%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 11	acc: 84.6%
* class: 67 (bearded iris)	total: 16	correct: 11	acc: 68.8%
* class: 68 (windflower)	total: 16	correct: 14	acc: 87.5%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 20	acc: 87.0%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 49	acc: 96.1%
* class: 74 (thorn apple)	total: 36	correct: 36	acc: 100.0%
* class: 75 (morning glory)	total: 32	correct: 27	acc: 84.4%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 34	acc: 100.0%
* class: 82 (hibiscus)	total: 39	correct: 37	acc: 94.9%
* class: 83 (columbine)	total: 26	correct: 25	acc: 96.2%
* class: 84 (desert-rose)	total: 18	correct: 14	acc: 77.8%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 35	acc: 76.1%
* class: 88 (watercress)	total: 55	correct: 42	acc: 76.4%
* class: 89 (canna lily)	total: 25	correct: 22	acc: 88.0%
* class: 90 (hippeastrum)	total: 23	correct: 18	acc: 78.3%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 34	acc: 89.5%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 19	acc: 95.0%
* class: 97 (mexican petunia)	total: 25	correct: 22	acc: 88.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 14	acc: 82.4%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 90.4%
Elapsed: 0:28:08
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '2', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_2-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 2
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.269 (1.083) data 0.000 (0.351) loss 4.2704 (4.4087) acc 6.2500 (6.2500) lr 1.0000e-05 eta 0:45:55
epoch [1/50] batch [10/51] time 0.260 (0.671) data 0.000 (0.176) loss 4.0867 (4.2878) acc 12.5000 (7.8125) lr 1.0000e-05 eta 0:28:25
epoch [1/50] batch [15/51] time 0.270 (0.535) data 0.000 (0.117) loss 4.0864 (4.2239) acc 9.3750 (8.7500) lr 1.0000e-05 eta 0:22:36
epoch [1/50] batch [20/51] time 0.278 (0.468) data 0.000 (0.088) loss 3.7825 (4.1569) acc 15.6250 (10.0000) lr 1.0000e-05 eta 0:19:44
epoch [1/50] batch [25/51] time 0.258 (0.427) data 0.000 (0.070) loss 3.4719 (4.1085) acc 18.7500 (11.3750) lr 1.0000e-05 eta 0:17:58
epoch [1/50] batch [30/51] time 0.257 (0.399) data 0.000 (0.059) loss 3.5123 (4.0417) acc 28.1250 (13.9583) lr 1.0000e-05 eta 0:16:45
epoch [1/50] batch [35/51] time 0.256 (0.379) data 0.000 (0.050) loss 2.8900 (3.9535) acc 40.6250 (17.2321) lr 1.0000e-05 eta 0:15:53
epoch [1/50] batch [40/51] time 0.258 (0.364) data 0.000 (0.044) loss 2.4318 (3.8159) acc 46.8750 (20.0781) lr 1.0000e-05 eta 0:15:14
epoch [1/50] batch [45/51] time 0.258 (0.352) data 0.000 (0.039) loss 3.1090 (3.7382) acc 37.5000 (21.8750) lr 1.0000e-05 eta 0:14:42
epoch [1/50] batch [50/51] time 0.259 (0.343) data 0.000 (0.035) loss 2.8376 (3.6595) acc 40.6250 (23.7500) lr 1.0000e-05 eta 0:14:17
epoch [2/50] batch [5/51] time 0.274 (0.610) data 0.000 (0.339) loss 2.8109 (2.7424) acc 56.2500 (50.6250) lr 2.0000e-03 eta 0:25:21
epoch [2/50] batch [10/51] time 0.261 (0.438) data 0.000 (0.169) loss 2.8052 (2.9199) acc 40.6250 (47.5000) lr 2.0000e-03 eta 0:18:10
epoch [2/50] batch [15/51] time 0.267 (0.381) data 0.000 (0.113) loss 2.6307 (2.7264) acc 56.2500 (50.6250) lr 2.0000e-03 eta 0:15:46
epoch [2/50] batch [20/51] time 0.269 (0.353) data 0.000 (0.085) loss 2.6339 (2.7140) acc 56.2500 (50.3125) lr 2.0000e-03 eta 0:14:35
epoch [2/50] batch [25/51] time 0.266 (0.335) data 0.000 (0.068) loss 2.6707 (2.6889) acc 53.1250 (51.1250) lr 2.0000e-03 eta 0:13:49
epoch [2/50] batch [30/51] time 0.262 (0.324) data 0.000 (0.057) loss 2.6492 (2.6610) acc 50.0000 (51.0417) lr 2.0000e-03 eta 0:13:18
epoch [2/50] batch [35/51] time 0.260 (0.316) data 0.000 (0.049) loss 2.3425 (2.6046) acc 50.0000 (51.8750) lr 2.0000e-03 eta 0:12:59
epoch [2/50] batch [40/51] time 0.257 (0.309) data 0.000 (0.043) loss 2.0880 (2.5809) acc 56.2500 (52.3438) lr 2.0000e-03 eta 0:12:40
epoch [2/50] batch [45/51] time 0.257 (0.303) data 0.000 (0.038) loss 2.4445 (2.5520) acc 53.1250 (52.6389) lr 2.0000e-03 eta 0:12:24
epoch [2/50] batch [50/51] time 0.257 (0.299) data 0.000 (0.034) loss 2.2209 (2.4915) acc 62.5000 (53.2500) lr 2.0000e-03 eta 0:12:11
epoch [3/50] batch [5/51] time 0.281 (0.617) data 0.000 (0.331) loss 2.2201 (2.0809) acc 56.2500 (58.7500) lr 1.9980e-03 eta 0:25:06
epoch [3/50] batch [10/51] time 0.260 (0.442) data 0.000 (0.166) loss 2.0690 (2.3899) acc 53.1250 (55.6250) lr 1.9980e-03 eta 0:17:57
epoch [3/50] batch [15/51] time 0.273 (0.384) data 0.000 (0.111) loss 2.1104 (2.3242) acc 56.2500 (55.2083) lr 1.9980e-03 eta 0:15:34
epoch [3/50] batch [20/51] time 0.265 (0.355) data 0.000 (0.083) loss 2.5768 (2.3008) acc 56.2500 (55.9375) lr 1.9980e-03 eta 0:14:20
epoch [3/50] batch [25/51] time 0.263 (0.336) data 0.000 (0.067) loss 1.8875 (2.2472) acc 59.3750 (57.5000) lr 1.9980e-03 eta 0:13:34
epoch [3/50] batch [30/51] time 0.271 (0.325) data 0.000 (0.055) loss 2.5370 (2.2522) acc 46.8750 (56.5625) lr 1.9980e-03 eta 0:13:05
epoch [3/50] batch [35/51] time 0.259 (0.316) data 0.000 (0.048) loss 1.6359 (2.2585) acc 81.2500 (57.5000) lr 1.9980e-03 eta 0:12:42
epoch [3/50] batch [40/51] time 0.259 (0.309) data 0.000 (0.042) loss 2.0478 (2.2324) acc 62.5000 (58.0469) lr 1.9980e-03 eta 0:12:23
epoch [3/50] batch [45/51] time 0.262 (0.303) data 0.000 (0.037) loss 4.0691 (2.2717) acc 37.5000 (58.2639) lr 1.9980e-03 eta 0:12:08
epoch [3/50] batch [50/51] time 0.261 (0.299) data 0.000 (0.033) loss 1.7110 (2.2465) acc 62.5000 (58.8750) lr 1.9980e-03 eta 0:11:57
epoch [4/50] batch [5/51] time 0.302 (0.599) data 0.000 (0.319) loss 1.2792 (1.6649) acc 71.8750 (68.1250) lr 1.9921e-03 eta 0:23:53
epoch [4/50] batch [10/51] time 0.272 (0.435) data 0.000 (0.160) loss 1.9967 (1.7723) acc 65.6250 (66.8750) lr 1.9921e-03 eta 0:17:19
epoch [4/50] batch [15/51] time 0.271 (0.379) data 0.000 (0.107) loss 2.2845 (1.9531) acc 56.2500 (65.0000) lr 1.9921e-03 eta 0:15:02
epoch [4/50] batch [20/51] time 0.272 (0.352) data 0.000 (0.080) loss 2.2941 (2.0487) acc 56.2500 (63.4375) lr 1.9921e-03 eta 0:13:55
epoch [4/50] batch [25/51] time 0.263 (0.335) data 0.000 (0.064) loss 2.3205 (2.1274) acc 65.6250 (61.8750) lr 1.9921e-03 eta 0:13:13
epoch [4/50] batch [30/51] time 0.267 (0.323) data 0.000 (0.053) loss 2.0881 (2.1473) acc 71.8750 (62.5000) lr 1.9921e-03 eta 0:12:44
epoch [4/50] batch [35/51] time 0.279 (0.315) data 0.000 (0.046) loss 2.3169 (2.1254) acc 56.2500 (63.2143) lr 1.9921e-03 eta 0:12:24
epoch [4/50] batch [40/51] time 0.262 (0.309) data 0.000 (0.040) loss 2.4734 (2.1119) acc 59.3750 (63.2031) lr 1.9921e-03 eta 0:12:07
epoch [4/50] batch [45/51] time 0.260 (0.303) data 0.000 (0.036) loss 1.4188 (2.1092) acc 75.0000 (63.0556) lr 1.9921e-03 eta 0:11:53
epoch [4/50] batch [50/51] time 0.258 (0.299) data 0.000 (0.032) loss 3.1359 (2.0878) acc 53.1250 (63.5625) lr 1.9921e-03 eta 0:11:41
epoch [5/50] batch [5/51] time 0.277 (0.604) data 0.000 (0.316) loss 2.1351 (2.2049) acc 68.7500 (60.0000) lr 1.9823e-03 eta 0:23:34
epoch [5/50] batch [10/51] time 0.263 (0.435) data 0.000 (0.158) loss 1.8281 (2.1023) acc 71.8750 (63.4375) lr 1.9823e-03 eta 0:16:56
epoch [5/50] batch [15/51] time 0.262 (0.380) data 0.000 (0.106) loss 1.9378 (2.0951) acc 68.7500 (63.5417) lr 1.9823e-03 eta 0:14:45
epoch [5/50] batch [20/51] time 0.267 (0.353) data 0.001 (0.079) loss 1.9579 (2.1035) acc 71.8750 (65.1562) lr 1.9823e-03 eta 0:13:40
epoch [5/50] batch [25/51] time 0.272 (0.335) data 0.000 (0.063) loss 1.1789 (2.0238) acc 78.1250 (66.2500) lr 1.9823e-03 eta 0:12:57
epoch [5/50] batch [30/51] time 0.267 (0.324) data 0.000 (0.053) loss 1.1192 (1.9235) acc 84.3750 (68.2292) lr 1.9823e-03 eta 0:12:29
epoch [5/50] batch [35/51] time 0.259 (0.315) data 0.000 (0.045) loss 1.6702 (1.9541) acc 78.1250 (67.7679) lr 1.9823e-03 eta 0:12:08
epoch [5/50] batch [40/51] time 0.256 (0.309) data 0.000 (0.040) loss 1.8008 (1.9729) acc 71.8750 (67.5781) lr 1.9823e-03 eta 0:11:51
epoch [5/50] batch [45/51] time 0.257 (0.303) data 0.000 (0.035) loss 1.8207 (1.9830) acc 65.6250 (67.2917) lr 1.9823e-03 eta 0:11:36
epoch [5/50] batch [50/51] time 0.256 (0.298) data 0.000 (0.032) loss 1.8372 (1.9945) acc 75.0000 (67.0625) lr 1.9823e-03 eta 0:11:24
epoch [6/50] batch [5/51] time 0.271 (0.562) data 0.000 (0.277) loss 1.3318 (1.5822) acc 75.0000 (76.8750) lr 1.9686e-03 eta 0:21:27
epoch [6/50] batch [10/51] time 0.259 (0.413) data 0.000 (0.139) loss 2.4746 (1.7680) acc 68.7500 (73.1250) lr 1.9686e-03 eta 0:15:44
epoch [6/50] batch [15/51] time 0.263 (0.364) data 0.000 (0.093) loss 1.3241 (1.7749) acc 81.2500 (72.7083) lr 1.9686e-03 eta 0:13:49
epoch [6/50] batch [20/51] time 0.264 (0.339) data 0.000 (0.070) loss 1.3929 (1.8071) acc 68.7500 (69.8438) lr 1.9686e-03 eta 0:12:51
epoch [6/50] batch [25/51] time 0.263 (0.325) data 0.000 (0.056) loss 2.2724 (1.8818) acc 53.1250 (68.0000) lr 1.9686e-03 eta 0:12:16
epoch [6/50] batch [30/51] time 0.259 (0.315) data 0.000 (0.046) loss 1.4245 (1.9268) acc 78.1250 (67.3958) lr 1.9686e-03 eta 0:11:52
epoch [6/50] batch [35/51] time 0.259 (0.307) data 0.000 (0.040) loss 1.4958 (1.8491) acc 78.1250 (68.6607) lr 1.9686e-03 eta 0:11:34
epoch [6/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.035) loss 2.8632 (1.8752) acc 59.3750 (68.0469) lr 1.9686e-03 eta 0:11:20
epoch [6/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.031) loss 2.2847 (1.8716) acc 59.3750 (68.1250) lr 1.9686e-03 eta 0:11:08
epoch [6/50] batch [50/51] time 0.259 (0.293) data 0.000 (0.028) loss 1.7968 (1.8653) acc 71.8750 (68.2500) lr 1.9686e-03 eta 0:10:58
epoch [7/50] batch [5/51] time 0.259 (0.592) data 0.000 (0.299) loss 1.8458 (2.0526) acc 75.0000 (66.2500) lr 1.9511e-03 eta 0:22:05
epoch [7/50] batch [10/51] time 0.258 (0.428) data 0.000 (0.150) loss 2.2943 (1.8421) acc 75.0000 (72.1875) lr 1.9511e-03 eta 0:15:55
epoch [7/50] batch [15/51] time 0.267 (0.374) data 0.000 (0.100) loss 2.4434 (1.8445) acc 62.5000 (71.2500) lr 1.9511e-03 eta 0:13:52
epoch [7/50] batch [20/51] time 0.268 (0.348) data 0.000 (0.075) loss 1.7381 (1.8038) acc 59.3750 (71.0938) lr 1.9511e-03 eta 0:12:52
epoch [7/50] batch [25/51] time 0.258 (0.330) data 0.000 (0.060) loss 1.4752 (1.7760) acc 75.0000 (71.3750) lr 1.9511e-03 eta 0:12:12
epoch [7/50] batch [30/51] time 0.259 (0.319) data 0.000 (0.050) loss 2.6516 (1.8241) acc 71.8750 (71.1458) lr 1.9511e-03 eta 0:11:46
epoch [7/50] batch [35/51] time 0.259 (0.312) data 0.000 (0.043) loss 2.2518 (1.8450) acc 68.7500 (70.9821) lr 1.9511e-03 eta 0:11:28
epoch [7/50] batch [40/51] time 0.258 (0.305) data 0.000 (0.038) loss 1.6287 (1.8437) acc 71.8750 (70.9375) lr 1.9511e-03 eta 0:11:12
epoch [7/50] batch [45/51] time 0.260 (0.300) data 0.000 (0.033) loss 1.6983 (1.8389) acc 68.7500 (70.5556) lr 1.9511e-03 eta 0:10:59
epoch [7/50] batch [50/51] time 0.259 (0.296) data 0.000 (0.030) loss 0.8249 (1.7865) acc 87.5000 (71.6250) lr 1.9511e-03 eta 0:10:49
epoch [8/50] batch [5/51] time 0.287 (0.569) data 0.001 (0.277) loss 1.4370 (1.5458) acc 71.8750 (73.7500) lr 1.9298e-03 eta 0:20:44
epoch [8/50] batch [10/51] time 0.275 (0.420) data 0.000 (0.139) loss 1.4869 (1.5867) acc 84.3750 (76.2500) lr 1.9298e-03 eta 0:15:16
epoch [8/50] batch [15/51] time 0.270 (0.369) data 0.000 (0.093) loss 1.9424 (1.5809) acc 65.6250 (75.6250) lr 1.9298e-03 eta 0:13:23
epoch [8/50] batch [20/51] time 0.271 (0.342) data 0.000 (0.070) loss 2.0027 (1.6327) acc 81.2500 (74.5312) lr 1.9298e-03 eta 0:12:23
epoch [8/50] batch [25/51] time 0.271 (0.327) data 0.000 (0.056) loss 1.8659 (1.6651) acc 78.1250 (75.0000) lr 1.9298e-03 eta 0:11:48
epoch [8/50] batch [30/51] time 0.270 (0.316) data 0.000 (0.046) loss 2.4051 (1.6828) acc 65.6250 (74.3750) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [35/51] time 0.269 (0.309) data 0.000 (0.040) loss 1.6532 (1.6998) acc 68.7500 (73.9286) lr 1.9298e-03 eta 0:11:07
epoch [8/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.035) loss 1.7258 (1.7028) acc 65.6250 (73.7500) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [45/51] time 0.258 (0.298) data 0.000 (0.031) loss 2.3349 (1.7157) acc 65.6250 (73.8194) lr 1.9298e-03 eta 0:10:40
epoch [8/50] batch [50/51] time 0.257 (0.294) data 0.000 (0.028) loss 0.9473 (1.7067) acc 84.3750 (73.7500) lr 1.9298e-03 eta 0:10:30
epoch [9/50] batch [5/51] time 0.292 (0.568) data 0.000 (0.274) loss 1.6885 (1.5374) acc 71.8750 (76.8750) lr 1.9048e-03 eta 0:20:13
epoch [9/50] batch [10/51] time 0.268 (0.416) data 0.000 (0.137) loss 2.2081 (1.6002) acc 68.7500 (76.5625) lr 1.9048e-03 eta 0:14:46
epoch [9/50] batch [15/51] time 0.259 (0.367) data 0.000 (0.092) loss 1.1887 (1.4852) acc 78.1250 (77.7083) lr 1.9048e-03 eta 0:12:59
epoch [9/50] batch [20/51] time 0.265 (0.341) data 0.000 (0.069) loss 1.5507 (1.5318) acc 81.2500 (77.8125) lr 1.9048e-03 eta 0:12:04
epoch [9/50] batch [25/51] time 0.275 (0.328) data 0.000 (0.055) loss 1.7864 (1.5524) acc 75.0000 (77.3750) lr 1.9048e-03 eta 0:11:33
epoch [9/50] batch [30/51] time 0.278 (0.318) data 0.000 (0.046) loss 1.8600 (1.5808) acc 78.1250 (76.8750) lr 1.9048e-03 eta 0:11:12
epoch [9/50] batch [35/51] time 0.260 (0.311) data 0.000 (0.040) loss 1.2733 (1.5715) acc 93.7500 (77.6786) lr 1.9048e-03 eta 0:10:54
epoch [9/50] batch [40/51] time 0.259 (0.304) data 0.000 (0.035) loss 2.1380 (1.5813) acc 71.8750 (77.5000) lr 1.9048e-03 eta 0:10:39
epoch [9/50] batch [45/51] time 0.262 (0.299) data 0.000 (0.031) loss 2.0065 (1.6271) acc 68.7500 (76.5278) lr 1.9048e-03 eta 0:10:27
epoch [9/50] batch [50/51] time 0.256 (0.295) data 0.000 (0.028) loss 1.3541 (1.6297) acc 78.1250 (76.1875) lr 1.9048e-03 eta 0:10:17
epoch [10/50] batch [5/51] time 0.263 (0.602) data 0.000 (0.302) loss 1.1578 (1.4451) acc 84.3750 (77.5000) lr 1.8763e-03 eta 0:20:56
epoch [10/50] batch [10/51] time 0.272 (0.443) data 0.000 (0.151) loss 1.3831 (1.4668) acc 75.0000 (78.7500) lr 1.8763e-03 eta 0:15:21
epoch [10/50] batch [15/51] time 0.274 (0.385) data 0.014 (0.102) loss 1.5576 (1.5327) acc 65.6250 (76.8750) lr 1.8763e-03 eta 0:13:19
epoch [10/50] batch [20/51] time 0.260 (0.355) data 0.000 (0.076) loss 1.1818 (1.5071) acc 81.2500 (77.0312) lr 1.8763e-03 eta 0:12:14
epoch [10/50] batch [25/51] time 0.261 (0.336) data 0.000 (0.061) loss 1.1101 (1.5224) acc 84.3750 (77.3750) lr 1.8763e-03 eta 0:11:34
epoch [10/50] batch [30/51] time 0.265 (0.326) data 0.000 (0.051) loss 1.7568 (1.5952) acc 68.7500 (76.4583) lr 1.8763e-03 eta 0:11:10
epoch [10/50] batch [35/51] time 0.260 (0.317) data 0.000 (0.044) loss 1.6363 (1.6310) acc 78.1250 (75.6250) lr 1.8763e-03 eta 0:10:52
epoch [10/50] batch [40/51] time 0.258 (0.310) data 0.000 (0.039) loss 2.6408 (1.6380) acc 59.3750 (75.3906) lr 1.8763e-03 eta 0:10:36
epoch [10/50] batch [45/51] time 0.258 (0.304) data 0.000 (0.034) loss 1.9557 (1.5998) acc 75.0000 (76.1111) lr 1.8763e-03 eta 0:10:22
epoch [10/50] batch [50/51] time 0.258 (0.300) data 0.000 (0.031) loss 1.7153 (1.5881) acc 78.1250 (76.3125) lr 1.8763e-03 eta 0:10:11
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.341  alpha2: -0.011 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.05 <<<
epoch [11/50] batch [5/51] time 0.825 (0.972) data 0.000 (0.316) loss 0.8562 (0.9519) acc 79.0816 (78.6091) lr 1.8443e-03 eta 0:32:57
epoch [11/50] batch [10/51] time 0.724 (0.693) data 0.000 (0.158) loss 1.0219 (0.9446) acc 73.8372 (78.3383) lr 1.8443e-03 eta 0:23:26
epoch [11/50] batch [15/51] time 0.161 (0.605) data 0.000 (0.105) loss 0.6947 (0.9240) acc 81.3830 (78.2991) lr 1.8443e-03 eta 0:20:24
epoch [11/50] batch [20/51] time 0.160 (0.526) data 0.000 (0.079) loss 1.0437 (0.9165) acc 75.5435 (78.9755) lr 1.8443e-03 eta 0:17:41
epoch [11/50] batch [25/51] time 0.163 (0.453) data 0.000 (0.063) loss 0.7329 (0.9065) acc 82.4468 (79.2300) lr 1.8443e-03 eta 0:15:13
epoch [11/50] batch [30/51] time 0.158 (0.406) data 0.000 (0.053) loss 0.8143 (0.9084) acc 82.7778 (79.2377) lr 1.8443e-03 eta 0:13:35
epoch [11/50] batch [35/51] time 0.840 (0.391) data 0.000 (0.045) loss 0.9227 (0.9169) acc 78.0000 (78.8513) lr 1.8443e-03 eta 0:13:04
epoch [11/50] batch [40/51] time 0.171 (0.381) data 0.000 (0.040) loss 1.2094 (0.9036) acc 69.6078 (79.1817) lr 1.8443e-03 eta 0:12:41
epoch [11/50] batch [45/51] time 0.167 (0.357) data 0.000 (0.035) loss 0.6935 (0.9037) acc 87.7660 (79.1701) lr 1.8443e-03 eta 0:11:53
epoch [11/50] batch [50/51] time 0.154 (0.338) data 0.000 (0.032) loss 0.8469 (0.8998) acc 79.0698 (79.2173) lr 1.8443e-03 eta 0:11:12
>>> alpha1: 0.287  alpha2: -0.044 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [12/50] batch [5/51] time 0.166 (0.515) data 0.000 (0.326) loss 0.7929 (0.7904) acc 82.8125 (82.8050) lr 1.8090e-03 eta 0:17:00
epoch [12/50] batch [10/51] time 0.915 (0.426) data 0.001 (0.163) loss 0.5914 (0.7811) acc 86.3636 (82.7929) lr 1.8090e-03 eta 0:14:02
epoch [12/50] batch [15/51] time 0.189 (0.342) data 0.000 (0.109) loss 0.7850 (0.7669) acc 80.7692 (82.7797) lr 1.8090e-03 eta 0:11:15
epoch [12/50] batch [20/51] time 0.166 (0.337) data 0.000 (0.082) loss 0.6738 (0.7508) acc 83.3333 (82.4096) lr 1.8090e-03 eta 0:11:04
epoch [12/50] batch [25/51] time 0.194 (0.305) data 0.000 (0.066) loss 0.5755 (0.7687) acc 84.8958 (81.4934) lr 1.8090e-03 eta 0:09:58
epoch [12/50] batch [30/51] time 0.182 (0.284) data 0.002 (0.055) loss 0.6564 (0.7630) acc 83.1633 (81.6464) lr 1.8090e-03 eta 0:09:16
epoch [12/50] batch [35/51] time 0.179 (0.270) data 0.000 (0.047) loss 0.5880 (0.7578) acc 89.6739 (81.7192) lr 1.8090e-03 eta 0:08:47
epoch [12/50] batch [40/51] time 0.165 (0.258) data 0.000 (0.042) loss 0.6079 (0.7517) acc 85.9375 (81.8919) lr 1.8090e-03 eta 0:08:23
epoch [12/50] batch [45/51] time 0.177 (0.249) data 0.000 (0.037) loss 0.6492 (0.7490) acc 86.3208 (81.8946) lr 1.8090e-03 eta 0:08:03
epoch [12/50] batch [50/51] time 0.167 (0.240) data 0.000 (0.033) loss 0.7289 (0.7555) acc 79.0000 (81.6246) lr 1.8090e-03 eta 0:07:45
>>> alpha1: 0.260  alpha2: -0.068 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [13/50] batch [5/51] time 0.164 (0.476) data 0.000 (0.300) loss 0.5654 (0.7069) acc 88.0435 (83.4077) lr 1.7705e-03 eta 0:15:20
epoch [13/50] batch [10/51] time 0.181 (0.324) data 0.000 (0.150) loss 0.8885 (0.7157) acc 84.3137 (83.4919) lr 1.7705e-03 eta 0:10:25
epoch [13/50] batch [15/51] time 0.176 (0.275) data 0.000 (0.100) loss 0.9275 (0.7156) acc 76.9231 (82.7047) lr 1.7705e-03 eta 0:08:49
epoch [13/50] batch [20/51] time 0.179 (0.250) data 0.000 (0.075) loss 0.5016 (0.6973) acc 87.7660 (83.1499) lr 1.7705e-03 eta 0:07:59
epoch [13/50] batch [25/51] time 0.176 (0.265) data 0.000 (0.060) loss 0.9309 (0.7038) acc 77.7778 (83.1864) lr 1.7705e-03 eta 0:08:27
epoch [13/50] batch [30/51] time 0.177 (0.251) data 0.000 (0.050) loss 0.8140 (0.7010) acc 78.5000 (83.1453) lr 1.7705e-03 eta 0:07:59
epoch [13/50] batch [35/51] time 0.179 (0.243) data 0.000 (0.043) loss 0.6102 (0.6952) acc 85.5769 (83.2629) lr 1.7705e-03 eta 0:07:41
epoch [13/50] batch [40/51] time 0.165 (0.233) data 0.000 (0.038) loss 0.9332 (0.7118) acc 78.6458 (82.7576) lr 1.7705e-03 eta 0:07:22
epoch [13/50] batch [45/51] time 0.157 (0.226) data 0.000 (0.034) loss 0.7699 (0.7045) acc 80.5556 (82.9734) lr 1.7705e-03 eta 0:07:07
epoch [13/50] batch [50/51] time 0.170 (0.220) data 0.000 (0.030) loss 0.6486 (0.7028) acc 85.2941 (83.0371) lr 1.7705e-03 eta 0:06:55
>>> alpha1: 0.245  alpha2: -0.071 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [14/50] batch [5/51] time 0.187 (0.557) data 0.000 (0.381) loss 0.6380 (0.6379) acc 82.0175 (84.3870) lr 1.7290e-03 eta 0:17:27
epoch [14/50] batch [10/51] time 0.171 (0.439) data 0.000 (0.191) loss 0.6604 (0.6463) acc 83.8235 (84.8734) lr 1.7290e-03 eta 0:13:43
epoch [14/50] batch [15/51] time 0.181 (0.355) data 0.000 (0.127) loss 0.5850 (0.6293) acc 89.8148 (85.4918) lr 1.7290e-03 eta 0:11:04
epoch [14/50] batch [20/51] time 0.164 (0.310) data 0.000 (0.095) loss 1.0158 (0.6663) acc 72.8723 (84.1459) lr 1.7290e-03 eta 0:09:39
epoch [14/50] batch [25/51] time 0.176 (0.284) data 0.000 (0.076) loss 0.8185 (0.7668) acc 83.3333 (82.7875) lr 1.7290e-03 eta 0:08:49
epoch [14/50] batch [30/51] time 0.214 (0.269) data 0.001 (0.064) loss 0.7236 (0.7565) acc 84.8039 (82.7421) lr 1.7290e-03 eta 0:08:18
epoch [14/50] batch [35/51] time 0.170 (0.257) data 0.000 (0.055) loss 0.6477 (0.7358) acc 84.5745 (83.1649) lr 1.7290e-03 eta 0:07:56
epoch [14/50] batch [40/51] time 0.156 (0.248) data 0.000 (0.048) loss 0.7552 (0.7153) acc 80.1136 (83.6056) lr 1.7290e-03 eta 0:07:37
epoch [14/50] batch [45/51] time 0.164 (0.239) data 0.000 (0.043) loss 0.6931 (0.7195) acc 81.7708 (83.2928) lr 1.7290e-03 eta 0:07:20
epoch [14/50] batch [50/51] time 0.169 (0.231) data 0.000 (0.038) loss 0.8019 (0.7207) acc 79.5000 (83.2196) lr 1.7290e-03 eta 0:07:04
>>> alpha1: 0.232  alpha2: -0.075 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [15/50] batch [5/51] time 0.153 (0.479) data 0.000 (0.300) loss 0.6505 (0.5866) acc 83.7209 (86.8122) lr 1.6845e-03 eta 0:14:37
epoch [15/50] batch [10/51] time 0.187 (0.328) data 0.015 (0.152) loss 0.7909 (0.6375) acc 82.0000 (85.0353) lr 1.6845e-03 eta 0:09:58
epoch [15/50] batch [15/51] time 0.893 (0.324) data 0.013 (0.102) loss 0.5613 (0.6562) acc 86.2069 (84.4097) lr 1.6845e-03 eta 0:09:49
epoch [15/50] batch [20/51] time 0.175 (0.287) data 0.000 (0.077) loss 0.5831 (0.6483) acc 86.7347 (84.1027) lr 1.6845e-03 eta 0:08:40
epoch [15/50] batch [25/51] time 0.177 (0.265) data 0.000 (0.061) loss 0.8602 (0.6524) acc 82.5000 (84.2501) lr 1.6845e-03 eta 0:08:00
epoch [15/50] batch [30/51] time 0.166 (0.251) data 0.000 (0.051) loss 0.7057 (0.6554) acc 85.2041 (84.2356) lr 1.6845e-03 eta 0:07:32
epoch [15/50] batch [35/51] time 0.168 (0.240) data 0.000 (0.044) loss 0.6403 (0.6421) acc 85.5000 (84.6743) lr 1.6845e-03 eta 0:07:11
epoch [15/50] batch [40/51] time 0.168 (0.232) data 0.000 (0.038) loss 0.8549 (0.6474) acc 75.5000 (84.2804) lr 1.6845e-03 eta 0:06:56
epoch [15/50] batch [45/51] time 0.160 (0.225) data 0.000 (0.034) loss 0.7502 (0.6497) acc 83.5106 (84.1169) lr 1.6845e-03 eta 0:06:42
epoch [15/50] batch [50/51] time 0.162 (0.219) data 0.000 (0.031) loss 0.4624 (0.6406) acc 89.8936 (84.4240) lr 1.6845e-03 eta 0:06:31
>>> alpha1: 0.208  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.09 <<<
epoch [16/50] batch [5/51] time 0.194 (0.493) data 0.001 (0.306) loss 0.6660 (0.5366) acc 85.8491 (86.8089) lr 1.6374e-03 eta 0:14:37
epoch [16/50] batch [10/51] time 0.191 (0.338) data 0.000 (0.153) loss 0.6399 (0.5911) acc 81.1225 (85.8946) lr 1.6374e-03 eta 0:10:00
epoch [16/50] batch [15/51] time 0.179 (0.285) data 0.000 (0.102) loss 0.7891 (0.6225) acc 86.2245 (85.5323) lr 1.6374e-03 eta 0:08:25
epoch [16/50] batch [20/51] time 0.175 (0.261) data 0.000 (0.077) loss 0.6756 (0.6191) acc 86.0577 (85.8489) lr 1.6374e-03 eta 0:07:40
epoch [16/50] batch [25/51] time 0.172 (0.244) data 0.000 (0.061) loss 0.8868 (0.6475) acc 73.9130 (84.8125) lr 1.6374e-03 eta 0:07:10
epoch [16/50] batch [30/51] time 0.171 (0.234) data 0.000 (0.051) loss 0.4904 (0.6411) acc 93.1373 (85.1854) lr 1.6374e-03 eta 0:06:49
epoch [16/50] batch [35/51] time 0.190 (0.226) data 0.001 (0.044) loss 0.5330 (0.6316) acc 86.7924 (85.3889) lr 1.6374e-03 eta 0:06:35
epoch [16/50] batch [40/51] time 0.173 (0.219) data 0.000 (0.038) loss 0.7322 (0.6412) acc 77.4038 (84.9202) lr 1.6374e-03 eta 0:06:22
epoch [16/50] batch [45/51] time 0.183 (0.214) data 0.000 (0.034) loss 0.4949 (0.6289) acc 87.7451 (85.1947) lr 1.6374e-03 eta 0:06:12
epoch [16/50] batch [50/51] time 0.177 (0.210) data 0.000 (0.031) loss 0.5066 (0.6265) acc 89.1509 (85.3186) lr 1.6374e-03 eta 0:06:04
>>> alpha1: 0.194  alpha2: -0.123 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [17/50] batch [5/51] time 0.179 (0.472) data 0.000 (0.288) loss 0.5741 (0.5042) acc 87.2642 (87.7534) lr 1.5878e-03 eta 0:13:36
epoch [17/50] batch [10/51] time 0.171 (0.324) data 0.000 (0.144) loss 0.6794 (0.5774) acc 86.5000 (86.8198) lr 1.5878e-03 eta 0:09:18
epoch [17/50] batch [15/51] time 0.175 (0.273) data 0.000 (0.096) loss 0.6826 (0.6035) acc 79.4118 (85.8898) lr 1.5878e-03 eta 0:07:49
epoch [17/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.072) loss 0.5464 (0.5938) acc 86.5741 (86.0061) lr 1.5878e-03 eta 0:07:03
epoch [17/50] batch [25/51] time 0.170 (0.234) data 0.000 (0.058) loss 0.5721 (0.5731) acc 86.9565 (86.4807) lr 1.5878e-03 eta 0:06:39
epoch [17/50] batch [30/51] time 0.172 (0.224) data 0.000 (0.048) loss 0.5331 (0.5833) acc 86.9792 (86.0193) lr 1.5878e-03 eta 0:06:22
epoch [17/50] batch [35/51] time 0.187 (0.218) data 0.001 (0.041) loss 0.6157 (0.5863) acc 87.2642 (85.9750) lr 1.5878e-03 eta 0:06:11
epoch [17/50] batch [40/51] time 0.169 (0.214) data 0.000 (0.036) loss 0.7699 (0.5872) acc 85.0000 (86.1019) lr 1.5878e-03 eta 0:06:01
epoch [17/50] batch [45/51] time 0.174 (0.209) data 0.000 (0.032) loss 0.6328 (0.5867) acc 81.7308 (85.9287) lr 1.5878e-03 eta 0:05:52
epoch [17/50] batch [50/51] time 0.174 (0.205) data 0.000 (0.029) loss 0.7589 (0.5977) acc 83.1731 (85.5914) lr 1.5878e-03 eta 0:05:44
>>> alpha1: 0.185  alpha2: -0.127 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [18/50] batch [5/51] time 0.212 (0.452) data 0.000 (0.260) loss 0.4782 (0.6180) acc 87.5000 (85.8739) lr 1.5358e-03 eta 0:12:37
epoch [18/50] batch [10/51] time 0.174 (0.319) data 0.000 (0.130) loss 0.7196 (0.6138) acc 84.0000 (86.4782) lr 1.5358e-03 eta 0:08:53
epoch [18/50] batch [15/51] time 0.182 (0.277) data 0.000 (0.087) loss 0.6858 (0.6034) acc 79.3269 (85.8153) lr 1.5358e-03 eta 0:07:41
epoch [18/50] batch [20/51] time 0.163 (0.251) data 0.000 (0.065) loss 0.9039 (0.6309) acc 76.0638 (84.9297) lr 1.5358e-03 eta 0:06:57
epoch [18/50] batch [25/51] time 0.172 (0.237) data 0.001 (0.052) loss 0.6660 (0.6251) acc 84.2391 (84.8687) lr 1.5358e-03 eta 0:06:33
epoch [18/50] batch [30/51] time 0.180 (0.227) data 0.000 (0.044) loss 0.6955 (0.6208) acc 81.3726 (85.3066) lr 1.5358e-03 eta 0:06:15
epoch [18/50] batch [35/51] time 0.197 (0.221) data 0.000 (0.037) loss 0.5569 (0.6080) acc 85.9091 (85.9919) lr 1.5358e-03 eta 0:06:04
epoch [18/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.033) loss 0.4657 (0.6040) acc 90.2778 (86.0452) lr 1.5358e-03 eta 0:05:53
epoch [18/50] batch [45/51] time 0.163 (0.210) data 0.000 (0.029) loss 0.4053 (0.5921) acc 91.6667 (86.3044) lr 1.5358e-03 eta 0:05:44
epoch [18/50] batch [50/51] time 0.162 (0.206) data 0.000 (0.026) loss 0.5662 (0.5917) acc 88.8298 (86.2036) lr 1.5358e-03 eta 0:05:36
>>> alpha1: 0.180  alpha2: -0.127 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [19/50] batch [5/51] time 0.186 (0.503) data 0.000 (0.317) loss 0.4211 (0.4843) acc 93.2692 (89.7625) lr 1.4818e-03 eta 0:13:38
epoch [19/50] batch [10/51] time 0.182 (0.342) data 0.000 (0.159) loss 0.6976 (0.5553) acc 86.5741 (89.4462) lr 1.4818e-03 eta 0:09:14
epoch [19/50] batch [15/51] time 0.179 (0.288) data 0.000 (0.106) loss 2.6185 (0.6905) acc 62.9808 (86.9439) lr 1.4818e-03 eta 0:07:46
epoch [19/50] batch [20/51] time 0.177 (0.265) data 0.000 (0.080) loss 0.6352 (0.7244) acc 84.5000 (86.2478) lr 1.4818e-03 eta 0:07:07
epoch [19/50] batch [25/51] time 0.172 (0.248) data 0.000 (0.064) loss 0.6199 (0.6841) acc 88.0000 (86.7523) lr 1.4818e-03 eta 0:06:38
epoch [19/50] batch [30/51] time 0.180 (0.236) data 0.000 (0.053) loss 0.3638 (0.6591) acc 90.6863 (87.0833) lr 1.4818e-03 eta 0:06:18
epoch [19/50] batch [35/51] time 0.182 (0.228) data 0.000 (0.046) loss 0.7067 (0.6370) acc 81.6038 (87.2885) lr 1.4818e-03 eta 0:06:04
epoch [19/50] batch [40/51] time 0.176 (0.222) data 0.000 (0.040) loss 0.6932 (0.6288) acc 83.5000 (87.2506) lr 1.4818e-03 eta 0:05:53
epoch [19/50] batch [45/51] time 0.180 (0.216) data 0.000 (0.035) loss 0.4324 (0.6198) acc 93.1373 (87.3456) lr 1.4818e-03 eta 0:05:43
epoch [19/50] batch [50/51] time 0.183 (0.213) data 0.000 (0.032) loss 0.5223 (0.6113) acc 88.2353 (87.5614) lr 1.4818e-03 eta 0:05:36
>>> alpha1: 0.174  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [20/50] batch [5/51] time 0.180 (0.469) data 0.000 (0.289) loss 0.6268 (0.6147) acc 84.2593 (85.4154) lr 1.4258e-03 eta 0:12:19
epoch [20/50] batch [10/51] time 0.179 (0.323) data 0.000 (0.144) loss 0.4745 (0.5776) acc 90.2778 (86.8835) lr 1.4258e-03 eta 0:08:28
epoch [20/50] batch [15/51] time 0.167 (0.278) data 0.000 (0.096) loss 0.3896 (0.5598) acc 91.8367 (87.0752) lr 1.4258e-03 eta 0:07:15
epoch [20/50] batch [20/51] time 0.176 (0.254) data 0.000 (0.072) loss 0.5863 (0.5612) acc 84.1837 (86.6066) lr 1.4258e-03 eta 0:06:36
epoch [20/50] batch [25/51] time 0.188 (0.242) data 0.000 (0.058) loss 0.5957 (0.5523) acc 89.9038 (86.9999) lr 1.4258e-03 eta 0:06:15
epoch [20/50] batch [30/51] time 0.170 (0.232) data 0.000 (0.048) loss 0.4916 (0.5658) acc 88.5417 (86.7066) lr 1.4258e-03 eta 0:05:59
epoch [20/50] batch [35/51] time 0.191 (0.224) data 0.000 (0.042) loss 0.6967 (0.5712) acc 80.8036 (86.2644) lr 1.4258e-03 eta 0:05:46
epoch [20/50] batch [40/51] time 0.171 (0.219) data 0.000 (0.036) loss 0.7849 (0.5657) acc 78.5714 (86.3150) lr 1.4258e-03 eta 0:05:37
epoch [20/50] batch [45/51] time 0.176 (0.215) data 0.001 (0.032) loss 0.5324 (0.5705) acc 90.0000 (86.1755) lr 1.4258e-03 eta 0:05:29
epoch [20/50] batch [50/51] time 0.183 (0.211) data 0.000 (0.029) loss 0.5174 (0.5653) acc 86.0577 (86.4052) lr 1.4258e-03 eta 0:05:23
>>> alpha1: 0.165  alpha2: -0.122 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [21/50] batch [5/51] time 0.181 (0.476) data 0.000 (0.293) loss 0.5301 (0.4902) acc 86.5741 (89.6741) lr 1.3681e-03 eta 0:12:05
epoch [21/50] batch [10/51] time 0.174 (0.328) data 0.000 (0.146) loss 0.7166 (0.5193) acc 85.2941 (88.3138) lr 1.3681e-03 eta 0:08:17
epoch [21/50] batch [15/51] time 0.169 (0.275) data 0.000 (0.098) loss 0.6904 (0.5448) acc 84.5000 (88.0610) lr 1.3681e-03 eta 0:06:56
epoch [21/50] batch [20/51] time 0.173 (0.249) data 0.000 (0.073) loss 0.5879 (0.5425) acc 83.1731 (87.8035) lr 1.3681e-03 eta 0:06:15
epoch [21/50] batch [25/51] time 0.178 (0.234) data 0.000 (0.059) loss 0.6175 (0.5403) acc 85.0962 (87.8676) lr 1.3681e-03 eta 0:05:51
epoch [21/50] batch [30/51] time 0.165 (0.224) data 0.000 (0.049) loss 0.5760 (0.5431) acc 83.8542 (87.7257) lr 1.3681e-03 eta 0:05:35
epoch [21/50] batch [35/51] time 0.182 (0.216) data 0.000 (0.042) loss 0.4727 (0.5402) acc 91.8269 (87.8090) lr 1.3681e-03 eta 0:05:23
epoch [21/50] batch [40/51] time 0.171 (0.211) data 0.001 (0.037) loss 0.5622 (0.5303) acc 84.6154 (87.9366) lr 1.3681e-03 eta 0:05:14
epoch [21/50] batch [45/51] time 0.167 (0.206) data 0.000 (0.033) loss 0.6316 (0.5296) acc 83.5000 (87.9916) lr 1.3681e-03 eta 0:05:05
epoch [21/50] batch [50/51] time 0.191 (0.202) data 0.000 (0.029) loss 0.3632 (0.5340) acc 91.9643 (87.7336) lr 1.3681e-03 eta 0:04:59
>>> alpha1: 0.163  alpha2: -0.119 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [22/50] batch [5/51] time 0.191 (0.508) data 0.000 (0.325) loss 0.3746 (0.4608) acc 90.5172 (88.5865) lr 1.3090e-03 eta 0:12:28
epoch [22/50] batch [10/51] time 0.169 (0.344) data 0.000 (0.163) loss 0.6228 (0.5380) acc 83.5000 (87.0609) lr 1.3090e-03 eta 0:08:25
epoch [22/50] batch [15/51] time 0.177 (0.286) data 0.000 (0.109) loss 0.4315 (0.5043) acc 92.3077 (88.3973) lr 1.3090e-03 eta 0:06:59
epoch [22/50] batch [20/51] time 0.207 (0.261) data 0.000 (0.082) loss 0.4745 (0.4876) acc 89.6226 (89.0690) lr 1.3090e-03 eta 0:06:20
epoch [22/50] batch [25/51] time 0.173 (0.264) data 0.000 (0.065) loss 0.5126 (0.5009) acc 89.5000 (88.8381) lr 1.3090e-03 eta 0:06:23
epoch [22/50] batch [30/51] time 0.164 (0.248) data 0.000 (0.054) loss 0.6289 (0.5147) acc 82.4468 (88.0117) lr 1.3090e-03 eta 0:06:00
epoch [22/50] batch [35/51] time 0.156 (0.237) data 0.000 (0.047) loss 0.5619 (0.5124) acc 86.3636 (87.8780) lr 1.3090e-03 eta 0:05:42
epoch [22/50] batch [40/51] time 0.164 (0.228) data 0.000 (0.041) loss 0.5573 (0.5218) acc 86.7021 (87.7247) lr 1.3090e-03 eta 0:05:28
epoch [22/50] batch [45/51] time 0.172 (0.221) data 0.000 (0.036) loss 0.4407 (0.5225) acc 90.1961 (87.6645) lr 1.3090e-03 eta 0:05:17
epoch [22/50] batch [50/51] time 0.171 (0.215) data 0.000 (0.033) loss 0.4647 (0.5189) acc 90.8654 (87.7531) lr 1.3090e-03 eta 0:05:07
>>> alpha1: 0.160  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [23/50] batch [5/51] time 0.177 (0.490) data 0.000 (0.314) loss 0.4361 (0.4819) acc 88.7755 (88.5281) lr 1.2487e-03 eta 0:11:37
epoch [23/50] batch [10/51] time 0.166 (0.333) data 0.000 (0.157) loss 0.5274 (0.4659) acc 83.8542 (88.8379) lr 1.2487e-03 eta 0:07:52
epoch [23/50] batch [15/51] time 0.179 (0.280) data 0.000 (0.105) loss 0.5907 (0.5941) acc 87.5000 (87.7083) lr 1.2487e-03 eta 0:06:35
epoch [23/50] batch [20/51] time 0.188 (0.254) data 0.001 (0.079) loss 0.6557 (0.5908) acc 86.5000 (87.1103) lr 1.2487e-03 eta 0:05:57
epoch [23/50] batch [25/51] time 0.189 (0.238) data 0.000 (0.063) loss 0.4732 (0.5626) acc 90.4546 (87.7227) lr 1.2487e-03 eta 0:05:33
epoch [23/50] batch [30/51] time 0.184 (0.228) data 0.000 (0.053) loss 0.4815 (0.5694) acc 90.5660 (87.5099) lr 1.2487e-03 eta 0:05:19
epoch [23/50] batch [35/51] time 0.165 (0.221) data 0.000 (0.045) loss 0.6206 (0.5656) acc 84.3750 (87.4188) lr 1.2487e-03 eta 0:05:07
epoch [23/50] batch [40/51] time 0.175 (0.216) data 0.000 (0.039) loss 0.4017 (0.5559) acc 92.3469 (87.5552) lr 1.2487e-03 eta 0:04:59
epoch [23/50] batch [45/51] time 0.178 (0.210) data 0.000 (0.035) loss 0.6352 (0.5570) acc 84.1346 (87.4594) lr 1.2487e-03 eta 0:04:50
epoch [23/50] batch [50/51] time 0.172 (0.207) data 0.000 (0.032) loss 0.4598 (0.5518) acc 91.3265 (87.7557) lr 1.2487e-03 eta 0:04:44
>>> alpha1: 0.157  alpha2: -0.113 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [24/50] batch [5/51] time 0.180 (0.454) data 0.000 (0.275) loss 0.5589 (0.5159) acc 87.2549 (86.8399) lr 1.1874e-03 eta 0:10:22
epoch [24/50] batch [10/51] time 0.169 (0.316) data 0.000 (0.138) loss 0.4018 (0.5108) acc 91.0000 (87.7511) lr 1.1874e-03 eta 0:07:12
epoch [24/50] batch [15/51] time 0.177 (0.271) data 0.000 (0.092) loss 0.5564 (0.5060) acc 88.2353 (88.0974) lr 1.1874e-03 eta 0:06:09
epoch [24/50] batch [20/51] time 0.176 (0.249) data 0.000 (0.069) loss 0.3853 (0.5076) acc 92.1569 (88.2538) lr 1.1874e-03 eta 0:05:37
epoch [24/50] batch [25/51] time 0.187 (0.235) data 0.000 (0.055) loss 0.4562 (0.5035) acc 85.6383 (88.1747) lr 1.1874e-03 eta 0:05:17
epoch [24/50] batch [30/51] time 0.170 (0.226) data 0.000 (0.046) loss 0.5021 (0.4989) acc 86.2745 (88.1748) lr 1.1874e-03 eta 0:05:03
epoch [24/50] batch [35/51] time 0.166 (0.218) data 0.000 (0.040) loss 0.4377 (0.4895) acc 90.3061 (88.5956) lr 1.1874e-03 eta 0:04:51
epoch [24/50] batch [40/51] time 0.178 (0.212) data 0.000 (0.035) loss 0.3060 (0.4965) acc 95.7547 (88.6610) lr 1.1874e-03 eta 0:04:43
epoch [24/50] batch [45/51] time 0.186 (0.208) data 0.000 (0.031) loss 0.3623 (0.4860) acc 93.8596 (88.9672) lr 1.1874e-03 eta 0:04:37
epoch [24/50] batch [50/51] time 0.165 (0.204) data 0.000 (0.028) loss 0.5760 (0.4876) acc 86.9792 (88.8550) lr 1.1874e-03 eta 0:04:30
>>> alpha1: 0.158  alpha2: -0.120 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [25/50] batch [5/51] time 0.171 (0.459) data 0.000 (0.278) loss 0.6267 (0.4899) acc 87.5000 (90.3528) lr 1.1253e-03 eta 0:10:06
epoch [25/50] batch [10/51] time 0.179 (0.318) data 0.000 (0.139) loss 0.4910 (0.4922) acc 91.2037 (90.4472) lr 1.1253e-03 eta 0:06:58
epoch [25/50] batch [15/51] time 0.173 (0.307) data 0.000 (0.093) loss 0.4025 (0.4782) acc 90.8163 (90.1503) lr 1.1253e-03 eta 0:06:42
epoch [25/50] batch [20/51] time 0.177 (0.275) data 0.000 (0.070) loss 0.4751 (0.4883) acc 90.5660 (89.8412) lr 1.1253e-03 eta 0:05:59
epoch [25/50] batch [25/51] time 0.170 (0.254) data 0.000 (0.056) loss 0.6198 (0.5020) acc 85.5000 (89.3598) lr 1.1253e-03 eta 0:05:30
epoch [25/50] batch [30/51] time 0.193 (0.242) data 0.000 (0.047) loss 0.3918 (0.4933) acc 89.5455 (89.4641) lr 1.1253e-03 eta 0:05:13
epoch [25/50] batch [35/51] time 0.185 (0.232) data 0.000 (0.040) loss 0.4170 (0.4930) acc 90.1786 (89.3974) lr 1.1253e-03 eta 0:04:59
epoch [25/50] batch [40/51] time 0.176 (0.225) data 0.000 (0.035) loss 0.4048 (0.5187) acc 91.5094 (89.3467) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [45/51] time 0.167 (0.219) data 0.000 (0.031) loss 0.3665 (0.5032) acc 92.8571 (89.6200) lr 1.1253e-03 eta 0:04:40
epoch [25/50] batch [50/51] time 0.179 (0.214) data 0.000 (0.028) loss 0.4790 (0.4990) acc 88.2353 (89.6866) lr 1.1253e-03 eta 0:04:33
>>> alpha1: 0.156  alpha2: -0.121 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [26/50] batch [5/51] time 0.177 (0.462) data 0.000 (0.285) loss 0.5474 (0.4386) acc 88.0208 (90.3842) lr 1.0628e-03 eta 0:09:47
epoch [26/50] batch [10/51] time 0.182 (0.322) data 0.000 (0.142) loss 0.4809 (0.4351) acc 89.7959 (90.5438) lr 1.0628e-03 eta 0:06:47
epoch [26/50] batch [15/51] time 0.159 (0.270) data 0.000 (0.095) loss 0.5215 (0.4409) acc 92.2222 (91.0682) lr 1.0628e-03 eta 0:05:39
epoch [26/50] batch [20/51] time 0.169 (0.245) data 0.000 (0.071) loss 0.4639 (0.4620) acc 86.0000 (90.0333) lr 1.0628e-03 eta 0:05:08
epoch [26/50] batch [25/51] time 0.161 (0.231) data 0.000 (0.057) loss 0.4979 (0.4698) acc 91.3043 (89.9583) lr 1.0628e-03 eta 0:04:48
epoch [26/50] batch [30/51] time 0.174 (0.222) data 0.000 (0.048) loss 0.5925 (0.4732) acc 85.0962 (89.7826) lr 1.0628e-03 eta 0:04:36
epoch [26/50] batch [35/51] time 0.160 (0.216) data 0.000 (0.041) loss 0.5622 (0.4758) acc 84.7826 (89.4665) lr 1.0628e-03 eta 0:04:27
epoch [26/50] batch [40/51] time 0.176 (0.210) data 0.000 (0.036) loss 0.3922 (0.4756) acc 86.7924 (89.2633) lr 1.0628e-03 eta 0:04:19
epoch [26/50] batch [45/51] time 0.161 (0.206) data 0.000 (0.032) loss 0.5506 (0.4696) acc 88.0435 (89.4598) lr 1.0628e-03 eta 0:04:13
epoch [26/50] batch [50/51] time 0.172 (0.202) data 0.000 (0.029) loss 0.5174 (0.4711) acc 87.9808 (89.4196) lr 1.0628e-03 eta 0:04:07
>>> alpha1: 0.152  alpha2: -0.117 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [27/50] batch [5/51] time 0.169 (0.480) data 0.000 (0.296) loss 0.4891 (0.5684) acc 89.7959 (89.6031) lr 1.0000e-03 eta 0:09:45
epoch [27/50] batch [10/51] time 0.161 (0.326) data 0.000 (0.148) loss 0.4828 (0.5385) acc 90.7609 (88.8515) lr 1.0000e-03 eta 0:06:36
epoch [27/50] batch [15/51] time 0.165 (0.275) data 0.000 (0.099) loss 0.4213 (0.5202) acc 92.1875 (88.7904) lr 1.0000e-03 eta 0:05:32
epoch [27/50] batch [20/51] time 0.191 (0.252) data 0.000 (0.074) loss 0.3615 (0.5013) acc 91.2037 (88.7355) lr 1.0000e-03 eta 0:05:03
epoch [27/50] batch [25/51] time 0.197 (0.237) data 0.001 (0.059) loss 0.3619 (0.5013) acc 93.1034 (88.8606) lr 1.0000e-03 eta 0:04:44
epoch [27/50] batch [30/51] time 0.172 (0.227) data 0.000 (0.050) loss 0.5552 (0.5085) acc 81.9149 (88.4677) lr 1.0000e-03 eta 0:04:30
epoch [27/50] batch [35/51] time 0.180 (0.220) data 0.000 (0.043) loss 0.5571 (0.5049) acc 85.2041 (88.3390) lr 1.0000e-03 eta 0:04:21
epoch [27/50] batch [40/51] time 0.172 (0.215) data 0.000 (0.037) loss 0.4848 (0.4970) acc 87.0192 (88.3799) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [45/51] time 0.169 (0.210) data 0.000 (0.033) loss 0.4712 (0.4922) acc 89.7059 (88.4432) lr 1.0000e-03 eta 0:04:07
epoch [27/50] batch [50/51] time 0.169 (0.206) data 0.000 (0.030) loss 0.7526 (0.4879) acc 82.0000 (88.5546) lr 1.0000e-03 eta 0:04:01
>>> alpha1: 0.149  alpha2: -0.124 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [28/50] batch [5/51] time 0.182 (0.462) data 0.000 (0.281) loss 0.5488 (0.4596) acc 85.3774 (89.8867) lr 9.3721e-04 eta 0:08:59
epoch [28/50] batch [10/51] time 0.171 (0.320) data 0.000 (0.141) loss 0.4223 (0.4162) acc 90.6863 (91.1700) lr 9.3721e-04 eta 0:06:12
epoch [28/50] batch [15/51] time 0.197 (0.274) data 0.000 (0.094) loss 0.2940 (0.4060) acc 94.1964 (91.2979) lr 9.3721e-04 eta 0:05:17
epoch [28/50] batch [20/51] time 0.187 (0.250) data 0.000 (0.071) loss 0.4827 (0.4268) acc 87.7551 (90.5537) lr 9.3721e-04 eta 0:04:48
epoch [28/50] batch [25/51] time 0.167 (0.235) data 0.000 (0.056) loss 0.4869 (0.4316) acc 85.9375 (89.8965) lr 9.3721e-04 eta 0:04:29
epoch [28/50] batch [30/51] time 0.185 (0.226) data 0.000 (0.047) loss 0.3632 (0.4373) acc 90.0943 (89.6151) lr 9.3721e-04 eta 0:04:18
epoch [28/50] batch [35/51] time 0.171 (0.219) data 0.000 (0.040) loss 0.4234 (0.4476) acc 89.2857 (89.3517) lr 9.3721e-04 eta 0:04:09
epoch [28/50] batch [40/51] time 0.160 (0.213) data 0.000 (0.035) loss 0.3481 (0.4527) acc 92.0213 (89.4148) lr 9.3721e-04 eta 0:04:01
epoch [28/50] batch [45/51] time 0.175 (0.209) data 0.000 (0.032) loss 0.5808 (0.4520) acc 83.4906 (89.3630) lr 9.3721e-04 eta 0:03:55
epoch [28/50] batch [50/51] time 0.187 (0.206) data 0.000 (0.028) loss 0.2751 (0.4450) acc 94.3966 (89.5495) lr 9.3721e-04 eta 0:03:51
>>> alpha1: 0.146  alpha2: -0.125 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.09 <<<
epoch [29/50] batch [5/51] time 0.171 (0.482) data 0.000 (0.295) loss 0.3121 (0.4140) acc 96.5686 (90.8488) lr 8.7467e-04 eta 0:08:58
epoch [29/50] batch [10/51] time 0.158 (0.325) data 0.000 (0.148) loss 0.6128 (0.4752) acc 88.3333 (89.8667) lr 8.7467e-04 eta 0:06:01
epoch [29/50] batch [15/51] time 0.182 (0.274) data 0.000 (0.099) loss 0.3987 (0.4659) acc 92.7273 (90.0974) lr 8.7467e-04 eta 0:05:03
epoch [29/50] batch [20/51] time 0.186 (0.251) data 0.000 (0.074) loss 0.7465 (0.4504) acc 79.8077 (90.0332) lr 8.7467e-04 eta 0:04:36
epoch [29/50] batch [25/51] time 0.180 (0.238) data 0.000 (0.059) loss 0.3519 (0.4564) acc 92.9245 (89.7718) lr 8.7467e-04 eta 0:04:21
epoch [29/50] batch [30/51] time 0.198 (0.229) data 0.000 (0.049) loss 0.4870 (0.4563) acc 89.2857 (89.7754) lr 8.7467e-04 eta 0:04:09
epoch [29/50] batch [35/51] time 0.168 (0.221) data 0.000 (0.042) loss 0.3980 (0.4522) acc 91.5000 (89.8157) lr 8.7467e-04 eta 0:04:00
epoch [29/50] batch [40/51] time 0.161 (0.233) data 0.000 (0.037) loss 0.5977 (0.4510) acc 81.3830 (89.7679) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [45/51] time 0.167 (0.226) data 0.000 (0.033) loss 0.3357 (0.4502) acc 93.3673 (89.8488) lr 8.7467e-04 eta 0:04:03
epoch [29/50] batch [50/51] time 0.171 (0.221) data 0.000 (0.030) loss 0.3978 (0.4477) acc 93.0851 (89.9164) lr 8.7467e-04 eta 0:03:56
>>> alpha1: 0.144  alpha2: -0.127 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [30/50] batch [5/51] time 0.173 (0.471) data 0.000 (0.285) loss 0.5038 (0.3941) acc 87.7451 (92.1482) lr 8.1262e-04 eta 0:08:21
epoch [30/50] batch [10/51] time 0.185 (0.324) data 0.000 (0.143) loss 0.3545 (0.4376) acc 92.0000 (90.0049) lr 8.1262e-04 eta 0:05:43
epoch [30/50] batch [15/51] time 0.199 (0.278) data 0.000 (0.095) loss 0.3569 (0.4389) acc 91.2037 (89.9423) lr 8.1262e-04 eta 0:04:53
epoch [30/50] batch [20/51] time 0.183 (0.253) data 0.000 (0.072) loss 0.5725 (0.4402) acc 86.1111 (89.7932) lr 8.1262e-04 eta 0:04:26
epoch [30/50] batch [25/51] time 0.167 (0.237) data 0.000 (0.057) loss 0.4254 (0.4392) acc 91.8367 (89.9915) lr 8.1262e-04 eta 0:04:08
epoch [30/50] batch [30/51] time 0.178 (0.228) data 0.000 (0.048) loss 0.4649 (0.4449) acc 89.8148 (89.7909) lr 8.1262e-04 eta 0:03:57
epoch [30/50] batch [35/51] time 0.173 (0.221) data 0.000 (0.041) loss 0.4253 (0.4350) acc 90.6863 (90.0482) lr 8.1262e-04 eta 0:03:48
epoch [30/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.036) loss 0.5451 (0.4425) acc 88.8889 (89.9467) lr 8.1262e-04 eta 0:03:42
epoch [30/50] batch [45/51] time 0.168 (0.211) data 0.000 (0.032) loss 0.3954 (0.4496) acc 92.0000 (89.6600) lr 8.1262e-04 eta 0:03:36
epoch [30/50] batch [50/51] time 0.166 (0.206) data 0.000 (0.029) loss 0.4123 (0.4494) acc 91.8367 (89.7567) lr 8.1262e-04 eta 0:03:30
>>> alpha1: 0.144  alpha2: -0.133 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.09 <<<
epoch [31/50] batch [5/51] time 0.187 (0.451) data 0.015 (0.272) loss 0.4911 (0.4268) acc 88.7255 (89.7163) lr 7.5131e-04 eta 0:07:37
epoch [31/50] batch [10/51] time 0.179 (0.315) data 0.000 (0.136) loss 0.4002 (0.4278) acc 93.0556 (90.4259) lr 7.5131e-04 eta 0:05:18
epoch [31/50] batch [15/51] time 0.169 (0.269) data 0.000 (0.091) loss 0.4114 (0.4050) acc 92.0000 (91.3997) lr 7.5131e-04 eta 0:04:29
epoch [31/50] batch [20/51] time 0.163 (0.246) data 0.000 (0.069) loss 0.4285 (0.4071) acc 88.8298 (91.1986) lr 7.5131e-04 eta 0:04:06
epoch [31/50] batch [25/51] time 0.176 (0.233) data 0.000 (0.055) loss 0.5090 (0.4118) acc 87.0000 (90.9436) lr 7.5131e-04 eta 0:03:52
epoch [31/50] batch [30/51] time 0.204 (0.227) data 0.000 (0.046) loss 0.3928 (0.4155) acc 92.3077 (90.8526) lr 7.5131e-04 eta 0:03:44
epoch [31/50] batch [35/51] time 0.176 (0.220) data 0.000 (0.039) loss 0.5534 (0.4258) acc 89.4231 (90.5420) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [40/51] time 0.167 (0.216) data 0.000 (0.035) loss 0.4736 (0.4328) acc 89.2857 (90.3182) lr 7.5131e-04 eta 0:03:31
epoch [31/50] batch [45/51] time 0.172 (0.211) data 0.000 (0.031) loss 0.6695 (0.4384) acc 87.2549 (90.2928) lr 7.5131e-04 eta 0:03:25
epoch [31/50] batch [50/51] time 0.167 (0.207) data 0.000 (0.028) loss 0.4109 (0.4389) acc 90.5000 (90.3993) lr 7.5131e-04 eta 0:03:21
>>> alpha1: 0.142  alpha2: -0.135 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [32/50] batch [5/51] time 0.186 (0.465) data 0.002 (0.278) loss 0.4578 (0.4142) acc 88.8393 (91.0498) lr 6.9098e-04 eta 0:07:27
epoch [32/50] batch [10/51] time 0.174 (0.322) data 0.000 (0.139) loss 0.4030 (0.4393) acc 90.3846 (89.6601) lr 6.9098e-04 eta 0:05:09
epoch [32/50] batch [15/51] time 0.182 (0.273) data 0.000 (0.093) loss 0.2616 (0.4279) acc 95.4546 (90.1192) lr 6.9098e-04 eta 0:04:20
epoch [32/50] batch [20/51] time 0.188 (0.249) data 0.000 (0.070) loss 0.5072 (0.4291) acc 87.0000 (90.1358) lr 6.9098e-04 eta 0:03:56
epoch [32/50] batch [25/51] time 0.174 (0.234) data 0.000 (0.056) loss 0.4766 (0.4463) acc 90.1961 (89.6466) lr 6.9098e-04 eta 0:03:40
epoch [32/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.047) loss 0.3769 (0.4474) acc 93.7500 (89.7644) lr 6.9098e-04 eta 0:03:31
epoch [32/50] batch [35/51] time 0.186 (0.220) data 0.000 (0.040) loss 0.3540 (0.4297) acc 92.4528 (90.4186) lr 6.9098e-04 eta 0:03:25
epoch [32/50] batch [40/51] time 0.172 (0.215) data 0.000 (0.035) loss 0.5502 (0.4292) acc 89.4231 (90.4365) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [45/51] time 0.162 (0.210) data 0.000 (0.031) loss 0.4375 (0.4379) acc 89.3617 (90.1993) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [50/51] time 0.169 (0.207) data 0.000 (0.028) loss 0.5557 (0.4403) acc 86.0000 (90.1559) lr 6.9098e-04 eta 0:03:10
>>> alpha1: 0.141  alpha2: -0.128 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [33/50] batch [5/51] time 0.185 (0.486) data 0.000 (0.308) loss 0.4443 (0.4014) acc 87.2549 (90.1350) lr 6.3188e-04 eta 0:07:24
epoch [33/50] batch [10/51] time 0.170 (0.328) data 0.000 (0.154) loss 0.4736 (0.4654) acc 92.0455 (89.0780) lr 6.3188e-04 eta 0:04:58
epoch [33/50] batch [15/51] time 0.176 (0.279) data 0.000 (0.103) loss 0.4517 (0.4531) acc 90.5000 (89.5365) lr 6.3188e-04 eta 0:04:11
epoch [33/50] batch [20/51] time 0.183 (0.254) data 0.001 (0.077) loss 0.4011 (0.4328) acc 92.1296 (90.0872) lr 6.3188e-04 eta 0:03:47
epoch [33/50] batch [25/51] time 0.174 (0.238) data 0.000 (0.062) loss 0.4769 (0.4215) acc 87.9808 (90.4462) lr 6.3188e-04 eta 0:03:32
epoch [33/50] batch [30/51] time 0.190 (0.229) data 0.001 (0.052) loss 0.3625 (0.4257) acc 93.4211 (90.2812) lr 6.3188e-04 eta 0:03:23
epoch [33/50] batch [35/51] time 0.173 (0.221) data 0.000 (0.044) loss 0.4159 (0.4298) acc 92.6471 (90.2617) lr 6.3188e-04 eta 0:03:15
epoch [33/50] batch [40/51] time 0.183 (0.215) data 0.000 (0.039) loss 0.5551 (0.4353) acc 87.2642 (90.0232) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.178 (0.211) data 0.000 (0.034) loss 0.4992 (0.4315) acc 90.7407 (90.2563) lr 6.3188e-04 eta 0:03:04
epoch [33/50] batch [50/51] time 0.170 (0.207) data 0.000 (0.031) loss 0.3018 (0.4319) acc 91.6667 (90.2776) lr 6.3188e-04 eta 0:02:59
>>> alpha1: 0.139  alpha2: -0.121 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [34/50] batch [5/51] time 0.181 (0.468) data 0.000 (0.285) loss 0.3313 (0.4098) acc 92.9245 (91.3753) lr 5.7422e-04 eta 0:06:43
epoch [34/50] batch [10/51] time 0.173 (0.323) data 0.000 (0.143) loss 0.5712 (0.4162) acc 86.0577 (90.6235) lr 5.7422e-04 eta 0:04:37
epoch [34/50] batch [15/51] time 0.189 (0.275) data 0.000 (0.095) loss 0.5646 (0.4247) acc 85.5769 (90.2305) lr 5.7422e-04 eta 0:03:54
epoch [34/50] batch [20/51] time 0.178 (0.251) data 0.000 (0.071) loss 0.4212 (0.4177) acc 92.6471 (90.7877) lr 5.7422e-04 eta 0:03:32
epoch [34/50] batch [25/51] time 0.171 (0.235) data 0.000 (0.057) loss 0.4767 (0.4194) acc 88.2353 (90.8721) lr 5.7422e-04 eta 0:03:17
epoch [34/50] batch [30/51] time 0.169 (0.228) data 0.000 (0.048) loss 0.3476 (0.4183) acc 90.5000 (90.9372) lr 5.7422e-04 eta 0:03:10
epoch [34/50] batch [35/51] time 0.175 (0.221) data 0.000 (0.041) loss 0.5180 (0.4257) acc 89.0000 (90.8804) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [40/51] time 0.181 (0.216) data 0.000 (0.036) loss 0.3968 (0.4313) acc 87.7273 (90.6243) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [45/51] time 0.159 (0.211) data 0.000 (0.032) loss 0.6193 (0.4367) acc 82.6087 (90.4511) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [50/51] time 0.164 (0.207) data 0.000 (0.029) loss 0.6910 (0.4408) acc 83.3333 (90.2384) lr 5.7422e-04 eta 0:02:48
>>> alpha1: 0.135  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [35/50] batch [5/51] time 0.187 (0.522) data 0.000 (0.328) loss 0.5514 (0.3774) acc 88.2353 (92.0532) lr 5.1825e-04 eta 0:07:03
epoch [35/50] batch [10/51] time 0.174 (0.351) data 0.000 (0.164) loss 0.2961 (0.3620) acc 96.1538 (92.5707) lr 5.1825e-04 eta 0:04:43
epoch [35/50] batch [15/51] time 0.182 (0.293) data 0.000 (0.109) loss 0.3497 (0.3779) acc 94.0909 (92.0450) lr 5.1825e-04 eta 0:03:54
epoch [35/50] batch [20/51] time 0.185 (0.263) data 0.000 (0.082) loss 0.4388 (0.3848) acc 89.4231 (91.8142) lr 5.1825e-04 eta 0:03:29
epoch [35/50] batch [25/51] time 0.188 (0.246) data 0.000 (0.066) loss 0.3850 (0.3914) acc 89.6226 (91.5692) lr 5.1825e-04 eta 0:03:14
epoch [35/50] batch [30/51] time 0.183 (0.234) data 0.000 (0.055) loss 0.4996 (0.4059) acc 92.1569 (91.3959) lr 5.1825e-04 eta 0:03:04
epoch [35/50] batch [35/51] time 0.178 (0.226) data 0.000 (0.047) loss 0.3565 (0.4079) acc 89.4231 (91.3842) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [40/51] time 0.168 (0.220) data 0.000 (0.041) loss 0.5084 (0.4044) acc 88.0000 (91.4068) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [45/51] time 0.162 (0.214) data 0.000 (0.037) loss 0.4279 (0.4102) acc 90.4255 (91.3461) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [50/51] time 0.173 (0.210) data 0.000 (0.033) loss 0.6178 (0.4140) acc 85.3261 (91.1603) lr 5.1825e-04 eta 0:02:41
>>> alpha1: 0.133  alpha2: -0.114 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [36/50] batch [5/51] time 0.182 (0.581) data 0.000 (0.287) loss 0.5460 (0.4136) acc 89.1509 (89.5911) lr 4.6417e-04 eta 0:07:21
epoch [36/50] batch [10/51] time 0.179 (0.378) data 0.000 (0.143) loss 0.2224 (0.3976) acc 96.2264 (90.6114) lr 4.6417e-04 eta 0:04:45
epoch [36/50] batch [15/51] time 0.181 (0.312) data 0.001 (0.096) loss 0.4441 (0.3832) acc 90.4255 (91.0475) lr 4.6417e-04 eta 0:03:54
epoch [36/50] batch [20/51] time 0.184 (0.279) data 0.000 (0.072) loss 0.4271 (0.3935) acc 88.8298 (91.0121) lr 4.6417e-04 eta 0:03:27
epoch [36/50] batch [25/51] time 0.171 (0.260) data 0.000 (0.058) loss 0.5177 (0.4035) acc 85.7143 (90.8887) lr 4.6417e-04 eta 0:03:12
epoch [36/50] batch [30/51] time 0.201 (0.248) data 0.000 (0.048) loss 0.3558 (0.4165) acc 91.6667 (90.6346) lr 4.6417e-04 eta 0:03:02
epoch [36/50] batch [35/51] time 0.195 (0.241) data 0.000 (0.041) loss 0.2012 (0.4058) acc 95.8333 (90.9731) lr 4.6417e-04 eta 0:02:55
epoch [36/50] batch [40/51] time 0.179 (0.233) data 0.000 (0.036) loss 0.3691 (0.4032) acc 91.6667 (91.1209) lr 4.6417e-04 eta 0:02:48
epoch [36/50] batch [45/51] time 0.176 (0.226) data 0.000 (0.032) loss 0.3347 (0.4051) acc 93.8679 (91.1873) lr 4.6417e-04 eta 0:02:42
epoch [36/50] batch [50/51] time 0.170 (0.220) data 0.000 (0.029) loss 0.5980 (0.4115) acc 86.7647 (91.0215) lr 4.6417e-04 eta 0:02:37
>>> alpha1: 0.132  alpha2: -0.108 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [37/50] batch [5/51] time 0.176 (0.500) data 0.000 (0.318) loss 0.3759 (0.4366) acc 90.1961 (90.8016) lr 4.1221e-04 eta 0:05:54
epoch [37/50] batch [10/51] time 0.170 (0.337) data 0.000 (0.159) loss 0.4228 (0.4070) acc 89.1304 (90.9474) lr 4.1221e-04 eta 0:03:57
epoch [37/50] batch [15/51] time 0.164 (0.283) data 0.000 (0.106) loss 0.2981 (0.3911) acc 92.7083 (91.0732) lr 4.1221e-04 eta 0:03:17
epoch [37/50] batch [20/51] time 0.194 (0.257) data 0.000 (0.080) loss 0.4208 (0.4066) acc 90.4546 (90.8226) lr 4.1221e-04 eta 0:02:58
epoch [37/50] batch [25/51] time 0.166 (0.239) data 0.000 (0.064) loss 0.3978 (0.4054) acc 90.8163 (91.0311) lr 4.1221e-04 eta 0:02:44
epoch [37/50] batch [30/51] time 0.164 (0.228) data 0.000 (0.053) loss 0.4372 (0.4105) acc 89.5833 (90.8922) lr 4.1221e-04 eta 0:02:35
epoch [37/50] batch [35/51] time 0.177 (0.221) data 0.000 (0.046) loss 0.3864 (0.4074) acc 94.2308 (90.9558) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [40/51] time 0.162 (0.214) data 0.000 (0.040) loss 0.3969 (0.4113) acc 92.9348 (90.9948) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [45/51] time 0.172 (0.210) data 0.000 (0.036) loss 0.3935 (0.4082) acc 91.3462 (91.0982) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [50/51] time 0.163 (0.206) data 0.000 (0.032) loss 0.5299 (0.4116) acc 86.9792 (90.9537) lr 4.1221e-04 eta 0:02:16
>>> alpha1: 0.131  alpha2: -0.105 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [38/50] batch [5/51] time 0.190 (0.492) data 0.000 (0.313) loss 0.4496 (0.3935) acc 92.5000 (91.4079) lr 3.6258e-04 eta 0:05:23
epoch [38/50] batch [10/51] time 0.185 (0.338) data 0.000 (0.156) loss 0.4944 (0.4249) acc 92.0000 (91.3213) lr 3.6258e-04 eta 0:03:40
epoch [38/50] batch [15/51] time 0.179 (0.285) data 0.000 (0.104) loss 0.2783 (0.4297) acc 93.8679 (91.1944) lr 3.6258e-04 eta 0:03:04
epoch [38/50] batch [20/51] time 0.185 (0.256) data 0.000 (0.078) loss 0.4528 (0.4437) acc 89.9038 (90.8938) lr 3.6258e-04 eta 0:02:44
epoch [38/50] batch [25/51] time 0.181 (0.241) data 0.000 (0.063) loss 0.4560 (0.4419) acc 91.3265 (90.6845) lr 3.6258e-04 eta 0:02:33
epoch [38/50] batch [30/51] time 0.194 (0.230) data 0.000 (0.052) loss 0.5549 (0.4477) acc 83.4906 (90.2889) lr 3.6258e-04 eta 0:02:25
epoch [38/50] batch [35/51] time 0.185 (0.224) data 0.000 (0.045) loss 0.3157 (0.4329) acc 96.0784 (90.7533) lr 3.6258e-04 eta 0:02:20
epoch [38/50] batch [40/51] time 0.176 (0.219) data 0.000 (0.039) loss 0.5054 (0.4319) acc 92.5000 (90.8928) lr 3.6258e-04 eta 0:02:16
epoch [38/50] batch [45/51] time 0.169 (0.213) data 0.000 (0.035) loss 0.3307 (0.4296) acc 92.0000 (90.8409) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [50/51] time 0.176 (0.209) data 0.000 (0.032) loss 0.4655 (0.4336) acc 90.5660 (90.7814) lr 3.6258e-04 eta 0:02:08
>>> alpha1: 0.129  alpha2: -0.103 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [39/50] batch [5/51] time 0.186 (0.434) data 0.000 (0.248) loss 0.3352 (0.3288) acc 93.7500 (92.9088) lr 3.1545e-04 eta 0:04:23
epoch [39/50] batch [10/51] time 0.181 (0.306) data 0.001 (0.124) loss 0.3485 (0.3660) acc 90.5660 (91.3999) lr 3.1545e-04 eta 0:03:03
epoch [39/50] batch [15/51] time 0.178 (0.264) data 0.000 (0.083) loss 0.4030 (0.3728) acc 92.7885 (91.2096) lr 3.1545e-04 eta 0:02:37
epoch [39/50] batch [20/51] time 0.177 (0.240) data 0.000 (0.062) loss 0.4143 (0.3786) acc 90.0000 (91.3320) lr 3.1545e-04 eta 0:02:22
epoch [39/50] batch [25/51] time 0.178 (0.227) data 0.000 (0.050) loss 0.6356 (0.3928) acc 84.1346 (90.9533) lr 3.1545e-04 eta 0:02:13
epoch [39/50] batch [30/51] time 0.161 (0.218) data 0.000 (0.042) loss 0.3545 (0.4162) acc 89.6341 (90.8802) lr 3.1545e-04 eta 0:02:06
epoch [39/50] batch [35/51] time 0.181 (0.213) data 0.000 (0.036) loss 0.3300 (0.4120) acc 93.5000 (90.9613) lr 3.1545e-04 eta 0:02:02
epoch [39/50] batch [40/51] time 0.168 (0.207) data 0.000 (0.032) loss 0.4709 (0.4200) acc 89.5000 (90.8176) lr 3.1545e-04 eta 0:01:58
epoch [39/50] batch [45/51] time 0.173 (0.203) data 0.000 (0.028) loss 0.4910 (0.4176) acc 91.8269 (90.9539) lr 3.1545e-04 eta 0:01:55
epoch [39/50] batch [50/51] time 0.161 (0.200) data 0.000 (0.025) loss 0.5337 (0.4166) acc 87.2340 (90.9423) lr 3.1545e-04 eta 0:01:52
>>> alpha1: 0.131  alpha2: -0.102 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [40/50] batch [5/51] time 0.170 (0.499) data 0.000 (0.319) loss 0.4399 (0.4063) acc 91.0000 (90.9893) lr 2.7103e-04 eta 0:04:37
epoch [40/50] batch [10/51] time 0.169 (0.336) data 0.000 (0.160) loss 0.4438 (0.4147) acc 88.5000 (91.3889) lr 2.7103e-04 eta 0:03:04
epoch [40/50] batch [15/51] time 0.171 (0.281) data 0.000 (0.107) loss 0.5467 (0.4415) acc 88.7255 (90.2579) lr 2.7103e-04 eta 0:02:33
epoch [40/50] batch [20/51] time 0.196 (0.256) data 0.000 (0.081) loss 0.3289 (0.4312) acc 94.5455 (90.5110) lr 2.7103e-04 eta 0:02:18
epoch [40/50] batch [25/51] time 0.169 (0.239) data 0.000 (0.065) loss 0.5442 (0.4439) acc 87.7551 (90.2162) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [30/51] time 0.177 (0.229) data 0.000 (0.054) loss 0.2591 (0.4298) acc 96.6981 (90.7504) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [35/51] time 0.170 (0.222) data 0.000 (0.046) loss 0.4577 (0.4183) acc 91.8478 (91.0928) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [40/51] time 0.165 (0.216) data 0.000 (0.041) loss 0.4198 (0.4090) acc 88.7755 (91.2154) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [45/51] time 0.173 (0.211) data 0.000 (0.036) loss 0.3232 (0.4142) acc 91.8269 (91.0783) lr 2.7103e-04 eta 0:01:48
epoch [40/50] batch [50/51] time 0.161 (0.207) data 0.000 (0.032) loss 0.3203 (0.4081) acc 94.1489 (91.2810) lr 2.7103e-04 eta 0:01:45
>>> alpha1: 0.130  alpha2: -0.103 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [41/50] batch [5/51] time 0.168 (0.480) data 0.000 (0.302) loss 0.2235 (0.3552) acc 97.0000 (93.7828) lr 2.2949e-04 eta 0:04:02
epoch [41/50] batch [10/51] time 0.176 (0.328) data 0.000 (0.151) loss 0.3890 (0.4103) acc 94.3396 (93.2192) lr 2.2949e-04 eta 0:02:44
epoch [41/50] batch [15/51] time 0.169 (0.277) data 0.000 (0.101) loss 0.4627 (0.4008) acc 92.5532 (92.9929) lr 2.2949e-04 eta 0:02:17
epoch [41/50] batch [20/51] time 0.165 (0.252) data 0.000 (0.076) loss 0.3336 (0.3870) acc 94.7917 (93.1464) lr 2.2949e-04 eta 0:02:03
epoch [41/50] batch [25/51] time 0.189 (0.238) data 0.000 (0.061) loss 0.3193 (0.4004) acc 94.3396 (92.3302) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [30/51] time 0.183 (0.228) data 0.001 (0.051) loss 0.4220 (0.4007) acc 89.1509 (92.0503) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [35/51] time 0.182 (0.221) data 0.000 (0.043) loss 0.4644 (0.4054) acc 91.0000 (91.6227) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [40/51] time 0.165 (0.214) data 0.000 (0.038) loss 0.3038 (0.4127) acc 94.3878 (91.2937) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [45/51] time 0.170 (0.209) data 0.000 (0.034) loss 0.3985 (0.4200) acc 93.6274 (91.0883) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [50/51] time 0.171 (0.205) data 0.000 (0.030) loss 0.4754 (0.4245) acc 89.7059 (90.9439) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.129  alpha2: -0.102 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [42/50] batch [5/51] time 0.189 (0.465) data 0.000 (0.281) loss 0.4642 (0.4055) acc 89.0000 (90.9336) lr 1.9098e-04 eta 0:03:31
epoch [42/50] batch [10/51] time 0.166 (0.319) data 0.000 (0.141) loss 0.2844 (0.3664) acc 94.2708 (91.9404) lr 1.9098e-04 eta 0:02:23
epoch [42/50] batch [15/51] time 0.162 (0.271) data 0.000 (0.094) loss 0.5415 (0.4178) acc 88.5870 (90.9238) lr 1.9098e-04 eta 0:02:00
epoch [42/50] batch [20/51] time 0.199 (0.249) data 0.000 (0.070) loss 0.1733 (0.3858) acc 97.6852 (91.4663) lr 1.9098e-04 eta 0:01:49
epoch [42/50] batch [25/51] time 0.170 (0.233) data 0.000 (0.056) loss 0.4208 (0.4008) acc 90.9574 (91.0971) lr 1.9098e-04 eta 0:01:40
epoch [42/50] batch [30/51] time 0.175 (0.223) data 0.000 (0.047) loss 0.4589 (0.4183) acc 89.7059 (91.0215) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [35/51] time 0.179 (0.217) data 0.000 (0.040) loss 0.2511 (0.4100) acc 96.2963 (91.1284) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.164 (0.211) data 0.000 (0.035) loss 0.4468 (0.4107) acc 91.1458 (91.1239) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.172 (0.207) data 0.000 (0.031) loss 0.2827 (0.4094) acc 93.2692 (91.1117) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.165 (0.203) data 0.000 (0.028) loss 0.4671 (0.4108) acc 90.1042 (91.1148) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.129  alpha2: -0.101 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [43/50] batch [5/51] time 0.169 (0.490) data 0.000 (0.304) loss 0.4356 (0.4074) acc 86.9792 (90.4769) lr 1.5567e-04 eta 0:03:17
epoch [43/50] batch [10/51] time 0.186 (0.335) data 0.000 (0.152) loss 0.1996 (0.3711) acc 95.1923 (91.7677) lr 1.5567e-04 eta 0:02:13
epoch [43/50] batch [15/51] time 0.177 (0.280) data 0.000 (0.101) loss 0.4832 (0.3943) acc 88.7255 (90.7710) lr 1.5567e-04 eta 0:01:49
epoch [43/50] batch [20/51] time 0.164 (0.253) data 0.000 (0.076) loss 0.4076 (0.3977) acc 91.3043 (91.0947) lr 1.5567e-04 eta 0:01:38
epoch [43/50] batch [25/51] time 0.161 (0.237) data 0.000 (0.061) loss 0.5007 (0.3953) acc 87.5000 (91.2768) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.173 (0.227) data 0.001 (0.051) loss 0.2960 (0.4068) acc 94.0000 (91.2513) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.178 (0.219) data 0.000 (0.044) loss 0.3988 (0.4022) acc 89.3617 (91.2172) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.038) loss 0.3892 (0.4094) acc 90.6863 (90.9483) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [45/51] time 0.167 (0.209) data 0.000 (0.034) loss 0.4218 (0.4065) acc 90.5000 (91.0476) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [50/51] time 0.176 (0.205) data 0.000 (0.031) loss 0.3519 (0.4068) acc 92.9245 (90.9786) lr 1.5567e-04 eta 0:01:13
>>> alpha1: 0.131  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [44/50] batch [5/51] time 0.175 (0.475) data 0.000 (0.280) loss 0.3462 (0.3663) acc 95.1923 (93.0177) lr 1.2369e-04 eta 0:02:47
epoch [44/50] batch [10/51] time 0.178 (0.325) data 0.000 (0.140) loss 0.4691 (0.3899) acc 88.2353 (92.1185) lr 1.2369e-04 eta 0:01:52
epoch [44/50] batch [15/51] time 0.174 (0.276) data 0.000 (0.094) loss 0.4062 (0.3846) acc 90.8654 (92.1216) lr 1.2369e-04 eta 0:01:34
epoch [44/50] batch [20/51] time 0.189 (0.251) data 0.000 (0.070) loss 0.2835 (0.3820) acc 94.3396 (92.2182) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [25/51] time 0.199 (0.239) data 0.000 (0.056) loss 0.4206 (0.4008) acc 90.7407 (91.8229) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [30/51] time 0.180 (0.231) data 0.001 (0.047) loss 0.3101 (0.4003) acc 94.2708 (91.8208) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [35/51] time 0.169 (0.223) data 0.000 (0.040) loss 0.3659 (0.4135) acc 89.7959 (91.3373) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.166 (0.217) data 0.000 (0.035) loss 0.4089 (0.4120) acc 93.3673 (91.3841) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.171 (0.211) data 0.000 (0.031) loss 0.4606 (0.4197) acc 88.2653 (91.1900) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [50/51] time 0.170 (0.207) data 0.000 (0.028) loss 0.3419 (0.4149) acc 96.5686 (91.2815) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.130  alpha2: -0.112 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [45/50] batch [5/51] time 0.211 (0.435) data 0.001 (0.247) loss 0.4650 (0.4572) acc 90.6863 (89.6177) lr 9.5173e-05 eta 0:02:11
epoch [45/50] batch [10/51] time 0.192 (0.310) data 0.000 (0.124) loss 0.4739 (0.4110) acc 88.5000 (90.5740) lr 9.5173e-05 eta 0:01:31
epoch [45/50] batch [15/51] time 0.188 (0.270) data 0.000 (0.083) loss 0.4385 (0.4134) acc 90.6863 (90.2073) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.062) loss 0.3258 (0.3917) acc 92.1296 (90.7777) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [25/51] time 0.181 (0.234) data 0.000 (0.050) loss 0.4867 (0.3990) acc 87.2449 (90.8566) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [30/51] time 0.165 (0.226) data 0.000 (0.042) loss 0.4958 (0.3900) acc 89.4445 (91.2331) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [35/51] time 0.162 (0.219) data 0.000 (0.036) loss 0.5701 (0.3949) acc 86.9565 (91.1369) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [40/51] time 0.179 (0.214) data 0.000 (0.031) loss 0.4375 (0.4031) acc 88.0000 (91.0380) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [45/51] time 0.164 (0.208) data 0.000 (0.028) loss 0.3919 (0.4060) acc 88.5417 (90.8331) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [50/51] time 0.172 (0.204) data 0.000 (0.025) loss 0.4104 (0.3996) acc 88.5000 (90.9730) lr 9.5173e-05 eta 0:00:52
>>> alpha1: 0.129  alpha2: -0.110 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [46/50] batch [5/51] time 0.177 (0.482) data 0.000 (0.297) loss 0.4078 (0.3970) acc 89.9038 (91.8467) lr 7.0224e-05 eta 0:02:00
epoch [46/50] batch [10/51] time 0.175 (0.331) data 0.000 (0.148) loss 0.4529 (0.3761) acc 86.0000 (91.7464) lr 7.0224e-05 eta 0:01:21
epoch [46/50] batch [15/51] time 0.173 (0.280) data 0.000 (0.099) loss 0.2803 (0.3496) acc 93.6170 (92.7804) lr 7.0224e-05 eta 0:01:07
epoch [46/50] batch [20/51] time 0.182 (0.255) data 0.000 (0.074) loss 0.3660 (0.3567) acc 93.0000 (92.3714) lr 7.0224e-05 eta 0:00:59
epoch [46/50] batch [25/51] time 0.166 (0.237) data 0.000 (0.060) loss 0.4902 (0.3730) acc 90.1042 (92.1479) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [30/51] time 0.170 (0.228) data 0.000 (0.050) loss 0.4143 (0.3725) acc 92.5000 (92.1188) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.190 (0.221) data 0.000 (0.043) loss 0.3715 (0.3773) acc 90.8654 (92.0069) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [40/51] time 0.165 (0.215) data 0.000 (0.038) loss 0.3344 (0.3762) acc 94.2708 (92.1726) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.169 (0.211) data 0.000 (0.034) loss 0.5226 (0.3885) acc 91.0000 (91.8657) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.169 (0.207) data 0.000 (0.030) loss 0.6176 (0.3912) acc 89.0000 (91.8943) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.130  alpha2: -0.113 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.07 <<<
epoch [47/50] batch [5/51] time 0.171 (0.454) data 0.000 (0.272) loss 0.3142 (0.3918) acc 93.1373 (91.5883) lr 4.8943e-05 eta 0:01:30
epoch [47/50] batch [10/51] time 0.171 (0.315) data 0.000 (0.136) loss 0.4087 (0.3685) acc 93.1373 (92.1919) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [15/51] time 0.169 (0.268) data 0.000 (0.091) loss 0.4320 (0.3528) acc 91.5000 (92.4014) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [20/51] time 0.206 (0.245) data 0.000 (0.068) loss 0.5946 (0.3716) acc 88.4259 (92.0156) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [25/51] time 0.180 (0.232) data 0.000 (0.055) loss 0.2651 (0.3706) acc 95.8333 (92.3249) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.201 (0.224) data 0.027 (0.047) loss 0.3397 (0.3734) acc 93.6274 (92.1513) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [35/51] time 0.158 (0.217) data 0.000 (0.040) loss 0.3480 (0.3765) acc 94.4445 (92.0171) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.177 (0.211) data 0.000 (0.035) loss 0.3399 (0.3809) acc 91.5094 (91.8945) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.163 (0.207) data 0.000 (0.031) loss 0.2729 (0.3796) acc 94.7917 (91.8546) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.172 (0.203) data 0.000 (0.028) loss 0.3996 (0.3771) acc 95.1923 (92.0482) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.131  alpha2: -0.115 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [48/50] batch [5/51] time 0.183 (0.469) data 0.001 (0.285) loss 0.2987 (0.4428) acc 94.3878 (90.0327) lr 3.1417e-05 eta 0:01:09
epoch [48/50] batch [10/51] time 0.174 (0.323) data 0.000 (0.145) loss 0.4690 (0.4325) acc 89.9038 (90.4908) lr 3.1417e-05 eta 0:00:46
epoch [48/50] batch [15/51] time 0.172 (0.274) data 0.001 (0.096) loss 0.4486 (0.4004) acc 89.7059 (90.8958) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [20/51] time 0.178 (0.250) data 0.000 (0.072) loss 0.3453 (0.3773) acc 92.4528 (91.5687) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.178 (0.235) data 0.000 (0.058) loss 0.2633 (0.3824) acc 95.7547 (91.8752) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.191 (0.226) data 0.000 (0.048) loss 0.2218 (0.3744) acc 95.9091 (92.0995) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.170 (0.218) data 0.001 (0.042) loss 0.5029 (0.3820) acc 85.2041 (91.7319) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.167 (0.212) data 0.000 (0.036) loss 0.4194 (0.3859) acc 95.5000 (91.6872) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [45/51] time 0.177 (0.208) data 0.000 (0.032) loss 0.4902 (0.3867) acc 90.5000 (91.7057) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.195 (0.205) data 0.000 (0.029) loss 0.4047 (0.3820) acc 89.2241 (91.8722) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.129  alpha2: -0.109 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [49/50] batch [5/51] time 0.192 (0.462) data 0.000 (0.277) loss 0.2344 (0.3257) acc 95.7547 (93.9437) lr 1.7713e-05 eta 0:00:44
epoch [49/50] batch [10/51] time 0.158 (0.316) data 0.000 (0.139) loss 0.2636 (0.3018) acc 93.8889 (94.1983) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.166 (0.268) data 0.000 (0.093) loss 0.4233 (0.3319) acc 92.3469 (93.7818) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.198 (0.246) data 0.000 (0.070) loss 0.4573 (0.3457) acc 90.0943 (93.2228) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.157 (0.233) data 0.000 (0.056) loss 0.4670 (0.3453) acc 91.6667 (93.0732) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.178 (0.223) data 0.000 (0.047) loss 0.3849 (0.3562) acc 92.8571 (92.6889) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.163 (0.217) data 0.000 (0.040) loss 0.4328 (0.3640) acc 91.4894 (92.4237) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.166 (0.211) data 0.000 (0.035) loss 0.4910 (0.3740) acc 89.2857 (92.1996) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.167 (0.207) data 0.000 (0.031) loss 0.2721 (0.3700) acc 93.5000 (92.1800) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.184 (0.203) data 0.000 (0.028) loss 0.3714 (0.3653) acc 91.6667 (92.3593) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.128  alpha2: -0.105 <<<
>>> noisy rate: 0.12 --> refined noisy rate: 0.04 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.07 <<<
epoch [50/50] batch [5/51] time 0.169 (0.511) data 0.000 (0.328) loss 0.5986 (0.4225) acc 85.4167 (90.4891) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.194 (0.345) data 0.000 (0.164) loss 0.4241 (0.3927) acc 89.9038 (91.1440) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.170 (0.290) data 0.000 (0.109) loss 0.6085 (0.4022) acc 85.0000 (90.9114) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.177 (0.261) data 0.000 (0.082) loss 0.2430 (0.3909) acc 95.2830 (91.5267) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.165 (0.243) data 0.000 (0.066) loss 0.3730 (0.3825) acc 91.1458 (91.7305) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.177 (0.231) data 0.000 (0.055) loss 0.5432 (0.3799) acc 87.5000 (91.7997) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.173 (0.223) data 0.000 (0.047) loss 0.3615 (0.3854) acc 92.3077 (91.7046) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.177 (0.217) data 0.000 (0.041) loss 0.5528 (0.3963) acc 86.1111 (91.2559) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.037) loss 0.5365 (0.3978) acc 85.2941 (91.0883) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.168 (0.207) data 0.000 (0.033) loss 0.3677 (0.3933) acc 93.5000 (91.2811) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_2FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.04, 0.05, 0.04, 0.04, 0.04, 0.05, 0.05, 0.04, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.04, 0.04, 0.04, 0.05, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.04, 0.04, 0.04]
* matched noise rate: [0.03, 0.02, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02]
* unmatched noise rate: [0.05, 0.07, 0.08, 0.08, 0.08, 0.09, 0.1, 0.08, 0.09, 0.09, 0.09, 0.08, 0.09, 0.09, 0.09, 0.08, 0.08, 0.08, 0.09, 0.09, 0.09, 0.09, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:05,  2.73s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.30it/s] 20%|██        | 5/25 [00:03<00:08,  2.40it/s] 28%|██▊       | 7/25 [00:03<00:04,  3.62it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.93it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.25it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.48it/s] 60%|██████    | 15/25 [00:03<00:01,  8.55it/s] 68%|██████▊   | 17/25 [00:04<00:01,  7.06it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.12it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.08it/s] 92%|█████████▏| 23/25 [00:04<00:00,  9.88it/s]100%|██████████| 25/25 [00:05<00:00,  6.90it/s]100%|██████████| 25/25 [00:05<00:00,  4.61it/s]
=> result
* total: 2,463
* correct: 2,231
* accuracy: 90.6%
* error: 9.4%
* macro_f1: 90.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 2	acc: 16.7%
* class: 3 (sweet pea)	total: 17	correct: 11	acc: 64.7%
* class: 4 (english marigold)	total: 20	correct: 15	acc: 75.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 11	acc: 78.6%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 25	acc: 96.2%
* class: 11 (colt's foot)	total: 26	correct: 24	acc: 92.3%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 12	acc: 92.3%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 18	acc: 72.0%
* class: 18 (balloon flower)	total: 15	correct: 11	acc: 73.3%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 11	acc: 91.7%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 24	acc: 88.9%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 25	acc: 96.2%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 13	acc: 92.9%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 19	acc: 86.4%
* class: 36 (cape flower)	total: 32	correct: 31	acc: 96.9%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 9	acc: 69.2%
* class: 39 (lenten rose)	total: 20	correct: 19	acc: 95.0%
* class: 40 (barbeton daisy)	total: 38	correct: 24	acc: 63.2%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 20	acc: 51.3%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 11	acc: 91.7%
* class: 45 (wallflower)	total: 59	correct: 54	acc: 91.5%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 24	acc: 85.7%
* class: 50 (petunia)	total: 77	correct: 54	acc: 70.1%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 28	acc: 100.0%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 15	acc: 93.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 30	acc: 96.8%
* class: 65 (osteospermum)	total: 19	correct: 18	acc: 94.7%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 14	acc: 87.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 28	acc: 77.8%
* class: 75 (morning glory)	total: 32	correct: 29	acc: 90.6%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 36	acc: 92.3%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 15	acc: 83.3%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 33	acc: 71.7%
* class: 88 (watercress)	total: 55	correct: 48	acc: 87.3%
* class: 89 (canna lily)	total: 25	correct: 19	acc: 76.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 46	acc: 93.9%
* class: 94 (bougainvillea)	total: 38	correct: 30	acc: 78.9%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 21	acc: 84.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 15	acc: 88.2%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 91.2%
Elapsed: 0:28:12
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_4-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.266 (1.116) data 0.000 (0.427) loss 4.4661 (4.4177) acc 6.2500 (11.8750) lr 1.0000e-05 eta 0:47:19
epoch [1/50] batch [10/51] time 0.266 (0.692) data 0.000 (0.213) loss 4.2662 (4.3371) acc 12.5000 (12.8125) lr 1.0000e-05 eta 0:29:16
epoch [1/50] batch [15/51] time 0.270 (0.550) data 0.000 (0.142) loss 4.3494 (4.2798) acc 15.6250 (14.1667) lr 1.0000e-05 eta 0:23:14
epoch [1/50] batch [20/51] time 0.259 (0.479) data 0.000 (0.107) loss 4.0113 (4.1832) acc 25.0000 (15.4688) lr 1.0000e-05 eta 0:20:11
epoch [1/50] batch [25/51] time 0.261 (0.435) data 0.000 (0.085) loss 3.4973 (4.0543) acc 34.3750 (19.3750) lr 1.0000e-05 eta 0:18:18
epoch [1/50] batch [30/51] time 0.268 (0.407) data 0.000 (0.071) loss 3.5636 (3.9392) acc 46.8750 (21.6667) lr 1.0000e-05 eta 0:17:04
epoch [1/50] batch [35/51] time 0.262 (0.387) data 0.000 (0.061) loss 3.1978 (3.8413) acc 50.0000 (24.4643) lr 1.0000e-05 eta 0:16:12
epoch [1/50] batch [40/51] time 0.257 (0.371) data 0.000 (0.054) loss 3.3312 (3.7757) acc 37.5000 (25.9375) lr 1.0000e-05 eta 0:15:30
epoch [1/50] batch [45/51] time 0.258 (0.358) data 0.000 (0.048) loss 3.8438 (3.7325) acc 31.2500 (27.7778) lr 1.0000e-05 eta 0:14:57
epoch [1/50] batch [50/51] time 0.257 (0.348) data 0.000 (0.043) loss 3.4150 (3.6718) acc 40.6250 (29.3125) lr 1.0000e-05 eta 0:14:30
epoch [2/50] batch [5/51] time 0.278 (0.560) data 0.000 (0.285) loss 3.9212 (4.1041) acc 12.5000 (15.6250) lr 2.0000e-03 eta 0:23:17
epoch [2/50] batch [10/51] time 0.275 (0.413) data 0.000 (0.143) loss 2.6408 (3.5747) acc 53.1250 (29.6875) lr 2.0000e-03 eta 0:17:07
epoch [2/50] batch [15/51] time 0.261 (0.364) data 0.000 (0.095) loss 2.5419 (3.4057) acc 50.0000 (35.0000) lr 2.0000e-03 eta 0:15:03
epoch [2/50] batch [20/51] time 0.266 (0.340) data 0.000 (0.071) loss 2.9773 (3.3357) acc 46.8750 (37.9688) lr 2.0000e-03 eta 0:14:03
epoch [2/50] batch [25/51] time 0.262 (0.325) data 0.000 (0.057) loss 2.8801 (3.3534) acc 37.5000 (37.1250) lr 2.0000e-03 eta 0:13:23
epoch [2/50] batch [30/51] time 0.260 (0.315) data 0.000 (0.048) loss 3.0964 (3.3180) acc 56.2500 (38.1250) lr 2.0000e-03 eta 0:12:57
epoch [2/50] batch [35/51] time 0.273 (0.308) data 0.000 (0.041) loss 2.7041 (3.3200) acc 59.3750 (38.5714) lr 2.0000e-03 eta 0:12:38
epoch [2/50] batch [40/51] time 0.257 (0.302) data 0.000 (0.036) loss 2.7561 (3.2968) acc 50.0000 (39.2188) lr 2.0000e-03 eta 0:12:22
epoch [2/50] batch [45/51] time 0.259 (0.297) data 0.000 (0.032) loss 2.7335 (3.2499) acc 31.2500 (39.7222) lr 2.0000e-03 eta 0:12:09
epoch [2/50] batch [50/51] time 0.258 (0.293) data 0.000 (0.029) loss 3.4414 (3.2263) acc 40.6250 (40.6250) lr 2.0000e-03 eta 0:11:58
epoch [3/50] batch [5/51] time 0.275 (0.568) data 0.000 (0.280) loss 2.6966 (2.9782) acc 59.3750 (47.5000) lr 1.9980e-03 eta 0:23:07
epoch [3/50] batch [10/51] time 0.271 (0.419) data 0.000 (0.140) loss 3.2257 (3.0962) acc 37.5000 (45.0000) lr 1.9980e-03 eta 0:17:01
epoch [3/50] batch [15/51] time 0.272 (0.368) data 0.000 (0.093) loss 3.4106 (3.0369) acc 43.7500 (46.2500) lr 1.9980e-03 eta 0:14:54
epoch [3/50] batch [20/51] time 0.263 (0.341) data 0.000 (0.070) loss 2.5759 (3.0479) acc 59.3750 (45.6250) lr 1.9980e-03 eta 0:13:48
epoch [3/50] batch [25/51] time 0.264 (0.326) data 0.000 (0.056) loss 2.6870 (3.0627) acc 46.8750 (45.8750) lr 1.9980e-03 eta 0:13:10
epoch [3/50] batch [30/51] time 0.266 (0.317) data 0.000 (0.047) loss 3.5048 (2.9939) acc 40.6250 (46.5625) lr 1.9980e-03 eta 0:12:45
epoch [3/50] batch [35/51] time 0.269 (0.309) data 0.000 (0.040) loss 2.6718 (2.9715) acc 56.2500 (47.5000) lr 1.9980e-03 eta 0:12:25
epoch [3/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.035) loss 2.5140 (2.9463) acc 46.8750 (47.8906) lr 1.9980e-03 eta 0:12:09
epoch [3/50] batch [45/51] time 0.258 (0.298) data 0.000 (0.031) loss 4.1790 (2.9691) acc 31.2500 (47.8472) lr 1.9980e-03 eta 0:11:55
epoch [3/50] batch [50/51] time 0.257 (0.294) data 0.000 (0.028) loss 3.8961 (2.9438) acc 25.0000 (48.5625) lr 1.9980e-03 eta 0:11:44
epoch [4/50] batch [5/51] time 0.272 (0.569) data 0.000 (0.273) loss 2.4694 (2.8426) acc 50.0000 (50.0000) lr 1.9921e-03 eta 0:22:40
epoch [4/50] batch [10/51] time 0.271 (0.418) data 0.000 (0.137) loss 3.1442 (2.9242) acc 56.2500 (49.0625) lr 1.9921e-03 eta 0:16:38
epoch [4/50] batch [15/51] time 0.275 (0.368) data 0.000 (0.091) loss 2.8221 (2.8427) acc 59.3750 (52.2917) lr 1.9921e-03 eta 0:14:36
epoch [4/50] batch [20/51] time 0.260 (0.342) data 0.000 (0.068) loss 2.3117 (2.8177) acc 50.0000 (51.2500) lr 1.9921e-03 eta 0:13:33
epoch [4/50] batch [25/51] time 0.268 (0.326) data 0.000 (0.055) loss 3.1190 (2.8276) acc 53.1250 (51.6250) lr 1.9921e-03 eta 0:12:53
epoch [4/50] batch [30/51] time 0.281 (0.317) data 0.000 (0.046) loss 2.7717 (2.8703) acc 56.2500 (50.7292) lr 1.9921e-03 eta 0:12:30
epoch [4/50] batch [35/51] time 0.272 (0.309) data 0.000 (0.039) loss 1.9557 (2.8197) acc 68.7500 (51.9643) lr 1.9921e-03 eta 0:12:10
epoch [4/50] batch [40/51] time 0.260 (0.303) data 0.000 (0.034) loss 2.6555 (2.8387) acc 46.8750 (51.1719) lr 1.9921e-03 eta 0:11:55
epoch [4/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.031) loss 3.0340 (2.8190) acc 53.1250 (51.0417) lr 1.9921e-03 eta 0:11:41
epoch [4/50] batch [50/51] time 0.259 (0.294) data 0.000 (0.028) loss 3.9612 (2.8505) acc 46.8750 (50.3750) lr 1.9921e-03 eta 0:11:30
epoch [5/50] batch [5/51] time 0.267 (0.624) data 0.000 (0.352) loss 1.8881 (2.3606) acc 65.6250 (58.7500) lr 1.9823e-03 eta 0:24:19
epoch [5/50] batch [10/51] time 0.280 (0.446) data 0.000 (0.176) loss 3.0077 (2.6587) acc 43.7500 (54.6875) lr 1.9823e-03 eta 0:17:22
epoch [5/50] batch [15/51] time 0.271 (0.386) data 0.000 (0.117) loss 2.6575 (2.6104) acc 46.8750 (54.3750) lr 1.9823e-03 eta 0:15:00
epoch [5/50] batch [20/51] time 0.264 (0.357) data 0.000 (0.088) loss 2.7891 (2.6867) acc 50.0000 (53.4375) lr 1.9823e-03 eta 0:13:50
epoch [5/50] batch [25/51] time 0.268 (0.338) data 0.000 (0.070) loss 3.1989 (2.6780) acc 46.8750 (53.3750) lr 1.9823e-03 eta 0:13:05
epoch [5/50] batch [30/51] time 0.287 (0.327) data 0.000 (0.059) loss 3.0276 (2.7379) acc 50.0000 (53.1250) lr 1.9823e-03 eta 0:12:37
epoch [5/50] batch [35/51] time 0.263 (0.319) data 0.000 (0.050) loss 2.2826 (2.7443) acc 65.6250 (53.3036) lr 1.9823e-03 eta 0:12:17
epoch [5/50] batch [40/51] time 0.259 (0.312) data 0.000 (0.044) loss 3.1927 (2.7402) acc 46.8750 (53.2031) lr 1.9823e-03 eta 0:11:59
epoch [5/50] batch [45/51] time 0.258 (0.306) data 0.000 (0.039) loss 3.4781 (2.7417) acc 40.6250 (53.3333) lr 1.9823e-03 eta 0:11:44
epoch [5/50] batch [50/51] time 0.259 (0.301) data 0.000 (0.035) loss 2.1733 (2.7743) acc 56.2500 (53.3750) lr 1.9823e-03 eta 0:11:31
epoch [6/50] batch [5/51] time 0.323 (0.614) data 0.001 (0.319) loss 3.0921 (2.4713) acc 53.1250 (60.6250) lr 1.9686e-03 eta 0:23:26
epoch [6/50] batch [10/51] time 0.261 (0.439) data 0.000 (0.160) loss 3.2683 (2.6339) acc 50.0000 (55.3125) lr 1.9686e-03 eta 0:16:44
epoch [6/50] batch [15/51] time 0.270 (0.383) data 0.000 (0.106) loss 2.5091 (2.5785) acc 50.0000 (55.6250) lr 1.9686e-03 eta 0:14:33
epoch [6/50] batch [20/51] time 0.263 (0.354) data 0.000 (0.080) loss 3.2400 (2.6303) acc 43.7500 (55.6250) lr 1.9686e-03 eta 0:13:24
epoch [6/50] batch [25/51] time 0.272 (0.336) data 0.000 (0.064) loss 2.9173 (2.6834) acc 56.2500 (55.5000) lr 1.9686e-03 eta 0:12:43
epoch [6/50] batch [30/51] time 0.278 (0.325) data 0.000 (0.053) loss 3.0490 (2.7076) acc 53.1250 (55.2083) lr 1.9686e-03 eta 0:12:16
epoch [6/50] batch [35/51] time 0.268 (0.317) data 0.000 (0.046) loss 2.5787 (2.7345) acc 53.1250 (54.8214) lr 1.9686e-03 eta 0:11:55
epoch [6/50] batch [40/51] time 0.258 (0.309) data 0.000 (0.040) loss 1.7824 (2.6974) acc 62.5000 (55.1562) lr 1.9686e-03 eta 0:11:37
epoch [6/50] batch [45/51] time 0.257 (0.304) data 0.000 (0.036) loss 2.2945 (2.7101) acc 65.6250 (55.0694) lr 1.9686e-03 eta 0:11:23
epoch [6/50] batch [50/51] time 0.258 (0.299) data 0.000 (0.032) loss 2.4331 (2.7026) acc 62.5000 (55.5000) lr 1.9686e-03 eta 0:11:11
epoch [7/50] batch [5/51] time 0.300 (0.572) data 0.000 (0.293) loss 3.1925 (2.7933) acc 46.8750 (54.3750) lr 1.9511e-03 eta 0:21:21
epoch [7/50] batch [10/51] time 0.262 (0.418) data 0.000 (0.146) loss 2.8908 (2.9698) acc 59.3750 (54.6875) lr 1.9511e-03 eta 0:15:33
epoch [7/50] batch [15/51] time 0.261 (0.369) data 0.000 (0.098) loss 2.3495 (2.7906) acc 56.2500 (56.0417) lr 1.9511e-03 eta 0:13:42
epoch [7/50] batch [20/51] time 0.261 (0.343) data 0.000 (0.073) loss 2.0834 (2.7389) acc 59.3750 (56.4062) lr 1.9511e-03 eta 0:12:42
epoch [7/50] batch [25/51] time 0.260 (0.327) data 0.000 (0.059) loss 2.1343 (2.6440) acc 68.7500 (57.8750) lr 1.9511e-03 eta 0:12:05
epoch [7/50] batch [30/51] time 0.270 (0.317) data 0.000 (0.049) loss 2.6394 (2.6777) acc 56.2500 (57.6042) lr 1.9511e-03 eta 0:11:41
epoch [7/50] batch [35/51] time 0.268 (0.311) data 0.000 (0.042) loss 1.9133 (2.6712) acc 68.7500 (57.8571) lr 1.9511e-03 eta 0:11:25
epoch [7/50] batch [40/51] time 0.259 (0.304) data 0.000 (0.037) loss 2.5419 (2.6469) acc 56.2500 (58.5156) lr 1.9511e-03 eta 0:11:10
epoch [7/50] batch [45/51] time 0.258 (0.299) data 0.000 (0.033) loss 2.5596 (2.6363) acc 59.3750 (58.4028) lr 1.9511e-03 eta 0:10:57
epoch [7/50] batch [50/51] time 0.258 (0.295) data 0.000 (0.029) loss 3.8667 (2.6687) acc 34.3750 (57.7500) lr 1.9511e-03 eta 0:10:47
epoch [8/50] batch [5/51] time 0.291 (0.588) data 0.000 (0.305) loss 2.2912 (2.5901) acc 43.7500 (57.5000) lr 1.9298e-03 eta 0:21:27
epoch [8/50] batch [10/51] time 0.261 (0.426) data 0.000 (0.153) loss 2.3582 (2.4942) acc 56.2500 (57.1875) lr 1.9298e-03 eta 0:15:30
epoch [8/50] batch [15/51] time 0.262 (0.373) data 0.000 (0.102) loss 2.4695 (2.5358) acc 56.2500 (57.0833) lr 1.9298e-03 eta 0:13:31
epoch [8/50] batch [20/51] time 0.274 (0.346) data 0.000 (0.076) loss 3.1123 (2.5961) acc 56.2500 (57.0312) lr 1.9298e-03 eta 0:12:31
epoch [8/50] batch [25/51] time 0.261 (0.330) data 0.000 (0.061) loss 2.5454 (2.6490) acc 56.2500 (56.0000) lr 1.9298e-03 eta 0:11:54
epoch [8/50] batch [30/51] time 0.260 (0.319) data 0.000 (0.051) loss 2.7851 (2.6138) acc 56.2500 (56.8750) lr 1.9298e-03 eta 0:11:29
epoch [8/50] batch [35/51] time 0.266 (0.310) data 0.000 (0.044) loss 1.6215 (2.6069) acc 75.0000 (57.5000) lr 1.9298e-03 eta 0:11:10
epoch [8/50] batch [40/51] time 0.258 (0.304) data 0.000 (0.038) loss 3.0807 (2.6055) acc 50.0000 (57.4219) lr 1.9298e-03 eta 0:10:55
epoch [8/50] batch [45/51] time 0.258 (0.299) data 0.000 (0.034) loss 2.0397 (2.5873) acc 59.3750 (57.7083) lr 1.9298e-03 eta 0:10:43
epoch [8/50] batch [50/51] time 0.258 (0.295) data 0.000 (0.031) loss 2.7724 (2.6053) acc 62.5000 (57.0625) lr 1.9298e-03 eta 0:10:32
epoch [9/50] batch [5/51] time 0.259 (0.533) data 0.000 (0.248) loss 2.2218 (2.0267) acc 65.6250 (68.7500) lr 1.9048e-03 eta 0:18:58
epoch [9/50] batch [10/51] time 0.264 (0.400) data 0.000 (0.124) loss 2.0999 (2.1561) acc 62.5000 (66.5625) lr 1.9048e-03 eta 0:14:13
epoch [9/50] batch [15/51] time 0.260 (0.355) data 0.000 (0.083) loss 2.0435 (2.2620) acc 62.5000 (64.7917) lr 1.9048e-03 eta 0:12:35
epoch [9/50] batch [20/51] time 0.274 (0.333) data 0.000 (0.062) loss 2.3381 (2.3696) acc 68.7500 (63.7500) lr 1.9048e-03 eta 0:11:46
epoch [9/50] batch [25/51] time 0.262 (0.320) data 0.000 (0.050) loss 1.9007 (2.3878) acc 65.6250 (63.1250) lr 1.9048e-03 eta 0:11:17
epoch [9/50] batch [30/51] time 0.271 (0.311) data 0.000 (0.042) loss 2.4265 (2.4129) acc 53.1250 (61.1458) lr 1.9048e-03 eta 0:10:57
epoch [9/50] batch [35/51] time 0.260 (0.305) data 0.000 (0.036) loss 3.3566 (2.4373) acc 46.8750 (60.6250) lr 1.9048e-03 eta 0:10:41
epoch [9/50] batch [40/51] time 0.258 (0.299) data 0.000 (0.031) loss 2.4363 (2.4522) acc 59.3750 (60.3906) lr 1.9048e-03 eta 0:10:28
epoch [9/50] batch [45/51] time 0.258 (0.294) data 0.000 (0.028) loss 2.6897 (2.4648) acc 50.0000 (60.2083) lr 1.9048e-03 eta 0:10:17
epoch [9/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.025) loss 2.8336 (2.5042) acc 56.2500 (59.8750) lr 1.9048e-03 eta 0:10:08
epoch [10/50] batch [5/51] time 0.263 (0.583) data 0.000 (0.310) loss 2.2256 (2.2977) acc 65.6250 (63.1250) lr 1.8763e-03 eta 0:20:16
epoch [10/50] batch [10/51] time 0.260 (0.425) data 0.000 (0.155) loss 3.0001 (2.4799) acc 50.0000 (61.2500) lr 1.8763e-03 eta 0:14:44
epoch [10/50] batch [15/51] time 0.274 (0.372) data 0.000 (0.104) loss 2.1996 (2.5039) acc 68.7500 (61.0417) lr 1.8763e-03 eta 0:12:52
epoch [10/50] batch [20/51] time 0.269 (0.347) data 0.000 (0.078) loss 1.9999 (2.5366) acc 71.8750 (61.0938) lr 1.8763e-03 eta 0:11:58
epoch [10/50] batch [25/51] time 0.260 (0.331) data 0.000 (0.062) loss 2.2348 (2.5284) acc 62.5000 (60.5000) lr 1.8763e-03 eta 0:11:23
epoch [10/50] batch [30/51] time 0.260 (0.320) data 0.000 (0.052) loss 2.1515 (2.5321) acc 59.3750 (59.8958) lr 1.8763e-03 eta 0:10:58
epoch [10/50] batch [35/51] time 0.261 (0.312) data 0.000 (0.045) loss 2.1581 (2.5182) acc 59.3750 (59.1964) lr 1.8763e-03 eta 0:10:41
epoch [10/50] batch [40/51] time 0.262 (0.306) data 0.000 (0.039) loss 1.8641 (2.4567) acc 68.7500 (60.0781) lr 1.8763e-03 eta 0:10:28
epoch [10/50] batch [45/51] time 0.261 (0.302) data 0.000 (0.035) loss 2.1725 (2.4656) acc 59.3750 (59.8611) lr 1.8763e-03 eta 0:10:16
epoch [10/50] batch [50/51] time 0.259 (0.297) data 0.000 (0.031) loss 2.7625 (2.4861) acc 53.1250 (59.9375) lr 1.8763e-03 eta 0:10:06
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.502  alpha2: 0.160 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [11/50] batch [5/51] time 0.908 (1.141) data 0.000 (0.324) loss 1.4004 (1.5366) acc 72.3214 (69.6670) lr 1.8443e-03 eta 0:38:42
epoch [11/50] batch [10/51] time 0.768 (0.853) data 0.000 (0.162) loss 0.8029 (1.3738) acc 84.5745 (72.9928) lr 1.8443e-03 eta 0:28:51
epoch [11/50] batch [15/51] time 0.160 (0.661) data 0.000 (0.108) loss 1.7522 (1.3508) acc 57.0652 (72.8348) lr 1.8443e-03 eta 0:22:18
epoch [11/50] batch [20/51] time 0.163 (0.573) data 0.000 (0.081) loss 0.9437 (1.2890) acc 83.3333 (74.0392) lr 1.8443e-03 eta 0:19:18
epoch [11/50] batch [25/51] time 0.165 (0.491) data 0.000 (0.065) loss 0.8948 (1.2716) acc 77.7174 (73.5302) lr 1.8443e-03 eta 0:16:28
epoch [11/50] batch [30/51] time 0.169 (0.437) data 0.000 (0.054) loss 1.2107 (1.2380) acc 71.8085 (74.1080) lr 1.8443e-03 eta 0:14:39
epoch [11/50] batch [35/51] time 0.160 (0.416) data 0.000 (0.046) loss 1.3958 (1.2447) acc 68.4783 (73.7986) lr 1.8443e-03 eta 0:13:53
epoch [11/50] batch [40/51] time 0.891 (0.402) data 0.000 (0.041) loss 1.2330 (1.2298) acc 71.7593 (73.8606) lr 1.8443e-03 eta 0:13:23
epoch [11/50] batch [45/51] time 0.160 (0.375) data 0.000 (0.036) loss 0.8627 (1.2203) acc 79.3478 (73.7362) lr 1.8443e-03 eta 0:12:28
epoch [11/50] batch [50/51] time 0.155 (0.379) data 0.000 (0.033) loss 1.4017 (1.2077) acc 65.1163 (73.9277) lr 1.8443e-03 eta 0:12:34
>>> alpha1: 0.406  alpha2: 0.137 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [12/50] batch [5/51] time 0.176 (0.496) data 0.000 (0.314) loss 0.8435 (1.0380) acc 79.3269 (74.0075) lr 1.8090e-03 eta 0:16:24
epoch [12/50] batch [10/51] time 0.174 (0.344) data 0.000 (0.157) loss 0.9139 (0.9468) acc 82.5000 (77.8811) lr 1.8090e-03 eta 0:11:19
epoch [12/50] batch [15/51] time 0.176 (0.290) data 0.000 (0.105) loss 0.6528 (0.9551) acc 84.5000 (77.8071) lr 1.8090e-03 eta 0:09:32
epoch [12/50] batch [20/51] time 0.168 (0.263) data 0.000 (0.079) loss 0.7550 (0.9632) acc 84.6939 (77.1305) lr 1.8090e-03 eta 0:08:38
epoch [12/50] batch [25/51] time 0.170 (0.274) data 0.000 (0.063) loss 0.9408 (0.9915) acc 77.9412 (76.7553) lr 1.8090e-03 eta 0:08:57
epoch [12/50] batch [30/51] time 0.186 (0.257) data 0.000 (0.053) loss 0.7150 (0.9716) acc 84.3137 (77.2756) lr 1.8090e-03 eta 0:08:23
epoch [12/50] batch [35/51] time 0.883 (0.266) data 0.001 (0.045) loss 0.8481 (0.9708) acc 81.1321 (77.1576) lr 1.8090e-03 eta 0:08:40
epoch [12/50] batch [40/51] time 0.179 (0.255) data 0.000 (0.040) loss 0.6860 (0.9411) acc 87.0370 (77.8488) lr 1.8090e-03 eta 0:08:16
epoch [12/50] batch [45/51] time 0.166 (0.245) data 0.000 (0.035) loss 0.7605 (0.9328) acc 83.1633 (78.0970) lr 1.8090e-03 eta 0:07:56
epoch [12/50] batch [50/51] time 0.170 (0.238) data 0.000 (0.032) loss 0.9585 (0.9453) acc 76.5000 (77.8233) lr 1.8090e-03 eta 0:07:40
>>> alpha1: 0.343  alpha2: 0.073 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [13/50] batch [5/51] time 0.184 (0.480) data 0.000 (0.298) loss 0.6863 (0.8223) acc 82.4074 (79.2421) lr 1.7705e-03 eta 0:15:27
epoch [13/50] batch [10/51] time 0.192 (0.332) data 0.000 (0.149) loss 0.6064 (0.7792) acc 84.8039 (80.5834) lr 1.7705e-03 eta 0:10:39
epoch [13/50] batch [15/51] time 0.167 (0.277) data 0.001 (0.100) loss 0.9399 (0.8024) acc 82.6531 (80.1585) lr 1.7705e-03 eta 0:08:52
epoch [13/50] batch [20/51] time 0.182 (0.251) data 0.000 (0.075) loss 0.8644 (0.7786) acc 81.5000 (80.9886) lr 1.7705e-03 eta 0:08:01
epoch [13/50] batch [25/51] time 0.175 (0.235) data 0.000 (0.060) loss 0.9118 (0.7779) acc 80.4348 (81.4640) lr 1.7705e-03 eta 0:07:30
epoch [13/50] batch [30/51] time 0.164 (0.225) data 0.000 (0.050) loss 0.6153 (0.7682) acc 84.8958 (81.6233) lr 1.7705e-03 eta 0:07:09
epoch [13/50] batch [35/51] time 0.188 (0.217) data 0.000 (0.043) loss 0.7991 (0.7695) acc 78.6458 (81.8721) lr 1.7705e-03 eta 0:06:53
epoch [13/50] batch [40/51] time 0.159 (0.211) data 0.000 (0.037) loss 0.9575 (0.7745) acc 75.5435 (81.7697) lr 1.7705e-03 eta 0:06:41
epoch [13/50] batch [45/51] time 0.157 (0.206) data 0.000 (0.033) loss 0.8431 (0.7725) acc 80.0000 (81.5236) lr 1.7705e-03 eta 0:06:30
epoch [13/50] batch [50/51] time 0.166 (0.202) data 0.000 (0.030) loss 0.9628 (0.7814) acc 80.1020 (81.4597) lr 1.7705e-03 eta 0:06:21
>>> alpha1: 0.304  alpha2: 0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [14/50] batch [5/51] time 0.174 (0.499) data 0.000 (0.317) loss 0.6247 (0.6255) acc 85.5769 (85.9604) lr 1.7290e-03 eta 0:15:38
epoch [14/50] batch [10/51] time 0.203 (0.340) data 0.000 (0.159) loss 0.5698 (0.7152) acc 90.1961 (82.9732) lr 1.7290e-03 eta 0:10:37
epoch [14/50] batch [15/51] time 0.172 (0.286) data 0.000 (0.106) loss 0.6727 (0.7024) acc 81.1225 (83.3153) lr 1.7290e-03 eta 0:08:55
epoch [14/50] batch [20/51] time 0.187 (0.259) data 0.000 (0.080) loss 0.8208 (0.7120) acc 82.2917 (83.3439) lr 1.7290e-03 eta 0:08:03
epoch [14/50] batch [25/51] time 0.171 (0.243) data 0.000 (0.064) loss 0.7862 (0.6985) acc 83.5106 (83.7965) lr 1.7290e-03 eta 0:07:32
epoch [14/50] batch [30/51] time 0.201 (0.234) data 0.001 (0.053) loss 0.6515 (0.6897) acc 87.2549 (83.9630) lr 1.7290e-03 eta 0:07:14
epoch [14/50] batch [35/51] time 0.168 (0.226) data 0.000 (0.046) loss 0.8549 (0.6840) acc 82.1429 (84.0411) lr 1.7290e-03 eta 0:06:57
epoch [14/50] batch [40/51] time 0.161 (0.219) data 0.000 (0.040) loss 0.6525 (0.6883) acc 82.4468 (83.8017) lr 1.7290e-03 eta 0:06:44
epoch [14/50] batch [45/51] time 0.163 (0.213) data 0.000 (0.035) loss 0.6836 (0.6973) acc 88.0208 (83.7067) lr 1.7290e-03 eta 0:06:31
epoch [14/50] batch [50/51] time 0.168 (0.208) data 0.000 (0.032) loss 0.7778 (0.7072) acc 76.0000 (83.3336) lr 1.7290e-03 eta 0:06:21
>>> alpha1: 0.288  alpha2: 0.022 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [15/50] batch [5/51] time 0.165 (0.492) data 0.002 (0.304) loss 0.6766 (0.5226) acc 79.2553 (87.5514) lr 1.6845e-03 eta 0:15:01
epoch [15/50] batch [10/51] time 0.175 (0.337) data 0.000 (0.152) loss 0.6155 (0.5982) acc 86.5000 (86.4896) lr 1.6845e-03 eta 0:10:15
epoch [15/50] batch [15/51] time 0.165 (0.284) data 0.000 (0.102) loss 0.5311 (0.6075) acc 86.7021 (86.1511) lr 1.6845e-03 eta 0:08:36
epoch [15/50] batch [20/51] time 0.181 (0.258) data 0.000 (0.076) loss 0.4953 (0.6383) acc 90.3846 (85.5133) lr 1.6845e-03 eta 0:07:47
epoch [15/50] batch [25/51] time 0.169 (0.241) data 0.000 (0.061) loss 0.6573 (0.6432) acc 83.5000 (85.3824) lr 1.6845e-03 eta 0:07:16
epoch [15/50] batch [30/51] time 0.193 (0.231) data 0.000 (0.051) loss 0.7254 (0.6534) acc 82.2917 (84.8625) lr 1.6845e-03 eta 0:06:56
epoch [15/50] batch [35/51] time 0.167 (0.223) data 0.000 (0.044) loss 0.5389 (0.6552) acc 86.2245 (84.4716) lr 1.6845e-03 eta 0:06:41
epoch [15/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.038) loss 0.5848 (0.6723) acc 89.0625 (84.0631) lr 1.6845e-03 eta 0:06:28
epoch [15/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.034) loss 0.7201 (0.6694) acc 83.3333 (84.1315) lr 1.6845e-03 eta 0:06:17
epoch [15/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.031) loss 0.6635 (0.6623) acc 84.3750 (84.3650) lr 1.6845e-03 eta 0:06:08
>>> alpha1: 0.236  alpha2: -0.023 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [16/50] batch [5/51] time 0.168 (0.503) data 0.000 (0.311) loss 0.6350 (0.6148) acc 87.2449 (85.7316) lr 1.6374e-03 eta 0:14:54
epoch [16/50] batch [10/51] time 0.178 (0.343) data 0.000 (0.157) loss 0.8558 (0.6892) acc 80.5000 (84.9718) lr 1.6374e-03 eta 0:10:09
epoch [16/50] batch [15/51] time 0.182 (0.290) data 0.000 (0.105) loss 0.4933 (0.6318) acc 90.5660 (86.3689) lr 1.6374e-03 eta 0:08:33
epoch [16/50] batch [20/51] time 0.170 (0.261) data 0.000 (0.079) loss 0.4811 (0.6060) acc 88.5000 (86.4985) lr 1.6374e-03 eta 0:07:40
epoch [16/50] batch [25/51] time 0.164 (0.244) data 0.000 (0.063) loss 0.6790 (0.5984) acc 87.5000 (86.7547) lr 1.6374e-03 eta 0:07:08
epoch [16/50] batch [30/51] time 0.175 (0.257) data 0.000 (0.053) loss 0.7143 (0.6102) acc 81.5000 (86.3693) lr 1.6374e-03 eta 0:07:31
epoch [16/50] batch [35/51] time 0.178 (0.245) data 0.000 (0.045) loss 0.6084 (0.6165) acc 85.5000 (86.2137) lr 1.6374e-03 eta 0:07:09
epoch [16/50] batch [40/51] time 0.166 (0.237) data 0.000 (0.040) loss 0.6944 (0.6055) acc 82.6531 (86.5039) lr 1.6374e-03 eta 0:06:54
epoch [16/50] batch [45/51] time 0.178 (0.230) data 0.000 (0.035) loss 0.6460 (0.6099) acc 86.7924 (86.3898) lr 1.6374e-03 eta 0:06:39
epoch [16/50] batch [50/51] time 0.167 (0.224) data 0.000 (0.032) loss 0.7330 (0.6201) acc 84.1837 (86.0418) lr 1.6374e-03 eta 0:06:28
>>> alpha1: 0.219  alpha2: -0.033 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [17/50] batch [5/51] time 0.178 (0.464) data 0.000 (0.284) loss 0.5320 (0.5817) acc 88.2075 (87.6744) lr 1.5878e-03 eta 0:13:21
epoch [17/50] batch [10/51] time 0.180 (0.323) data 0.000 (0.142) loss 0.5161 (0.6190) acc 88.4259 (86.5448) lr 1.5878e-03 eta 0:09:16
epoch [17/50] batch [15/51] time 0.175 (0.272) data 0.000 (0.095) loss 0.8053 (0.6219) acc 82.0000 (86.0744) lr 1.5878e-03 eta 0:07:47
epoch [17/50] batch [20/51] time 0.170 (0.285) data 0.000 (0.071) loss 0.8805 (0.6187) acc 83.5000 (86.2764) lr 1.5878e-03 eta 0:08:09
epoch [17/50] batch [25/51] time 0.180 (0.262) data 0.000 (0.057) loss 0.6768 (0.6339) acc 82.9787 (85.7808) lr 1.5878e-03 eta 0:07:28
epoch [17/50] batch [30/51] time 0.183 (0.249) data 0.001 (0.048) loss 0.7690 (0.6321) acc 82.3529 (85.7211) lr 1.5878e-03 eta 0:07:04
epoch [17/50] batch [35/51] time 0.178 (0.239) data 0.000 (0.041) loss 0.5116 (0.6157) acc 87.2642 (86.0635) lr 1.5878e-03 eta 0:06:45
epoch [17/50] batch [40/51] time 0.180 (0.231) data 0.000 (0.036) loss 0.5167 (0.6044) acc 89.0909 (86.3360) lr 1.5878e-03 eta 0:06:32
epoch [17/50] batch [45/51] time 0.171 (0.224) data 0.000 (0.032) loss 0.8431 (0.6150) acc 85.2941 (86.0648) lr 1.5878e-03 eta 0:06:18
epoch [17/50] batch [50/51] time 0.177 (0.219) data 0.000 (0.029) loss 0.5339 (0.6163) acc 88.2353 (86.0452) lr 1.5878e-03 eta 0:06:08
>>> alpha1: 0.209  alpha2: -0.034 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [18/50] batch [5/51] time 0.171 (0.538) data 0.001 (0.348) loss 0.5493 (0.5235) acc 88.5000 (87.6916) lr 1.5358e-03 eta 0:15:03
epoch [18/50] batch [10/51] time 0.167 (0.359) data 0.001 (0.174) loss 0.5144 (0.5845) acc 92.3469 (86.4046) lr 1.5358e-03 eta 0:10:00
epoch [18/50] batch [15/51] time 0.172 (0.294) data 0.000 (0.116) loss 0.6127 (0.6045) acc 86.2745 (86.1247) lr 1.5358e-03 eta 0:08:11
epoch [18/50] batch [20/51] time 0.183 (0.265) data 0.000 (0.087) loss 0.7149 (0.6043) acc 82.4074 (86.2615) lr 1.5358e-03 eta 0:07:21
epoch [18/50] batch [25/51] time 0.170 (0.246) data 0.000 (0.070) loss 0.5757 (0.5917) acc 88.2979 (86.6436) lr 1.5358e-03 eta 0:06:48
epoch [18/50] batch [30/51] time 0.176 (0.235) data 0.000 (0.058) loss 0.5457 (0.6011) acc 89.9038 (86.5862) lr 1.5358e-03 eta 0:06:28
epoch [18/50] batch [35/51] time 0.177 (0.248) data 0.000 (0.050) loss 0.3700 (0.6076) acc 91.5094 (86.2447) lr 1.5358e-03 eta 0:06:49
epoch [18/50] batch [40/51] time 0.185 (0.239) data 0.000 (0.044) loss 0.5177 (0.5981) acc 92.1053 (86.5093) lr 1.5358e-03 eta 0:06:32
epoch [18/50] batch [45/51] time 0.170 (0.231) data 0.000 (0.039) loss 0.5951 (0.5976) acc 84.3137 (86.4343) lr 1.5358e-03 eta 0:06:18
epoch [18/50] batch [50/51] time 0.159 (0.224) data 0.000 (0.035) loss 0.5997 (0.5968) acc 84.7826 (86.4239) lr 1.5358e-03 eta 0:06:06
>>> alpha1: 0.203  alpha2: -0.029 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [19/50] batch [5/51] time 0.171 (0.486) data 0.000 (0.295) loss 0.8612 (0.6456) acc 75.0000 (82.5565) lr 1.4818e-03 eta 0:13:11
epoch [19/50] batch [10/51] time 0.169 (0.332) data 0.000 (0.148) loss 0.5907 (0.6433) acc 90.0000 (84.4277) lr 1.4818e-03 eta 0:08:59
epoch [19/50] batch [15/51] time 0.167 (0.280) data 0.000 (0.099) loss 0.6592 (0.6321) acc 83.1633 (85.0253) lr 1.4818e-03 eta 0:07:32
epoch [19/50] batch [20/51] time 0.179 (0.255) data 0.000 (0.074) loss 0.7577 (0.6148) acc 80.1887 (85.7415) lr 1.4818e-03 eta 0:06:50
epoch [19/50] batch [25/51] time 0.185 (0.241) data 0.000 (0.059) loss 0.5124 (0.6112) acc 88.7255 (86.1076) lr 1.4818e-03 eta 0:06:26
epoch [19/50] batch [30/51] time 0.181 (0.231) data 0.000 (0.049) loss 0.4075 (0.6172) acc 90.8654 (85.8323) lr 1.4818e-03 eta 0:06:10
epoch [19/50] batch [35/51] time 0.176 (0.223) data 0.000 (0.042) loss 0.5629 (0.6184) acc 86.2245 (85.9310) lr 1.4818e-03 eta 0:05:56
epoch [19/50] batch [40/51] time 0.159 (0.216) data 0.000 (0.037) loss 0.4491 (0.6069) acc 90.9574 (86.2071) lr 1.4818e-03 eta 0:05:43
epoch [19/50] batch [45/51] time 0.163 (0.210) data 0.000 (0.033) loss 0.5758 (0.6011) acc 86.9792 (86.3569) lr 1.4818e-03 eta 0:05:33
epoch [19/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.030) loss 0.4847 (0.5986) acc 91.8367 (86.4980) lr 1.4818e-03 eta 0:05:25
>>> alpha1: 0.192  alpha2: -0.039 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [20/50] batch [5/51] time 0.729 (0.654) data 0.000 (0.366) loss 0.8728 (0.6137) acc 75.6250 (85.3333) lr 1.4258e-03 eta 0:17:10
epoch [20/50] batch [10/51] time 0.165 (0.412) data 0.000 (0.183) loss 0.6168 (0.5753) acc 92.7083 (87.1547) lr 1.4258e-03 eta 0:10:48
epoch [20/50] batch [15/51] time 0.174 (0.334) data 0.000 (0.122) loss 0.5726 (0.5681) acc 89.4231 (87.5144) lr 1.4258e-03 eta 0:08:43
epoch [20/50] batch [20/51] time 0.174 (0.294) data 0.000 (0.092) loss 0.6618 (0.5621) acc 87.0192 (87.2543) lr 1.4258e-03 eta 0:07:39
epoch [20/50] batch [25/51] time 0.166 (0.272) data 0.000 (0.073) loss 0.7406 (0.5669) acc 83.1633 (86.9103) lr 1.4258e-03 eta 0:07:03
epoch [20/50] batch [30/51] time 0.163 (0.256) data 0.000 (0.061) loss 0.5535 (0.5820) acc 84.5745 (86.3522) lr 1.4258e-03 eta 0:06:36
epoch [20/50] batch [35/51] time 0.169 (0.244) data 0.000 (0.053) loss 0.6140 (0.5845) acc 88.0000 (86.3011) lr 1.4258e-03 eta 0:06:17
epoch [20/50] batch [40/51] time 0.180 (0.237) data 0.000 (0.046) loss 0.6763 (0.5867) acc 85.0962 (86.3366) lr 1.4258e-03 eta 0:06:04
epoch [20/50] batch [45/51] time 0.176 (0.231) data 0.001 (0.041) loss 0.5342 (0.5765) acc 87.5000 (86.5106) lr 1.4258e-03 eta 0:05:54
epoch [20/50] batch [50/51] time 0.185 (0.225) data 0.000 (0.037) loss 0.4641 (0.5687) acc 91.6667 (86.9533) lr 1.4258e-03 eta 0:05:44
>>> alpha1: 0.186  alpha2: -0.039 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [21/50] batch [5/51] time 0.178 (0.478) data 0.000 (0.294) loss 0.7264 (0.6070) acc 80.1887 (84.8777) lr 1.3681e-03 eta 0:12:08
epoch [21/50] batch [10/51] time 0.174 (0.330) data 0.000 (0.147) loss 0.4645 (0.5939) acc 89.4231 (85.9896) lr 1.3681e-03 eta 0:08:20
epoch [21/50] batch [15/51] time 0.177 (0.279) data 0.000 (0.098) loss 0.6080 (0.5789) acc 88.2075 (86.5037) lr 1.3681e-03 eta 0:07:02
epoch [21/50] batch [20/51] time 0.182 (0.254) data 0.000 (0.074) loss 0.4801 (0.5752) acc 88.2075 (86.9142) lr 1.3681e-03 eta 0:06:23
epoch [21/50] batch [25/51] time 0.176 (0.238) data 0.000 (0.059) loss 0.7966 (0.5806) acc 81.7708 (87.0699) lr 1.3681e-03 eta 0:05:58
epoch [21/50] batch [30/51] time 0.176 (0.227) data 0.000 (0.049) loss 0.5571 (0.5639) acc 89.7059 (87.6321) lr 1.3681e-03 eta 0:05:41
epoch [21/50] batch [35/51] time 0.167 (0.220) data 0.000 (0.042) loss 0.6515 (0.5622) acc 83.8889 (87.5181) lr 1.3681e-03 eta 0:05:28
epoch [21/50] batch [40/51] time 0.164 (0.215) data 0.000 (0.037) loss 0.4402 (0.5594) acc 87.5000 (87.3844) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [45/51] time 0.166 (0.209) data 0.000 (0.033) loss 0.7736 (0.5609) acc 82.6531 (87.3701) lr 1.3681e-03 eta 0:05:11
epoch [21/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.030) loss 0.6875 (0.5710) acc 88.0435 (87.1780) lr 1.3681e-03 eta 0:05:02
>>> alpha1: 0.179  alpha2: -0.040 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.11 <<<
epoch [22/50] batch [5/51] time 0.191 (0.438) data 0.000 (0.261) loss 0.3647 (0.5349) acc 95.3704 (89.5892) lr 1.3090e-03 eta 0:10:45
epoch [22/50] batch [10/51] time 0.187 (0.310) data 0.000 (0.131) loss 0.6744 (0.5714) acc 84.4340 (87.9426) lr 1.3090e-03 eta 0:07:35
epoch [22/50] batch [15/51] time 0.188 (0.266) data 0.000 (0.087) loss 0.6075 (0.5868) acc 84.5000 (86.5596) lr 1.3090e-03 eta 0:06:30
epoch [22/50] batch [20/51] time 0.178 (0.244) data 0.000 (0.065) loss 0.5434 (0.5632) acc 87.2642 (86.8614) lr 1.3090e-03 eta 0:05:55
epoch [22/50] batch [25/51] time 0.167 (0.231) data 0.000 (0.052) loss 0.7090 (0.5616) acc 81.1225 (86.9327) lr 1.3090e-03 eta 0:05:35
epoch [22/50] batch [30/51] time 0.176 (0.221) data 0.000 (0.044) loss 0.6552 (0.5561) acc 87.0000 (87.0701) lr 1.3090e-03 eta 0:05:20
epoch [22/50] batch [35/51] time 0.182 (0.216) data 0.000 (0.037) loss 0.4742 (0.5486) acc 89.9038 (87.4030) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [40/51] time 0.173 (0.210) data 0.000 (0.033) loss 0.7531 (0.5482) acc 85.2941 (87.4610) lr 1.3090e-03 eta 0:05:01
epoch [22/50] batch [45/51] time 0.154 (0.205) data 0.000 (0.029) loss 0.7658 (0.5511) acc 88.3721 (87.5063) lr 1.3090e-03 eta 0:04:54
epoch [22/50] batch [50/51] time 0.180 (0.203) data 0.000 (0.026) loss 0.4454 (0.5388) acc 87.7273 (87.7293) lr 1.3090e-03 eta 0:04:49
>>> alpha1: 0.173  alpha2: -0.044 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [23/50] batch [5/51] time 0.167 (0.490) data 0.000 (0.308) loss 0.7586 (0.9500) acc 81.5217 (81.0436) lr 1.2487e-03 eta 0:11:36
epoch [23/50] batch [10/51] time 0.171 (0.332) data 0.000 (0.154) loss 0.5657 (0.6937) acc 87.0000 (85.9677) lr 1.2487e-03 eta 0:07:50
epoch [23/50] batch [15/51] time 0.178 (0.279) data 0.000 (0.103) loss 0.5273 (0.6450) acc 90.5660 (87.1185) lr 1.2487e-03 eta 0:06:34
epoch [23/50] batch [20/51] time 0.177 (0.254) data 0.000 (0.077) loss 0.5937 (0.6251) acc 83.0189 (86.7475) lr 1.2487e-03 eta 0:05:56
epoch [23/50] batch [25/51] time 0.177 (0.237) data 0.000 (0.062) loss 0.4324 (0.6004) acc 91.9811 (87.0183) lr 1.2487e-03 eta 0:05:32
epoch [23/50] batch [30/51] time 0.173 (0.228) data 0.000 (0.052) loss 0.5013 (0.5853) acc 90.3846 (87.4690) lr 1.2487e-03 eta 0:05:18
epoch [23/50] batch [35/51] time 0.188 (0.220) data 0.000 (0.044) loss 0.3049 (0.5816) acc 93.8679 (87.2899) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [40/51] time 0.161 (0.214) data 0.000 (0.039) loss 0.4045 (0.5733) acc 91.4894 (87.3879) lr 1.2487e-03 eta 0:04:56
epoch [23/50] batch [45/51] time 0.176 (0.208) data 0.000 (0.034) loss 0.4235 (0.5670) acc 88.2075 (87.4387) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.031) loss 0.8053 (0.5786) acc 82.5000 (87.0621) lr 1.2487e-03 eta 0:04:41
>>> alpha1: 0.167  alpha2: -0.040 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [24/50] batch [5/51] time 0.181 (0.479) data 0.000 (0.289) loss 0.5617 (0.6073) acc 90.1961 (90.4248) lr 1.1874e-03 eta 0:10:56
epoch [24/50] batch [10/51] time 0.172 (0.333) data 0.000 (0.145) loss 0.5461 (0.5691) acc 91.6667 (89.3965) lr 1.1874e-03 eta 0:07:35
epoch [24/50] batch [15/51] time 0.178 (0.280) data 0.000 (0.097) loss 0.5353 (0.5617) acc 86.7647 (88.3473) lr 1.1874e-03 eta 0:06:21
epoch [24/50] batch [20/51] time 0.168 (0.254) data 0.000 (0.072) loss 0.3985 (0.5471) acc 89.5833 (88.2229) lr 1.1874e-03 eta 0:05:45
epoch [24/50] batch [25/51] time 0.171 (0.239) data 0.000 (0.058) loss 0.5301 (0.5347) acc 92.0000 (88.3342) lr 1.1874e-03 eta 0:05:22
epoch [24/50] batch [30/51] time 0.198 (0.231) data 0.000 (0.048) loss 0.3212 (0.5377) acc 94.8113 (88.3216) lr 1.1874e-03 eta 0:05:11
epoch [24/50] batch [35/51] time 0.196 (0.225) data 0.000 (0.042) loss 0.6025 (0.5432) acc 81.6327 (88.0299) lr 1.1874e-03 eta 0:05:01
epoch [24/50] batch [40/51] time 0.179 (0.220) data 0.000 (0.036) loss 0.3326 (0.5568) acc 94.9074 (88.0770) lr 1.1874e-03 eta 0:04:53
epoch [24/50] batch [45/51] time 0.169 (0.214) data 0.000 (0.032) loss 0.4243 (0.5543) acc 87.5000 (87.8545) lr 1.1874e-03 eta 0:04:44
epoch [24/50] batch [50/51] time 0.166 (0.209) data 0.000 (0.029) loss 0.4610 (0.5473) acc 88.7755 (88.0814) lr 1.1874e-03 eta 0:04:37
>>> alpha1: 0.163  alpha2: -0.036 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.11 <<<
epoch [25/50] batch [5/51] time 0.182 (0.474) data 0.000 (0.288) loss 0.5249 (0.5322) acc 90.6250 (87.9042) lr 1.1253e-03 eta 0:10:25
epoch [25/50] batch [10/51] time 0.171 (0.324) data 0.000 (0.144) loss 0.6410 (0.5462) acc 88.0208 (87.7977) lr 1.1253e-03 eta 0:07:05
epoch [25/50] batch [15/51] time 0.178 (0.274) data 0.000 (0.096) loss 0.6089 (0.5496) acc 86.2245 (87.7102) lr 1.1253e-03 eta 0:05:59
epoch [25/50] batch [20/51] time 0.168 (0.249) data 0.000 (0.072) loss 0.4959 (0.5210) acc 87.7551 (88.4221) lr 1.1253e-03 eta 0:05:25
epoch [25/50] batch [25/51] time 0.173 (0.235) data 0.000 (0.058) loss 0.4081 (0.5161) acc 91.3462 (88.6134) lr 1.1253e-03 eta 0:05:05
epoch [25/50] batch [30/51] time 0.174 (0.226) data 0.000 (0.048) loss 0.5954 (0.5086) acc 89.9038 (88.8831) lr 1.1253e-03 eta 0:04:52
epoch [25/50] batch [35/51] time 0.174 (0.219) data 0.000 (0.041) loss 0.5407 (0.5086) acc 87.5000 (88.6572) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [40/51] time 0.166 (0.213) data 0.000 (0.036) loss 0.2918 (0.5039) acc 93.8775 (88.7261) lr 1.1253e-03 eta 0:04:33
epoch [25/50] batch [45/51] time 0.170 (0.208) data 0.000 (0.032) loss 0.5364 (0.4965) acc 84.8039 (88.7781) lr 1.1253e-03 eta 0:04:26
epoch [25/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.029) loss 0.5690 (0.5031) acc 87.0000 (88.7526) lr 1.1253e-03 eta 0:04:20
>>> alpha1: 0.158  alpha2: -0.034 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.11 <<<
epoch [26/50] batch [5/51] time 0.179 (0.524) data 0.000 (0.334) loss 0.4732 (0.5333) acc 89.1509 (89.3268) lr 1.0628e-03 eta 0:11:06
epoch [26/50] batch [10/51] time 0.173 (0.352) data 0.000 (0.167) loss 0.4833 (0.5442) acc 89.9038 (88.0747) lr 1.0628e-03 eta 0:07:24
epoch [26/50] batch [15/51] time 0.175 (0.295) data 0.000 (0.111) loss 0.5838 (0.5177) acc 86.9565 (88.7191) lr 1.0628e-03 eta 0:06:11
epoch [26/50] batch [20/51] time 0.187 (0.267) data 0.000 (0.084) loss 0.6241 (0.5241) acc 82.6923 (88.0843) lr 1.0628e-03 eta 0:05:35
epoch [26/50] batch [25/51] time 0.181 (0.249) data 0.000 (0.067) loss 0.4135 (0.5887) acc 91.6667 (87.4231) lr 1.0628e-03 eta 0:05:11
epoch [26/50] batch [30/51] time 0.186 (0.238) data 0.000 (0.056) loss 0.5759 (0.5811) acc 85.0962 (87.4239) lr 1.0628e-03 eta 0:04:56
epoch [26/50] batch [35/51] time 0.170 (0.230) data 0.000 (0.048) loss 0.2762 (0.5631) acc 94.5000 (87.6819) lr 1.0628e-03 eta 0:04:44
epoch [26/50] batch [40/51] time 0.186 (0.224) data 0.000 (0.042) loss 0.3705 (0.5541) acc 93.2692 (87.8167) lr 1.0628e-03 eta 0:04:36
epoch [26/50] batch [45/51] time 0.173 (0.218) data 0.000 (0.037) loss 0.4312 (0.5436) acc 90.0000 (87.9818) lr 1.0628e-03 eta 0:04:27
epoch [26/50] batch [50/51] time 0.174 (0.215) data 0.001 (0.034) loss 0.4865 (0.5570) acc 91.1458 (87.9802) lr 1.0628e-03 eta 0:04:22
>>> alpha1: 0.153  alpha2: -0.036 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.09 <<<
epoch [27/50] batch [5/51] time 0.173 (0.434) data 0.000 (0.247) loss 0.6317 (0.4746) acc 83.8235 (89.3538) lr 1.0000e-03 eta 0:08:48
epoch [27/50] batch [10/51] time 0.176 (0.304) data 0.000 (0.124) loss 0.2893 (0.4384) acc 95.5882 (90.3939) lr 1.0000e-03 eta 0:06:09
epoch [27/50] batch [15/51] time 0.172 (0.262) data 0.000 (0.083) loss 0.4996 (0.4677) acc 86.7647 (89.3341) lr 1.0000e-03 eta 0:05:16
epoch [27/50] batch [20/51] time 0.172 (0.239) data 0.001 (0.062) loss 0.4647 (0.4897) acc 90.5000 (88.5277) lr 1.0000e-03 eta 0:04:47
epoch [27/50] batch [25/51] time 0.179 (0.225) data 0.000 (0.050) loss 0.5090 (0.4857) acc 89.8148 (88.7867) lr 1.0000e-03 eta 0:04:29
epoch [27/50] batch [30/51] time 0.170 (0.216) data 0.000 (0.041) loss 0.4975 (0.4941) acc 86.5000 (88.5800) lr 1.0000e-03 eta 0:04:18
epoch [27/50] batch [35/51] time 0.156 (0.210) data 0.000 (0.036) loss 0.5199 (0.4971) acc 88.6364 (88.7124) lr 1.0000e-03 eta 0:04:09
epoch [27/50] batch [40/51] time 0.172 (0.206) data 0.000 (0.031) loss 0.4934 (0.4848) acc 87.5000 (88.8855) lr 1.0000e-03 eta 0:04:04
epoch [27/50] batch [45/51] time 0.176 (0.203) data 0.000 (0.028) loss 0.3009 (0.4844) acc 94.3878 (88.8989) lr 1.0000e-03 eta 0:03:59
epoch [27/50] batch [50/51] time 0.177 (0.200) data 0.000 (0.025) loss 0.6768 (0.4885) acc 84.4340 (88.6964) lr 1.0000e-03 eta 0:03:54
>>> alpha1: 0.150  alpha2: -0.033 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.10 <<<
epoch [28/50] batch [5/51] time 0.166 (0.489) data 0.001 (0.301) loss 0.6986 (0.5315) acc 79.2553 (86.3289) lr 9.3721e-04 eta 0:09:31
epoch [28/50] batch [10/51] time 0.173 (0.333) data 0.000 (0.151) loss 0.5720 (0.5264) acc 86.5000 (87.3462) lr 9.3721e-04 eta 0:06:27
epoch [28/50] batch [15/51] time 0.195 (0.286) data 0.000 (0.101) loss 0.4858 (0.5096) acc 91.8269 (88.2479) lr 9.3721e-04 eta 0:05:31
epoch [28/50] batch [20/51] time 0.192 (0.262) data 0.001 (0.076) loss 0.3473 (0.5004) acc 91.8269 (88.3764) lr 9.3721e-04 eta 0:05:02
epoch [28/50] batch [25/51] time 0.191 (0.249) data 0.000 (0.061) loss 0.5100 (0.5006) acc 92.7083 (88.4010) lr 9.3721e-04 eta 0:04:45
epoch [28/50] batch [30/51] time 0.179 (0.238) data 0.000 (0.050) loss 0.4717 (0.5038) acc 88.5000 (88.3225) lr 9.3721e-04 eta 0:04:31
epoch [28/50] batch [35/51] time 0.180 (0.230) data 0.001 (0.043) loss 0.4847 (0.4971) acc 89.8148 (88.6087) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [40/51] time 0.180 (0.223) data 0.000 (0.038) loss 0.5370 (0.5034) acc 88.4259 (88.4640) lr 9.3721e-04 eta 0:04:12
epoch [28/50] batch [45/51] time 0.171 (0.217) data 0.000 (0.034) loss 0.3749 (0.4987) acc 95.0980 (88.7575) lr 9.3721e-04 eta 0:04:04
epoch [28/50] batch [50/51] time 0.173 (0.212) data 0.000 (0.030) loss 0.3728 (0.4950) acc 92.7885 (88.7936) lr 9.3721e-04 eta 0:03:58
>>> alpha1: 0.149  alpha2: -0.030 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.09 <<<
epoch [29/50] batch [5/51] time 0.171 (0.459) data 0.000 (0.274) loss 0.4383 (0.4970) acc 92.5000 (88.9506) lr 8.7467e-04 eta 0:08:32
epoch [29/50] batch [10/51] time 0.175 (0.317) data 0.000 (0.137) loss 0.7460 (0.4787) acc 81.2500 (89.5864) lr 8.7467e-04 eta 0:05:52
epoch [29/50] batch [15/51] time 0.203 (0.270) data 0.000 (0.092) loss 0.5745 (0.4733) acc 87.0000 (89.9157) lr 8.7467e-04 eta 0:04:59
epoch [29/50] batch [20/51] time 0.182 (0.248) data 0.000 (0.069) loss 0.3010 (0.5032) acc 94.7115 (88.6335) lr 8.7467e-04 eta 0:04:33
epoch [29/50] batch [25/51] time 0.174 (0.234) data 0.000 (0.055) loss 0.4065 (0.5084) acc 94.2308 (88.8857) lr 8.7467e-04 eta 0:04:16
epoch [29/50] batch [30/51] time 0.178 (0.224) data 0.000 (0.046) loss 0.5293 (0.4963) acc 86.2745 (89.1590) lr 8.7467e-04 eta 0:04:04
epoch [29/50] batch [35/51] time 0.174 (0.217) data 0.000 (0.039) loss 0.3964 (0.4834) acc 90.3846 (89.1979) lr 8.7467e-04 eta 0:03:55
epoch [29/50] batch [40/51] time 0.162 (0.212) data 0.000 (0.035) loss 0.5004 (0.4842) acc 89.3617 (89.1486) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [45/51] time 0.178 (0.207) data 0.000 (0.031) loss 0.4162 (0.4762) acc 89.1509 (89.2690) lr 8.7467e-04 eta 0:03:42
epoch [29/50] batch [50/51] time 0.169 (0.204) data 0.000 (0.028) loss 0.3873 (0.4783) acc 93.3673 (89.2069) lr 8.7467e-04 eta 0:03:38
>>> alpha1: 0.146  alpha2: -0.036 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [30/50] batch [5/51] time 0.162 (0.477) data 0.001 (0.296) loss 0.5946 (0.5533) acc 86.9318 (86.6132) lr 8.1262e-04 eta 0:08:28
epoch [30/50] batch [10/51] time 0.170 (0.332) data 0.000 (0.148) loss 0.6386 (0.5416) acc 85.2273 (86.5854) lr 8.1262e-04 eta 0:05:52
epoch [30/50] batch [15/51] time 0.189 (0.284) data 0.000 (0.099) loss 0.3765 (0.4996) acc 89.9038 (88.1739) lr 8.1262e-04 eta 0:04:59
epoch [30/50] batch [20/51] time 0.192 (0.256) data 0.000 (0.074) loss 0.2752 (0.4719) acc 94.8113 (89.2977) lr 8.1262e-04 eta 0:04:29
epoch [30/50] batch [25/51] time 0.199 (0.243) data 0.000 (0.059) loss 0.2860 (0.4526) acc 94.1964 (90.0749) lr 8.1262e-04 eta 0:04:14
epoch [30/50] batch [30/51] time 0.181 (0.233) data 0.000 (0.050) loss 0.3695 (0.4428) acc 92.7273 (90.4115) lr 8.1262e-04 eta 0:04:02
epoch [30/50] batch [35/51] time 0.185 (0.225) data 0.000 (0.043) loss 0.4827 (0.4525) acc 88.5000 (90.2349) lr 8.1262e-04 eta 0:03:53
epoch [30/50] batch [40/51] time 0.180 (0.219) data 0.000 (0.037) loss 0.4331 (0.4580) acc 91.8182 (90.0740) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [45/51] time 0.165 (0.214) data 0.000 (0.033) loss 0.5303 (0.4550) acc 89.5833 (90.0961) lr 8.1262e-04 eta 0:03:39
epoch [30/50] batch [50/51] time 0.163 (0.209) data 0.000 (0.030) loss 0.4926 (0.4637) acc 86.7021 (89.8291) lr 8.1262e-04 eta 0:03:33
>>> alpha1: 0.147  alpha2: -0.030 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [31/50] batch [5/51] time 0.169 (0.417) data 0.000 (0.245) loss 0.5622 (0.4349) acc 90.1042 (92.3831) lr 7.5131e-04 eta 0:07:03
epoch [31/50] batch [10/51] time 0.184 (0.297) data 0.000 (0.122) loss 0.3285 (0.4161) acc 92.1569 (92.2026) lr 7.5131e-04 eta 0:05:00
epoch [31/50] batch [15/51] time 0.182 (0.255) data 0.000 (0.082) loss 0.5036 (0.4257) acc 88.6364 (91.4860) lr 7.5131e-04 eta 0:04:16
epoch [31/50] batch [20/51] time 0.167 (0.235) data 0.000 (0.061) loss 0.4420 (0.4309) acc 90.3061 (91.2125) lr 7.5131e-04 eta 0:03:55
epoch [31/50] batch [25/51] time 0.178 (0.222) data 0.000 (0.049) loss 0.3980 (0.4365) acc 94.2308 (90.9614) lr 7.5131e-04 eta 0:03:41
epoch [31/50] batch [30/51] time 0.177 (0.216) data 0.000 (0.042) loss 0.3800 (0.4430) acc 89.2157 (90.7462) lr 7.5131e-04 eta 0:03:33
epoch [31/50] batch [35/51] time 0.173 (0.211) data 0.001 (0.036) loss 0.4693 (0.4532) acc 88.5417 (90.3396) lr 7.5131e-04 eta 0:03:27
epoch [31/50] batch [40/51] time 0.166 (0.207) data 0.000 (0.031) loss 0.4741 (0.4497) acc 92.8571 (90.4868) lr 7.5131e-04 eta 0:03:22
epoch [31/50] batch [45/51] time 0.167 (0.203) data 0.000 (0.028) loss 0.3965 (0.4469) acc 91.3265 (90.4816) lr 7.5131e-04 eta 0:03:17
epoch [31/50] batch [50/51] time 0.164 (0.199) data 0.000 (0.025) loss 0.5645 (0.4542) acc 89.0625 (90.3821) lr 7.5131e-04 eta 0:03:13
>>> alpha1: 0.147  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [32/50] batch [5/51] time 0.179 (0.496) data 0.000 (0.308) loss 0.3880 (0.3888) acc 92.4528 (92.3714) lr 6.9098e-04 eta 0:07:58
epoch [32/50] batch [10/51] time 0.182 (0.336) data 0.001 (0.154) loss 0.5554 (0.4407) acc 84.0425 (90.3616) lr 6.9098e-04 eta 0:05:22
epoch [32/50] batch [15/51] time 0.172 (0.282) data 0.000 (0.103) loss 0.4647 (0.4423) acc 89.0000 (90.3022) lr 6.9098e-04 eta 0:04:28
epoch [32/50] batch [20/51] time 0.173 (0.258) data 0.000 (0.078) loss 0.4028 (0.4370) acc 91.5000 (90.4346) lr 6.9098e-04 eta 0:04:04
epoch [32/50] batch [25/51] time 0.182 (0.243) data 0.000 (0.062) loss 0.2630 (0.4185) acc 94.5455 (90.9478) lr 6.9098e-04 eta 0:03:49
epoch [32/50] batch [30/51] time 0.177 (0.232) data 0.000 (0.052) loss 0.6712 (0.4289) acc 83.4906 (90.7193) lr 6.9098e-04 eta 0:03:37
epoch [32/50] batch [35/51] time 0.167 (0.224) data 0.001 (0.045) loss 0.5611 (0.4360) acc 86.7347 (90.5633) lr 6.9098e-04 eta 0:03:29
epoch [32/50] batch [40/51] time 0.164 (0.218) data 0.000 (0.039) loss 0.4502 (0.4394) acc 89.5833 (90.5614) lr 6.9098e-04 eta 0:03:22
epoch [32/50] batch [45/51] time 0.181 (0.213) data 0.000 (0.035) loss 0.3832 (0.4386) acc 93.6364 (90.7761) lr 6.9098e-04 eta 0:03:16
epoch [32/50] batch [50/51] time 0.177 (0.208) data 0.001 (0.031) loss 0.3555 (0.4399) acc 93.1373 (90.8032) lr 6.9098e-04 eta 0:03:11
>>> alpha1: 0.147  alpha2: -0.020 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [33/50] batch [5/51] time 0.170 (0.472) data 0.000 (0.287) loss 0.4382 (0.4764) acc 89.5000 (90.6941) lr 6.3188e-04 eta 0:07:11
epoch [33/50] batch [10/51] time 0.178 (0.326) data 0.000 (0.143) loss 0.5343 (0.4946) acc 89.6226 (90.3319) lr 6.3188e-04 eta 0:04:55
epoch [33/50] batch [15/51] time 0.177 (0.276) data 0.000 (0.096) loss 0.3931 (0.4583) acc 93.0000 (90.9616) lr 6.3188e-04 eta 0:04:08
epoch [33/50] batch [20/51] time 0.166 (0.250) data 0.000 (0.072) loss 0.3759 (0.4429) acc 88.7755 (90.9416) lr 6.3188e-04 eta 0:03:44
epoch [33/50] batch [25/51] time 0.179 (0.237) data 0.000 (0.057) loss 0.3993 (0.4417) acc 90.2778 (90.9993) lr 6.3188e-04 eta 0:03:31
epoch [33/50] batch [30/51] time 0.176 (0.228) data 0.000 (0.048) loss 0.4207 (0.4600) acc 89.7059 (90.5549) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [35/51] time 0.175 (0.220) data 0.000 (0.041) loss 0.6568 (0.4584) acc 82.2115 (90.3042) lr 6.3188e-04 eta 0:03:14
epoch [33/50] batch [40/51] time 0.173 (0.215) data 0.000 (0.036) loss 0.4557 (0.4572) acc 86.5385 (90.2431) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.168 (0.210) data 0.000 (0.032) loss 0.3694 (0.4514) acc 93.5000 (90.4213) lr 6.3188e-04 eta 0:03:03
epoch [33/50] batch [50/51] time 0.173 (0.206) data 0.000 (0.029) loss 0.5766 (0.4588) acc 86.5000 (90.3046) lr 6.3188e-04 eta 0:02:58
>>> alpha1: 0.146  alpha2: -0.017 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.06 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [34/50] batch [5/51] time 0.167 (0.447) data 0.000 (0.267) loss 0.4185 (0.4156) acc 93.3673 (91.4046) lr 5.7422e-04 eta 0:06:25
epoch [34/50] batch [10/51] time 0.171 (0.310) data 0.000 (0.134) loss 0.6663 (0.4488) acc 87.5000 (91.0611) lr 5.7422e-04 eta 0:04:26
epoch [34/50] batch [15/51] time 0.173 (0.266) data 0.001 (0.090) loss 0.4138 (0.4234) acc 89.5000 (91.0211) lr 5.7422e-04 eta 0:03:46
epoch [34/50] batch [20/51] time 0.188 (0.244) data 0.000 (0.068) loss 0.5423 (0.4312) acc 89.0625 (91.0455) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [25/51] time 0.194 (0.232) data 0.000 (0.054) loss 0.5469 (0.4257) acc 89.0000 (91.1488) lr 5.7422e-04 eta 0:03:15
epoch [34/50] batch [30/51] time 0.184 (0.224) data 0.000 (0.045) loss 0.3758 (0.4310) acc 95.1923 (91.1266) lr 5.7422e-04 eta 0:03:07
epoch [34/50] batch [35/51] time 0.171 (0.218) data 0.000 (0.039) loss 0.5549 (0.4353) acc 85.5000 (90.9071) lr 5.7422e-04 eta 0:03:01
epoch [34/50] batch [40/51] time 0.172 (0.213) data 0.000 (0.034) loss 0.6132 (0.4361) acc 83.6538 (90.7408) lr 5.7422e-04 eta 0:02:55
epoch [34/50] batch [45/51] time 0.172 (0.208) data 0.000 (0.030) loss 0.4304 (0.4361) acc 92.7885 (90.7550) lr 5.7422e-04 eta 0:02:50
epoch [34/50] batch [50/51] time 0.173 (0.203) data 0.000 (0.027) loss 0.4351 (0.4352) acc 92.3077 (90.7403) lr 5.7422e-04 eta 0:02:46
>>> alpha1: 0.145  alpha2: -0.017 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [35/50] batch [5/51] time 0.165 (0.481) data 0.000 (0.295) loss 0.3789 (0.4121) acc 94.7917 (91.2960) lr 5.1825e-04 eta 0:06:29
epoch [35/50] batch [10/51] time 0.177 (0.328) data 0.000 (0.147) loss 0.3759 (0.4600) acc 93.3962 (91.2166) lr 5.1825e-04 eta 0:04:24
epoch [35/50] batch [15/51] time 0.172 (0.278) data 0.000 (0.098) loss 0.5071 (0.4530) acc 92.3469 (91.1228) lr 5.1825e-04 eta 0:03:42
epoch [35/50] batch [20/51] time 0.172 (0.250) data 0.001 (0.074) loss 0.4695 (0.4528) acc 89.7059 (90.9397) lr 5.1825e-04 eta 0:03:18
epoch [35/50] batch [25/51] time 0.179 (0.234) data 0.000 (0.059) loss 0.5020 (0.4895) acc 90.2174 (90.6950) lr 5.1825e-04 eta 0:03:04
epoch [35/50] batch [30/51] time 0.181 (0.225) data 0.000 (0.049) loss 0.4259 (0.4803) acc 92.1296 (90.6234) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.170 (0.219) data 0.000 (0.042) loss 0.6555 (0.4795) acc 85.5000 (90.6078) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [40/51] time 0.175 (0.213) data 0.000 (0.037) loss 0.2530 (0.4680) acc 95.1923 (90.8103) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [45/51] time 0.173 (0.209) data 0.000 (0.033) loss 0.3803 (0.4635) acc 91.3462 (90.8121) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [50/51] time 0.171 (0.205) data 0.000 (0.030) loss 0.3587 (0.4604) acc 92.1569 (90.8373) lr 5.1825e-04 eta 0:02:36
>>> alpha1: 0.144  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [36/50] batch [5/51] time 0.170 (0.444) data 0.000 (0.266) loss 0.3756 (0.3879) acc 94.3182 (91.8736) lr 4.6417e-04 eta 0:05:37
epoch [36/50] batch [10/51] time 0.173 (0.311) data 0.000 (0.133) loss 0.3700 (0.3771) acc 94.2308 (92.0364) lr 4.6417e-04 eta 0:03:54
epoch [36/50] batch [15/51] time 0.181 (0.266) data 0.000 (0.089) loss 0.4197 (0.4230) acc 91.8269 (91.3156) lr 4.6417e-04 eta 0:03:19
epoch [36/50] batch [20/51] time 0.173 (0.245) data 0.000 (0.067) loss 0.3937 (0.4242) acc 89.5000 (91.1896) lr 4.6417e-04 eta 0:03:02
epoch [36/50] batch [25/51] time 0.170 (0.232) data 0.000 (0.053) loss 0.4071 (0.4201) acc 90.0000 (91.0276) lr 4.6417e-04 eta 0:02:51
epoch [36/50] batch [30/51] time 0.156 (0.223) data 0.000 (0.045) loss 0.4485 (0.4243) acc 90.9091 (90.8533) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [35/51] time 0.193 (0.217) data 0.000 (0.038) loss 0.4306 (0.4205) acc 92.2727 (90.9449) lr 4.6417e-04 eta 0:02:38
epoch [36/50] batch [40/51] time 0.159 (0.211) data 0.000 (0.034) loss 0.4682 (0.4276) acc 87.5000 (90.6704) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [45/51] time 0.170 (0.207) data 0.000 (0.030) loss 0.4122 (0.4265) acc 90.5000 (90.7038) lr 4.6417e-04 eta 0:02:28
epoch [36/50] batch [50/51] time 0.178 (0.204) data 0.000 (0.027) loss 0.5423 (0.4276) acc 88.2075 (90.6794) lr 4.6417e-04 eta 0:02:25
>>> alpha1: 0.144  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [37/50] batch [5/51] time 0.170 (0.461) data 0.000 (0.274) loss 0.5528 (0.5142) acc 91.1458 (88.9725) lr 4.1221e-04 eta 0:05:26
epoch [37/50] batch [10/51] time 0.189 (0.320) data 0.000 (0.139) loss 0.3550 (0.4754) acc 92.1296 (89.2002) lr 4.1221e-04 eta 0:03:45
epoch [37/50] batch [15/51] time 0.205 (0.273) data 0.000 (0.093) loss 0.3942 (0.4963) acc 90.6863 (89.6420) lr 4.1221e-04 eta 0:03:10
epoch [37/50] batch [20/51] time 0.189 (0.249) data 0.000 (0.069) loss 0.4596 (0.5092) acc 91.9811 (89.7756) lr 4.1221e-04 eta 0:02:52
epoch [37/50] batch [25/51] time 0.171 (0.235) data 0.001 (0.056) loss 0.6790 (0.4993) acc 80.5000 (89.9084) lr 4.1221e-04 eta 0:02:42
epoch [37/50] batch [30/51] time 0.180 (0.226) data 0.000 (0.046) loss 0.2284 (0.4850) acc 96.6981 (90.1737) lr 4.1221e-04 eta 0:02:34
epoch [37/50] batch [35/51] time 0.198 (0.220) data 0.000 (0.040) loss 0.4259 (0.4687) acc 94.0909 (90.2980) lr 4.1221e-04 eta 0:02:29
epoch [37/50] batch [40/51] time 0.176 (0.215) data 0.000 (0.035) loss 0.3598 (0.4653) acc 92.4528 (90.2901) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [45/51] time 0.159 (0.210) data 0.000 (0.031) loss 0.4024 (0.4538) acc 88.0435 (90.5161) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [50/51] time 0.176 (0.206) data 0.000 (0.028) loss 0.3418 (0.4473) acc 94.3396 (90.6493) lr 4.1221e-04 eta 0:02:16
>>> alpha1: 0.143  alpha2: -0.031 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.08 <<<
epoch [38/50] batch [5/51] time 0.176 (0.456) data 0.000 (0.283) loss 0.4481 (0.4642) acc 92.9245 (91.1559) lr 3.6258e-04 eta 0:04:59
epoch [38/50] batch [10/51] time 0.178 (0.317) data 0.000 (0.142) loss 0.3262 (0.4550) acc 93.5185 (90.5836) lr 3.6258e-04 eta 0:03:27
epoch [38/50] batch [15/51] time 0.197 (0.271) data 0.000 (0.094) loss 0.3166 (0.4556) acc 92.7885 (90.1514) lr 3.6258e-04 eta 0:02:55
epoch [38/50] batch [20/51] time 0.185 (0.247) data 0.000 (0.071) loss 0.5618 (0.4520) acc 86.7647 (90.2975) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [25/51] time 0.173 (0.233) data 0.000 (0.057) loss 0.5872 (0.4516) acc 87.2549 (90.2449) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.214 (0.225) data 0.000 (0.047) loss 0.5001 (0.4466) acc 88.8889 (90.1974) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.186 (0.218) data 0.000 (0.041) loss 0.5166 (0.4504) acc 90.5000 (90.2126) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [40/51] time 0.175 (0.213) data 0.000 (0.036) loss 0.4866 (0.4547) acc 86.3208 (90.0630) lr 3.6258e-04 eta 0:02:12
epoch [38/50] batch [45/51] time 0.172 (0.209) data 0.001 (0.032) loss 0.3909 (0.4449) acc 93.1373 (90.2798) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [50/51] time 0.188 (0.205) data 0.000 (0.029) loss 0.2772 (0.4464) acc 94.5455 (90.3650) lr 3.6258e-04 eta 0:02:05
>>> alpha1: 0.143  alpha2: -0.028 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [39/50] batch [5/51] time 0.173 (0.435) data 0.000 (0.255) loss 0.5033 (0.4542) acc 89.6739 (91.9203) lr 3.1545e-04 eta 0:04:24
epoch [39/50] batch [10/51] time 0.184 (0.308) data 0.000 (0.128) loss 0.4336 (0.4171) acc 92.8571 (92.4315) lr 3.1545e-04 eta 0:03:05
epoch [39/50] batch [15/51] time 0.187 (0.263) data 0.001 (0.085) loss 0.4769 (0.4409) acc 90.6863 (91.2968) lr 3.1545e-04 eta 0:02:37
epoch [39/50] batch [20/51] time 0.178 (0.240) data 0.000 (0.064) loss 0.6277 (0.4546) acc 86.3208 (90.8022) lr 3.1545e-04 eta 0:02:22
epoch [39/50] batch [25/51] time 0.178 (0.229) data 0.001 (0.051) loss 0.3797 (0.4644) acc 89.6226 (90.4521) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [30/51] time 0.177 (0.222) data 0.000 (0.043) loss 0.4335 (0.4637) acc 89.6226 (90.3981) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [35/51] time 0.174 (0.215) data 0.001 (0.037) loss 0.4622 (0.4668) acc 89.7059 (90.1803) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [40/51] time 0.167 (0.210) data 0.000 (0.032) loss 0.3569 (0.4596) acc 92.8571 (90.1849) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [45/51] time 0.170 (0.206) data 0.000 (0.029) loss 0.3627 (0.4456) acc 93.6274 (90.6187) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [50/51] time 0.169 (0.203) data 0.000 (0.026) loss 0.2928 (0.4416) acc 96.0000 (90.8514) lr 3.1545e-04 eta 0:01:53
>>> alpha1: 0.142  alpha2: -0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [40/50] batch [5/51] time 0.170 (0.425) data 0.000 (0.245) loss 0.5943 (0.4673) acc 85.0000 (88.7083) lr 2.7103e-04 eta 0:03:56
epoch [40/50] batch [10/51] time 0.185 (0.301) data 0.000 (0.123) loss 0.5626 (0.4846) acc 85.0000 (88.5593) lr 2.7103e-04 eta 0:02:45
epoch [40/50] batch [15/51] time 0.174 (0.258) data 0.000 (0.082) loss 0.4373 (0.4672) acc 88.4615 (89.0808) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [20/51] time 0.171 (0.238) data 0.000 (0.061) loss 0.2407 (0.4342) acc 94.6078 (89.9670) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [25/51] time 0.172 (0.225) data 0.000 (0.049) loss 0.4520 (0.4379) acc 88.2353 (90.0720) lr 2.7103e-04 eta 0:02:00
epoch [40/50] batch [30/51] time 0.174 (0.217) data 0.000 (0.041) loss 0.3742 (0.4395) acc 93.0000 (90.1052) lr 2.7103e-04 eta 0:01:55
epoch [40/50] batch [35/51] time 0.193 (0.212) data 0.001 (0.035) loss 0.3941 (0.4351) acc 88.8889 (90.3280) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [40/51] time 0.161 (0.208) data 0.000 (0.031) loss 0.3515 (0.4264) acc 92.5532 (90.5944) lr 2.7103e-04 eta 0:01:48
epoch [40/50] batch [45/51] time 0.178 (0.204) data 0.000 (0.027) loss 0.6292 (0.4312) acc 84.7222 (90.4373) lr 2.7103e-04 eta 0:01:45
epoch [40/50] batch [50/51] time 0.190 (0.201) data 0.000 (0.025) loss 0.4314 (0.4301) acc 91.9811 (90.4945) lr 2.7103e-04 eta 0:01:42
>>> alpha1: 0.141  alpha2: -0.017 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [41/50] batch [5/51] time 0.186 (0.452) data 0.001 (0.269) loss 0.5074 (0.4591) acc 85.1852 (89.3042) lr 2.2949e-04 eta 0:03:48
epoch [41/50] batch [10/51] time 0.176 (0.317) data 0.001 (0.135) loss 0.3438 (0.4540) acc 92.5000 (89.9927) lr 2.2949e-04 eta 0:02:38
epoch [41/50] batch [15/51] time 0.161 (0.268) data 0.000 (0.090) loss 0.4838 (0.4288) acc 89.6739 (90.6369) lr 2.2949e-04 eta 0:02:12
epoch [41/50] batch [20/51] time 0.178 (0.244) data 0.000 (0.068) loss 0.3496 (0.4127) acc 93.9815 (91.3844) lr 2.2949e-04 eta 0:01:59
epoch [41/50] batch [25/51] time 0.224 (0.232) data 0.000 (0.054) loss 0.4916 (0.4166) acc 86.7347 (91.0872) lr 2.2949e-04 eta 0:01:52
epoch [41/50] batch [30/51] time 0.173 (0.223) data 0.000 (0.045) loss 0.2801 (0.4051) acc 94.5652 (91.3882) lr 2.2949e-04 eta 0:01:47
epoch [41/50] batch [35/51] time 0.174 (0.218) data 0.000 (0.039) loss 0.3898 (0.4124) acc 95.1923 (91.3261) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [40/51] time 0.181 (0.213) data 0.000 (0.034) loss 0.5201 (0.4108) acc 88.3929 (91.3790) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [45/51] time 0.172 (0.208) data 0.000 (0.030) loss 0.3520 (0.4174) acc 90.8654 (91.0856) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.027) loss 0.5624 (0.4232) acc 89.2157 (90.9709) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.139  alpha2: -0.021 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [42/50] batch [5/51] time 0.178 (0.460) data 0.000 (0.276) loss 0.3475 (0.3628) acc 92.4528 (93.1716) lr 1.9098e-04 eta 0:03:28
epoch [42/50] batch [10/51] time 0.178 (0.320) data 0.000 (0.138) loss 0.4062 (0.3612) acc 89.6226 (93.2035) lr 1.9098e-04 eta 0:02:23
epoch [42/50] batch [15/51] time 0.177 (0.273) data 0.000 (0.092) loss 0.3305 (0.4124) acc 92.4528 (92.5338) lr 1.9098e-04 eta 0:02:01
epoch [42/50] batch [20/51] time 0.179 (0.249) data 0.000 (0.069) loss 0.3322 (0.4097) acc 92.9245 (91.9613) lr 1.9098e-04 eta 0:01:49
epoch [42/50] batch [25/51] time 0.187 (0.234) data 0.000 (0.056) loss 0.3185 (0.4014) acc 93.5185 (92.1342) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [30/51] time 0.175 (0.224) data 0.000 (0.046) loss 0.4570 (0.4097) acc 87.5000 (91.7477) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [35/51] time 0.177 (0.216) data 0.000 (0.040) loss 0.3933 (0.4047) acc 93.7500 (91.7745) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.163 (0.211) data 0.000 (0.035) loss 0.4005 (0.4048) acc 91.1458 (91.7259) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.167 (0.206) data 0.000 (0.031) loss 0.5805 (0.4103) acc 90.3061 (91.6992) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.171 (0.203) data 0.000 (0.028) loss 0.4974 (0.4161) acc 90.1961 (91.4132) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.138  alpha2: -0.018 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [43/50] batch [5/51] time 0.176 (0.461) data 0.001 (0.281) loss 0.5297 (0.3899) acc 89.2157 (92.4034) lr 1.5567e-04 eta 0:03:05
epoch [43/50] batch [10/51] time 0.170 (0.317) data 0.000 (0.141) loss 0.4496 (0.4127) acc 91.2791 (91.5959) lr 1.5567e-04 eta 0:02:06
epoch [43/50] batch [15/51] time 0.175 (0.270) data 0.000 (0.094) loss 0.2351 (0.3965) acc 97.5000 (92.4696) lr 1.5567e-04 eta 0:01:46
epoch [43/50] batch [20/51] time 0.180 (0.249) data 0.000 (0.070) loss 0.3843 (0.3988) acc 91.1765 (91.8971) lr 1.5567e-04 eta 0:01:36
epoch [43/50] batch [25/51] time 0.201 (0.235) data 0.000 (0.056) loss 0.4879 (0.4038) acc 87.9808 (91.5810) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.192 (0.227) data 0.000 (0.047) loss 0.3267 (0.3974) acc 95.6140 (91.7532) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.211 (0.223) data 0.000 (0.040) loss 0.4082 (0.3954) acc 91.6667 (91.9580) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.172 (0.217) data 0.000 (0.035) loss 0.3335 (0.4021) acc 95.4082 (91.7809) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.161 (0.211) data 0.000 (0.031) loss 0.4545 (0.4086) acc 90.4255 (91.5838) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.163 (0.207) data 0.000 (0.028) loss 0.3751 (0.4024) acc 93.2292 (91.7905) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.136  alpha2: -0.019 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [44/50] batch [5/51] time 0.171 (0.470) data 0.000 (0.296) loss 0.2989 (0.4019) acc 92.0000 (90.6712) lr 1.2369e-04 eta 0:02:45
epoch [44/50] batch [10/51] time 0.171 (0.323) data 0.000 (0.148) loss 0.4655 (0.4140) acc 87.2549 (90.4306) lr 1.2369e-04 eta 0:01:51
epoch [44/50] batch [15/51] time 0.175 (0.273) data 0.000 (0.099) loss 0.4341 (0.4160) acc 95.9184 (90.6227) lr 1.2369e-04 eta 0:01:33
epoch [44/50] batch [20/51] time 0.177 (0.250) data 0.000 (0.074) loss 0.5042 (0.4229) acc 90.1042 (90.8306) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [25/51] time 0.173 (0.235) data 0.000 (0.059) loss 0.5035 (0.4176) acc 89.0625 (90.9261) lr 1.2369e-04 eta 0:01:17
epoch [44/50] batch [30/51] time 0.181 (0.226) data 0.000 (0.050) loss 0.4181 (0.4125) acc 90.4546 (91.0845) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [35/51] time 0.173 (0.218) data 0.000 (0.042) loss 0.4568 (0.4094) acc 86.7021 (91.1361) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [40/51] time 0.176 (0.212) data 0.000 (0.037) loss 0.3740 (0.4082) acc 93.3962 (91.2146) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [45/51] time 0.165 (0.207) data 0.000 (0.033) loss 0.3068 (0.4088) acc 92.1875 (91.1949) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.167 (0.220) data 0.000 (0.030) loss 0.3259 (0.4042) acc 94.3878 (91.2374) lr 1.2369e-04 eta 0:01:07
>>> alpha1: 0.136  alpha2: -0.016 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.08 <<<
epoch [45/50] batch [5/51] time 0.178 (0.536) data 0.000 (0.356) loss 0.2582 (0.3821) acc 95.7547 (91.9272) lr 9.5173e-05 eta 0:02:41
epoch [45/50] batch [10/51] time 0.172 (0.358) data 0.000 (0.178) loss 0.4073 (0.4179) acc 95.0980 (91.8639) lr 9.5173e-05 eta 0:01:45
epoch [45/50] batch [15/51] time 0.161 (0.295) data 0.000 (0.119) loss 0.6943 (0.4374) acc 82.6087 (90.8366) lr 9.5173e-05 eta 0:01:25
epoch [45/50] batch [20/51] time 0.173 (0.268) data 0.000 (0.089) loss 0.4036 (0.4235) acc 90.3061 (91.1658) lr 9.5173e-05 eta 0:01:16
epoch [45/50] batch [25/51] time 0.182 (0.251) data 0.000 (0.071) loss 0.4724 (0.4171) acc 88.7255 (91.3186) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [30/51] time 0.188 (0.239) data 0.000 (0.060) loss 0.4495 (0.4137) acc 89.0351 (91.3297) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [35/51] time 0.170 (0.230) data 0.000 (0.051) loss 0.4945 (0.4064) acc 88.5000 (91.4817) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [40/51] time 0.171 (0.222) data 0.000 (0.045) loss 0.3928 (0.4077) acc 91.6667 (91.2404) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [45/51] time 0.170 (0.216) data 0.000 (0.040) loss 0.2875 (0.4058) acc 96.5686 (91.5420) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.176 (0.212) data 0.000 (0.036) loss 0.4696 (0.4078) acc 89.6226 (91.5538) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.135  alpha2: -0.012 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [46/50] batch [5/51] time 0.167 (0.441) data 0.000 (0.263) loss 0.4491 (0.4714) acc 90.3061 (89.9001) lr 7.0224e-05 eta 0:01:50
epoch [46/50] batch [10/51] time 0.192 (0.309) data 0.000 (0.132) loss 0.5890 (0.4619) acc 86.5000 (89.9834) lr 7.0224e-05 eta 0:01:15
epoch [46/50] batch [15/51] time 0.170 (0.264) data 0.000 (0.088) loss 0.2583 (0.4495) acc 95.4082 (90.4249) lr 7.0224e-05 eta 0:01:03
epoch [46/50] batch [20/51] time 0.184 (0.242) data 0.000 (0.066) loss 0.3086 (0.4480) acc 91.2037 (90.0045) lr 7.0224e-05 eta 0:00:56
epoch [46/50] batch [25/51] time 0.198 (0.230) data 0.000 (0.053) loss 0.4109 (0.4475) acc 90.4546 (90.1351) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [30/51] time 0.174 (0.221) data 0.000 (0.044) loss 0.2213 (0.4353) acc 98.4694 (90.4735) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [35/51] time 0.192 (0.214) data 0.000 (0.038) loss 0.2811 (0.4289) acc 94.5833 (90.5741) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [40/51] time 0.175 (0.209) data 0.000 (0.033) loss 0.4537 (0.4252) acc 87.7358 (90.6955) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.175 (0.205) data 0.000 (0.029) loss 0.4817 (0.4217) acc 88.6792 (90.8697) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [50/51] time 0.165 (0.202) data 0.000 (0.027) loss 0.4709 (0.4197) acc 87.7551 (91.0105) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.135  alpha2: -0.010 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [47/50] batch [5/51] time 0.192 (0.458) data 0.000 (0.275) loss 0.2386 (0.3479) acc 94.9153 (92.3002) lr 4.8943e-05 eta 0:01:31
epoch [47/50] batch [10/51] time 0.165 (0.318) data 0.000 (0.138) loss 0.3111 (0.3643) acc 95.8333 (92.1815) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [15/51] time 0.172 (0.270) data 0.000 (0.092) loss 0.2749 (0.3750) acc 91.6667 (91.9059) lr 4.8943e-05 eta 0:00:51
epoch [47/50] batch [20/51] time 0.173 (0.244) data 0.000 (0.069) loss 0.3073 (0.3688) acc 92.7885 (92.0759) lr 4.8943e-05 eta 0:00:44
epoch [47/50] batch [25/51] time 0.183 (0.232) data 0.000 (0.055) loss 0.3758 (0.3782) acc 94.0000 (91.8612) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.180 (0.222) data 0.000 (0.046) loss 0.3619 (0.3887) acc 92.7885 (91.6484) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [35/51] time 0.191 (0.216) data 0.000 (0.040) loss 0.4580 (0.3941) acc 87.5000 (91.5385) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.164 (0.211) data 0.000 (0.035) loss 0.3781 (0.3982) acc 93.2292 (91.5617) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.164 (0.206) data 0.000 (0.031) loss 0.2931 (0.3964) acc 94.7917 (91.7213) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.166 (0.203) data 0.000 (0.028) loss 0.3406 (0.3919) acc 90.8163 (91.7968) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.134  alpha2: -0.008 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.09 <<<
epoch [48/50] batch [5/51] time 0.182 (0.447) data 0.001 (0.259) loss 0.3741 (0.3822) acc 93.8679 (93.2409) lr 3.1417e-05 eta 0:01:06
epoch [48/50] batch [10/51] time 0.174 (0.311) data 0.000 (0.129) loss 0.5798 (0.3847) acc 88.5417 (93.2417) lr 3.1417e-05 eta 0:00:44
epoch [48/50] batch [15/51] time 0.166 (0.265) data 0.000 (0.086) loss 0.4094 (0.3780) acc 89.5833 (92.9703) lr 3.1417e-05 eta 0:00:36
epoch [48/50] batch [20/51] time 0.171 (0.244) data 0.000 (0.065) loss 0.5208 (0.3997) acc 85.5000 (92.1959) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [25/51] time 0.178 (0.230) data 0.000 (0.052) loss 0.4249 (0.4124) acc 91.6667 (91.9588) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [30/51] time 0.203 (0.222) data 0.000 (0.043) loss 0.4130 (0.4062) acc 88.2075 (91.9907) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.166 (0.215) data 0.000 (0.037) loss 0.4218 (0.4112) acc 93.7500 (91.8660) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.164 (0.210) data 0.000 (0.033) loss 0.3691 (0.4010) acc 89.5833 (92.0642) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [45/51] time 0.165 (0.206) data 0.001 (0.029) loss 0.5261 (0.4059) acc 89.5833 (91.9380) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.185 (0.203) data 0.000 (0.026) loss 0.3413 (0.4067) acc 89.9123 (91.7654) lr 3.1417e-05 eta 0:00:20
>>> alpha1: 0.135  alpha2: -0.009 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.09 <<<
epoch [49/50] batch [5/51] time 0.180 (0.468) data 0.000 (0.282) loss 0.3769 (0.4512) acc 92.1296 (91.2906) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.181 (0.327) data 0.000 (0.141) loss 0.4214 (0.4163) acc 92.7273 (91.0965) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.178 (0.275) data 0.000 (0.094) loss 0.3769 (0.4145) acc 91.8269 (91.3085) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.167 (0.251) data 0.000 (0.071) loss 0.6060 (0.4309) acc 90.8163 (91.4125) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.167 (0.237) data 0.000 (0.057) loss 0.4692 (0.4224) acc 87.2449 (91.4237) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.169 (0.227) data 0.000 (0.047) loss 0.3890 (0.4193) acc 92.3469 (91.5294) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.179 (0.220) data 0.000 (0.040) loss 0.4396 (0.4153) acc 90.1961 (91.5062) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.177 (0.215) data 0.000 (0.035) loss 0.3042 (0.4190) acc 94.8113 (91.3593) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.164 (0.210) data 0.000 (0.031) loss 0.3653 (0.4182) acc 92.1875 (91.1896) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.183 (0.207) data 0.000 (0.028) loss 0.3404 (0.4175) acc 95.9821 (91.2273) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.133  alpha2: -0.013 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.05 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.10 <<<
epoch [50/50] batch [5/51] time 0.172 (0.451) data 0.001 (0.268) loss 0.2622 (0.3781) acc 94.3878 (91.4103) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [10/51] time 0.186 (0.313) data 0.000 (0.134) loss 0.4544 (0.3938) acc 90.0000 (91.5051) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [15/51] time 0.171 (0.269) data 0.000 (0.089) loss 0.4770 (0.3784) acc 91.1765 (91.9457) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.172 (0.246) data 0.000 (0.067) loss 0.4332 (0.3993) acc 90.1961 (91.5739) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.181 (0.233) data 0.000 (0.054) loss 0.3102 (0.3995) acc 94.0000 (91.2943) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.186 (0.226) data 0.000 (0.045) loss 0.6012 (0.4119) acc 86.4130 (91.0389) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.185 (0.220) data 0.000 (0.038) loss 0.4765 (0.4068) acc 87.7358 (91.0991) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.034) loss 0.4645 (0.4059) acc 88.8889 (91.0137) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.173 (0.210) data 0.000 (0.030) loss 0.2974 (0.3995) acc 96.1538 (91.3248) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.173 (0.206) data 0.000 (0.027) loss 0.5065 (0.4090) acc 88.9423 (91.0678) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.09, 0.07, 0.05, 0.05, 0.05, 0.05, 0.06, 0.06, 0.05, 0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
* matched noise rate: [0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.03, 0.02]
* unmatched noise rate: [0.14, 0.11, 0.09, 0.09, 0.09, 0.09, 0.11, 0.1, 0.1, 0.1, 0.1, 0.11, 0.1, 0.1, 0.11, 0.11, 0.09, 0.1, 0.09, 0.08, 0.08, 0.09, 0.09, 0.1, 0.1, 0.09, 0.08, 0.08, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.08, 0.1, 0.09, 0.09, 0.09, 0.1]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:04,  2.67s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.31it/s] 20%|██        | 5/25 [00:03<00:08,  2.42it/s] 28%|██▊       | 7/25 [00:03<00:04,  3.66it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.98it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.29it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.51it/s] 60%|██████    | 15/25 [00:03<00:01,  8.60it/s] 68%|██████▊   | 17/25 [00:04<00:01,  6.63it/s] 76%|███████▌  | 19/25 [00:04<00:00,  7.75it/s] 84%|████████▍ | 21/25 [00:04<00:00,  8.76it/s] 92%|█████████▏| 23/25 [00:04<00:00,  9.61it/s]100%|██████████| 25/25 [00:05<00:00,  7.22it/s]100%|██████████| 25/25 [00:05<00:00,  4.65it/s]
=> result
* total: 2,463
* correct: 2,246
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 90.7%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 7	acc: 58.3%
* class: 3 (sweet pea)	total: 17	correct: 14	acc: 82.4%
* class: 4 (english marigold)	total: 20	correct: 13	acc: 65.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 23	acc: 88.5%
* class: 11 (colt's foot)	total: 26	correct: 20	acc: 76.9%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 11	acc: 84.6%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 23	acc: 92.0%
* class: 18 (balloon flower)	total: 15	correct: 12	acc: 80.0%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 25	acc: 92.6%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 23	acc: 88.5%
* class: 30 (carnation)	total: 16	correct: 13	acc: 81.2%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 13	acc: 92.9%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 20	acc: 90.9%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 6	acc: 46.2%
* class: 39 (lenten rose)	total: 20	correct: 16	acc: 80.0%
* class: 40 (barbeton daisy)	total: 38	correct: 17	acc: 44.7%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 17	acc: 43.6%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 10	acc: 83.3%
* class: 45 (wallflower)	total: 59	correct: 56	acc: 94.9%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 27	acc: 96.4%
* class: 50 (petunia)	total: 77	correct: 74	acc: 96.1%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 25	acc: 89.3%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 31	acc: 100.0%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 15	acc: 93.8%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 23	acc: 79.3%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 29	acc: 80.6%
* class: 75 (morning glory)	total: 32	correct: 31	acc: 96.9%
* class: 76 (passion flower)	total: 75	correct: 69	acc: 92.0%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 38	acc: 97.4%
* class: 83 (columbine)	total: 26	correct: 23	acc: 88.5%
* class: 84 (desert-rose)	total: 18	correct: 15	acc: 83.3%
* class: 85 (tree mallow)	total: 17	correct: 12	acc: 70.6%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 35	acc: 76.1%
* class: 88 (watercress)	total: 55	correct: 48	acc: 87.3%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 16	acc: 69.6%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 46	acc: 93.9%
* class: 94 (bougainvillea)	total: 38	correct: 33	acc: 86.8%
* class: 95 (camellia)	total: 27	correct: 23	acc: 85.2%
* class: 96 (mallow)	total: 20	correct: 17	acc: 85.0%
* class: 97 (mexican petunia)	total: 25	correct: 24	acc: 96.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 13	acc: 76.5%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 91.6%
Elapsed: 0:28:20
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_4-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.259 (1.094) data 0.000 (0.372) loss 4.1417 (4.3884) acc 15.6250 (6.2500) lr 1.0000e-05 eta 0:46:25
epoch [1/50] batch [10/51] time 0.265 (0.681) data 0.000 (0.186) loss 4.0590 (4.3283) acc 9.3750 (8.4375) lr 1.0000e-05 eta 0:28:49
epoch [1/50] batch [15/51] time 0.273 (0.542) data 0.000 (0.124) loss 3.8088 (4.2408) acc 21.8750 (10.6250) lr 1.0000e-05 eta 0:22:55
epoch [1/50] batch [20/51] time 0.274 (0.474) data 0.000 (0.093) loss 3.8609 (4.1854) acc 18.7500 (11.5625) lr 1.0000e-05 eta 0:19:58
epoch [1/50] batch [25/51] time 0.264 (0.433) data 0.000 (0.075) loss 3.9522 (4.0681) acc 18.7500 (14.5000) lr 1.0000e-05 eta 0:18:13
epoch [1/50] batch [30/51] time 0.267 (0.406) data 0.000 (0.062) loss 3.6671 (4.0080) acc 37.5000 (16.7708) lr 1.0000e-05 eta 0:17:03
epoch [1/50] batch [35/51] time 0.274 (0.387) data 0.000 (0.053) loss 3.5764 (3.9976) acc 34.3750 (18.0357) lr 1.0000e-05 eta 0:16:14
epoch [1/50] batch [40/51] time 0.257 (0.371) data 0.000 (0.047) loss 3.4998 (3.9307) acc 28.1250 (19.6094) lr 1.0000e-05 eta 0:15:32
epoch [1/50] batch [45/51] time 0.256 (0.359) data 0.000 (0.042) loss 3.4100 (3.8859) acc 37.5000 (21.1806) lr 1.0000e-05 eta 0:14:58
epoch [1/50] batch [50/51] time 0.257 (0.349) data 0.000 (0.037) loss 3.8259 (3.8574) acc 28.1250 (22.5000) lr 1.0000e-05 eta 0:14:31
epoch [2/50] batch [5/51] time 0.334 (0.537) data 0.000 (0.245) loss 2.9631 (3.5812) acc 46.8750 (38.1250) lr 2.0000e-03 eta 0:22:19
epoch [2/50] batch [10/51] time 0.274 (0.402) data 0.000 (0.122) loss 2.8921 (3.2235) acc 40.6250 (40.3125) lr 2.0000e-03 eta 0:16:40
epoch [2/50] batch [15/51] time 0.263 (0.356) data 0.000 (0.082) loss 2.7062 (3.1999) acc 43.7500 (41.0417) lr 2.0000e-03 eta 0:14:44
epoch [2/50] batch [20/51] time 0.263 (0.334) data 0.000 (0.061) loss 3.3446 (3.2182) acc 43.7500 (42.5000) lr 2.0000e-03 eta 0:13:47
epoch [2/50] batch [25/51] time 0.260 (0.320) data 0.000 (0.049) loss 2.3086 (3.1904) acc 59.3750 (42.7500) lr 2.0000e-03 eta 0:13:11
epoch [2/50] batch [30/51] time 0.260 (0.311) data 0.000 (0.041) loss 2.9725 (3.1748) acc 50.0000 (43.2292) lr 2.0000e-03 eta 0:12:48
epoch [2/50] batch [35/51] time 0.261 (0.305) data 0.000 (0.035) loss 2.3882 (3.1810) acc 53.1250 (43.9286) lr 2.0000e-03 eta 0:12:30
epoch [2/50] batch [40/51] time 0.259 (0.299) data 0.000 (0.031) loss 3.1570 (3.1928) acc 43.7500 (43.9062) lr 2.0000e-03 eta 0:12:15
epoch [2/50] batch [45/51] time 0.257 (0.295) data 0.000 (0.027) loss 3.1245 (3.1666) acc 43.7500 (44.3750) lr 2.0000e-03 eta 0:12:02
epoch [2/50] batch [50/51] time 0.257 (0.291) data 0.000 (0.025) loss 2.9299 (3.1357) acc 43.7500 (44.8125) lr 2.0000e-03 eta 0:11:52
epoch [3/50] batch [5/51] time 0.269 (0.547) data 0.000 (0.267) loss 2.3311 (2.8698) acc 59.3750 (50.6250) lr 1.9980e-03 eta 0:22:17
epoch [3/50] batch [10/51] time 0.268 (0.406) data 0.000 (0.134) loss 2.7353 (3.1436) acc 46.8750 (44.0625) lr 1.9980e-03 eta 0:16:30
epoch [3/50] batch [15/51] time 0.259 (0.359) data 0.000 (0.089) loss 2.6865 (3.1151) acc 59.3750 (46.0417) lr 1.9980e-03 eta 0:14:33
epoch [3/50] batch [20/51] time 0.259 (0.335) data 0.000 (0.067) loss 3.1743 (3.0844) acc 46.8750 (47.0312) lr 1.9980e-03 eta 0:13:32
epoch [3/50] batch [25/51] time 0.259 (0.320) data 0.000 (0.054) loss 3.0774 (3.1000) acc 50.0000 (46.5000) lr 1.9980e-03 eta 0:12:55
epoch [3/50] batch [30/51] time 0.270 (0.311) data 0.000 (0.045) loss 3.0850 (3.0299) acc 50.0000 (48.2292) lr 1.9980e-03 eta 0:12:31
epoch [3/50] batch [35/51] time 0.268 (0.304) data 0.000 (0.038) loss 2.9930 (2.9879) acc 56.2500 (49.2857) lr 1.9980e-03 eta 0:12:12
epoch [3/50] batch [40/51] time 0.258 (0.298) data 0.000 (0.034) loss 3.1639 (2.9842) acc 43.7500 (49.4531) lr 1.9980e-03 eta 0:11:57
epoch [3/50] batch [45/51] time 0.260 (0.294) data 0.000 (0.030) loss 2.7823 (2.9768) acc 53.1250 (49.4444) lr 1.9980e-03 eta 0:11:45
epoch [3/50] batch [50/51] time 0.258 (0.290) data 0.000 (0.027) loss 2.2871 (2.9757) acc 59.3750 (49.3750) lr 1.9980e-03 eta 0:11:35
epoch [4/50] batch [5/51] time 0.279 (0.545) data 0.000 (0.253) loss 3.7538 (3.0068) acc 43.7500 (51.2500) lr 1.9921e-03 eta 0:21:42
epoch [4/50] batch [10/51] time 0.265 (0.404) data 0.000 (0.127) loss 2.8013 (2.8783) acc 56.2500 (52.1875) lr 1.9921e-03 eta 0:16:04
epoch [4/50] batch [15/51] time 0.261 (0.358) data 0.000 (0.085) loss 3.2532 (2.9507) acc 37.5000 (49.1667) lr 1.9921e-03 eta 0:14:12
epoch [4/50] batch [20/51] time 0.259 (0.334) data 0.000 (0.063) loss 2.4176 (2.9502) acc 62.5000 (48.7500) lr 1.9921e-03 eta 0:13:13
epoch [4/50] batch [25/51] time 0.271 (0.321) data 0.000 (0.051) loss 2.0336 (2.8242) acc 50.0000 (50.7500) lr 1.9921e-03 eta 0:12:40
epoch [4/50] batch [30/51] time 0.262 (0.311) data 0.000 (0.042) loss 2.8771 (2.8263) acc 53.1250 (51.3542) lr 1.9921e-03 eta 0:12:16
epoch [4/50] batch [35/51] time 0.267 (0.305) data 0.000 (0.036) loss 3.7290 (2.9260) acc 31.2500 (50.5357) lr 1.9921e-03 eta 0:12:00
epoch [4/50] batch [40/51] time 0.257 (0.299) data 0.000 (0.032) loss 1.9359 (2.9187) acc 78.1250 (50.8594) lr 1.9921e-03 eta 0:11:45
epoch [4/50] batch [45/51] time 0.258 (0.295) data 0.000 (0.028) loss 2.3340 (2.9250) acc 59.3750 (50.7639) lr 1.9921e-03 eta 0:11:32
epoch [4/50] batch [50/51] time 0.257 (0.291) data 0.000 (0.025) loss 2.8279 (2.9003) acc 62.5000 (50.9375) lr 1.9921e-03 eta 0:11:22
epoch [5/50] batch [5/51] time 0.271 (0.593) data 0.000 (0.315) loss 2.5046 (2.7385) acc 56.2500 (51.2500) lr 1.9823e-03 eta 0:23:07
epoch [5/50] batch [10/51] time 0.266 (0.429) data 0.000 (0.157) loss 2.2271 (2.8135) acc 62.5000 (51.8750) lr 1.9823e-03 eta 0:16:42
epoch [5/50] batch [15/51] time 0.275 (0.374) data 0.000 (0.105) loss 2.3197 (2.6639) acc 75.0000 (54.7917) lr 1.9823e-03 eta 0:14:31
epoch [5/50] batch [20/51] time 0.261 (0.347) data 0.000 (0.079) loss 1.9893 (2.7352) acc 65.6250 (52.8125) lr 1.9823e-03 eta 0:13:26
epoch [5/50] batch [25/51] time 0.260 (0.330) data 0.000 (0.063) loss 2.4850 (2.7436) acc 56.2500 (52.6250) lr 1.9823e-03 eta 0:12:45
epoch [5/50] batch [30/51] time 0.262 (0.319) data 0.000 (0.053) loss 3.6625 (2.8380) acc 31.2500 (51.1458) lr 1.9823e-03 eta 0:12:19
epoch [5/50] batch [35/51] time 0.259 (0.311) data 0.000 (0.045) loss 2.4408 (2.7921) acc 65.6250 (52.5893) lr 1.9823e-03 eta 0:11:59
epoch [5/50] batch [40/51] time 0.257 (0.305) data 0.000 (0.040) loss 2.5015 (2.7886) acc 53.1250 (52.3438) lr 1.9823e-03 eta 0:11:42
epoch [5/50] batch [45/51] time 0.258 (0.299) data 0.000 (0.035) loss 2.4271 (2.8054) acc 46.8750 (51.8750) lr 1.9823e-03 eta 0:11:28
epoch [5/50] batch [50/51] time 0.259 (0.295) data 0.000 (0.032) loss 3.0407 (2.8433) acc 56.2500 (51.5625) lr 1.9823e-03 eta 0:11:18
epoch [6/50] batch [5/51] time 0.267 (0.610) data 0.000 (0.333) loss 2.5041 (2.8079) acc 50.0000 (50.6250) lr 1.9686e-03 eta 0:23:17
epoch [6/50] batch [10/51] time 0.273 (0.440) data 0.000 (0.166) loss 2.7548 (2.7355) acc 56.2500 (53.4375) lr 1.9686e-03 eta 0:16:44
epoch [6/50] batch [15/51] time 0.273 (0.382) data 0.000 (0.111) loss 2.3589 (2.6594) acc 65.6250 (53.5417) lr 1.9686e-03 eta 0:14:31
epoch [6/50] batch [20/51] time 0.260 (0.353) data 0.000 (0.083) loss 2.8076 (2.6782) acc 56.2500 (53.9062) lr 1.9686e-03 eta 0:13:23
epoch [6/50] batch [25/51] time 0.272 (0.337) data 0.000 (0.067) loss 3.3355 (2.7674) acc 43.7500 (53.2500) lr 1.9686e-03 eta 0:12:45
epoch [6/50] batch [30/51] time 0.271 (0.325) data 0.000 (0.056) loss 2.8149 (2.7816) acc 50.0000 (52.9167) lr 1.9686e-03 eta 0:12:16
epoch [6/50] batch [35/51] time 0.261 (0.316) data 0.000 (0.048) loss 2.7874 (2.8010) acc 62.5000 (52.9464) lr 1.9686e-03 eta 0:11:54
epoch [6/50] batch [40/51] time 0.260 (0.310) data 0.000 (0.042) loss 2.7015 (2.8075) acc 56.2500 (52.2656) lr 1.9686e-03 eta 0:11:38
epoch [6/50] batch [45/51] time 0.258 (0.304) data 0.000 (0.037) loss 1.8260 (2.8054) acc 71.8750 (52.9167) lr 1.9686e-03 eta 0:11:23
epoch [6/50] batch [50/51] time 0.259 (0.300) data 0.000 (0.033) loss 2.7025 (2.7778) acc 56.2500 (53.3125) lr 1.9686e-03 eta 0:11:12
epoch [7/50] batch [5/51] time 0.261 (0.546) data 0.000 (0.260) loss 2.0737 (2.8966) acc 65.6250 (50.6250) lr 1.9511e-03 eta 0:20:23
epoch [7/50] batch [10/51] time 0.259 (0.404) data 0.000 (0.130) loss 2.5284 (2.8188) acc 65.6250 (51.2500) lr 1.9511e-03 eta 0:15:03
epoch [7/50] batch [15/51] time 0.273 (0.360) data 0.000 (0.087) loss 3.8091 (2.7762) acc 37.5000 (51.6667) lr 1.9511e-03 eta 0:13:21
epoch [7/50] batch [20/51] time 0.271 (0.337) data 0.000 (0.065) loss 3.2813 (2.7746) acc 46.8750 (52.6562) lr 1.9511e-03 eta 0:12:28
epoch [7/50] batch [25/51] time 0.262 (0.322) data 0.000 (0.052) loss 2.2095 (2.7502) acc 59.3750 (53.2500) lr 1.9511e-03 eta 0:11:55
epoch [7/50] batch [30/51] time 0.261 (0.312) data 0.000 (0.043) loss 3.1603 (2.7412) acc 50.0000 (54.2708) lr 1.9511e-03 eta 0:11:31
epoch [7/50] batch [35/51] time 0.270 (0.306) data 0.000 (0.037) loss 2.8960 (2.7657) acc 56.2500 (53.7500) lr 1.9511e-03 eta 0:11:15
epoch [7/50] batch [40/51] time 0.258 (0.300) data 0.000 (0.033) loss 2.5425 (2.7387) acc 59.3750 (54.5312) lr 1.9511e-03 eta 0:11:01
epoch [7/50] batch [45/51] time 0.261 (0.296) data 0.000 (0.029) loss 3.2663 (2.7169) acc 43.7500 (54.9306) lr 1.9511e-03 eta 0:10:50
epoch [7/50] batch [50/51] time 0.258 (0.292) data 0.000 (0.026) loss 2.0595 (2.7265) acc 65.6250 (55.1250) lr 1.9511e-03 eta 0:10:41
epoch [8/50] batch [5/51] time 0.278 (0.539) data 0.000 (0.256) loss 1.9789 (2.6367) acc 65.6250 (60.0000) lr 1.9298e-03 eta 0:19:39
epoch [8/50] batch [10/51] time 0.260 (0.401) data 0.000 (0.128) loss 2.4951 (2.5725) acc 56.2500 (57.8125) lr 1.9298e-03 eta 0:14:35
epoch [8/50] batch [15/51] time 0.260 (0.355) data 0.000 (0.085) loss 2.6521 (2.5998) acc 62.5000 (57.7083) lr 1.9298e-03 eta 0:12:54
epoch [8/50] batch [20/51] time 0.270 (0.333) data 0.000 (0.064) loss 2.2183 (2.6423) acc 68.7500 (58.1250) lr 1.9298e-03 eta 0:12:03
epoch [8/50] batch [25/51] time 0.260 (0.319) data 0.000 (0.051) loss 2.4623 (2.6446) acc 56.2500 (57.6250) lr 1.9298e-03 eta 0:11:31
epoch [8/50] batch [30/51] time 0.277 (0.310) data 0.000 (0.043) loss 2.7263 (2.6241) acc 59.3750 (57.3958) lr 1.9298e-03 eta 0:11:11
epoch [8/50] batch [35/51] time 0.259 (0.304) data 0.000 (0.037) loss 3.0557 (2.6213) acc 53.1250 (57.5893) lr 1.9298e-03 eta 0:10:55
epoch [8/50] batch [40/51] time 0.256 (0.298) data 0.000 (0.032) loss 2.8888 (2.6615) acc 56.2500 (57.5000) lr 1.9298e-03 eta 0:10:42
epoch [8/50] batch [45/51] time 0.256 (0.294) data 0.000 (0.029) loss 3.1009 (2.6686) acc 46.8750 (56.9444) lr 1.9298e-03 eta 0:10:31
epoch [8/50] batch [50/51] time 0.258 (0.290) data 0.000 (0.026) loss 2.4631 (2.6521) acc 59.3750 (57.4375) lr 1.9298e-03 eta 0:10:21
epoch [9/50] batch [5/51] time 0.261 (0.553) data 0.000 (0.259) loss 2.9894 (2.5727) acc 50.0000 (58.1250) lr 1.9048e-03 eta 0:19:41
epoch [9/50] batch [10/51] time 0.265 (0.410) data 0.000 (0.130) loss 2.9061 (2.6520) acc 62.5000 (57.5000) lr 1.9048e-03 eta 0:14:34
epoch [9/50] batch [15/51] time 0.260 (0.361) data 0.000 (0.086) loss 2.6852 (2.6800) acc 56.2500 (57.9167) lr 1.9048e-03 eta 0:12:47
epoch [9/50] batch [20/51] time 0.272 (0.338) data 0.000 (0.065) loss 2.6737 (2.7104) acc 59.3750 (56.7188) lr 1.9048e-03 eta 0:11:56
epoch [9/50] batch [25/51] time 0.262 (0.324) data 0.000 (0.052) loss 1.6388 (2.6056) acc 81.2500 (58.0000) lr 1.9048e-03 eta 0:11:25
epoch [9/50] batch [30/51] time 0.266 (0.315) data 0.000 (0.043) loss 2.0118 (2.5746) acc 65.6250 (58.6458) lr 1.9048e-03 eta 0:11:05
epoch [9/50] batch [35/51] time 0.272 (0.308) data 0.000 (0.037) loss 2.3894 (2.5860) acc 78.1250 (59.0179) lr 1.9048e-03 eta 0:10:49
epoch [9/50] batch [40/51] time 0.258 (0.302) data 0.000 (0.033) loss 2.1361 (2.5735) acc 53.1250 (58.8281) lr 1.9048e-03 eta 0:10:35
epoch [9/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.029) loss 2.2796 (2.6014) acc 65.6250 (58.5417) lr 1.9048e-03 eta 0:10:23
epoch [9/50] batch [50/51] time 0.257 (0.293) data 0.000 (0.026) loss 3.1922 (2.6060) acc 43.7500 (58.5625) lr 1.9048e-03 eta 0:10:13
epoch [10/50] batch [5/51] time 0.295 (0.580) data 0.000 (0.290) loss 2.2938 (2.7183) acc 68.7500 (55.0000) lr 1.8763e-03 eta 0:20:09
epoch [10/50] batch [10/51] time 0.272 (0.423) data 0.000 (0.145) loss 2.6931 (2.7356) acc 62.5000 (56.2500) lr 1.8763e-03 eta 0:14:41
epoch [10/50] batch [15/51] time 0.260 (0.371) data 0.000 (0.097) loss 2.2886 (2.7135) acc 65.6250 (57.0833) lr 1.8763e-03 eta 0:12:49
epoch [10/50] batch [20/51] time 0.260 (0.345) data 0.000 (0.073) loss 2.3522 (2.6100) acc 68.7500 (58.5938) lr 1.8763e-03 eta 0:11:55
epoch [10/50] batch [25/51] time 0.261 (0.329) data 0.000 (0.058) loss 2.5257 (2.5819) acc 56.2500 (58.8750) lr 1.8763e-03 eta 0:11:20
epoch [10/50] batch [30/51] time 0.272 (0.319) data 0.000 (0.048) loss 3.0209 (2.5998) acc 53.1250 (58.9583) lr 1.8763e-03 eta 0:10:58
epoch [10/50] batch [35/51] time 0.269 (0.312) data 0.000 (0.042) loss 2.3117 (2.5603) acc 65.6250 (59.8214) lr 1.8763e-03 eta 0:10:40
epoch [10/50] batch [40/51] time 0.259 (0.306) data 0.000 (0.036) loss 2.2514 (2.5531) acc 62.5000 (59.5312) lr 1.8763e-03 eta 0:10:27
epoch [10/50] batch [45/51] time 0.257 (0.300) data 0.000 (0.032) loss 2.4246 (2.5564) acc 62.5000 (59.7222) lr 1.8763e-03 eta 0:10:14
epoch [10/50] batch [50/51] time 0.258 (0.296) data 0.000 (0.029) loss 2.9126 (2.5505) acc 59.3750 (59.7500) lr 1.8763e-03 eta 0:10:04
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.517  alpha2: 0.168 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [11/50] batch [5/51] time 0.714 (0.783) data 0.000 (0.264) loss 1.2036 (1.2971) acc 87.2093 (76.2095) lr 1.8443e-03 eta 0:26:33
epoch [11/50] batch [10/51] time 0.856 (0.726) data 0.000 (0.132) loss 1.2069 (1.2585) acc 73.4694 (76.1027) lr 1.8443e-03 eta 0:24:34
epoch [11/50] batch [15/51] time 0.851 (0.585) data 0.000 (0.088) loss 1.0651 (1.2643) acc 79.4118 (74.8008) lr 1.8443e-03 eta 0:19:43
epoch [11/50] batch [20/51] time 0.164 (0.478) data 0.000 (0.066) loss 1.3570 (1.2600) acc 72.8723 (74.3105) lr 1.8443e-03 eta 0:16:05
epoch [11/50] batch [25/51] time 0.166 (0.414) data 0.000 (0.053) loss 0.8497 (1.2214) acc 82.0652 (74.4248) lr 1.8443e-03 eta 0:13:54
epoch [11/50] batch [30/51] time 0.819 (0.419) data 0.000 (0.044) loss 1.1109 (1.2402) acc 78.1250 (73.8698) lr 1.8443e-03 eta 0:14:02
epoch [11/50] batch [35/51] time 0.164 (0.398) data 0.000 (0.038) loss 1.0838 (1.2322) acc 81.2500 (73.8598) lr 1.8443e-03 eta 0:13:17
epoch [11/50] batch [40/51] time 0.165 (0.369) data 0.000 (0.033) loss 1.2014 (1.2257) acc 70.9184 (74.1138) lr 1.8443e-03 eta 0:12:17
epoch [11/50] batch [45/51] time 0.156 (0.346) data 0.000 (0.030) loss 1.2843 (1.2202) acc 73.3333 (74.2344) lr 1.8443e-03 eta 0:11:29
epoch [11/50] batch [50/51] time 0.163 (0.327) data 0.000 (0.027) loss 1.1212 (1.2259) acc 79.6875 (74.1393) lr 1.8443e-03 eta 0:10:51
>>> alpha1: 0.402  alpha2: 0.079 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.16 <<<
epoch [12/50] batch [5/51] time 0.162 (0.859) data 0.000 (0.260) loss 1.2211 (0.9632) acc 68.0851 (74.7185) lr 1.8090e-03 eta 0:28:25
epoch [12/50] batch [10/51] time 0.168 (0.513) data 0.000 (0.130) loss 1.1224 (0.9953) acc 70.6522 (75.8491) lr 1.8090e-03 eta 0:16:54
epoch [12/50] batch [15/51] time 0.160 (0.398) data 0.000 (0.087) loss 1.1385 (0.9599) acc 67.7778 (77.0403) lr 1.8090e-03 eta 0:13:05
epoch [12/50] batch [20/51] time 0.158 (0.342) data 0.000 (0.065) loss 1.1875 (0.9856) acc 70.0000 (76.6466) lr 1.8090e-03 eta 0:11:12
epoch [12/50] batch [25/51] time 0.191 (0.309) data 0.000 (0.052) loss 0.6829 (0.9615) acc 85.6481 (77.5233) lr 1.8090e-03 eta 0:10:07
epoch [12/50] batch [30/51] time 0.162 (0.286) data 0.000 (0.044) loss 1.3090 (0.9807) acc 66.4894 (76.8507) lr 1.8090e-03 eta 0:09:20
epoch [12/50] batch [35/51] time 0.166 (0.271) data 0.000 (0.037) loss 1.0772 (1.0376) acc 74.4792 (75.9273) lr 1.8090e-03 eta 0:08:48
epoch [12/50] batch [40/51] time 0.171 (0.277) data 0.000 (0.033) loss 0.7700 (1.0105) acc 75.4902 (76.5086) lr 1.8090e-03 eta 0:08:59
epoch [12/50] batch [45/51] time 0.174 (0.265) data 0.000 (0.029) loss 0.9988 (0.9954) acc 75.5000 (76.7638) lr 1.8090e-03 eta 0:08:34
epoch [12/50] batch [50/51] time 0.170 (0.255) data 0.000 (0.026) loss 0.8738 (0.9820) acc 83.3333 (77.3305) lr 1.8090e-03 eta 0:08:14
>>> alpha1: 0.347  alpha2: 0.015 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.16 <<<
epoch [13/50] batch [5/51] time 0.177 (0.438) data 0.000 (0.264) loss 0.9172 (0.8357) acc 80.1887 (80.5696) lr 1.7705e-03 eta 0:14:07
epoch [13/50] batch [10/51] time 0.161 (0.308) data 0.000 (0.132) loss 0.7761 (0.8476) acc 81.5217 (79.6282) lr 1.7705e-03 eta 0:09:54
epoch [13/50] batch [15/51] time 0.169 (0.263) data 0.000 (0.088) loss 0.9163 (0.8422) acc 81.0000 (79.8035) lr 1.7705e-03 eta 0:08:25
epoch [13/50] batch [20/51] time 0.164 (0.240) data 0.000 (0.066) loss 0.7731 (0.8191) acc 83.3333 (80.8162) lr 1.7705e-03 eta 0:07:39
epoch [13/50] batch [25/51] time 0.176 (0.226) data 0.000 (0.053) loss 0.7122 (0.8166) acc 86.9565 (80.7575) lr 1.7705e-03 eta 0:07:12
epoch [13/50] batch [30/51] time 0.212 (0.218) data 0.000 (0.044) loss 0.7848 (0.8067) acc 84.4340 (81.0597) lr 1.7705e-03 eta 0:06:56
epoch [13/50] batch [35/51] time 0.182 (0.213) data 0.000 (0.038) loss 0.6181 (0.8142) acc 88.7255 (80.7894) lr 1.7705e-03 eta 0:06:45
epoch [13/50] batch [40/51] time 0.158 (0.208) data 0.000 (0.033) loss 1.1700 (0.8331) acc 78.4091 (80.5430) lr 1.7705e-03 eta 0:06:34
epoch [13/50] batch [45/51] time 0.171 (0.204) data 0.000 (0.030) loss 0.9111 (0.8282) acc 76.4706 (80.6662) lr 1.7705e-03 eta 0:06:26
epoch [13/50] batch [50/51] time 0.179 (0.201) data 0.000 (0.027) loss 0.6255 (0.8281) acc 87.9630 (80.5614) lr 1.7705e-03 eta 0:06:19
>>> alpha1: 0.312  alpha2: -0.006 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.01 & unmatched refined noisy rate: 0.16 <<<
epoch [14/50] batch [5/51] time 0.182 (0.462) data 0.000 (0.283) loss 0.8525 (0.7731) acc 76.0417 (82.3521) lr 1.7290e-03 eta 0:14:28
epoch [14/50] batch [10/51] time 0.196 (0.318) data 0.000 (0.142) loss 0.6957 (0.7685) acc 83.8235 (81.5444) lr 1.7290e-03 eta 0:09:55
epoch [14/50] batch [15/51] time 0.158 (0.268) data 0.000 (0.094) loss 0.7765 (0.7726) acc 83.3333 (82.3101) lr 1.7290e-03 eta 0:08:22
epoch [14/50] batch [20/51] time 0.165 (0.244) data 0.000 (0.071) loss 0.7385 (0.7414) acc 84.6939 (82.8533) lr 1.7290e-03 eta 0:07:35
epoch [14/50] batch [25/51] time 0.181 (0.229) data 0.000 (0.057) loss 0.8974 (0.7509) acc 72.0000 (82.1809) lr 1.7290e-03 eta 0:07:06
epoch [14/50] batch [30/51] time 0.180 (0.219) data 0.000 (0.047) loss 0.6989 (0.7557) acc 83.8235 (81.8826) lr 1.7290e-03 eta 0:06:47
epoch [14/50] batch [35/51] time 0.168 (0.212) data 0.000 (0.041) loss 0.7750 (0.7518) acc 81.5000 (82.1994) lr 1.7290e-03 eta 0:06:33
epoch [14/50] batch [40/51] time 0.156 (0.206) data 0.000 (0.036) loss 0.8125 (0.7478) acc 81.6667 (82.4493) lr 1.7290e-03 eta 0:06:21
epoch [14/50] batch [45/51] time 0.166 (0.201) data 0.000 (0.032) loss 0.6261 (0.7362) acc 88.5000 (82.6701) lr 1.7290e-03 eta 0:06:10
epoch [14/50] batch [50/51] time 0.164 (0.198) data 0.000 (0.028) loss 0.7065 (0.7360) acc 84.6939 (82.9159) lr 1.7290e-03 eta 0:06:03
>>> alpha1: 0.289  alpha2: -0.020 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.14 <<<
epoch [15/50] batch [5/51] time 0.158 (0.492) data 0.000 (0.313) loss 0.7449 (0.6886) acc 85.5556 (84.7107) lr 1.6845e-03 eta 0:15:00
epoch [15/50] batch [10/51] time 0.169 (0.332) data 0.000 (0.157) loss 0.8970 (0.7109) acc 83.8542 (84.9171) lr 1.6845e-03 eta 0:10:06
epoch [15/50] batch [15/51] time 0.161 (0.280) data 0.000 (0.105) loss 0.6733 (0.7009) acc 84.7826 (84.7289) lr 1.6845e-03 eta 0:08:29
epoch [15/50] batch [20/51] time 0.174 (0.254) data 0.000 (0.078) loss 0.6355 (0.6972) acc 89.2157 (84.6490) lr 1.6845e-03 eta 0:07:41
epoch [15/50] batch [25/51] time 0.173 (0.238) data 0.000 (0.063) loss 0.7694 (0.6756) acc 80.0000 (85.1676) lr 1.6845e-03 eta 0:07:11
epoch [15/50] batch [30/51] time 0.175 (0.227) data 0.000 (0.052) loss 0.7672 (0.6859) acc 86.5385 (84.5454) lr 1.6845e-03 eta 0:06:49
epoch [15/50] batch [35/51] time 0.178 (0.219) data 0.000 (0.045) loss 0.5677 (0.6988) acc 87.7358 (84.3488) lr 1.6845e-03 eta 0:06:33
epoch [15/50] batch [40/51] time 0.169 (0.213) data 0.000 (0.039) loss 0.7081 (0.7072) acc 81.0000 (83.8484) lr 1.6845e-03 eta 0:06:22
epoch [15/50] batch [45/51] time 0.159 (0.207) data 0.000 (0.035) loss 0.7215 (0.6976) acc 83.1522 (84.0199) lr 1.6845e-03 eta 0:06:11
epoch [15/50] batch [50/51] time 0.162 (0.203) data 0.000 (0.032) loss 0.6997 (0.7018) acc 86.1702 (84.0990) lr 1.6845e-03 eta 0:06:02
>>> alpha1: 0.238  alpha2: -0.062 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.16 <<<
epoch [16/50] batch [5/51] time 0.181 (0.438) data 0.000 (0.251) loss 0.6485 (0.5730) acc 84.6939 (88.7012) lr 1.6374e-03 eta 0:12:59
epoch [16/50] batch [10/51] time 0.166 (0.306) data 0.000 (0.126) loss 0.6029 (0.5709) acc 88.5417 (87.8254) lr 1.6374e-03 eta 0:09:02
epoch [16/50] batch [15/51] time 0.171 (0.261) data 0.000 (0.084) loss 0.8319 (0.6491) acc 78.9216 (85.6564) lr 1.6374e-03 eta 0:07:41
epoch [16/50] batch [20/51] time 0.160 (0.239) data 0.000 (0.063) loss 0.7757 (0.6612) acc 81.5217 (85.4677) lr 1.6374e-03 eta 0:07:01
epoch [16/50] batch [25/51] time 0.164 (0.256) data 0.000 (0.050) loss 0.8464 (0.6496) acc 82.2917 (85.3766) lr 1.6374e-03 eta 0:07:29
epoch [16/50] batch [30/51] time 0.184 (0.242) data 0.001 (0.042) loss 0.8283 (0.6592) acc 78.2407 (85.0320) lr 1.6374e-03 eta 0:07:05
epoch [16/50] batch [35/51] time 0.181 (0.232) data 0.000 (0.036) loss 0.7572 (0.6715) acc 82.8704 (84.6853) lr 1.6374e-03 eta 0:06:46
epoch [16/50] batch [40/51] time 0.169 (0.224) data 0.000 (0.032) loss 1.0062 (0.6792) acc 75.5000 (84.5291) lr 1.6374e-03 eta 0:06:31
epoch [16/50] batch [45/51] time 0.165 (0.217) data 0.000 (0.028) loss 0.7780 (0.6712) acc 75.0000 (84.4452) lr 1.6374e-03 eta 0:06:18
epoch [16/50] batch [50/51] time 0.165 (0.213) data 0.000 (0.025) loss 0.4933 (0.6684) acc 87.7551 (84.5854) lr 1.6374e-03 eta 0:06:08
>>> alpha1: 0.219  alpha2: -0.066 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [17/50] batch [5/51] time 0.178 (0.413) data 0.000 (0.233) loss 0.5862 (0.6334) acc 88.2653 (84.4479) lr 1.5878e-03 eta 0:11:54
epoch [17/50] batch [10/51] time 0.188 (0.296) data 0.000 (0.117) loss 0.4617 (0.6169) acc 90.0000 (85.1401) lr 1.5878e-03 eta 0:08:30
epoch [17/50] batch [15/51] time 0.178 (0.257) data 0.000 (0.078) loss 0.8360 (0.6333) acc 78.6458 (84.9980) lr 1.5878e-03 eta 0:07:22
epoch [17/50] batch [20/51] time 0.171 (0.238) data 0.000 (0.059) loss 0.5341 (0.6142) acc 88.0000 (85.7718) lr 1.5878e-03 eta 0:06:47
epoch [17/50] batch [25/51] time 0.172 (0.225) data 0.000 (0.047) loss 0.5013 (0.6169) acc 85.2941 (85.5471) lr 1.5878e-03 eta 0:06:23
epoch [17/50] batch [30/51] time 0.165 (0.216) data 0.000 (0.039) loss 0.7547 (0.6239) acc 81.6327 (85.4997) lr 1.5878e-03 eta 0:06:08
epoch [17/50] batch [35/51] time 0.188 (0.211) data 0.000 (0.034) loss 0.6482 (0.6262) acc 85.5769 (85.5626) lr 1.5878e-03 eta 0:05:57
epoch [17/50] batch [40/51] time 0.162 (0.205) data 0.000 (0.029) loss 0.7463 (0.6383) acc 85.1064 (85.2632) lr 1.5878e-03 eta 0:05:47
epoch [17/50] batch [45/51] time 0.173 (0.202) data 0.000 (0.026) loss 0.8352 (0.6335) acc 83.6538 (85.7276) lr 1.5878e-03 eta 0:05:40
epoch [17/50] batch [50/51] time 0.165 (0.198) data 0.000 (0.024) loss 1.1377 (0.6525) acc 71.4286 (85.1975) lr 1.5878e-03 eta 0:05:33
>>> alpha1: 0.203  alpha2: -0.061 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [18/50] batch [5/51] time 0.190 (0.461) data 0.000 (0.284) loss 0.5954 (0.6363) acc 87.7451 (86.0230) lr 1.5358e-03 eta 0:12:52
epoch [18/50] batch [10/51] time 0.177 (0.319) data 0.000 (0.142) loss 0.6664 (0.6306) acc 85.0962 (85.4591) lr 1.5358e-03 eta 0:08:54
epoch [18/50] batch [15/51] time 0.169 (0.271) data 0.000 (0.095) loss 0.6851 (0.6115) acc 87.7551 (86.4641) lr 1.5358e-03 eta 0:07:31
epoch [18/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.071) loss 0.6288 (0.6227) acc 81.4815 (85.9167) lr 1.5358e-03 eta 0:06:50
epoch [18/50] batch [25/51] time 0.173 (0.231) data 0.000 (0.057) loss 0.4545 (0.6219) acc 92.7885 (86.0015) lr 1.5358e-03 eta 0:06:23
epoch [18/50] batch [30/51] time 0.177 (0.222) data 0.000 (0.047) loss 0.6911 (0.6273) acc 83.9623 (85.9230) lr 1.5358e-03 eta 0:06:07
epoch [18/50] batch [35/51] time 0.172 (0.237) data 0.000 (0.041) loss 0.7458 (0.6363) acc 79.0816 (85.2483) lr 1.5358e-03 eta 0:06:30
epoch [18/50] batch [40/51] time 0.159 (0.228) data 0.000 (0.036) loss 0.9715 (0.6445) acc 71.1956 (84.9440) lr 1.5358e-03 eta 0:06:15
epoch [18/50] batch [45/51] time 0.168 (0.222) data 0.000 (0.032) loss 0.5744 (0.6363) acc 87.5000 (85.0140) lr 1.5358e-03 eta 0:06:03
epoch [18/50] batch [50/51] time 0.161 (0.216) data 0.000 (0.029) loss 0.7105 (0.6421) acc 75.5319 (84.8891) lr 1.5358e-03 eta 0:05:53
>>> alpha1: 0.193  alpha2: -0.060 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [19/50] batch [5/51] time 0.164 (0.473) data 0.001 (0.292) loss 0.8277 (0.6527) acc 79.8913 (83.9301) lr 1.4818e-03 eta 0:12:49
epoch [19/50] batch [10/51] time 0.182 (0.327) data 0.000 (0.146) loss 0.5196 (0.6538) acc 89.0000 (84.4767) lr 1.4818e-03 eta 0:08:50
epoch [19/50] batch [15/51] time 0.168 (0.276) data 0.000 (0.097) loss 0.7309 (0.6386) acc 85.5000 (85.9126) lr 1.4818e-03 eta 0:07:26
epoch [19/50] batch [20/51] time 0.171 (0.252) data 0.000 (0.073) loss 0.6484 (0.6321) acc 85.2941 (86.2162) lr 1.4818e-03 eta 0:06:46
epoch [19/50] batch [25/51] time 0.188 (0.238) data 0.000 (0.059) loss 0.4758 (0.6270) acc 90.1042 (86.3375) lr 1.4818e-03 eta 0:06:22
epoch [19/50] batch [30/51] time 0.196 (0.230) data 0.000 (0.049) loss 0.6521 (0.6144) acc 85.0000 (86.3463) lr 1.4818e-03 eta 0:06:08
epoch [19/50] batch [35/51] time 0.169 (0.222) data 0.000 (0.042) loss 0.6948 (0.6105) acc 83.0000 (86.1660) lr 1.4818e-03 eta 0:05:54
epoch [19/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.037) loss 0.6516 (0.6127) acc 84.5000 (86.0182) lr 1.4818e-03 eta 0:05:44
epoch [19/50] batch [45/51] time 0.166 (0.212) data 0.000 (0.033) loss 0.4841 (0.6050) acc 89.7959 (86.1896) lr 1.4818e-03 eta 0:05:35
epoch [19/50] batch [50/51] time 0.159 (0.207) data 0.000 (0.029) loss 0.5348 (0.6118) acc 87.5000 (85.9413) lr 1.4818e-03 eta 0:05:26
>>> alpha1: 0.185  alpha2: -0.054 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [20/50] batch [5/51] time 0.196 (0.462) data 0.000 (0.263) loss 0.4186 (0.5828) acc 91.3636 (86.3965) lr 1.4258e-03 eta 0:12:08
epoch [20/50] batch [10/51] time 0.176 (0.319) data 0.000 (0.132) loss 0.5095 (0.5655) acc 89.2157 (86.8587) lr 1.4258e-03 eta 0:08:21
epoch [20/50] batch [15/51] time 0.185 (0.273) data 0.000 (0.088) loss 0.3519 (0.5518) acc 91.2281 (87.3766) lr 1.4258e-03 eta 0:07:06
epoch [20/50] batch [20/51] time 0.187 (0.248) data 0.000 (0.066) loss 0.7174 (0.5813) acc 80.6122 (86.3252) lr 1.4258e-03 eta 0:06:27
epoch [20/50] batch [25/51] time 0.166 (0.232) data 0.000 (0.053) loss 0.7980 (0.6144) acc 82.6531 (85.6602) lr 1.4258e-03 eta 0:06:01
epoch [20/50] batch [30/51] time 0.158 (0.223) data 0.000 (0.044) loss 0.5956 (0.6150) acc 82.2222 (85.5829) lr 1.4258e-03 eta 0:05:45
epoch [20/50] batch [35/51] time 0.179 (0.217) data 0.001 (0.038) loss 0.7169 (0.6102) acc 82.4074 (85.7192) lr 1.4258e-03 eta 0:05:35
epoch [20/50] batch [40/51] time 0.161 (0.211) data 0.000 (0.033) loss 0.6412 (0.6140) acc 81.9149 (85.6309) lr 1.4258e-03 eta 0:05:25
epoch [20/50] batch [45/51] time 0.164 (0.207) data 0.000 (0.029) loss 0.4703 (0.6128) acc 90.1042 (85.7459) lr 1.4258e-03 eta 0:05:17
epoch [20/50] batch [50/51] time 0.170 (0.203) data 0.000 (0.026) loss 0.5152 (0.6064) acc 87.2549 (85.8137) lr 1.4258e-03 eta 0:05:10
>>> alpha1: 0.176  alpha2: -0.049 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [21/50] batch [5/51] time 0.180 (0.447) data 0.000 (0.264) loss 0.4696 (0.5537) acc 90.0943 (87.1190) lr 1.3681e-03 eta 0:11:21
epoch [21/50] batch [10/51] time 0.182 (0.310) data 0.000 (0.132) loss 0.4799 (0.5691) acc 93.9815 (87.8172) lr 1.3681e-03 eta 0:07:51
epoch [21/50] batch [15/51] time 0.178 (0.264) data 0.000 (0.088) loss 0.4681 (0.5725) acc 90.5660 (87.5646) lr 1.3681e-03 eta 0:06:40
epoch [21/50] batch [20/51] time 0.178 (0.243) data 0.001 (0.066) loss 0.3531 (0.5703) acc 96.6981 (87.5849) lr 1.3681e-03 eta 0:06:06
epoch [21/50] batch [25/51] time 0.190 (0.230) data 0.000 (0.053) loss 0.6880 (0.5735) acc 85.6481 (87.4514) lr 1.3681e-03 eta 0:05:45
epoch [21/50] batch [30/51] time 0.200 (0.223) data 0.000 (0.044) loss 0.6366 (0.5593) acc 87.2642 (87.9720) lr 1.3681e-03 eta 0:05:34
epoch [21/50] batch [35/51] time 0.186 (0.237) data 0.000 (0.038) loss 0.5266 (0.5551) acc 87.5000 (88.1029) lr 1.3681e-03 eta 0:05:54
epoch [21/50] batch [40/51] time 0.169 (0.229) data 0.000 (0.033) loss 0.4884 (0.5547) acc 89.2157 (88.2015) lr 1.3681e-03 eta 0:05:41
epoch [21/50] batch [45/51] time 0.167 (0.223) data 0.000 (0.030) loss 0.6252 (0.5674) acc 86.0000 (87.8753) lr 1.3681e-03 eta 0:05:31
epoch [21/50] batch [50/51] time 0.180 (0.217) data 0.000 (0.027) loss 0.5960 (0.5704) acc 82.4074 (87.5957) lr 1.3681e-03 eta 0:05:21
>>> alpha1: 0.169  alpha2: -0.043 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [22/50] batch [5/51] time 0.184 (0.494) data 0.000 (0.310) loss 0.7570 (0.5910) acc 83.0000 (86.8194) lr 1.3090e-03 eta 0:12:07
epoch [22/50] batch [10/51] time 0.165 (0.333) data 0.000 (0.155) loss 0.6788 (0.5762) acc 83.8542 (87.1588) lr 1.3090e-03 eta 0:08:08
epoch [22/50] batch [15/51] time 0.185 (0.282) data 0.000 (0.104) loss 0.5990 (0.5621) acc 84.3137 (87.2905) lr 1.3090e-03 eta 0:06:52
epoch [22/50] batch [20/51] time 0.168 (0.254) data 0.000 (0.078) loss 0.5362 (0.5611) acc 84.6939 (87.4112) lr 1.3090e-03 eta 0:06:11
epoch [22/50] batch [25/51] time 0.183 (0.239) data 0.000 (0.062) loss 0.5174 (0.5533) acc 83.6538 (87.1957) lr 1.3090e-03 eta 0:05:47
epoch [22/50] batch [30/51] time 0.154 (0.227) data 0.000 (0.052) loss 0.5585 (0.5575) acc 86.0465 (87.0233) lr 1.3090e-03 eta 0:05:28
epoch [22/50] batch [35/51] time 0.170 (0.219) data 0.000 (0.045) loss 0.4439 (0.5632) acc 90.5000 (87.0484) lr 1.3090e-03 eta 0:05:15
epoch [22/50] batch [40/51] time 0.179 (0.213) data 0.000 (0.039) loss 0.3619 (0.5610) acc 91.3636 (87.0824) lr 1.3090e-03 eta 0:05:06
epoch [22/50] batch [45/51] time 0.159 (0.208) data 0.000 (0.035) loss 0.5002 (0.5619) acc 88.0435 (86.9473) lr 1.3090e-03 eta 0:04:58
epoch [22/50] batch [50/51] time 0.173 (0.204) data 0.001 (0.031) loss 0.4930 (0.5591) acc 87.9808 (87.2220) lr 1.3090e-03 eta 0:04:51
>>> alpha1: 0.166  alpha2: -0.040 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [23/50] batch [5/51] time 0.178 (0.486) data 0.000 (0.302) loss 0.5372 (0.5583) acc 83.9623 (87.2470) lr 1.2487e-03 eta 0:11:32
epoch [23/50] batch [10/51] time 0.184 (0.332) data 0.000 (0.151) loss 0.4676 (0.5559) acc 88.2075 (85.8530) lr 1.2487e-03 eta 0:07:50
epoch [23/50] batch [15/51] time 0.160 (0.278) data 0.000 (0.101) loss 0.5361 (0.6711) acc 86.9565 (85.2809) lr 1.2487e-03 eta 0:06:33
epoch [23/50] batch [20/51] time 0.177 (0.254) data 0.000 (0.076) loss 0.5308 (0.6312) acc 85.3774 (85.5963) lr 1.2487e-03 eta 0:05:57
epoch [23/50] batch [25/51] time 0.178 (0.239) data 0.000 (0.061) loss 0.4227 (0.5982) acc 89.7059 (86.4793) lr 1.2487e-03 eta 0:05:35
epoch [23/50] batch [30/51] time 0.179 (0.228) data 0.000 (0.051) loss 0.7334 (0.5912) acc 84.5745 (86.6143) lr 1.2487e-03 eta 0:05:19
epoch [23/50] batch [35/51] time 0.166 (0.220) data 0.000 (0.044) loss 0.5587 (0.5977) acc 88.2979 (86.3905) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [40/51] time 0.168 (0.214) data 0.000 (0.038) loss 0.4959 (0.5858) acc 86.5000 (86.5348) lr 1.2487e-03 eta 0:04:57
epoch [23/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.034) loss 0.4562 (0.5815) acc 91.0000 (86.7420) lr 1.2487e-03 eta 0:04:49
epoch [23/50] batch [50/51] time 0.161 (0.205) data 0.000 (0.031) loss 0.4573 (0.5765) acc 90.7609 (86.8942) lr 1.2487e-03 eta 0:04:42
>>> alpha1: 0.162  alpha2: -0.043 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [24/50] batch [5/51] time 0.180 (0.447) data 0.001 (0.264) loss 0.3134 (0.5103) acc 93.8679 (89.9546) lr 1.1874e-03 eta 0:10:13
epoch [24/50] batch [10/51] time 0.171 (0.311) data 0.000 (0.132) loss 0.4803 (0.5140) acc 90.6863 (89.2755) lr 1.1874e-03 eta 0:07:04
epoch [24/50] batch [15/51] time 0.168 (0.263) data 0.000 (0.088) loss 0.5194 (0.5150) acc 86.2245 (88.5857) lr 1.1874e-03 eta 0:05:57
epoch [24/50] batch [20/51] time 0.169 (0.242) data 0.000 (0.066) loss 0.4232 (0.5089) acc 89.2857 (88.6632) lr 1.1874e-03 eta 0:05:27
epoch [24/50] batch [25/51] time 0.171 (0.228) data 0.000 (0.053) loss 0.3293 (0.4969) acc 93.6274 (88.8846) lr 1.1874e-03 eta 0:05:08
epoch [24/50] batch [30/51] time 0.174 (0.219) data 0.000 (0.044) loss 0.4422 (0.4954) acc 89.8936 (88.9812) lr 1.1874e-03 eta 0:04:54
epoch [24/50] batch [35/51] time 0.163 (0.213) data 0.000 (0.038) loss 0.6204 (0.5050) acc 87.7660 (88.8181) lr 1.1874e-03 eta 0:04:45
epoch [24/50] batch [40/51] time 0.169 (0.208) data 0.000 (0.033) loss 0.5452 (0.5167) acc 87.7451 (88.4974) lr 1.1874e-03 eta 0:04:37
epoch [24/50] batch [45/51] time 0.167 (0.203) data 0.000 (0.030) loss 0.6509 (0.5263) acc 87.2449 (88.1786) lr 1.1874e-03 eta 0:04:30
epoch [24/50] batch [50/51] time 0.171 (0.200) data 0.000 (0.027) loss 0.5982 (0.5312) acc 87.2549 (87.9538) lr 1.1874e-03 eta 0:04:25
>>> alpha1: 0.156  alpha2: -0.042 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [25/50] batch [5/51] time 0.169 (0.466) data 0.000 (0.296) loss 0.6832 (0.5380) acc 86.0000 (89.3362) lr 1.1253e-03 eta 0:10:15
epoch [25/50] batch [10/51] time 0.173 (0.322) data 0.000 (0.148) loss 0.4873 (0.5148) acc 85.5769 (89.2300) lr 1.1253e-03 eta 0:07:03
epoch [25/50] batch [15/51] time 0.189 (0.272) data 0.000 (0.099) loss 0.5399 (0.5326) acc 89.5000 (88.8017) lr 1.1253e-03 eta 0:05:56
epoch [25/50] batch [20/51] time 0.173 (0.247) data 0.000 (0.074) loss 0.3927 (0.5221) acc 93.3673 (88.9799) lr 1.1253e-03 eta 0:05:22
epoch [25/50] batch [25/51] time 0.176 (0.234) data 0.000 (0.059) loss 0.6578 (0.5256) acc 85.1064 (89.0456) lr 1.1253e-03 eta 0:05:04
epoch [25/50] batch [30/51] time 0.156 (0.224) data 0.000 (0.050) loss 0.7816 (0.5366) acc 79.5455 (88.8491) lr 1.1253e-03 eta 0:04:50
epoch [25/50] batch [35/51] time 0.162 (0.217) data 0.000 (0.043) loss 0.4537 (0.5358) acc 94.1489 (88.7934) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [40/51] time 0.164 (0.211) data 0.000 (0.038) loss 0.4787 (0.5346) acc 87.5000 (88.5065) lr 1.1253e-03 eta 0:04:31
epoch [25/50] batch [45/51] time 0.184 (0.207) data 0.000 (0.033) loss 0.4563 (0.5416) acc 87.5000 (88.2866) lr 1.1253e-03 eta 0:04:25
epoch [25/50] batch [50/51] time 0.165 (0.203) data 0.000 (0.030) loss 0.4214 (0.5345) acc 91.6667 (88.4196) lr 1.1253e-03 eta 0:04:19
>>> alpha1: 0.155  alpha2: -0.040 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [26/50] batch [5/51] time 0.169 (0.439) data 0.000 (0.263) loss 0.6974 (0.5246) acc 83.5000 (89.3570) lr 1.0628e-03 eta 0:09:17
epoch [26/50] batch [10/51] time 0.176 (0.307) data 0.000 (0.132) loss 0.4714 (0.5545) acc 90.3846 (88.4284) lr 1.0628e-03 eta 0:06:28
epoch [26/50] batch [15/51] time 0.173 (0.264) data 0.001 (0.088) loss 0.3927 (0.5458) acc 90.1961 (87.7345) lr 1.0628e-03 eta 0:05:32
epoch [26/50] batch [20/51] time 0.178 (0.242) data 0.000 (0.066) loss 0.5687 (0.5294) acc 88.5000 (88.3796) lr 1.0628e-03 eta 0:05:03
epoch [26/50] batch [25/51] time 0.166 (0.228) data 0.001 (0.053) loss 0.3813 (0.5285) acc 88.0208 (88.2834) lr 1.0628e-03 eta 0:04:45
epoch [26/50] batch [30/51] time 0.184 (0.219) data 0.000 (0.044) loss 0.4835 (0.5264) acc 87.0192 (88.2120) lr 1.0628e-03 eta 0:04:32
epoch [26/50] batch [35/51] time 0.178 (0.213) data 0.000 (0.038) loss 0.5983 (0.5224) acc 81.1321 (88.1991) lr 1.0628e-03 eta 0:04:23
epoch [26/50] batch [40/51] time 0.169 (0.207) data 0.000 (0.033) loss 0.5354 (0.5321) acc 87.7551 (87.9072) lr 1.0628e-03 eta 0:04:15
epoch [26/50] batch [45/51] time 0.166 (0.203) data 0.000 (0.029) loss 0.4688 (0.5206) acc 87.7551 (88.1815) lr 1.0628e-03 eta 0:04:09
epoch [26/50] batch [50/51] time 0.177 (0.200) data 0.000 (0.027) loss 0.3743 (0.5156) acc 89.1509 (88.2522) lr 1.0628e-03 eta 0:04:05
>>> alpha1: 0.152  alpha2: -0.034 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [27/50] batch [5/51] time 0.171 (0.531) data 0.001 (0.358) loss 0.3623 (0.8738) acc 91.0000 (84.2796) lr 1.0000e-03 eta 0:10:47
epoch [27/50] batch [10/51] time 0.178 (0.354) data 0.000 (0.179) loss 0.2836 (0.6595) acc 93.2692 (87.3361) lr 1.0000e-03 eta 0:07:09
epoch [27/50] batch [15/51] time 0.212 (0.296) data 0.000 (0.120) loss 0.6435 (0.6338) acc 82.3529 (87.0022) lr 1.0000e-03 eta 0:05:57
epoch [27/50] batch [20/51] time 0.168 (0.268) data 0.001 (0.090) loss 0.4111 (0.5941) acc 91.3265 (87.6871) lr 1.0000e-03 eta 0:05:22
epoch [27/50] batch [25/51] time 0.187 (0.250) data 0.000 (0.072) loss 0.3781 (0.5889) acc 92.4107 (87.8410) lr 1.0000e-03 eta 0:04:59
epoch [27/50] batch [30/51] time 0.169 (0.238) data 0.000 (0.060) loss 0.5292 (0.5798) acc 89.0625 (87.9992) lr 1.0000e-03 eta 0:04:44
epoch [27/50] batch [35/51] time 0.186 (0.231) data 0.001 (0.052) loss 0.5595 (0.5656) acc 87.5000 (87.9777) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [40/51] time 0.168 (0.224) data 0.000 (0.045) loss 0.3586 (0.5532) acc 95.9184 (88.1082) lr 1.0000e-03 eta 0:04:24
epoch [27/50] batch [45/51] time 0.168 (0.217) data 0.000 (0.040) loss 0.5503 (0.5464) acc 88.7755 (88.2727) lr 1.0000e-03 eta 0:04:16
epoch [27/50] batch [50/51] time 0.180 (0.213) data 0.000 (0.036) loss 0.6050 (0.5499) acc 88.8889 (88.2023) lr 1.0000e-03 eta 0:04:10
>>> alpha1: 0.148  alpha2: -0.035 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [28/50] batch [5/51] time 0.167 (0.440) data 0.000 (0.269) loss 0.5742 (0.5091) acc 87.5000 (88.3977) lr 9.3721e-04 eta 0:08:34
epoch [28/50] batch [10/51] time 0.171 (0.308) data 0.000 (0.135) loss 0.4181 (0.4924) acc 89.7059 (88.5337) lr 9.3721e-04 eta 0:05:58
epoch [28/50] batch [15/51] time 0.172 (0.264) data 0.000 (0.090) loss 0.2246 (0.4720) acc 97.5490 (89.1519) lr 9.3721e-04 eta 0:05:05
epoch [28/50] batch [20/51] time 0.171 (0.242) data 0.000 (0.068) loss 0.2660 (0.4501) acc 93.5000 (89.7417) lr 9.3721e-04 eta 0:04:39
epoch [28/50] batch [25/51] time 0.169 (0.228) data 0.000 (0.054) loss 0.3857 (0.4609) acc 91.5000 (89.6345) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [30/51] time 0.177 (0.219) data 0.000 (0.045) loss 0.4512 (0.4675) acc 89.6226 (89.3159) lr 9.3721e-04 eta 0:04:10
epoch [28/50] batch [35/51] time 0.191 (0.213) data 0.000 (0.039) loss 0.6127 (0.4658) acc 88.8298 (89.4615) lr 9.3721e-04 eta 0:04:02
epoch [28/50] batch [40/51] time 0.175 (0.208) data 0.000 (0.034) loss 0.3672 (0.4714) acc 88.2075 (89.1319) lr 9.3721e-04 eta 0:03:56
epoch [28/50] batch [45/51] time 0.175 (0.204) data 0.000 (0.030) loss 0.4581 (0.4757) acc 89.4231 (88.8780) lr 9.3721e-04 eta 0:03:50
epoch [28/50] batch [50/51] time 0.175 (0.201) data 0.000 (0.027) loss 0.4269 (0.4722) acc 90.5660 (89.0188) lr 9.3721e-04 eta 0:03:45
>>> alpha1: 0.146  alpha2: -0.034 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [29/50] batch [5/51] time 0.211 (0.466) data 0.000 (0.275) loss 0.2913 (0.4347) acc 94.1964 (89.7709) lr 8.7467e-04 eta 0:08:40
epoch [29/50] batch [10/51] time 0.178 (0.327) data 0.001 (0.138) loss 0.6830 (0.4664) acc 86.2245 (89.3189) lr 8.7467e-04 eta 0:06:03
epoch [29/50] batch [15/51] time 0.173 (0.276) data 0.000 (0.092) loss 0.4859 (0.4874) acc 88.2353 (88.2487) lr 8.7467e-04 eta 0:05:05
epoch [29/50] batch [20/51] time 0.170 (0.251) data 0.000 (0.069) loss 0.3531 (0.4806) acc 94.0000 (88.4853) lr 8.7467e-04 eta 0:04:36
epoch [29/50] batch [25/51] time 0.171 (0.237) data 0.000 (0.055) loss 0.5951 (0.4852) acc 87.7451 (88.4637) lr 8.7467e-04 eta 0:04:19
epoch [29/50] batch [30/51] time 0.178 (0.226) data 0.001 (0.046) loss 0.4252 (0.5013) acc 91.8269 (88.2929) lr 8.7467e-04 eta 0:04:07
epoch [29/50] batch [35/51] time 0.188 (0.220) data 0.000 (0.040) loss 0.5443 (0.4924) acc 89.5000 (88.6824) lr 8.7467e-04 eta 0:03:58
epoch [29/50] batch [40/51] time 0.168 (0.214) data 0.000 (0.035) loss 0.6166 (0.5004) acc 84.5000 (88.2783) lr 8.7467e-04 eta 0:03:51
epoch [29/50] batch [45/51] time 0.166 (0.209) data 0.000 (0.031) loss 0.4334 (0.4983) acc 94.8980 (88.2752) lr 8.7467e-04 eta 0:03:45
epoch [29/50] batch [50/51] time 0.196 (0.206) data 0.000 (0.028) loss 0.3305 (0.4932) acc 93.8596 (88.5509) lr 8.7467e-04 eta 0:03:41
>>> alpha1: 0.145  alpha2: -0.034 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [30/50] batch [5/51] time 0.177 (0.450) data 0.001 (0.265) loss 0.4258 (0.4521) acc 90.8654 (89.2521) lr 8.1262e-04 eta 0:07:59
epoch [30/50] batch [10/51] time 0.176 (0.316) data 0.000 (0.132) loss 0.3158 (0.4642) acc 94.0000 (89.8313) lr 8.1262e-04 eta 0:05:35
epoch [30/50] batch [15/51] time 0.174 (0.275) data 0.000 (0.089) loss 0.4517 (0.4564) acc 88.9423 (90.0737) lr 8.1262e-04 eta 0:04:49
epoch [30/50] batch [20/51] time 0.193 (0.250) data 0.000 (0.067) loss 0.4946 (0.4508) acc 92.9245 (90.3618) lr 8.1262e-04 eta 0:04:22
epoch [30/50] batch [25/51] time 0.181 (0.235) data 0.000 (0.054) loss 0.5246 (0.4466) acc 86.3636 (90.3812) lr 8.1262e-04 eta 0:04:05
epoch [30/50] batch [30/51] time 0.175 (0.225) data 0.000 (0.045) loss 0.4323 (0.4524) acc 91.6667 (90.4623) lr 8.1262e-04 eta 0:03:53
epoch [30/50] batch [35/51] time 0.180 (0.218) data 0.000 (0.038) loss 0.5836 (0.4720) acc 88.8889 (90.2060) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [40/51] time 0.165 (0.212) data 0.000 (0.034) loss 0.4696 (0.4686) acc 88.2653 (90.1246) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [45/51] time 0.170 (0.208) data 0.000 (0.030) loss 0.4006 (0.4738) acc 94.6078 (90.0082) lr 8.1262e-04 eta 0:03:33
epoch [30/50] batch [50/51] time 0.172 (0.204) data 0.000 (0.027) loss 0.4263 (0.4724) acc 89.9038 (90.0752) lr 8.1262e-04 eta 0:03:28
>>> alpha1: 0.143  alpha2: -0.035 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [31/50] batch [5/51] time 0.179 (0.498) data 0.001 (0.312) loss 0.4399 (0.4641) acc 92.4528 (92.0433) lr 7.5131e-04 eta 0:08:25
epoch [31/50] batch [10/51] time 0.212 (0.340) data 0.000 (0.156) loss 0.2753 (0.4362) acc 94.0909 (92.0513) lr 7.5131e-04 eta 0:05:43
epoch [31/50] batch [15/51] time 0.175 (0.284) data 0.000 (0.104) loss 0.6751 (0.4583) acc 89.6739 (91.1076) lr 7.5131e-04 eta 0:04:45
epoch [31/50] batch [20/51] time 0.187 (0.257) data 0.000 (0.078) loss 0.4570 (0.4717) acc 90.1786 (90.5673) lr 7.5131e-04 eta 0:04:16
epoch [31/50] batch [25/51] time 0.172 (0.241) data 0.000 (0.063) loss 0.5641 (0.4691) acc 86.1702 (90.1531) lr 7.5131e-04 eta 0:04:00
epoch [31/50] batch [30/51] time 0.161 (0.231) data 0.000 (0.052) loss 0.3493 (0.4728) acc 94.0217 (89.9493) lr 7.5131e-04 eta 0:03:48
epoch [31/50] batch [35/51] time 0.171 (0.223) data 0.000 (0.045) loss 0.5337 (0.4755) acc 90.6863 (89.6581) lr 7.5131e-04 eta 0:03:39
epoch [31/50] batch [40/51] time 0.169 (0.217) data 0.000 (0.039) loss 0.5690 (0.4729) acc 87.0000 (89.6964) lr 7.5131e-04 eta 0:03:33
epoch [31/50] batch [45/51] time 0.184 (0.213) data 0.000 (0.035) loss 0.3430 (0.4706) acc 92.7273 (89.5451) lr 7.5131e-04 eta 0:03:27
epoch [31/50] batch [50/51] time 0.181 (0.209) data 0.000 (0.031) loss 0.4381 (0.4726) acc 90.0000 (89.3935) lr 7.5131e-04 eta 0:03:22
>>> alpha1: 0.143  alpha2: -0.033 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [32/50] batch [5/51] time 0.184 (0.482) data 0.001 (0.298) loss 0.5044 (0.4542) acc 87.7551 (90.7714) lr 6.9098e-04 eta 0:07:44
epoch [32/50] batch [10/51] time 0.183 (0.331) data 0.000 (0.149) loss 0.3724 (0.4181) acc 91.8269 (91.3173) lr 6.9098e-04 eta 0:05:17
epoch [32/50] batch [15/51] time 0.180 (0.279) data 0.000 (0.100) loss 0.4128 (0.4051) acc 92.1296 (91.7316) lr 6.9098e-04 eta 0:04:26
epoch [32/50] batch [20/51] time 0.217 (0.256) data 0.018 (0.076) loss 0.5924 (0.4300) acc 85.0000 (90.4122) lr 6.9098e-04 eta 0:04:03
epoch [32/50] batch [25/51] time 0.196 (0.242) data 0.000 (0.061) loss 0.3637 (0.4393) acc 95.0000 (90.4162) lr 6.9098e-04 eta 0:03:48
epoch [32/50] batch [30/51] time 0.171 (0.232) data 0.000 (0.051) loss 0.5877 (0.4523) acc 85.0000 (90.0666) lr 6.9098e-04 eta 0:03:37
epoch [32/50] batch [35/51] time 0.159 (0.224) data 0.000 (0.043) loss 0.5598 (0.4502) acc 89.6739 (90.1734) lr 6.9098e-04 eta 0:03:28
epoch [32/50] batch [40/51] time 0.165 (0.218) data 0.000 (0.038) loss 0.4843 (0.4516) acc 92.7083 (90.1794) lr 6.9098e-04 eta 0:03:22
epoch [32/50] batch [45/51] time 0.173 (0.213) data 0.000 (0.034) loss 0.5699 (0.4567) acc 86.5385 (89.8607) lr 6.9098e-04 eta 0:03:16
epoch [32/50] batch [50/51] time 0.176 (0.208) data 0.000 (0.030) loss 0.3737 (0.4515) acc 91.0377 (89.9691) lr 6.9098e-04 eta 0:03:11
>>> alpha1: 0.142  alpha2: -0.032 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [33/50] batch [5/51] time 0.183 (0.470) data 0.000 (0.283) loss 0.7542 (0.5057) acc 84.0000 (89.0514) lr 6.3188e-04 eta 0:07:09
epoch [33/50] batch [10/51] time 0.188 (0.326) data 0.000 (0.142) loss 0.4794 (0.4652) acc 89.7059 (89.3030) lr 6.3188e-04 eta 0:04:56
epoch [33/50] batch [15/51] time 0.173 (0.277) data 0.000 (0.095) loss 0.5032 (0.4676) acc 89.2857 (89.8551) lr 6.3188e-04 eta 0:04:10
epoch [33/50] batch [20/51] time 0.201 (0.253) data 0.000 (0.071) loss 0.2736 (0.4598) acc 91.1765 (89.7530) lr 6.3188e-04 eta 0:03:47
epoch [33/50] batch [25/51] time 0.168 (0.238) data 0.001 (0.057) loss 0.4351 (0.4414) acc 89.2857 (90.1258) lr 6.3188e-04 eta 0:03:32
epoch [33/50] batch [30/51] time 0.174 (0.228) data 0.000 (0.047) loss 0.6582 (0.4496) acc 88.0000 (89.9622) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [35/51] time 0.174 (0.221) data 0.000 (0.041) loss 0.6251 (0.4582) acc 85.7843 (89.8436) lr 6.3188e-04 eta 0:03:15
epoch [33/50] batch [40/51] time 0.157 (0.216) data 0.000 (0.036) loss 0.5056 (0.4512) acc 87.2222 (89.9247) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.032) loss 0.4538 (0.4559) acc 89.7959 (89.7004) lr 6.3188e-04 eta 0:03:03
epoch [33/50] batch [50/51] time 0.900 (0.221) data 0.000 (0.029) loss 0.4458 (0.4583) acc 91.1017 (89.7244) lr 6.3188e-04 eta 0:03:11
>>> alpha1: 0.140  alpha2: -0.027 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [34/50] batch [5/51] time 0.193 (0.444) data 0.000 (0.262) loss 0.6334 (0.4773) acc 84.3023 (89.3286) lr 5.7422e-04 eta 0:06:22
epoch [34/50] batch [10/51] time 0.167 (0.311) data 0.000 (0.131) loss 0.5549 (0.4590) acc 88.7755 (89.9476) lr 5.7422e-04 eta 0:04:26
epoch [34/50] batch [15/51] time 0.176 (0.267) data 0.000 (0.088) loss 0.3456 (0.4316) acc 95.3125 (90.4810) lr 5.7422e-04 eta 0:03:47
epoch [34/50] batch [20/51] time 0.171 (0.245) data 0.000 (0.066) loss 0.4072 (0.4439) acc 90.1961 (90.2052) lr 5.7422e-04 eta 0:03:27
epoch [34/50] batch [25/51] time 0.164 (0.232) data 0.000 (0.053) loss 0.6716 (0.4497) acc 85.6383 (89.9938) lr 5.7422e-04 eta 0:03:14
epoch [34/50] batch [30/51] time 0.173 (0.222) data 0.000 (0.044) loss 0.3541 (0.4429) acc 91.6667 (90.0510) lr 5.7422e-04 eta 0:03:05
epoch [34/50] batch [35/51] time 0.167 (0.217) data 0.001 (0.038) loss 0.6931 (0.4389) acc 85.2041 (90.1757) lr 5.7422e-04 eta 0:03:00
epoch [34/50] batch [40/51] time 0.169 (0.212) data 0.000 (0.033) loss 0.3899 (0.4424) acc 90.5000 (90.0357) lr 5.7422e-04 eta 0:02:55
epoch [34/50] batch [45/51] time 0.183 (0.207) data 0.000 (0.030) loss 0.2806 (0.4455) acc 93.3036 (89.9707) lr 5.7422e-04 eta 0:02:49
epoch [34/50] batch [50/51] time 0.164 (0.203) data 0.000 (0.027) loss 0.3024 (0.4464) acc 94.2708 (90.0526) lr 5.7422e-04 eta 0:02:45
>>> alpha1: 0.137  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [35/50] batch [5/51] time 0.185 (0.484) data 0.000 (0.298) loss 0.3050 (0.3780) acc 92.4528 (92.9200) lr 5.1825e-04 eta 0:06:32
epoch [35/50] batch [10/51] time 0.202 (0.332) data 0.000 (0.149) loss 0.3586 (0.3949) acc 93.9815 (92.3118) lr 5.1825e-04 eta 0:04:27
epoch [35/50] batch [15/51] time 0.197 (0.281) data 0.000 (0.100) loss 0.5591 (0.4216) acc 88.6364 (91.5544) lr 5.1825e-04 eta 0:03:44
epoch [35/50] batch [20/51] time 0.188 (0.254) data 0.000 (0.075) loss 0.5035 (0.4164) acc 87.7551 (91.5584) lr 5.1825e-04 eta 0:03:21
epoch [35/50] batch [25/51] time 0.179 (0.240) data 0.001 (0.060) loss 0.3755 (0.5158) acc 92.6471 (90.0031) lr 5.1825e-04 eta 0:03:09
epoch [35/50] batch [30/51] time 0.209 (0.231) data 0.000 (0.050) loss 0.5856 (0.5775) acc 86.0577 (89.1771) lr 5.1825e-04 eta 0:03:01
epoch [35/50] batch [35/51] time 0.192 (0.224) data 0.000 (0.043) loss 0.5589 (0.5634) acc 82.0755 (89.0522) lr 5.1825e-04 eta 0:02:54
epoch [35/50] batch [40/51] time 0.169 (0.218) data 0.000 (0.038) loss 0.4607 (0.5469) acc 89.0000 (89.1090) lr 5.1825e-04 eta 0:02:49
epoch [35/50] batch [45/51] time 0.171 (0.213) data 0.000 (0.033) loss 0.2626 (0.5393) acc 97.5490 (89.2863) lr 5.1825e-04 eta 0:02:43
epoch [35/50] batch [50/51] time 0.170 (0.208) data 0.000 (0.030) loss 0.4996 (0.5291) acc 88.7755 (89.3531) lr 5.1825e-04 eta 0:02:39
>>> alpha1: 0.136  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [36/50] batch [5/51] time 0.175 (0.456) data 0.000 (0.261) loss 0.6122 (0.4463) acc 87.5000 (89.9563) lr 4.6417e-04 eta 0:05:46
epoch [36/50] batch [10/51] time 0.162 (0.316) data 0.000 (0.131) loss 0.3320 (0.4180) acc 93.4783 (91.0680) lr 4.6417e-04 eta 0:03:58
epoch [36/50] batch [15/51] time 0.176 (0.267) data 0.000 (0.087) loss 0.3139 (0.4126) acc 95.1923 (91.1575) lr 4.6417e-04 eta 0:03:20
epoch [36/50] batch [20/51] time 0.170 (0.245) data 0.000 (0.066) loss 0.5547 (0.4029) acc 87.0000 (91.1167) lr 4.6417e-04 eta 0:03:02
epoch [36/50] batch [25/51] time 0.172 (0.232) data 0.000 (0.053) loss 0.2615 (0.4026) acc 95.2128 (91.3070) lr 4.6417e-04 eta 0:02:51
epoch [36/50] batch [30/51] time 0.168 (0.223) data 0.000 (0.044) loss 0.5209 (0.4102) acc 89.0000 (91.2524) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [35/51] time 0.178 (0.215) data 0.000 (0.038) loss 0.7029 (0.4244) acc 84.8039 (90.8692) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [40/51] time 0.169 (0.210) data 0.000 (0.033) loss 0.5054 (0.4280) acc 88.0000 (90.7503) lr 4.6417e-04 eta 0:02:31
epoch [36/50] batch [45/51] time 0.165 (0.205) data 0.000 (0.029) loss 0.5849 (0.4417) acc 87.5000 (90.2889) lr 4.6417e-04 eta 0:02:27
epoch [36/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.026) loss 0.4694 (0.4403) acc 90.5000 (90.3649) lr 4.6417e-04 eta 0:02:23
>>> alpha1: 0.136  alpha2: -0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [37/50] batch [5/51] time 0.166 (0.463) data 0.000 (0.289) loss 0.4217 (0.4377) acc 91.1458 (90.0252) lr 4.1221e-04 eta 0:05:28
epoch [37/50] batch [10/51] time 0.180 (0.322) data 0.000 (0.144) loss 0.3858 (0.4225) acc 88.7255 (90.3358) lr 4.1221e-04 eta 0:03:46
epoch [37/50] batch [15/51] time 0.157 (0.271) data 0.000 (0.096) loss 0.5338 (0.4166) acc 86.9318 (90.7642) lr 4.1221e-04 eta 0:03:09
epoch [37/50] batch [20/51] time 0.187 (0.248) data 0.000 (0.072) loss 0.3765 (0.4047) acc 91.8182 (91.0895) lr 4.1221e-04 eta 0:02:52
epoch [37/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.058) loss 0.5497 (0.4041) acc 87.5000 (91.1341) lr 4.1221e-04 eta 0:02:40
epoch [37/50] batch [30/51] time 0.178 (0.223) data 0.000 (0.048) loss 0.3737 (0.4124) acc 90.5660 (90.6586) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [35/51] time 0.183 (0.217) data 0.001 (0.041) loss 0.2464 (0.4168) acc 96.3542 (90.6050) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [40/51] time 0.165 (0.213) data 0.000 (0.036) loss 0.5222 (0.4248) acc 90.1163 (90.5375) lr 4.1221e-04 eta 0:02:23
epoch [37/50] batch [45/51] time 0.183 (0.209) data 0.000 (0.032) loss 0.3363 (0.4333) acc 96.2264 (90.3673) lr 4.1221e-04 eta 0:02:19
epoch [37/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.029) loss 0.3216 (0.4283) acc 91.8478 (90.4519) lr 4.1221e-04 eta 0:02:15
>>> alpha1: 0.134  alpha2: -0.023 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [38/50] batch [5/51] time 0.181 (0.501) data 0.000 (0.317) loss 0.2405 (0.3968) acc 95.3704 (91.6473) lr 3.6258e-04 eta 0:05:29
epoch [38/50] batch [10/51] time 0.173 (0.341) data 0.000 (0.159) loss 0.3300 (0.3959) acc 93.1373 (90.9329) lr 3.6258e-04 eta 0:03:42
epoch [38/50] batch [15/51] time 0.178 (0.287) data 0.000 (0.106) loss 0.5210 (0.4168) acc 88.6792 (90.2503) lr 3.6258e-04 eta 0:03:06
epoch [38/50] batch [20/51] time 0.177 (0.261) data 0.000 (0.080) loss 0.5345 (0.4315) acc 89.5000 (90.2249) lr 3.6258e-04 eta 0:02:48
epoch [38/50] batch [25/51] time 0.174 (0.245) data 0.000 (0.064) loss 0.4265 (0.4272) acc 89.9038 (90.6691) lr 3.6258e-04 eta 0:02:36
epoch [38/50] batch [30/51] time 0.195 (0.235) data 0.000 (0.053) loss 0.3464 (0.4197) acc 93.8679 (90.8536) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [35/51] time 0.173 (0.225) data 0.000 (0.046) loss 0.3533 (0.4202) acc 90.8654 (90.6383) lr 3.6258e-04 eta 0:02:21
epoch [38/50] batch [40/51] time 0.151 (0.218) data 0.000 (0.040) loss 0.4794 (0.4198) acc 91.0714 (90.8349) lr 3.6258e-04 eta 0:02:15
epoch [38/50] batch [45/51] time 0.161 (0.213) data 0.000 (0.036) loss 0.3369 (0.4170) acc 94.1489 (90.9402) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [50/51] time 0.170 (0.208) data 0.000 (0.032) loss 0.3279 (0.4132) acc 91.1765 (90.9515) lr 3.6258e-04 eta 0:02:07
>>> alpha1: 0.133  alpha2: -0.027 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [39/50] batch [5/51] time 0.172 (0.488) data 0.000 (0.304) loss 0.3893 (0.4029) acc 88.5417 (90.3008) lr 3.1545e-04 eta 0:04:56
epoch [39/50] batch [10/51] time 0.170 (0.334) data 0.001 (0.152) loss 0.4128 (0.3916) acc 88.0000 (90.7315) lr 3.1545e-04 eta 0:03:20
epoch [39/50] batch [15/51] time 0.183 (0.283) data 0.000 (0.102) loss 0.3230 (0.4165) acc 93.7500 (90.5755) lr 3.1545e-04 eta 0:02:49
epoch [39/50] batch [20/51] time 0.167 (0.257) data 0.000 (0.076) loss 0.5368 (0.4218) acc 87.7660 (90.5805) lr 3.1545e-04 eta 0:02:32
epoch [39/50] batch [25/51] time 0.169 (0.242) data 0.000 (0.061) loss 0.2880 (0.4122) acc 92.8571 (91.0614) lr 3.1545e-04 eta 0:02:22
epoch [39/50] batch [30/51] time 0.180 (0.230) data 0.000 (0.051) loss 0.4166 (0.4130) acc 88.9423 (90.9898) lr 3.1545e-04 eta 0:02:13
epoch [39/50] batch [35/51] time 0.168 (0.222) data 0.000 (0.044) loss 0.4407 (0.4151) acc 90.3061 (91.0265) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [40/51] time 0.185 (0.218) data 0.000 (0.038) loss 0.3148 (0.4111) acc 92.9825 (91.1776) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [45/51] time 0.166 (0.213) data 0.000 (0.034) loss 0.4785 (0.4155) acc 85.4167 (90.9074) lr 3.1545e-04 eta 0:02:00
epoch [39/50] batch [50/51] time 0.165 (0.209) data 0.000 (0.031) loss 0.5247 (0.4233) acc 89.5833 (90.8138) lr 3.1545e-04 eta 0:01:57
>>> alpha1: 0.132  alpha2: -0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [40/50] batch [5/51] time 0.171 (0.440) data 0.000 (0.266) loss 0.4761 (0.4590) acc 89.2857 (88.3832) lr 2.7103e-04 eta 0:04:04
epoch [40/50] batch [10/51] time 0.183 (0.307) data 0.000 (0.133) loss 0.4101 (0.4638) acc 91.1458 (88.5954) lr 2.7103e-04 eta 0:02:49
epoch [40/50] batch [15/51] time 0.187 (0.265) data 0.000 (0.089) loss 0.3592 (0.4124) acc 91.8367 (90.4925) lr 2.7103e-04 eta 0:02:24
epoch [40/50] batch [20/51] time 0.169 (0.243) data 0.001 (0.067) loss 0.5024 (0.4165) acc 90.8163 (90.6938) lr 2.7103e-04 eta 0:02:11
epoch [40/50] batch [25/51] time 0.177 (0.231) data 0.001 (0.053) loss 0.3683 (0.4195) acc 91.1458 (90.5971) lr 2.7103e-04 eta 0:02:03
epoch [40/50] batch [30/51] time 0.186 (0.223) data 0.000 (0.045) loss 0.2710 (0.4132) acc 96.5686 (90.8132) lr 2.7103e-04 eta 0:01:58
epoch [40/50] batch [35/51] time 0.186 (0.218) data 0.016 (0.039) loss 0.4525 (0.4107) acc 90.7609 (90.7521) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [40/51] time 0.168 (0.214) data 0.000 (0.034) loss 0.3902 (0.4107) acc 90.0000 (90.7448) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [45/51] time 0.179 (0.209) data 0.001 (0.030) loss 0.2514 (0.4031) acc 95.8333 (91.0213) lr 2.7103e-04 eta 0:01:47
epoch [40/50] batch [50/51] time 0.181 (0.205) data 0.001 (0.027) loss 0.3695 (0.4143) acc 92.2727 (90.6949) lr 2.7103e-04 eta 0:01:44
>>> alpha1: 0.131  alpha2: -0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [41/50] batch [5/51] time 0.166 (0.504) data 0.000 (0.315) loss 0.3067 (0.3345) acc 96.2766 (93.4620) lr 2.2949e-04 eta 0:04:14
epoch [41/50] batch [10/51] time 0.171 (0.340) data 0.000 (0.158) loss 0.3431 (0.3490) acc 93.7500 (92.9789) lr 2.2949e-04 eta 0:02:50
epoch [41/50] batch [15/51] time 0.165 (0.283) data 0.000 (0.105) loss 0.6117 (0.4243) acc 84.3750 (91.0899) lr 2.2949e-04 eta 0:02:19
epoch [41/50] batch [20/51] time 0.168 (0.257) data 0.000 (0.079) loss 0.4558 (0.4371) acc 89.6739 (90.7191) lr 2.2949e-04 eta 0:02:05
epoch [41/50] batch [25/51] time 0.190 (0.242) data 0.000 (0.063) loss 0.3493 (0.4432) acc 91.9811 (90.6155) lr 2.2949e-04 eta 0:01:57
epoch [41/50] batch [30/51] time 0.176 (0.230) data 0.001 (0.053) loss 0.4518 (0.4371) acc 90.8654 (90.7769) lr 2.2949e-04 eta 0:01:50
epoch [41/50] batch [35/51] time 0.175 (0.222) data 0.000 (0.045) loss 0.3976 (0.4381) acc 91.1765 (90.6357) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [40/51] time 0.165 (0.216) data 0.000 (0.040) loss 0.2928 (0.4278) acc 93.7500 (90.8456) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [45/51] time 0.164 (0.210) data 0.000 (0.035) loss 0.3509 (0.4249) acc 90.1042 (90.6897) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [50/51] time 0.166 (0.206) data 0.000 (0.032) loss 0.3116 (0.4197) acc 93.3673 (90.8774) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.128  alpha2: -0.027 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [42/50] batch [5/51] time 0.188 (0.499) data 0.000 (0.314) loss 0.4048 (0.3325) acc 91.6667 (93.6392) lr 1.9098e-04 eta 0:03:46
epoch [42/50] batch [10/51] time 0.167 (0.336) data 0.000 (0.157) loss 0.3309 (0.3697) acc 92.7083 (92.3928) lr 1.9098e-04 eta 0:02:30
epoch [42/50] batch [15/51] time 0.171 (0.283) data 0.000 (0.105) loss 0.3484 (0.3694) acc 91.4773 (92.3184) lr 1.9098e-04 eta 0:02:05
epoch [42/50] batch [20/51] time 0.171 (0.255) data 0.000 (0.079) loss 0.5176 (0.3891) acc 89.0000 (91.7438) lr 1.9098e-04 eta 0:01:51
epoch [42/50] batch [25/51] time 0.173 (0.241) data 0.000 (0.063) loss 0.4287 (0.3949) acc 90.1042 (91.6396) lr 1.9098e-04 eta 0:01:44
epoch [42/50] batch [30/51] time 0.188 (0.231) data 0.000 (0.053) loss 0.4685 (0.4048) acc 88.0208 (91.3024) lr 1.9098e-04 eta 0:01:39
epoch [42/50] batch [35/51] time 0.171 (0.225) data 0.000 (0.045) loss 0.5605 (0.4034) acc 85.3261 (91.1703) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [40/51] time 0.187 (0.219) data 0.000 (0.040) loss 0.2537 (0.4030) acc 94.3966 (91.2793) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [45/51] time 0.171 (0.213) data 0.000 (0.035) loss 0.3058 (0.4049) acc 92.6471 (91.2326) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [50/51] time 0.162 (0.208) data 0.000 (0.032) loss 0.5077 (0.4147) acc 87.2340 (91.0740) lr 1.9098e-04 eta 0:01:25
>>> alpha1: 0.128  alpha2: -0.028 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [43/50] batch [5/51] time 0.170 (0.482) data 0.001 (0.308) loss 0.6952 (0.4185) acc 85.2041 (91.5182) lr 1.5567e-04 eta 0:03:14
epoch [43/50] batch [10/51] time 0.175 (0.332) data 0.000 (0.154) loss 0.2978 (0.3974) acc 94.6078 (92.6918) lr 1.5567e-04 eta 0:02:12
epoch [43/50] batch [15/51] time 0.170 (0.280) data 0.000 (0.103) loss 0.3797 (0.3886) acc 90.8163 (92.4594) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [20/51] time 0.176 (0.252) data 0.000 (0.077) loss 0.7120 (0.3988) acc 81.6667 (91.9863) lr 1.5567e-04 eta 0:01:37
epoch [43/50] batch [25/51] time 0.170 (0.236) data 0.000 (0.062) loss 0.3371 (0.3950) acc 89.7959 (91.6596) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.179 (0.226) data 0.000 (0.052) loss 0.3541 (0.4027) acc 91.9811 (91.3861) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.167 (0.219) data 0.001 (0.045) loss 0.5271 (0.4020) acc 91.6667 (91.3988) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [40/51] time 0.169 (0.213) data 0.000 (0.039) loss 0.4535 (0.4068) acc 89.5000 (91.0980) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [45/51] time 0.168 (0.208) data 0.000 (0.035) loss 0.3063 (0.4058) acc 91.0000 (91.0774) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [50/51] time 0.173 (0.204) data 0.000 (0.031) loss 0.3317 (0.3994) acc 93.2692 (91.1997) lr 1.5567e-04 eta 0:01:13
>>> alpha1: 0.128  alpha2: -0.025 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [44/50] batch [5/51] time 0.176 (0.418) data 0.000 (0.237) loss 0.4710 (0.4383) acc 92.5532 (90.7158) lr 1.2369e-04 eta 0:02:27
epoch [44/50] batch [10/51] time 0.177 (0.297) data 0.000 (0.119) loss 0.4397 (0.4527) acc 93.8775 (91.0030) lr 1.2369e-04 eta 0:01:42
epoch [44/50] batch [15/51] time 0.173 (0.257) data 0.000 (0.079) loss 0.4465 (0.4371) acc 87.5000 (90.8384) lr 1.2369e-04 eta 0:01:27
epoch [44/50] batch [20/51] time 0.171 (0.235) data 0.000 (0.060) loss 0.3753 (0.4461) acc 90.5000 (90.7367) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [25/51] time 0.187 (0.222) data 0.000 (0.048) loss 0.4287 (0.4409) acc 90.8654 (90.9562) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [30/51] time 0.184 (0.215) data 0.000 (0.040) loss 0.3017 (0.4293) acc 90.4546 (90.9022) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [35/51] time 0.170 (0.209) data 0.000 (0.034) loss 0.4957 (0.4223) acc 89.5833 (91.0725) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [40/51] time 0.165 (0.204) data 0.000 (0.030) loss 0.5091 (0.4265) acc 91.3043 (91.0235) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [45/51] time 0.167 (0.200) data 0.001 (0.027) loss 0.2983 (0.4268) acc 96.4286 (91.2227) lr 1.2369e-04 eta 0:01:02
epoch [44/50] batch [50/51] time 0.177 (0.197) data 0.001 (0.024) loss 0.2379 (0.4291) acc 93.8679 (91.0777) lr 1.2369e-04 eta 0:01:00
>>> alpha1: 0.127  alpha2: -0.026 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [45/50] batch [5/51] time 0.180 (0.457) data 0.000 (0.272) loss 0.1594 (0.4550) acc 97.2222 (91.1216) lr 9.5173e-05 eta 0:02:17
epoch [45/50] batch [10/51] time 0.171 (0.314) data 0.000 (0.136) loss 0.4690 (0.4253) acc 91.6667 (91.3795) lr 9.5173e-05 eta 0:01:32
epoch [45/50] batch [15/51] time 0.179 (0.270) data 0.000 (0.091) loss 0.2050 (0.4156) acc 96.2264 (91.3872) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [20/51] time 0.153 (0.245) data 0.000 (0.068) loss 0.5485 (0.4155) acc 85.7143 (91.1806) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [25/51] time 0.180 (0.230) data 0.000 (0.055) loss 0.3062 (0.4084) acc 91.9811 (91.2303) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [30/51] time 0.177 (0.221) data 0.000 (0.045) loss 0.4143 (0.4029) acc 93.2692 (91.3553) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [35/51] time 0.175 (0.215) data 0.000 (0.039) loss 0.3313 (0.4026) acc 90.3061 (91.4008) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [40/51] time 0.169 (0.222) data 0.000 (0.034) loss 0.4048 (0.4007) acc 93.0000 (91.5507) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [45/51] time 0.169 (0.216) data 0.000 (0.030) loss 0.3271 (0.4029) acc 93.1373 (91.4121) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.166 (0.212) data 0.000 (0.027) loss 0.2936 (0.4396) acc 93.3673 (91.1609) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.125  alpha2: -0.029 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [46/50] batch [5/51] time 0.165 (0.455) data 0.000 (0.277) loss 0.4006 (0.3801) acc 91.6667 (92.1515) lr 7.0224e-05 eta 0:01:53
epoch [46/50] batch [10/51] time 0.159 (0.317) data 0.000 (0.139) loss 0.2817 (0.3743) acc 93.4524 (92.1918) lr 7.0224e-05 eta 0:01:17
epoch [46/50] batch [15/51] time 0.172 (0.269) data 0.000 (0.092) loss 0.4745 (0.3723) acc 93.5000 (92.4500) lr 7.0224e-05 eta 0:01:04
epoch [46/50] batch [20/51] time 0.186 (0.247) data 0.001 (0.069) loss 0.4250 (0.3774) acc 92.7273 (92.3540) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [25/51] time 0.206 (0.233) data 0.000 (0.056) loss 0.3702 (0.3755) acc 93.1373 (92.4389) lr 7.0224e-05 eta 0:00:53
epoch [46/50] batch [30/51] time 0.175 (0.223) data 0.000 (0.046) loss 0.4396 (0.3835) acc 88.8889 (92.0136) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [35/51] time 0.166 (0.216) data 0.000 (0.040) loss 0.3914 (0.3947) acc 90.1042 (91.6684) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [40/51] time 0.179 (0.210) data 0.000 (0.035) loss 0.4198 (0.4003) acc 89.6226 (91.5170) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [45/51] time 0.174 (0.206) data 0.000 (0.031) loss 0.3786 (0.4053) acc 91.8269 (91.4425) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [50/51] time 0.173 (0.202) data 0.000 (0.028) loss 0.3732 (0.4101) acc 88.4615 (91.0892) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.125  alpha2: -0.027 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [47/50] batch [5/51] time 0.163 (0.473) data 0.000 (0.295) loss 0.3739 (0.3132) acc 92.3913 (93.7840) lr 4.8943e-05 eta 0:01:34
epoch [47/50] batch [10/51] time 0.164 (0.327) data 0.000 (0.148) loss 0.4288 (0.3723) acc 89.3617 (91.7837) lr 4.8943e-05 eta 0:01:03
epoch [47/50] batch [15/51] time 0.170 (0.278) data 0.001 (0.099) loss 0.3863 (0.3795) acc 91.4894 (91.9488) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [20/51] time 0.165 (0.253) data 0.000 (0.074) loss 0.3398 (0.3809) acc 96.8085 (92.1067) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.172 (0.237) data 0.000 (0.059) loss 0.4495 (0.3899) acc 92.1569 (91.7617) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [30/51] time 0.182 (0.227) data 0.000 (0.049) loss 0.5527 (0.4015) acc 87.0000 (91.2067) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.184 (0.220) data 0.000 (0.042) loss 0.3219 (0.4334) acc 94.5000 (91.0869) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.178 (0.214) data 0.000 (0.037) loss 0.3571 (0.4345) acc 91.2037 (90.9186) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.170 (0.208) data 0.000 (0.033) loss 0.3566 (0.4281) acc 93.1373 (91.0766) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.163 (0.205) data 0.000 (0.030) loss 0.5022 (0.4298) acc 88.5417 (91.0074) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.127  alpha2: -0.027 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [48/50] batch [5/51] time 0.176 (0.479) data 0.000 (0.300) loss 0.3073 (0.4687) acc 96.1956 (90.1111) lr 3.1417e-05 eta 0:01:10
epoch [48/50] batch [10/51] time 0.176 (0.329) data 0.000 (0.150) loss 0.4298 (0.4239) acc 90.1042 (90.8167) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.181 (0.280) data 0.000 (0.100) loss 0.4035 (0.4492) acc 91.2037 (90.2442) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [20/51] time 0.177 (0.253) data 0.001 (0.075) loss 0.4118 (0.4412) acc 90.8654 (90.5563) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.167 (0.239) data 0.000 (0.060) loss 0.4712 (0.4203) acc 91.3265 (91.2245) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.188 (0.229) data 0.001 (0.050) loss 0.4216 (0.4157) acc 88.5000 (91.3375) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.178 (0.222) data 0.000 (0.043) loss 0.2848 (0.4151) acc 96.6346 (91.4277) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.170 (0.216) data 0.000 (0.038) loss 0.2592 (0.4170) acc 96.5686 (91.3138) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.168 (0.211) data 0.000 (0.034) loss 0.4861 (0.4163) acc 90.0000 (91.2643) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.168 (0.206) data 0.001 (0.030) loss 0.3422 (0.4221) acc 93.5000 (91.1513) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.126  alpha2: -0.030 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [49/50] batch [5/51] time 0.185 (0.449) data 0.000 (0.263) loss 0.3816 (0.4210) acc 91.3265 (91.7787) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [10/51] time 0.182 (0.316) data 0.000 (0.132) loss 0.3362 (0.3979) acc 93.5185 (92.1440) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.197 (0.268) data 0.000 (0.088) loss 0.3793 (0.4154) acc 91.2037 (91.5314) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.172 (0.246) data 0.000 (0.066) loss 0.4881 (0.4093) acc 90.0000 (91.4334) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.170 (0.232) data 0.000 (0.053) loss 0.2378 (0.4004) acc 97.2826 (91.7668) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.174 (0.222) data 0.000 (0.044) loss 0.4722 (0.4041) acc 88.9423 (91.5300) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.170 (0.216) data 0.000 (0.038) loss 0.4206 (0.3998) acc 88.7755 (91.4931) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.163 (0.212) data 0.000 (0.033) loss 0.4085 (0.3995) acc 89.5833 (91.3615) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.174 (0.207) data 0.000 (0.029) loss 0.3752 (0.3933) acc 93.7500 (91.6206) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.169 (0.203) data 0.000 (0.027) loss 0.7459 (0.4061) acc 79.5000 (91.1853) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.127  alpha2: -0.032 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.12 <<<
epoch [50/50] batch [5/51] time 0.174 (0.488) data 0.000 (0.309) loss 0.2825 (0.3906) acc 95.6731 (91.3176) lr 7.8853e-06 eta 0:00:22
epoch [50/50] batch [10/51] time 0.165 (0.334) data 0.001 (0.155) loss 0.4257 (0.3935) acc 93.2292 (91.9627) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.173 (0.284) data 0.001 (0.103) loss 0.2786 (0.3882) acc 95.0980 (92.2006) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.181 (0.259) data 0.000 (0.077) loss 0.4845 (0.3904) acc 88.0208 (92.1224) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.170 (0.242) data 0.000 (0.062) loss 0.4502 (0.3971) acc 89.5000 (91.7485) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.171 (0.232) data 0.000 (0.052) loss 0.3739 (0.4025) acc 93.0000 (91.7243) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.166 (0.224) data 0.000 (0.044) loss 0.4042 (0.4014) acc 90.6250 (91.5982) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.168 (0.217) data 0.000 (0.039) loss 0.2774 (0.3932) acc 93.0000 (91.6664) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.035) loss 0.3326 (0.3937) acc 92.8571 (91.6517) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.167 (0.206) data 0.000 (0.031) loss 0.3869 (0.3965) acc 91.5000 (91.6266) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.09, 0.09, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08]
* matched noise rate: [0.03, 0.02, 0.01, 0.01, 0.02, 0.02, 0.03, 0.03, 0.03, 0.04, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.05, 0.05, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]
* unmatched noise rate: [0.15, 0.16, 0.16, 0.16, 0.14, 0.16, 0.14, 0.14, 0.13, 0.14, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.13, 0.13, 0.14, 0.13, 0.14, 0.13, 0.12, 0.13, 0.13, 0.12, 0.12, 0.13, 0.12, 0.12, 0.13, 0.12, 0.12, 0.13, 0.12, 0.13, 0.12]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:11,  2.98s/it] 12%|█▏        | 3/25 [00:03<00:18,  1.19it/s] 20%|██        | 5/25 [00:03<00:09,  2.21it/s] 24%|██▍       | 6/25 [00:03<00:06,  2.79it/s] 32%|███▏      | 8/25 [00:03<00:04,  4.19it/s] 40%|████      | 10/25 [00:03<00:02,  5.61it/s] 48%|████▊     | 12/25 [00:03<00:01,  6.95it/s] 56%|█████▌    | 14/25 [00:04<00:01,  8.11it/s] 64%|██████▍   | 16/25 [00:04<00:00,  9.13it/s] 72%|███████▏  | 18/25 [00:04<00:00,  7.78it/s] 80%|████████  | 20/25 [00:04<00:00,  8.80it/s] 88%|████████▊ | 22/25 [00:04<00:00,  9.67it/s] 96%|█████████▌| 24/25 [00:05<00:00, 10.37it/s]100%|██████████| 25/25 [00:05<00:00,  4.43it/s]
=> result
* total: 2,463
* correct: 2,189
* accuracy: 88.9%
* error: 11.1%
* macro_f1: 87.5%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 11	acc: 91.7%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 10	acc: 83.3%
* class: 3 (sweet pea)	total: 17	correct: 11	acc: 64.7%
* class: 4 (english marigold)	total: 20	correct: 13	acc: 65.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 13	acc: 92.9%
* class: 10 (snapdragon)	total: 26	correct: 20	acc: 76.9%
* class: 11 (colt's foot)	total: 26	correct: 26	acc: 100.0%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 25	acc: 100.0%
* class: 18 (balloon flower)	total: 15	correct: 12	acc: 80.0%
* class: 19 (giant white arum lily)	total: 17	correct: 12	acc: 70.6%
* class: 20 (fire lily)	total: 12	correct: 11	acc: 91.7%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 23	acc: 85.2%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 21	acc: 80.8%
* class: 30 (carnation)	total: 16	correct: 13	acc: 81.2%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 7	acc: 53.8%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 26	acc: 68.4%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 27	acc: 96.4%
* class: 44 (bolero deep blue)	total: 12	correct: 9	acc: 75.0%
* class: 45 (wallflower)	total: 59	correct: 56	acc: 94.9%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 24	acc: 85.7%
* class: 50 (petunia)	total: 77	correct: 67	acc: 87.0%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 13	acc: 81.2%
* class: 68 (windflower)	total: 16	correct: 15	acc: 93.8%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 23	acc: 100.0%
* class: 71 (azalea)	total: 29	correct: 28	acc: 96.6%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 35	acc: 97.2%
* class: 75 (morning glory)	total: 32	correct: 29	acc: 90.6%
* class: 76 (passion flower)	total: 75	correct: 74	acc: 98.7%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 34	acc: 100.0%
* class: 82 (hibiscus)	total: 39	correct: 35	acc: 89.7%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 14	acc: 77.8%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 32	acc: 69.6%
* class: 88 (watercress)	total: 55	correct: 34	acc: 61.8%
* class: 89 (canna lily)	total: 25	correct: 19	acc: 76.0%
* class: 90 (hippeastrum)	total: 23	correct: 21	acc: 91.3%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 13	acc: 92.9%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 35	acc: 92.1%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 23	acc: 92.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 14	acc: 82.4%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 89.7%
Elapsed: 0:28:12
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '4', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_4-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 4
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.260 (1.102) data 0.000 (0.308) loss 4.4374 (4.5256) acc 3.1250 (4.3750) lr 1.0000e-05 eta 0:46:43
epoch [1/50] batch [10/51] time 0.261 (0.682) data 0.000 (0.154) loss 4.1374 (4.4597) acc 15.6250 (5.9375) lr 1.0000e-05 eta 0:28:51
epoch [1/50] batch [15/51] time 0.269 (0.542) data 0.000 (0.103) loss 4.2307 (4.3795) acc 6.2500 (6.2500) lr 1.0000e-05 eta 0:22:55
epoch [1/50] batch [20/51] time 0.264 (0.473) data 0.000 (0.077) loss 4.1076 (4.3315) acc 15.6250 (7.0312) lr 1.0000e-05 eta 0:19:55
epoch [1/50] batch [25/51] time 0.259 (0.430) data 0.000 (0.062) loss 3.9435 (4.3093) acc 12.5000 (7.6250) lr 1.0000e-05 eta 0:18:06
epoch [1/50] batch [30/51] time 0.259 (0.403) data 0.000 (0.052) loss 3.9121 (4.2855) acc 9.3750 (8.0208) lr 1.0000e-05 eta 0:16:55
epoch [1/50] batch [35/51] time 0.272 (0.383) data 0.000 (0.044) loss 3.6459 (4.2426) acc 21.8750 (9.7321) lr 1.0000e-05 eta 0:16:04
epoch [1/50] batch [40/51] time 0.257 (0.368) data 0.000 (0.039) loss 3.4754 (4.1797) acc 34.3750 (11.5625) lr 1.0000e-05 eta 0:15:23
epoch [1/50] batch [45/51] time 0.257 (0.356) data 0.000 (0.034) loss 3.3524 (4.1015) acc 34.3750 (13.7500) lr 1.0000e-05 eta 0:14:50
epoch [1/50] batch [50/51] time 0.258 (0.346) data 0.000 (0.031) loss 3.5904 (4.0768) acc 34.3750 (14.8750) lr 1.0000e-05 eta 0:14:24
epoch [2/50] batch [5/51] time 0.303 (0.552) data 0.000 (0.256) loss 3.6194 (3.4503) acc 37.5000 (35.6250) lr 2.0000e-03 eta 0:22:57
epoch [2/50] batch [10/51] time 0.260 (0.409) data 0.000 (0.128) loss 3.1991 (3.4407) acc 31.2500 (34.0625) lr 2.0000e-03 eta 0:16:57
epoch [2/50] batch [15/51] time 0.260 (0.360) data 0.000 (0.086) loss 2.8951 (3.3018) acc 43.7500 (37.0833) lr 2.0000e-03 eta 0:14:55
epoch [2/50] batch [20/51] time 0.266 (0.337) data 0.000 (0.064) loss 3.6224 (3.4382) acc 37.5000 (36.8750) lr 2.0000e-03 eta 0:13:55
epoch [2/50] batch [25/51] time 0.270 (0.323) data 0.000 (0.051) loss 3.3834 (3.4520) acc 50.0000 (36.5000) lr 2.0000e-03 eta 0:13:19
epoch [2/50] batch [30/51] time 0.261 (0.314) data 0.000 (0.043) loss 3.4203 (3.4161) acc 50.0000 (37.5000) lr 2.0000e-03 eta 0:12:56
epoch [2/50] batch [35/51] time 0.259 (0.307) data 0.000 (0.037) loss 3.1280 (3.3809) acc 37.5000 (38.1250) lr 2.0000e-03 eta 0:12:36
epoch [2/50] batch [40/51] time 0.257 (0.301) data 0.000 (0.032) loss 2.8856 (3.3608) acc 50.0000 (38.6719) lr 2.0000e-03 eta 0:12:20
epoch [2/50] batch [45/51] time 0.257 (0.296) data 0.000 (0.029) loss 3.8352 (3.3474) acc 40.6250 (39.1667) lr 2.0000e-03 eta 0:12:06
epoch [2/50] batch [50/51] time 0.259 (0.292) data 0.000 (0.026) loss 2.9308 (3.2664) acc 50.0000 (40.2500) lr 2.0000e-03 eta 0:11:56
epoch [3/50] batch [5/51] time 0.273 (0.622) data 0.000 (0.342) loss 3.3929 (2.9846) acc 40.6250 (46.2500) lr 1.9980e-03 eta 0:25:20
epoch [3/50] batch [10/51] time 0.260 (0.445) data 0.000 (0.171) loss 2.6028 (3.0703) acc 59.3750 (45.3125) lr 1.9980e-03 eta 0:18:04
epoch [3/50] batch [15/51] time 0.259 (0.388) data 0.000 (0.114) loss 3.0594 (3.0888) acc 53.1250 (45.0000) lr 1.9980e-03 eta 0:15:44
epoch [3/50] batch [20/51] time 0.265 (0.357) data 0.000 (0.086) loss 2.6013 (3.0698) acc 50.0000 (45.0000) lr 1.9980e-03 eta 0:14:26
epoch [3/50] batch [25/51] time 0.261 (0.340) data 0.000 (0.069) loss 2.6484 (3.0149) acc 56.2500 (46.5000) lr 1.9980e-03 eta 0:13:43
epoch [3/50] batch [30/51] time 0.270 (0.327) data 0.000 (0.057) loss 2.8568 (3.0086) acc 34.3750 (46.5625) lr 1.9980e-03 eta 0:13:11
epoch [3/50] batch [35/51] time 0.273 (0.319) data 0.013 (0.049) loss 2.8161 (3.0529) acc 53.1250 (46.3393) lr 1.9980e-03 eta 0:12:49
epoch [3/50] batch [40/51] time 0.258 (0.312) data 0.000 (0.043) loss 2.2404 (2.9896) acc 68.7500 (47.5781) lr 1.9980e-03 eta 0:12:30
epoch [3/50] batch [45/51] time 0.259 (0.306) data 0.000 (0.038) loss 3.9904 (3.0179) acc 37.5000 (47.5000) lr 1.9980e-03 eta 0:12:14
epoch [3/50] batch [50/51] time 0.256 (0.301) data 0.000 (0.035) loss 2.9137 (3.0205) acc 40.6250 (47.1875) lr 1.9980e-03 eta 0:12:01
epoch [4/50] batch [5/51] time 0.275 (0.537) data 0.000 (0.257) loss 2.3740 (2.8467) acc 50.0000 (55.0000) lr 1.9921e-03 eta 0:21:24
epoch [4/50] batch [10/51] time 0.267 (0.402) data 0.000 (0.128) loss 3.2911 (2.7727) acc 40.6250 (53.1250) lr 1.9921e-03 eta 0:16:00
epoch [4/50] batch [15/51] time 0.258 (0.356) data 0.000 (0.086) loss 2.8711 (2.8935) acc 46.8750 (50.6250) lr 1.9921e-03 eta 0:14:08
epoch [4/50] batch [20/51] time 0.259 (0.333) data 0.000 (0.064) loss 3.1202 (2.9071) acc 43.7500 (50.1562) lr 1.9921e-03 eta 0:13:10
epoch [4/50] batch [25/51] time 0.260 (0.318) data 0.000 (0.051) loss 2.7585 (2.9413) acc 56.2500 (49.3750) lr 1.9921e-03 eta 0:12:35
epoch [4/50] batch [30/51] time 0.269 (0.310) data 0.000 (0.043) loss 2.5543 (2.9068) acc 50.0000 (50.1042) lr 1.9921e-03 eta 0:12:13
epoch [4/50] batch [35/51] time 0.269 (0.303) data 0.000 (0.037) loss 3.4794 (2.9170) acc 37.5000 (50.2679) lr 1.9921e-03 eta 0:11:56
epoch [4/50] batch [40/51] time 0.258 (0.298) data 0.000 (0.032) loss 3.1322 (2.9012) acc 56.2500 (50.6250) lr 1.9921e-03 eta 0:11:41
epoch [4/50] batch [45/51] time 0.258 (0.293) data 0.000 (0.029) loss 2.3426 (2.9068) acc 62.5000 (50.6944) lr 1.9921e-03 eta 0:11:29
epoch [4/50] batch [50/51] time 0.256 (0.290) data 0.000 (0.026) loss 3.3318 (2.8906) acc 40.6250 (50.9375) lr 1.9921e-03 eta 0:11:19
epoch [5/50] batch [5/51] time 0.260 (0.491) data 0.000 (0.215) loss 2.4723 (2.6792) acc 62.5000 (53.1250) lr 1.9823e-03 eta 0:19:09
epoch [5/50] batch [10/51] time 0.259 (0.380) data 0.000 (0.108) loss 2.3385 (2.6156) acc 65.6250 (55.9375) lr 1.9823e-03 eta 0:14:46
epoch [5/50] batch [15/51] time 0.260 (0.341) data 0.000 (0.072) loss 3.4389 (2.7247) acc 40.6250 (54.3750) lr 1.9823e-03 eta 0:13:14
epoch [5/50] batch [20/51] time 0.275 (0.323) data 0.000 (0.054) loss 3.8159 (2.8139) acc 46.8750 (52.9688) lr 1.9823e-03 eta 0:12:30
epoch [5/50] batch [25/51] time 0.266 (0.312) data 0.000 (0.043) loss 2.8999 (2.8263) acc 53.1250 (51.7500) lr 1.9823e-03 eta 0:12:04
epoch [5/50] batch [30/51] time 0.273 (0.305) data 0.000 (0.036) loss 2.1921 (2.7567) acc 71.8750 (54.0625) lr 1.9823e-03 eta 0:11:46
epoch [5/50] batch [35/51] time 0.262 (0.299) data 0.000 (0.031) loss 2.4948 (2.8094) acc 56.2500 (53.2143) lr 1.9823e-03 eta 0:11:30
epoch [5/50] batch [40/51] time 0.258 (0.294) data 0.000 (0.027) loss 2.4417 (2.8373) acc 56.2500 (52.9688) lr 1.9823e-03 eta 0:11:17
epoch [5/50] batch [45/51] time 0.257 (0.290) data 0.000 (0.024) loss 2.5385 (2.8054) acc 50.0000 (53.4028) lr 1.9823e-03 eta 0:11:07
epoch [5/50] batch [50/51] time 0.259 (0.287) data 0.000 (0.022) loss 2.5422 (2.8019) acc 59.3750 (53.3125) lr 1.9823e-03 eta 0:10:58
epoch [6/50] batch [5/51] time 0.285 (0.576) data 0.000 (0.280) loss 1.9319 (2.6695) acc 78.1250 (61.8750) lr 1.9686e-03 eta 0:21:58
epoch [6/50] batch [10/51] time 0.268 (0.422) data 0.000 (0.140) loss 2.8084 (2.8879) acc 56.2500 (56.2500) lr 1.9686e-03 eta 0:16:04
epoch [6/50] batch [15/51] time 0.271 (0.371) data 0.000 (0.094) loss 2.5379 (2.8811) acc 53.1250 (55.0000) lr 1.9686e-03 eta 0:14:05
epoch [6/50] batch [20/51] time 0.276 (0.345) data 0.000 (0.070) loss 2.5644 (2.8820) acc 40.6250 (53.1250) lr 1.9686e-03 eta 0:13:04
epoch [6/50] batch [25/51] time 0.259 (0.329) data 0.000 (0.056) loss 2.9563 (2.8497) acc 40.6250 (53.1250) lr 1.9686e-03 eta 0:12:26
epoch [6/50] batch [30/51] time 0.273 (0.318) data 0.000 (0.047) loss 3.0014 (2.8435) acc 50.0000 (52.7083) lr 1.9686e-03 eta 0:12:01
epoch [6/50] batch [35/51] time 0.259 (0.310) data 0.000 (0.040) loss 2.6678 (2.7501) acc 56.2500 (54.6429) lr 1.9686e-03 eta 0:11:41
epoch [6/50] batch [40/51] time 0.258 (0.304) data 0.000 (0.035) loss 2.9644 (2.7342) acc 46.8750 (54.2969) lr 1.9686e-03 eta 0:11:25
epoch [6/50] batch [45/51] time 0.259 (0.299) data 0.000 (0.031) loss 3.1043 (2.7236) acc 56.2500 (54.7222) lr 1.9686e-03 eta 0:11:12
epoch [6/50] batch [50/51] time 0.257 (0.295) data 0.000 (0.028) loss 2.6463 (2.7275) acc 56.2500 (54.5625) lr 1.9686e-03 eta 0:11:01
epoch [7/50] batch [5/51] time 0.269 (0.522) data 0.000 (0.243) loss 3.1204 (3.1653) acc 56.2500 (48.7500) lr 1.9511e-03 eta 0:19:27
epoch [7/50] batch [10/51] time 0.267 (0.396) data 0.000 (0.122) loss 2.4238 (2.7090) acc 65.6250 (57.8125) lr 1.9511e-03 eta 0:14:43
epoch [7/50] batch [15/51] time 0.274 (0.353) data 0.000 (0.081) loss 2.4534 (2.7182) acc 62.5000 (56.4583) lr 1.9511e-03 eta 0:13:07
epoch [7/50] batch [20/51] time 0.269 (0.331) data 0.000 (0.061) loss 2.8350 (2.6996) acc 43.7500 (56.0938) lr 1.9511e-03 eta 0:12:15
epoch [7/50] batch [25/51] time 0.269 (0.320) data 0.000 (0.049) loss 2.1255 (2.7309) acc 62.5000 (55.7500) lr 1.9511e-03 eta 0:11:49
epoch [7/50] batch [30/51] time 0.264 (0.312) data 0.000 (0.041) loss 3.3210 (2.7626) acc 43.7500 (55.2083) lr 1.9511e-03 eta 0:11:30
epoch [7/50] batch [35/51] time 0.274 (0.306) data 0.000 (0.035) loss 2.5740 (2.7409) acc 50.0000 (55.0893) lr 1.9511e-03 eta 0:11:16
epoch [7/50] batch [40/51] time 0.260 (0.301) data 0.000 (0.031) loss 2.3877 (2.7263) acc 65.6250 (55.1562) lr 1.9511e-03 eta 0:11:03
epoch [7/50] batch [45/51] time 0.258 (0.296) data 0.000 (0.027) loss 2.6356 (2.7183) acc 50.0000 (55.2778) lr 1.9511e-03 eta 0:10:51
epoch [7/50] batch [50/51] time 0.257 (0.292) data 0.000 (0.025) loss 2.1466 (2.6810) acc 65.6250 (56.1250) lr 1.9511e-03 eta 0:10:41
epoch [8/50] batch [5/51] time 0.284 (0.554) data 0.000 (0.264) loss 1.5840 (2.3780) acc 71.8750 (57.5000) lr 1.9298e-03 eta 0:20:12
epoch [8/50] batch [10/51] time 0.259 (0.410) data 0.000 (0.132) loss 2.3905 (2.4503) acc 59.3750 (58.4375) lr 1.9298e-03 eta 0:14:55
epoch [8/50] batch [15/51] time 0.260 (0.363) data 0.000 (0.088) loss 2.6848 (2.4995) acc 53.1250 (56.4583) lr 1.9298e-03 eta 0:13:10
epoch [8/50] batch [20/51] time 0.260 (0.338) data 0.000 (0.066) loss 2.3220 (2.5074) acc 65.6250 (55.9375) lr 1.9298e-03 eta 0:12:14
epoch [8/50] batch [25/51] time 0.274 (0.324) data 0.000 (0.053) loss 2.1671 (2.4682) acc 62.5000 (57.0000) lr 1.9298e-03 eta 0:11:41
epoch [8/50] batch [30/51] time 0.261 (0.315) data 0.000 (0.044) loss 3.0021 (2.5207) acc 56.2500 (56.9792) lr 1.9298e-03 eta 0:11:20
epoch [8/50] batch [35/51] time 0.286 (0.308) data 0.000 (0.038) loss 2.5803 (2.5210) acc 59.3750 (57.5000) lr 1.9298e-03 eta 0:11:05
epoch [8/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.033) loss 2.7404 (2.5291) acc 59.3750 (58.2812) lr 1.9298e-03 eta 0:10:51
epoch [8/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.030) loss 2.6003 (2.5361) acc 59.3750 (58.7500) lr 1.9298e-03 eta 0:10:39
epoch [8/50] batch [50/51] time 0.259 (0.294) data 0.000 (0.027) loss 2.5865 (2.5635) acc 62.5000 (58.6250) lr 1.9298e-03 eta 0:10:29
epoch [9/50] batch [5/51] time 0.286 (0.554) data 0.000 (0.273) loss 1.8434 (2.4132) acc 71.8750 (65.6250) lr 1.9048e-03 eta 0:19:44
epoch [9/50] batch [10/51] time 0.271 (0.411) data 0.000 (0.137) loss 2.3214 (2.3627) acc 62.5000 (63.1250) lr 1.9048e-03 eta 0:14:35
epoch [9/50] batch [15/51] time 0.260 (0.363) data 0.000 (0.091) loss 1.7493 (2.3753) acc 65.6250 (61.8750) lr 1.9048e-03 eta 0:12:51
epoch [9/50] batch [20/51] time 0.271 (0.339) data 0.000 (0.069) loss 2.7621 (2.3814) acc 62.5000 (60.9375) lr 1.9048e-03 eta 0:11:58
epoch [9/50] batch [25/51] time 0.272 (0.324) data 0.000 (0.055) loss 2.3152 (2.4033) acc 62.5000 (60.8750) lr 1.9048e-03 eta 0:11:25
epoch [9/50] batch [30/51] time 0.264 (0.314) data 0.000 (0.046) loss 2.3763 (2.4328) acc 62.5000 (60.3125) lr 1.9048e-03 eta 0:11:02
epoch [9/50] batch [35/51] time 0.267 (0.307) data 0.000 (0.039) loss 1.7453 (2.4713) acc 71.8750 (60.6250) lr 1.9048e-03 eta 0:10:46
epoch [9/50] batch [40/51] time 0.259 (0.301) data 0.000 (0.034) loss 2.3060 (2.4933) acc 65.6250 (60.3125) lr 1.9048e-03 eta 0:10:33
epoch [9/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.031) loss 2.4420 (2.5361) acc 59.3750 (59.5833) lr 1.9048e-03 eta 0:10:22
epoch [9/50] batch [50/51] time 0.259 (0.293) data 0.000 (0.028) loss 2.2782 (2.5311) acc 65.6250 (59.6250) lr 1.9048e-03 eta 0:10:12
epoch [10/50] batch [5/51] time 0.279 (0.590) data 0.000 (0.302) loss 2.0480 (2.2091) acc 71.8750 (63.1250) lr 1.8763e-03 eta 0:20:31
epoch [10/50] batch [10/51] time 0.259 (0.427) data 0.000 (0.151) loss 1.9451 (2.2423) acc 68.7500 (65.3125) lr 1.8763e-03 eta 0:14:49
epoch [10/50] batch [15/51] time 0.268 (0.374) data 0.000 (0.101) loss 3.0629 (2.3322) acc 56.2500 (63.5417) lr 1.8763e-03 eta 0:12:55
epoch [10/50] batch [20/51] time 0.273 (0.347) data 0.000 (0.076) loss 2.0791 (2.3079) acc 68.7500 (64.0625) lr 1.8763e-03 eta 0:11:57
epoch [10/50] batch [25/51] time 0.269 (0.331) data 0.000 (0.061) loss 3.1643 (2.3740) acc 56.2500 (63.5000) lr 1.8763e-03 eta 0:11:24
epoch [10/50] batch [30/51] time 0.261 (0.320) data 0.000 (0.050) loss 2.8436 (2.4218) acc 46.8750 (62.2917) lr 1.8763e-03 eta 0:11:00
epoch [10/50] batch [35/51] time 0.264 (0.313) data 0.000 (0.043) loss 2.6015 (2.4680) acc 56.2500 (61.8750) lr 1.8763e-03 eta 0:10:43
epoch [10/50] batch [40/51] time 0.258 (0.306) data 0.000 (0.038) loss 4.0188 (2.5141) acc 40.6250 (61.3281) lr 1.8763e-03 eta 0:10:27
epoch [10/50] batch [45/51] time 0.258 (0.301) data 0.000 (0.034) loss 2.4112 (2.4722) acc 59.3750 (61.8056) lr 1.8763e-03 eta 0:10:15
epoch [10/50] batch [50/51] time 0.259 (0.297) data 0.000 (0.030) loss 2.7734 (2.4748) acc 53.1250 (61.3750) lr 1.8763e-03 eta 0:10:05
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.430  alpha2: 0.076 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [11/50] batch [5/51] time 0.154 (0.809) data 0.000 (0.272) loss 1.3703 (1.2455) acc 78.9773 (75.3842) lr 1.8443e-03 eta 0:27:25
epoch [11/50] batch [10/51] time 0.781 (0.688) data 0.000 (0.136) loss 1.4272 (1.2533) acc 69.6808 (72.9376) lr 1.8443e-03 eta 0:23:15
epoch [11/50] batch [15/51] time 0.162 (0.551) data 0.000 (0.091) loss 1.3037 (1.2644) acc 68.0851 (72.0162) lr 1.8443e-03 eta 0:18:35
epoch [11/50] batch [20/51] time 0.164 (0.487) data 0.000 (0.068) loss 1.3025 (1.2447) acc 71.2766 (72.9071) lr 1.8443e-03 eta 0:16:23
epoch [11/50] batch [25/51] time 0.158 (0.453) data 0.000 (0.055) loss 1.0067 (1.2132) acc 76.1111 (73.6909) lr 1.8443e-03 eta 0:15:12
epoch [11/50] batch [30/51] time 0.157 (0.448) data 0.000 (0.046) loss 0.8999 (1.2162) acc 76.7045 (73.3996) lr 1.8443e-03 eta 0:14:59
epoch [11/50] batch [35/51] time 0.895 (0.428) data 0.000 (0.039) loss 1.1010 (1.2040) acc 79.0000 (73.5132) lr 1.8443e-03 eta 0:14:17
epoch [11/50] batch [40/51] time 0.166 (0.395) data 0.000 (0.034) loss 1.1718 (1.1800) acc 72.9592 (74.1666) lr 1.8443e-03 eta 0:13:10
epoch [11/50] batch [45/51] time 0.174 (0.385) data 0.000 (0.030) loss 0.9033 (1.1804) acc 80.5000 (74.0677) lr 1.8443e-03 eta 0:12:47
epoch [11/50] batch [50/51] time 0.152 (0.362) data 0.000 (0.027) loss 1.2116 (1.1769) acc 80.2326 (74.3000) lr 1.8443e-03 eta 0:12:01
>>> alpha1: 0.345  alpha2: 0.048 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [12/50] batch [5/51] time 0.166 (0.457) data 0.000 (0.285) loss 1.0309 (0.9585) acc 75.5208 (77.5686) lr 1.8090e-03 eta 0:15:06
epoch [12/50] batch [10/51] time 0.865 (0.449) data 0.000 (0.143) loss 0.6683 (0.9365) acc 85.0000 (78.3533) lr 1.8090e-03 eta 0:14:48
epoch [12/50] batch [15/51] time 0.189 (0.355) data 0.000 (0.095) loss 0.7244 (0.9396) acc 84.7222 (78.3788) lr 1.8090e-03 eta 0:11:41
epoch [12/50] batch [20/51] time 0.181 (0.311) data 0.000 (0.071) loss 0.6850 (0.9216) acc 84.1346 (78.7760) lr 1.8090e-03 eta 0:10:11
epoch [12/50] batch [25/51] time 0.176 (0.282) data 0.000 (0.057) loss 0.8356 (0.9233) acc 80.5000 (78.9073) lr 1.8090e-03 eta 0:09:14
epoch [12/50] batch [30/51] time 0.179 (0.265) data 0.000 (0.048) loss 0.8652 (0.9235) acc 79.0816 (78.8810) lr 1.8090e-03 eta 0:08:39
epoch [12/50] batch [35/51] time 0.164 (0.252) data 0.000 (0.041) loss 0.9521 (0.9074) acc 80.1136 (79.2152) lr 1.8090e-03 eta 0:08:12
epoch [12/50] batch [40/51] time 0.168 (0.242) data 0.000 (0.036) loss 0.7736 (0.8966) acc 80.3922 (79.6380) lr 1.8090e-03 eta 0:07:51
epoch [12/50] batch [45/51] time 0.176 (0.234) data 0.000 (0.032) loss 0.8772 (0.9077) acc 77.8302 (79.1722) lr 1.8090e-03 eta 0:07:34
epoch [12/50] batch [50/51] time 0.165 (0.227) data 0.001 (0.029) loss 1.3151 (0.9130) acc 62.5000 (78.8610) lr 1.8090e-03 eta 0:07:20
>>> alpha1: 0.311  alpha2: 0.019 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [13/50] batch [5/51] time 0.170 (0.444) data 0.001 (0.258) loss 0.8427 (0.8091) acc 78.8889 (82.3779) lr 1.7705e-03 eta 0:14:19
epoch [13/50] batch [10/51] time 0.168 (0.311) data 0.001 (0.129) loss 0.9224 (0.8049) acc 78.5714 (81.7045) lr 1.7705e-03 eta 0:09:58
epoch [13/50] batch [15/51] time 0.187 (0.267) data 0.000 (0.086) loss 0.8242 (0.8041) acc 82.2115 (82.1652) lr 1.7705e-03 eta 0:08:33
epoch [13/50] batch [20/51] time 0.196 (0.245) data 0.000 (0.065) loss 0.6161 (0.7903) acc 86.7347 (81.8903) lr 1.7705e-03 eta 0:07:50
epoch [13/50] batch [25/51] time 0.163 (0.262) data 0.001 (0.052) loss 0.7791 (0.7877) acc 82.9787 (82.1452) lr 1.7705e-03 eta 0:08:20
epoch [13/50] batch [30/51] time 0.166 (0.246) data 0.000 (0.043) loss 0.8253 (0.7847) acc 78.0612 (82.0212) lr 1.7705e-03 eta 0:07:50
epoch [13/50] batch [35/51] time 0.179 (0.237) data 0.000 (0.037) loss 0.7397 (0.7894) acc 83.3333 (81.8730) lr 1.7705e-03 eta 0:07:30
epoch [13/50] batch [40/51] time 0.162 (0.227) data 0.000 (0.033) loss 0.9578 (0.8031) acc 85.6383 (81.5646) lr 1.7705e-03 eta 0:07:11
epoch [13/50] batch [45/51] time 0.162 (0.221) data 0.000 (0.029) loss 0.8489 (0.8087) acc 85.1064 (81.3182) lr 1.7705e-03 eta 0:06:58
epoch [13/50] batch [50/51] time 0.161 (0.215) data 0.000 (0.026) loss 0.7462 (0.8034) acc 85.6383 (81.6461) lr 1.7705e-03 eta 0:06:46
>>> alpha1: 0.287  alpha2: -0.010 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [14/50] batch [5/51] time 0.177 (0.439) data 0.000 (0.262) loss 0.7340 (0.8203) acc 85.0962 (82.4489) lr 1.7290e-03 eta 0:13:45
epoch [14/50] batch [10/51] time 0.167 (0.305) data 0.000 (0.131) loss 0.7070 (0.7263) acc 83.8542 (84.0871) lr 1.7290e-03 eta 0:09:32
epoch [14/50] batch [15/51] time 0.175 (0.261) data 0.000 (0.087) loss 0.7260 (0.7597) acc 81.2500 (82.4887) lr 1.7290e-03 eta 0:08:07
epoch [14/50] batch [20/51] time 0.161 (0.241) data 0.000 (0.066) loss 1.1036 (0.7926) acc 76.5957 (81.5096) lr 1.7290e-03 eta 0:07:29
epoch [14/50] batch [25/51] time 0.181 (0.228) data 0.000 (0.053) loss 0.7433 (0.8504) acc 83.1633 (80.7806) lr 1.7290e-03 eta 0:07:03
epoch [14/50] batch [30/51] time 0.179 (0.219) data 0.000 (0.044) loss 0.7082 (0.8303) acc 82.0000 (81.1313) lr 1.7290e-03 eta 0:06:46
epoch [14/50] batch [35/51] time 0.176 (0.212) data 0.000 (0.038) loss 0.8326 (0.8052) acc 76.5957 (81.6875) lr 1.7290e-03 eta 0:06:33
epoch [14/50] batch [40/51] time 0.156 (0.227) data 0.000 (0.033) loss 0.9258 (0.7873) acc 73.2955 (81.8230) lr 1.7290e-03 eta 0:06:58
epoch [14/50] batch [45/51] time 0.159 (0.220) data 0.000 (0.029) loss 0.8410 (0.7915) acc 78.8043 (81.7134) lr 1.7290e-03 eta 0:06:45
epoch [14/50] batch [50/51] time 0.169 (0.214) data 0.000 (0.026) loss 0.6723 (0.7979) acc 81.5000 (81.3789) lr 1.7290e-03 eta 0:06:33
>>> alpha1: 0.272  alpha2: -0.024 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.02 & unmatched refined noisy rate: 0.15 <<<
epoch [15/50] batch [5/51] time 0.161 (0.431) data 0.000 (0.252) loss 0.6569 (0.7193) acc 84.4445 (84.1196) lr 1.6845e-03 eta 0:13:08
epoch [15/50] batch [10/51] time 0.183 (0.302) data 0.000 (0.126) loss 0.6844 (0.7047) acc 82.5472 (83.6193) lr 1.6845e-03 eta 0:09:10
epoch [15/50] batch [15/51] time 0.177 (0.259) data 0.000 (0.084) loss 0.6217 (0.6906) acc 85.3774 (84.0528) lr 1.6845e-03 eta 0:07:51
epoch [15/50] batch [20/51] time 0.169 (0.239) data 0.000 (0.063) loss 0.8259 (0.7057) acc 82.4468 (83.3176) lr 1.6845e-03 eta 0:07:13
epoch [15/50] batch [25/51] time 0.169 (0.257) data 0.000 (0.051) loss 0.6597 (0.7036) acc 82.0000 (83.6326) lr 1.6845e-03 eta 0:07:45
epoch [15/50] batch [30/51] time 0.191 (0.245) data 0.000 (0.042) loss 0.7209 (0.7015) acc 81.2500 (83.6517) lr 1.6845e-03 eta 0:07:21
epoch [15/50] batch [35/51] time 0.167 (0.233) data 0.000 (0.036) loss 1.0430 (0.7214) acc 77.5510 (83.1496) lr 1.6845e-03 eta 0:07:00
epoch [15/50] batch [40/51] time 0.159 (0.226) data 0.000 (0.032) loss 0.9682 (0.7236) acc 75.5435 (83.0642) lr 1.6845e-03 eta 0:06:45
epoch [15/50] batch [45/51] time 0.163 (0.219) data 0.000 (0.028) loss 1.0084 (0.7262) acc 77.0833 (83.0936) lr 1.6845e-03 eta 0:06:32
epoch [15/50] batch [50/51] time 0.163 (0.214) data 0.000 (0.025) loss 0.5574 (0.7189) acc 84.3750 (83.1831) lr 1.6845e-03 eta 0:06:22
>>> alpha1: 0.226  alpha2: -0.060 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [16/50] batch [5/51] time 0.181 (0.460) data 0.000 (0.284) loss 0.5595 (0.6331) acc 88.4259 (84.7651) lr 1.6374e-03 eta 0:13:38
epoch [16/50] batch [10/51] time 0.170 (0.318) data 0.000 (0.142) loss 0.6398 (0.6369) acc 85.5000 (85.4470) lr 1.6374e-03 eta 0:09:24
epoch [16/50] batch [15/51] time 0.175 (0.270) data 0.000 (0.095) loss 0.9581 (0.6585) acc 81.3830 (85.3673) lr 1.6374e-03 eta 0:07:57
epoch [16/50] batch [20/51] time 0.170 (0.246) data 0.000 (0.071) loss 0.7563 (0.6417) acc 84.5000 (85.7408) lr 1.6374e-03 eta 0:07:13
epoch [16/50] batch [25/51] time 0.161 (0.230) data 0.000 (0.057) loss 0.8221 (0.6832) acc 81.5217 (84.5987) lr 1.6374e-03 eta 0:06:45
epoch [16/50] batch [30/51] time 0.182 (0.223) data 0.000 (0.048) loss 0.6502 (0.6813) acc 84.6154 (84.6813) lr 1.6374e-03 eta 0:06:30
epoch [16/50] batch [35/51] time 0.172 (0.215) data 0.000 (0.041) loss 0.6068 (0.6835) acc 81.7308 (84.2932) lr 1.6374e-03 eta 0:06:16
epoch [16/50] batch [40/51] time 0.163 (0.209) data 0.000 (0.036) loss 0.7114 (0.6841) acc 86.4583 (84.3744) lr 1.6374e-03 eta 0:06:04
epoch [16/50] batch [45/51] time 0.169 (0.205) data 0.000 (0.032) loss 0.6176 (0.6805) acc 83.8235 (84.3287) lr 1.6374e-03 eta 0:05:56
epoch [16/50] batch [50/51] time 0.170 (0.201) data 0.000 (0.029) loss 0.6095 (0.6746) acc 86.2745 (84.5456) lr 1.6374e-03 eta 0:05:49
>>> alpha1: 0.211  alpha2: -0.065 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.16 <<<
epoch [17/50] batch [5/51] time 0.182 (0.448) data 0.000 (0.269) loss 0.4927 (0.5770) acc 90.4546 (85.4045) lr 1.5878e-03 eta 0:12:55
epoch [17/50] batch [10/51] time 0.171 (0.312) data 0.001 (0.135) loss 0.6120 (0.6201) acc 83.8542 (84.7386) lr 1.5878e-03 eta 0:08:58
epoch [17/50] batch [15/51] time 0.196 (0.266) data 0.000 (0.090) loss 0.5540 (0.6650) acc 85.0877 (83.7061) lr 1.5878e-03 eta 0:07:37
epoch [17/50] batch [20/51] time 0.165 (0.244) data 0.000 (0.067) loss 0.5721 (0.6710) acc 87.2340 (83.5998) lr 1.5878e-03 eta 0:06:59
epoch [17/50] batch [25/51] time 0.163 (0.230) data 0.000 (0.054) loss 0.6156 (0.6516) acc 86.7021 (84.2432) lr 1.5878e-03 eta 0:06:33
epoch [17/50] batch [30/51] time 0.180 (0.222) data 0.000 (0.045) loss 0.5325 (0.6450) acc 89.0000 (84.4394) lr 1.5878e-03 eta 0:06:18
epoch [17/50] batch [35/51] time 0.173 (0.215) data 0.000 (0.039) loss 0.7189 (0.6486) acc 84.6154 (84.6253) lr 1.5878e-03 eta 0:06:06
epoch [17/50] batch [40/51] time 0.157 (0.210) data 0.000 (0.034) loss 0.5209 (0.6463) acc 88.3333 (84.8946) lr 1.5878e-03 eta 0:05:55
epoch [17/50] batch [45/51] time 0.175 (0.205) data 0.000 (0.030) loss 0.6765 (0.6480) acc 82.0755 (84.9009) lr 1.5878e-03 eta 0:05:46
epoch [17/50] batch [50/51] time 0.173 (0.201) data 0.000 (0.027) loss 0.6362 (0.6556) acc 89.9038 (84.7028) lr 1.5878e-03 eta 0:05:38
>>> alpha1: 0.193  alpha2: -0.069 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.16 <<<
epoch [18/50] batch [5/51] time 0.194 (0.538) data 0.000 (0.348) loss 0.6994 (0.6252) acc 83.0189 (84.7161) lr 1.5358e-03 eta 0:15:01
epoch [18/50] batch [10/51] time 0.187 (0.357) data 0.000 (0.174) loss 0.9404 (0.6795) acc 77.5510 (84.8436) lr 1.5358e-03 eta 0:09:57
epoch [18/50] batch [15/51] time 0.184 (0.299) data 0.000 (0.117) loss 0.6364 (0.6399) acc 86.5385 (85.6097) lr 1.5358e-03 eta 0:08:19
epoch [18/50] batch [20/51] time 0.163 (0.266) data 0.001 (0.088) loss 0.5852 (0.6605) acc 88.8298 (85.2491) lr 1.5358e-03 eta 0:07:22
epoch [18/50] batch [25/51] time 0.159 (0.248) data 0.000 (0.070) loss 0.6922 (0.6543) acc 82.0652 (85.1721) lr 1.5358e-03 eta 0:06:50
epoch [18/50] batch [30/51] time 0.174 (0.236) data 0.000 (0.059) loss 0.6969 (0.6597) acc 81.0000 (85.0370) lr 1.5358e-03 eta 0:06:30
epoch [18/50] batch [35/51] time 0.189 (0.227) data 0.000 (0.050) loss 0.5713 (0.6474) acc 88.6792 (85.4021) lr 1.5358e-03 eta 0:06:14
epoch [18/50] batch [40/51] time 0.173 (0.220) data 0.000 (0.044) loss 0.6594 (0.6572) acc 83.1731 (85.0758) lr 1.5358e-03 eta 0:06:02
epoch [18/50] batch [45/51] time 0.161 (0.214) data 0.000 (0.039) loss 0.7487 (0.6513) acc 80.8511 (85.1028) lr 1.5358e-03 eta 0:05:51
epoch [18/50] batch [50/51] time 0.161 (0.210) data 0.000 (0.035) loss 0.7883 (0.6496) acc 81.9149 (84.9588) lr 1.5358e-03 eta 0:05:43
>>> alpha1: 0.183  alpha2: -0.061 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.16 <<<
epoch [19/50] batch [5/51] time 0.175 (0.463) data 0.000 (0.285) loss 0.5170 (0.5755) acc 89.4231 (88.3199) lr 1.4818e-03 eta 0:12:33
epoch [19/50] batch [10/51] time 0.173 (0.318) data 0.000 (0.143) loss 0.5622 (0.6203) acc 87.0192 (88.3651) lr 1.4818e-03 eta 0:08:36
epoch [19/50] batch [15/51] time 0.189 (0.270) data 0.000 (0.095) loss 2.6728 (0.7458) acc 62.7451 (86.4172) lr 1.4818e-03 eta 0:07:16
epoch [19/50] batch [20/51] time 0.162 (0.246) data 0.000 (0.071) loss 0.7399 (0.7773) acc 78.2609 (86.1943) lr 1.4818e-03 eta 0:06:35
epoch [19/50] batch [25/51] time 0.173 (0.231) data 0.000 (0.057) loss 0.5668 (0.7308) acc 88.7255 (86.6614) lr 1.4818e-03 eta 0:06:11
epoch [19/50] batch [30/51] time 0.174 (0.221) data 0.000 (0.048) loss 0.5477 (0.7188) acc 87.5000 (86.4688) lr 1.4818e-03 eta 0:05:54
epoch [19/50] batch [35/51] time 0.182 (0.215) data 0.001 (0.041) loss 0.6256 (0.6966) acc 83.1818 (86.4118) lr 1.4818e-03 eta 0:05:43
epoch [19/50] batch [40/51] time 0.166 (0.210) data 0.000 (0.036) loss 0.7763 (0.6928) acc 79.0698 (86.1700) lr 1.4818e-03 eta 0:05:34
epoch [19/50] batch [45/51] time 0.185 (0.206) data 0.000 (0.032) loss 0.7186 (0.6826) acc 86.2745 (86.3022) lr 1.4818e-03 eta 0:05:26
epoch [19/50] batch [50/51] time 0.174 (0.203) data 0.000 (0.029) loss 0.7803 (0.6802) acc 82.1429 (86.1278) lr 1.4818e-03 eta 0:05:21
>>> alpha1: 0.175  alpha2: -0.057 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [20/50] batch [5/51] time 0.181 (0.471) data 0.000 (0.299) loss 0.4863 (0.6709) acc 90.0000 (85.1484) lr 1.4258e-03 eta 0:12:22
epoch [20/50] batch [10/51] time 0.165 (0.323) data 0.000 (0.151) loss 0.5893 (0.6379) acc 90.1042 (87.0974) lr 1.4258e-03 eta 0:08:26
epoch [20/50] batch [15/51] time 0.171 (0.275) data 0.000 (0.102) loss 0.4073 (0.6154) acc 93.0000 (87.1508) lr 1.4258e-03 eta 0:07:10
epoch [20/50] batch [20/51] time 0.170 (0.250) data 0.000 (0.076) loss 0.6134 (0.6205) acc 85.5000 (86.7541) lr 1.4258e-03 eta 0:06:30
epoch [20/50] batch [25/51] time 0.186 (0.236) data 0.001 (0.061) loss 0.7966 (0.6370) acc 79.2553 (85.8896) lr 1.4258e-03 eta 0:06:06
epoch [20/50] batch [30/51] time 0.167 (0.225) data 0.000 (0.051) loss 0.5792 (0.6289) acc 85.9375 (85.8875) lr 1.4258e-03 eta 0:05:49
epoch [20/50] batch [35/51] time 0.198 (0.219) data 0.020 (0.044) loss 0.5834 (0.6239) acc 86.0577 (86.1666) lr 1.4258e-03 eta 0:05:38
epoch [20/50] batch [40/51] time 0.164 (0.213) data 0.000 (0.039) loss 0.6522 (0.6303) acc 81.7708 (85.8431) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.034) loss 0.5340 (0.6232) acc 88.2653 (85.9713) lr 1.4258e-03 eta 0:05:20
epoch [20/50] batch [50/51] time 0.172 (0.205) data 0.000 (0.031) loss 0.5315 (0.6135) acc 86.5385 (86.1172) lr 1.4258e-03 eta 0:05:14
>>> alpha1: 0.169  alpha2: -0.056 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [21/50] batch [5/51] time 0.185 (0.457) data 0.000 (0.272) loss 0.5424 (0.4879) acc 87.7551 (90.4857) lr 1.3681e-03 eta 0:11:36
epoch [21/50] batch [10/51] time 0.177 (0.315) data 0.000 (0.136) loss 0.7103 (0.5338) acc 82.6923 (88.2735) lr 1.3681e-03 eta 0:07:59
epoch [21/50] batch [15/51] time 0.176 (0.268) data 0.000 (0.091) loss 0.7856 (0.5628) acc 80.3191 (87.3422) lr 1.3681e-03 eta 0:06:46
epoch [21/50] batch [20/51] time 0.193 (0.246) data 0.000 (0.068) loss 0.7650 (0.5838) acc 80.5000 (86.6432) lr 1.3681e-03 eta 0:06:11
epoch [21/50] batch [25/51] time 0.195 (0.235) data 0.000 (0.055) loss 0.5131 (0.5778) acc 90.0000 (86.7512) lr 1.3681e-03 eta 0:05:53
epoch [21/50] batch [30/51] time 0.171 (0.225) data 0.000 (0.046) loss 0.6018 (0.5841) acc 86.4583 (86.5077) lr 1.3681e-03 eta 0:05:37
epoch [21/50] batch [35/51] time 0.167 (0.217) data 0.000 (0.039) loss 0.7196 (0.5899) acc 83.3333 (86.3693) lr 1.3681e-03 eta 0:05:24
epoch [21/50] batch [40/51] time 0.176 (0.212) data 0.000 (0.034) loss 0.5259 (0.5782) acc 91.9811 (86.9161) lr 1.3681e-03 eta 0:05:16
epoch [21/50] batch [45/51] time 0.165 (0.207) data 0.000 (0.031) loss 0.7178 (0.5766) acc 83.1633 (86.9358) lr 1.3681e-03 eta 0:05:07
epoch [21/50] batch [50/51] time 0.177 (0.203) data 0.000 (0.027) loss 0.4749 (0.5790) acc 90.5660 (86.8567) lr 1.3681e-03 eta 0:05:00
>>> alpha1: 0.163  alpha2: -0.061 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [22/50] batch [5/51] time 1.135 (0.679) data 0.000 (0.299) loss 0.4383 (0.4504) acc 90.6780 (89.3978) lr 1.3090e-03 eta 0:16:41
epoch [22/50] batch [10/51] time 0.166 (0.423) data 0.000 (0.150) loss 0.5585 (0.5226) acc 86.9792 (88.2344) lr 1.3090e-03 eta 0:10:20
epoch [22/50] batch [15/51] time 0.171 (0.340) data 0.001 (0.100) loss 0.4392 (0.5243) acc 90.3061 (89.0111) lr 1.3090e-03 eta 0:08:17
epoch [22/50] batch [20/51] time 0.181 (0.299) data 0.000 (0.075) loss 0.4991 (0.5297) acc 87.0370 (88.6216) lr 1.3090e-03 eta 0:07:16
epoch [22/50] batch [25/51] time 0.178 (0.274) data 0.000 (0.060) loss 0.4552 (0.5443) acc 90.3846 (88.1453) lr 1.3090e-03 eta 0:06:37
epoch [22/50] batch [30/51] time 0.163 (0.257) data 0.000 (0.050) loss 0.6341 (0.5537) acc 88.5870 (88.0740) lr 1.3090e-03 eta 0:06:12
epoch [22/50] batch [35/51] time 0.151 (0.244) data 0.000 (0.043) loss 0.5895 (0.5567) acc 85.1190 (87.5836) lr 1.3090e-03 eta 0:05:52
epoch [22/50] batch [40/51] time 0.162 (0.235) data 0.001 (0.038) loss 0.6917 (0.5709) acc 85.3261 (87.4011) lr 1.3090e-03 eta 0:05:37
epoch [22/50] batch [45/51] time 0.169 (0.227) data 0.000 (0.034) loss 0.3160 (0.5740) acc 95.0980 (87.0956) lr 1.3090e-03 eta 0:05:25
epoch [22/50] batch [50/51] time 0.187 (0.221) data 0.000 (0.030) loss 0.3710 (0.5712) acc 93.4211 (87.1913) lr 1.3090e-03 eta 0:05:16
>>> alpha1: 0.161  alpha2: -0.068 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [23/50] batch [5/51] time 0.167 (0.465) data 0.000 (0.288) loss 0.5090 (0.5406) acc 89.5833 (89.1256) lr 1.2487e-03 eta 0:11:01
epoch [23/50] batch [10/51] time 0.164 (0.320) data 0.000 (0.144) loss 0.5345 (0.5234) acc 91.1458 (89.1427) lr 1.2487e-03 eta 0:07:34
epoch [23/50] batch [15/51] time 0.168 (0.270) data 0.000 (0.096) loss 0.7270 (0.6980) acc 83.6735 (86.4378) lr 1.2487e-03 eta 0:06:21
epoch [23/50] batch [20/51] time 0.174 (0.247) data 0.000 (0.072) loss 0.8295 (0.6944) acc 79.2553 (85.9478) lr 1.2487e-03 eta 0:05:48
epoch [23/50] batch [25/51] time 0.186 (0.234) data 0.000 (0.058) loss 0.6212 (0.6566) acc 82.4561 (86.4459) lr 1.2487e-03 eta 0:05:27
epoch [23/50] batch [30/51] time 0.164 (0.223) data 0.000 (0.048) loss 0.6178 (0.6398) acc 86.7021 (86.4773) lr 1.2487e-03 eta 0:05:11
epoch [23/50] batch [35/51] time 0.163 (0.217) data 0.000 (0.041) loss 0.7805 (0.6328) acc 80.8511 (86.4456) lr 1.2487e-03 eta 0:05:01
epoch [23/50] batch [40/51] time 0.171 (0.211) data 0.000 (0.036) loss 0.4097 (0.6217) acc 90.6863 (86.4420) lr 1.2487e-03 eta 0:04:53
epoch [23/50] batch [45/51] time 0.166 (0.206) data 0.000 (0.032) loss 0.7339 (0.6258) acc 83.6735 (86.3552) lr 1.2487e-03 eta 0:04:44
epoch [23/50] batch [50/51] time 0.167 (0.202) data 0.000 (0.029) loss 0.5899 (0.6148) acc 87.7551 (86.5957) lr 1.2487e-03 eta 0:04:38
>>> alpha1: 0.156  alpha2: -0.068 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [24/50] batch [5/51] time 0.180 (0.435) data 0.000 (0.251) loss 0.7858 (0.5818) acc 84.0000 (86.8701) lr 1.1874e-03 eta 0:09:56
epoch [24/50] batch [10/51] time 0.171 (0.305) data 0.000 (0.126) loss 0.1831 (0.5289) acc 99.0000 (88.2422) lr 1.1874e-03 eta 0:06:57
epoch [24/50] batch [15/51] time 0.197 (0.266) data 0.001 (0.084) loss 0.5425 (0.5531) acc 89.2857 (87.5651) lr 1.1874e-03 eta 0:06:01
epoch [24/50] batch [20/51] time 0.183 (0.248) data 0.000 (0.063) loss 0.3931 (0.5403) acc 92.3469 (87.9893) lr 1.1874e-03 eta 0:05:36
epoch [24/50] batch [25/51] time 0.189 (0.236) data 0.000 (0.051) loss 0.5751 (0.5449) acc 89.0000 (88.1349) lr 1.1874e-03 eta 0:05:18
epoch [24/50] batch [30/51] time 0.196 (0.229) data 0.000 (0.042) loss 0.4797 (0.5453) acc 89.2857 (87.9569) lr 1.1874e-03 eta 0:05:09
epoch [24/50] batch [35/51] time 0.188 (0.223) data 0.000 (0.036) loss 0.5182 (0.5343) acc 89.7959 (88.1918) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [40/51] time 0.198 (0.219) data 0.000 (0.032) loss 0.4709 (0.5439) acc 89.8148 (88.0364) lr 1.1874e-03 eta 0:04:52
epoch [24/50] batch [45/51] time 0.200 (0.216) data 0.000 (0.028) loss 0.4292 (0.5389) acc 90.2778 (88.0137) lr 1.1874e-03 eta 0:04:47
epoch [24/50] batch [50/51] time 0.184 (0.213) data 0.001 (0.026) loss 0.5692 (0.5431) acc 86.4583 (87.9517) lr 1.1874e-03 eta 0:04:42
>>> alpha1: 0.150  alpha2: -0.065 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [25/50] batch [5/51] time 0.166 (0.432) data 0.000 (0.257) loss 0.7211 (0.6116) acc 84.8958 (85.8703) lr 1.1253e-03 eta 0:09:31
epoch [25/50] batch [10/51] time 0.180 (0.302) data 0.000 (0.129) loss 0.6148 (0.5679) acc 87.0370 (87.3162) lr 1.1253e-03 eta 0:06:37
epoch [25/50] batch [15/51] time 0.163 (0.256) data 0.000 (0.086) loss 0.4959 (0.5727) acc 91.4894 (87.6313) lr 1.1253e-03 eta 0:05:35
epoch [25/50] batch [20/51] time 0.179 (0.237) data 0.000 (0.065) loss 0.4781 (0.5537) acc 90.0943 (87.7943) lr 1.1253e-03 eta 0:05:09
epoch [25/50] batch [25/51] time 0.167 (0.223) data 0.000 (0.052) loss 0.5728 (0.5569) acc 89.7959 (87.9939) lr 1.1253e-03 eta 0:04:50
epoch [25/50] batch [30/51] time 0.182 (0.215) data 0.000 (0.043) loss 0.3875 (0.5589) acc 91.6667 (88.0452) lr 1.1253e-03 eta 0:04:38
epoch [25/50] batch [35/51] time 0.178 (0.209) data 0.000 (0.037) loss 0.4334 (0.5559) acc 92.9245 (88.1518) lr 1.1253e-03 eta 0:04:29
epoch [25/50] batch [40/51] time 0.166 (0.205) data 0.000 (0.032) loss 0.5107 (0.5968) acc 88.0208 (87.4998) lr 1.1253e-03 eta 0:04:23
epoch [25/50] batch [45/51] time 0.166 (0.201) data 0.000 (0.029) loss 0.3939 (0.5782) acc 91.6667 (87.8529) lr 1.1253e-03 eta 0:04:17
epoch [25/50] batch [50/51] time 0.173 (0.198) data 0.000 (0.026) loss 0.5369 (0.5727) acc 87.5000 (87.7971) lr 1.1253e-03 eta 0:04:13
>>> alpha1: 0.146  alpha2: -0.069 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [26/50] batch [5/51] time 0.169 (0.462) data 0.002 (0.276) loss 0.5403 (0.4604) acc 90.1042 (90.8980) lr 1.0628e-03 eta 0:09:46
epoch [26/50] batch [10/51] time 0.166 (0.318) data 0.000 (0.138) loss 0.6359 (0.4995) acc 87.5000 (89.7152) lr 1.0628e-03 eta 0:06:42
epoch [26/50] batch [15/51] time 0.171 (0.269) data 0.000 (0.092) loss 0.5364 (0.5128) acc 86.1702 (89.2024) lr 1.0628e-03 eta 0:05:39
epoch [26/50] batch [20/51] time 0.194 (0.245) data 0.001 (0.069) loss 0.5595 (0.5326) acc 87.7451 (88.5884) lr 1.0628e-03 eta 0:05:07
epoch [26/50] batch [25/51] time 0.161 (0.231) data 0.000 (0.055) loss 0.5057 (0.5379) acc 90.7609 (88.4952) lr 1.0628e-03 eta 0:04:49
epoch [26/50] batch [30/51] time 0.176 (0.222) data 0.000 (0.046) loss 0.5750 (0.5377) acc 87.5000 (88.4614) lr 1.0628e-03 eta 0:04:36
epoch [26/50] batch [35/51] time 0.183 (0.216) data 0.000 (0.040) loss 0.3887 (0.5279) acc 91.5000 (88.6143) lr 1.0628e-03 eta 0:04:27
epoch [26/50] batch [40/51] time 0.177 (0.210) data 0.001 (0.035) loss 0.5108 (0.5222) acc 85.3774 (88.5516) lr 1.0628e-03 eta 0:04:19
epoch [26/50] batch [45/51] time 0.157 (0.206) data 0.000 (0.031) loss 0.5528 (0.5262) acc 88.6364 (88.4933) lr 1.0628e-03 eta 0:04:12
epoch [26/50] batch [50/51] time 0.181 (0.202) data 0.000 (0.028) loss 0.4861 (0.5323) acc 90.8654 (88.3301) lr 1.0628e-03 eta 0:04:07
>>> alpha1: 0.145  alpha2: -0.062 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [27/50] batch [5/51] time 0.171 (0.455) data 0.000 (0.279) loss 0.4392 (0.6114) acc 90.6863 (87.7531) lr 1.0000e-03 eta 0:09:14
epoch [27/50] batch [10/51] time 0.171 (0.311) data 0.000 (0.139) loss 0.5691 (0.6027) acc 87.5000 (87.6486) lr 1.0000e-03 eta 0:06:17
epoch [27/50] batch [15/51] time 0.170 (0.267) data 0.000 (0.093) loss 0.4938 (0.5609) acc 87.0000 (88.2682) lr 1.0000e-03 eta 0:05:22
epoch [27/50] batch [20/51] time 0.173 (0.243) data 0.000 (0.070) loss 0.4048 (0.5385) acc 88.7255 (88.6288) lr 1.0000e-03 eta 0:04:52
epoch [27/50] batch [25/51] time 0.177 (0.229) data 0.000 (0.056) loss 0.4859 (0.5405) acc 88.2075 (88.2921) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [30/51] time 0.190 (0.219) data 0.000 (0.047) loss 0.5212 (0.5397) acc 86.7021 (88.1042) lr 1.0000e-03 eta 0:04:21
epoch [27/50] batch [35/51] time 0.168 (0.214) data 0.000 (0.040) loss 0.5900 (0.5405) acc 83.6735 (88.1143) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [40/51] time 0.170 (0.209) data 0.000 (0.035) loss 0.3966 (0.5369) acc 91.5000 (88.2512) lr 1.0000e-03 eta 0:04:07
epoch [27/50] batch [45/51] time 0.167 (0.204) data 0.000 (0.031) loss 0.6048 (0.5384) acc 87.7551 (88.0822) lr 1.0000e-03 eta 0:04:01
epoch [27/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.028) loss 0.7447 (0.5370) acc 83.5000 (88.2198) lr 1.0000e-03 eta 0:03:55
>>> alpha1: 0.142  alpha2: -0.062 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [28/50] batch [5/51] time 0.182 (0.501) data 0.001 (0.310) loss 0.6586 (0.5092) acc 81.2500 (87.5392) lr 9.3721e-04 eta 0:09:45
epoch [28/50] batch [10/51] time 0.181 (0.339) data 0.000 (0.155) loss 0.4814 (0.4957) acc 88.2353 (88.4853) lr 9.3721e-04 eta 0:06:33
epoch [28/50] batch [15/51] time 0.189 (0.284) data 0.000 (0.103) loss 0.5559 (0.4858) acc 88.0000 (88.9755) lr 9.3721e-04 eta 0:05:28
epoch [28/50] batch [20/51] time 0.171 (0.258) data 0.001 (0.078) loss 0.4754 (0.4822) acc 89.0000 (88.9169) lr 9.3721e-04 eta 0:04:57
epoch [28/50] batch [25/51] time 0.172 (0.240) data 0.000 (0.062) loss 0.5361 (0.4818) acc 88.5417 (89.0416) lr 9.3721e-04 eta 0:04:35
epoch [28/50] batch [30/51] time 0.192 (0.230) data 0.000 (0.052) loss 0.5203 (0.4876) acc 93.3962 (88.9812) lr 9.3721e-04 eta 0:04:22
epoch [28/50] batch [35/51] time 0.175 (0.222) data 0.000 (0.044) loss 0.4533 (0.5005) acc 94.2708 (88.8747) lr 9.3721e-04 eta 0:04:12
epoch [28/50] batch [40/51] time 0.161 (0.215) data 0.000 (0.039) loss 0.4199 (0.4989) acc 89.3617 (88.9146) lr 9.3721e-04 eta 0:04:03
epoch [28/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.035) loss 0.5139 (0.4937) acc 87.2549 (89.0438) lr 9.3721e-04 eta 0:03:56
epoch [28/50] batch [50/51] time 0.181 (0.206) data 0.000 (0.031) loss 0.6109 (0.4873) acc 87.7273 (89.2515) lr 9.3721e-04 eta 0:03:51
>>> alpha1: 0.142  alpha2: -0.060 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [29/50] batch [5/51] time 0.172 (0.448) data 0.000 (0.271) loss 0.4121 (0.4395) acc 90.0000 (90.0008) lr 8.7467e-04 eta 0:08:20
epoch [29/50] batch [10/51] time 0.162 (0.308) data 0.000 (0.136) loss 0.6970 (0.4588) acc 84.4445 (89.9377) lr 8.7467e-04 eta 0:05:42
epoch [29/50] batch [15/51] time 0.175 (0.261) data 0.000 (0.091) loss 0.4073 (0.4883) acc 88.9423 (89.2015) lr 8.7467e-04 eta 0:04:49
epoch [29/50] batch [20/51] time 0.177 (0.239) data 0.000 (0.068) loss 0.8034 (0.4833) acc 79.1667 (89.1826) lr 8.7467e-04 eta 0:04:23
epoch [29/50] batch [25/51] time 0.188 (0.226) data 0.000 (0.054) loss 0.5319 (0.4956) acc 86.7924 (88.8110) lr 8.7467e-04 eta 0:04:08
epoch [29/50] batch [30/51] time 0.176 (0.217) data 0.000 (0.045) loss 0.4106 (0.4997) acc 89.1509 (88.5434) lr 8.7467e-04 eta 0:03:57
epoch [29/50] batch [35/51] time 0.158 (0.211) data 0.000 (0.039) loss 0.4156 (0.4950) acc 90.5556 (88.6740) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [40/51] time 0.161 (0.206) data 0.000 (0.034) loss 0.6745 (0.4997) acc 84.0425 (88.7272) lr 8.7467e-04 eta 0:03:42
epoch [29/50] batch [45/51] time 0.164 (0.201) data 0.000 (0.030) loss 0.3365 (0.4990) acc 95.3125 (88.9016) lr 8.7467e-04 eta 0:03:36
epoch [29/50] batch [50/51] time 0.173 (0.198) data 0.000 (0.027) loss 0.4341 (0.5069) acc 91.8269 (88.6566) lr 8.7467e-04 eta 0:03:31
>>> alpha1: 0.143  alpha2: -0.063 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [30/50] batch [5/51] time 0.170 (0.430) data 0.000 (0.252) loss 0.5893 (0.5009) acc 85.0000 (89.7048) lr 8.1262e-04 eta 0:07:38
epoch [30/50] batch [10/51] time 0.167 (0.302) data 0.000 (0.126) loss 0.4101 (0.4944) acc 93.3673 (89.5298) lr 8.1262e-04 eta 0:05:20
epoch [30/50] batch [15/51] time 0.226 (0.264) data 0.001 (0.084) loss 0.3493 (0.4847) acc 89.9123 (89.3638) lr 8.1262e-04 eta 0:04:39
epoch [30/50] batch [20/51] time 0.171 (0.243) data 0.000 (0.063) loss 0.5480 (0.4857) acc 88.2353 (89.2764) lr 8.1262e-04 eta 0:04:15
epoch [30/50] batch [25/51] time 0.164 (0.230) data 0.000 (0.051) loss 0.4201 (0.4934) acc 90.9574 (89.1481) lr 8.1262e-04 eta 0:04:00
epoch [30/50] batch [30/51] time 0.174 (0.220) data 0.000 (0.043) loss 0.5577 (0.4979) acc 89.4231 (89.1862) lr 8.1262e-04 eta 0:03:49
epoch [30/50] batch [35/51] time 0.171 (0.214) data 0.000 (0.037) loss 0.3471 (0.4883) acc 94.1176 (89.3941) lr 8.1262e-04 eta 0:03:42
epoch [30/50] batch [40/51] time 0.168 (0.209) data 0.000 (0.032) loss 0.6499 (0.4912) acc 87.0000 (89.0536) lr 8.1262e-04 eta 0:03:35
epoch [30/50] batch [45/51] time 0.178 (0.204) data 0.000 (0.029) loss 0.4748 (0.4890) acc 87.5000 (89.0297) lr 8.1262e-04 eta 0:03:29
epoch [30/50] batch [50/51] time 0.163 (0.200) data 0.000 (0.026) loss 0.5968 (0.4906) acc 85.1064 (89.0607) lr 8.1262e-04 eta 0:03:24
>>> alpha1: 0.146  alpha2: -0.063 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [31/50] batch [5/51] time 0.170 (0.484) data 0.000 (0.310) loss 0.5510 (0.4594) acc 87.0000 (88.4045) lr 7.5131e-04 eta 0:08:11
epoch [31/50] batch [10/51] time 0.181 (0.331) data 0.000 (0.155) loss 0.4769 (0.4620) acc 89.5000 (89.2142) lr 7.5131e-04 eta 0:05:34
epoch [31/50] batch [15/51] time 0.165 (0.280) data 0.000 (0.103) loss 0.5832 (0.4725) acc 89.0625 (89.6677) lr 7.5131e-04 eta 0:04:41
epoch [31/50] batch [20/51] time 0.774 (0.283) data 0.000 (0.078) loss 0.5658 (0.4638) acc 89.0244 (89.9424) lr 7.5131e-04 eta 0:04:43
epoch [31/50] batch [25/51] time 0.168 (0.261) data 0.000 (0.062) loss 0.4465 (0.4567) acc 89.2857 (90.1473) lr 7.5131e-04 eta 0:04:19
epoch [31/50] batch [30/51] time 0.184 (0.247) data 0.000 (0.052) loss 0.4422 (0.4647) acc 94.6078 (90.1095) lr 7.5131e-04 eta 0:04:04
epoch [31/50] batch [35/51] time 0.163 (0.236) data 0.000 (0.044) loss 0.5693 (0.4739) acc 88.2979 (89.8415) lr 7.5131e-04 eta 0:03:52
epoch [31/50] batch [40/51] time 0.169 (0.228) data 0.000 (0.039) loss 0.4214 (0.4806) acc 90.6863 (89.5959) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [45/51] time 0.167 (0.222) data 0.000 (0.035) loss 0.4156 (0.4773) acc 93.0000 (89.5831) lr 7.5131e-04 eta 0:03:36
epoch [31/50] batch [50/51] time 0.181 (0.218) data 0.000 (0.031) loss 0.4581 (0.4817) acc 94.2308 (89.6877) lr 7.5131e-04 eta 0:03:31
>>> alpha1: 0.143  alpha2: -0.068 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [32/50] batch [5/51] time 0.178 (0.448) data 0.001 (0.260) loss 0.4254 (0.4259) acc 90.0943 (91.3486) lr 6.9098e-04 eta 0:07:12
epoch [32/50] batch [10/51] time 0.168 (0.313) data 0.000 (0.130) loss 0.4837 (0.4454) acc 88.2653 (90.3439) lr 6.9098e-04 eta 0:05:00
epoch [32/50] batch [15/51] time 0.172 (0.265) data 0.000 (0.087) loss 0.4254 (0.4613) acc 90.1961 (89.6431) lr 6.9098e-04 eta 0:04:13
epoch [32/50] batch [20/51] time 0.166 (0.243) data 0.000 (0.065) loss 0.4857 (0.4787) acc 90.4255 (89.5612) lr 6.9098e-04 eta 0:03:50
epoch [32/50] batch [25/51] time 0.168 (0.229) data 0.000 (0.052) loss 0.4714 (0.4718) acc 89.8936 (89.7938) lr 6.9098e-04 eta 0:03:36
epoch [32/50] batch [30/51] time 0.178 (0.221) data 0.000 (0.044) loss 0.3257 (0.4708) acc 93.3962 (89.8956) lr 6.9098e-04 eta 0:03:27
epoch [32/50] batch [35/51] time 0.176 (0.215) data 0.000 (0.037) loss 0.3324 (0.4645) acc 93.5000 (90.1651) lr 6.9098e-04 eta 0:03:20
epoch [32/50] batch [40/51] time 0.169 (0.210) data 0.000 (0.033) loss 0.4795 (0.4661) acc 91.0000 (90.1022) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [45/51] time 0.160 (0.205) data 0.001 (0.029) loss 0.5018 (0.4721) acc 89.4445 (90.0696) lr 6.9098e-04 eta 0:03:09
epoch [32/50] batch [50/51] time 0.170 (0.202) data 0.000 (0.026) loss 0.5349 (0.4727) acc 85.2941 (89.9559) lr 6.9098e-04 eta 0:03:05
>>> alpha1: 0.142  alpha2: -0.068 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [33/50] batch [5/51] time 0.167 (0.461) data 0.001 (0.286) loss 0.6788 (0.5115) acc 83.8542 (88.1635) lr 6.3188e-04 eta 0:07:00
epoch [33/50] batch [10/51] time 0.158 (0.315) data 0.000 (0.143) loss 0.6485 (0.5126) acc 84.8837 (88.4390) lr 6.3188e-04 eta 0:04:46
epoch [33/50] batch [15/51] time 0.167 (0.269) data 0.000 (0.096) loss 0.6597 (0.5044) acc 81.1225 (88.5150) lr 6.3188e-04 eta 0:04:02
epoch [33/50] batch [20/51] time 0.171 (0.247) data 0.000 (0.072) loss 0.3703 (0.4769) acc 92.1569 (89.1374) lr 6.3188e-04 eta 0:03:41
epoch [33/50] batch [25/51] time 0.176 (0.233) data 0.000 (0.057) loss 0.4215 (0.4748) acc 89.1509 (89.2159) lr 6.3188e-04 eta 0:03:28
epoch [33/50] batch [30/51] time 0.189 (0.225) data 0.000 (0.048) loss 0.4058 (0.4735) acc 89.1509 (89.2708) lr 6.3188e-04 eta 0:03:19
epoch [33/50] batch [35/51] time 0.176 (0.218) data 0.000 (0.041) loss 0.4978 (0.4871) acc 92.1875 (89.0412) lr 6.3188e-04 eta 0:03:12
epoch [33/50] batch [40/51] time 0.168 (0.212) data 0.000 (0.036) loss 0.5523 (0.4854) acc 84.0000 (89.0161) lr 6.3188e-04 eta 0:03:06
epoch [33/50] batch [45/51] time 0.179 (0.207) data 0.000 (0.032) loss 0.4923 (0.4868) acc 89.0909 (89.0564) lr 6.3188e-04 eta 0:03:01
epoch [33/50] batch [50/51] time 0.165 (0.204) data 0.000 (0.029) loss 0.4028 (0.4931) acc 91.8367 (88.9636) lr 6.3188e-04 eta 0:02:56
>>> alpha1: 0.138  alpha2: -0.060 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [34/50] batch [5/51] time 0.175 (0.435) data 0.000 (0.262) loss 0.4148 (0.5200) acc 93.2692 (88.9464) lr 5.7422e-04 eta 0:06:15
epoch [34/50] batch [10/51] time 0.179 (0.305) data 0.000 (0.131) loss 0.5180 (0.5004) acc 87.7451 (89.1523) lr 5.7422e-04 eta 0:04:21
epoch [34/50] batch [15/51] time 0.175 (0.263) data 0.001 (0.087) loss 0.6651 (0.5066) acc 82.1429 (88.8300) lr 5.7422e-04 eta 0:03:44
epoch [34/50] batch [20/51] time 0.167 (0.242) data 0.000 (0.066) loss 0.5416 (0.4984) acc 89.7959 (88.9648) lr 5.7422e-04 eta 0:03:25
epoch [34/50] batch [25/51] time 0.169 (0.228) data 0.001 (0.053) loss 0.4291 (0.4801) acc 92.5000 (89.4742) lr 5.7422e-04 eta 0:03:11
epoch [34/50] batch [30/51] time 0.165 (0.219) data 0.000 (0.044) loss 0.4644 (0.4847) acc 90.6250 (89.4688) lr 5.7422e-04 eta 0:03:03
epoch [34/50] batch [35/51] time 0.172 (0.212) data 0.000 (0.038) loss 0.5065 (0.4795) acc 89.5000 (89.7214) lr 5.7422e-04 eta 0:02:56
epoch [34/50] batch [40/51] time 0.177 (0.208) data 0.000 (0.033) loss 0.3587 (0.4704) acc 91.9811 (89.8413) lr 5.7422e-04 eta 0:02:51
epoch [34/50] batch [45/51] time 0.162 (0.204) data 0.000 (0.029) loss 0.6134 (0.4738) acc 84.2391 (89.6752) lr 5.7422e-04 eta 0:02:47
epoch [34/50] batch [50/51] time 0.161 (0.200) data 0.000 (0.026) loss 0.4738 (0.4756) acc 87.7660 (89.6244) lr 5.7422e-04 eta 0:02:43
>>> alpha1: 0.136  alpha2: -0.059 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [35/50] batch [5/51] time 0.172 (0.450) data 0.000 (0.260) loss 0.5151 (0.4592) acc 88.2353 (89.2893) lr 5.1825e-04 eta 0:06:05
epoch [35/50] batch [10/51] time 0.179 (0.314) data 0.000 (0.130) loss 0.2602 (0.4317) acc 95.3704 (90.2489) lr 5.1825e-04 eta 0:04:13
epoch [35/50] batch [15/51] time 0.195 (0.269) data 0.000 (0.087) loss 0.4347 (0.4306) acc 95.4546 (90.7673) lr 5.1825e-04 eta 0:03:35
epoch [35/50] batch [20/51] time 0.198 (0.248) data 0.001 (0.065) loss 0.4781 (0.4281) acc 92.5926 (90.8028) lr 5.1825e-04 eta 0:03:17
epoch [35/50] batch [25/51] time 0.195 (0.234) data 0.000 (0.052) loss 0.5349 (0.4384) acc 86.4130 (90.5712) lr 5.1825e-04 eta 0:03:05
epoch [35/50] batch [30/51] time 0.190 (0.224) data 0.000 (0.044) loss 0.5686 (0.4556) acc 89.0000 (90.0565) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.197 (0.218) data 0.000 (0.037) loss 0.2854 (0.4582) acc 96.0784 (90.0243) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [40/51] time 0.159 (0.212) data 0.000 (0.033) loss 0.6727 (0.4656) acc 86.9565 (89.7244) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [45/51] time 0.169 (0.207) data 0.000 (0.029) loss 0.5100 (0.4676) acc 89.0000 (89.7278) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [50/51] time 0.164 (0.203) data 0.000 (0.026) loss 0.4816 (0.4673) acc 86.4583 (89.7764) lr 5.1825e-04 eta 0:02:35
>>> alpha1: 0.134  alpha2: -0.056 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [36/50] batch [5/51] time 0.195 (0.472) data 0.000 (0.281) loss 0.4341 (0.4279) acc 88.0000 (90.4384) lr 4.6417e-04 eta 0:05:58
epoch [36/50] batch [10/51] time 0.192 (0.336) data 0.000 (0.141) loss 0.3435 (0.4431) acc 94.2308 (90.8166) lr 4.6417e-04 eta 0:04:13
epoch [36/50] batch [15/51] time 0.189 (0.288) data 0.001 (0.094) loss 0.5479 (0.4524) acc 87.7551 (90.0035) lr 4.6417e-04 eta 0:03:36
epoch [36/50] batch [20/51] time 0.182 (0.264) data 0.000 (0.070) loss 0.5167 (0.4496) acc 90.2174 (90.0315) lr 4.6417e-04 eta 0:03:16
epoch [36/50] batch [25/51] time 0.183 (0.248) data 0.000 (0.056) loss 0.6348 (0.4488) acc 88.7755 (90.2030) lr 4.6417e-04 eta 0:03:03
epoch [36/50] batch [30/51] time 0.178 (0.236) data 0.000 (0.047) loss 0.5539 (0.4644) acc 87.0192 (90.0117) lr 4.6417e-04 eta 0:02:53
epoch [36/50] batch [35/51] time 0.176 (0.228) data 0.000 (0.040) loss 0.4411 (0.4658) acc 92.1569 (89.9501) lr 4.6417e-04 eta 0:02:46
epoch [36/50] batch [40/51] time 0.180 (0.221) data 0.000 (0.035) loss 0.3317 (0.4566) acc 94.0909 (90.3317) lr 4.6417e-04 eta 0:02:40
epoch [36/50] batch [45/51] time 0.195 (0.216) data 0.000 (0.031) loss 0.4741 (0.4635) acc 88.2075 (90.1393) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [50/51] time 0.169 (0.211) data 0.001 (0.028) loss 0.5096 (0.4677) acc 89.0000 (90.0496) lr 4.6417e-04 eta 0:02:31
>>> alpha1: 0.135  alpha2: -0.056 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.13 <<<
epoch [37/50] batch [5/51] time 0.165 (0.443) data 0.000 (0.266) loss 0.4096 (0.4870) acc 92.7083 (89.6860) lr 4.1221e-04 eta 0:05:14
epoch [37/50] batch [10/51] time 0.166 (0.313) data 0.000 (0.134) loss 0.6201 (0.4788) acc 86.4583 (89.5771) lr 4.1221e-04 eta 0:03:40
epoch [37/50] batch [15/51] time 0.165 (0.267) data 0.000 (0.090) loss 0.3387 (0.4400) acc 92.7083 (90.3281) lr 4.1221e-04 eta 0:03:06
epoch [37/50] batch [20/51] time 0.181 (0.244) data 0.000 (0.067) loss 0.4452 (0.4535) acc 91.5094 (90.3659) lr 4.1221e-04 eta 0:02:49
epoch [37/50] batch [25/51] time 0.167 (0.231) data 0.000 (0.054) loss 0.4447 (0.4613) acc 91.4894 (89.9055) lr 4.1221e-04 eta 0:02:39
epoch [37/50] batch [30/51] time 0.169 (0.223) data 0.000 (0.045) loss 0.5228 (0.4562) acc 90.4255 (90.2339) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [35/51] time 0.187 (0.218) data 0.000 (0.039) loss 0.5089 (0.4614) acc 87.5000 (89.7751) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [40/51] time 0.164 (0.212) data 0.000 (0.034) loss 0.5917 (0.4707) acc 85.4167 (89.4832) lr 4.1221e-04 eta 0:02:22
epoch [37/50] batch [45/51] time 0.179 (0.208) data 0.000 (0.030) loss 0.4981 (0.4661) acc 89.8148 (89.5940) lr 4.1221e-04 eta 0:02:18
epoch [37/50] batch [50/51] time 0.171 (0.204) data 0.000 (0.027) loss 0.4501 (0.4655) acc 90.1961 (89.6116) lr 4.1221e-04 eta 0:02:15
>>> alpha1: 0.135  alpha2: -0.055 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [38/50] batch [5/51] time 0.171 (0.452) data 0.000 (0.275) loss 0.3482 (0.4383) acc 92.5000 (91.3556) lr 3.6258e-04 eta 0:04:57
epoch [38/50] batch [10/51] time 0.180 (0.318) data 0.000 (0.138) loss 0.3119 (0.4296) acc 96.7593 (91.9830) lr 3.6258e-04 eta 0:03:27
epoch [38/50] batch [15/51] time 0.193 (0.272) data 0.001 (0.092) loss 0.4456 (0.4493) acc 90.3846 (91.0448) lr 3.6258e-04 eta 0:02:56
epoch [38/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.069) loss 0.4110 (0.4739) acc 93.1373 (90.2755) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [25/51] time 0.177 (0.233) data 0.000 (0.055) loss 0.4988 (0.4846) acc 90.0000 (89.8001) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.188 (0.223) data 0.000 (0.046) loss 0.4564 (0.4807) acc 89.8148 (89.7996) lr 3.6258e-04 eta 0:02:21
epoch [38/50] batch [35/51] time 0.164 (0.216) data 0.000 (0.040) loss 0.4881 (0.4671) acc 91.1458 (90.1867) lr 3.6258e-04 eta 0:02:15
epoch [38/50] batch [40/51] time 0.171 (0.211) data 0.000 (0.035) loss 0.4336 (0.4649) acc 89.4231 (90.0901) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [45/51] time 0.165 (0.206) data 0.000 (0.031) loss 0.2495 (0.4617) acc 96.4286 (90.2256) lr 3.6258e-04 eta 0:02:07
epoch [38/50] batch [50/51] time 0.175 (0.202) data 0.000 (0.028) loss 0.4195 (0.4680) acc 90.5660 (90.0387) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.137  alpha2: -0.057 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [39/50] batch [5/51] time 0.198 (0.459) data 0.000 (0.279) loss 0.4192 (0.4176) acc 91.9811 (92.3526) lr 3.1545e-04 eta 0:04:38
epoch [39/50] batch [10/51] time 0.179 (0.316) data 0.000 (0.140) loss 0.5491 (0.4726) acc 88.8889 (90.2121) lr 3.1545e-04 eta 0:03:09
epoch [39/50] batch [15/51] time 0.177 (0.269) data 0.000 (0.093) loss 0.3550 (0.4565) acc 94.0000 (90.7218) lr 3.1545e-04 eta 0:02:40
epoch [39/50] batch [20/51] time 0.176 (0.245) data 0.000 (0.070) loss 0.3533 (0.4392) acc 94.8980 (91.4728) lr 3.1545e-04 eta 0:02:24
epoch [39/50] batch [25/51] time 0.182 (0.231) data 0.000 (0.056) loss 0.5822 (0.4496) acc 85.7843 (91.0077) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [30/51] time 0.151 (0.222) data 0.001 (0.047) loss 0.3728 (0.4682) acc 93.9024 (90.9974) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [35/51] time 0.186 (0.216) data 0.000 (0.040) loss 0.4413 (0.4602) acc 95.0980 (91.1209) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [40/51] time 0.167 (0.211) data 0.000 (0.035) loss 0.4022 (0.4604) acc 88.5000 (90.9119) lr 3.1545e-04 eta 0:02:00
epoch [39/50] batch [45/51] time 0.176 (0.206) data 0.000 (0.031) loss 0.6526 (0.4668) acc 86.3208 (90.3958) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [50/51] time 0.165 (0.202) data 0.000 (0.028) loss 0.4821 (0.4631) acc 87.7551 (90.3725) lr 3.1545e-04 eta 0:01:53
>>> alpha1: 0.139  alpha2: -0.057 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [40/50] batch [5/51] time 0.172 (0.442) data 0.000 (0.265) loss 0.3986 (0.3976) acc 91.6667 (92.2929) lr 2.7103e-04 eta 0:04:05
epoch [40/50] batch [10/51] time 0.188 (0.309) data 0.000 (0.132) loss 0.3768 (0.4268) acc 90.5000 (90.9718) lr 2.7103e-04 eta 0:02:50
epoch [40/50] batch [15/51] time 0.175 (0.263) data 0.001 (0.088) loss 0.5413 (0.4637) acc 84.0000 (89.4957) lr 2.7103e-04 eta 0:02:23
epoch [40/50] batch [20/51] time 0.181 (0.241) data 0.000 (0.066) loss 0.3416 (0.4483) acc 93.5185 (89.9471) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [25/51] time 0.177 (0.229) data 0.000 (0.053) loss 0.5325 (0.4449) acc 88.9423 (90.2535) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [30/51] time 0.180 (0.220) data 0.000 (0.044) loss 0.2810 (0.4385) acc 97.2727 (90.4406) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [35/51] time 0.173 (0.212) data 0.000 (0.038) loss 0.5834 (0.4439) acc 91.4894 (90.4757) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [40/51] time 0.164 (0.207) data 0.000 (0.033) loss 0.4998 (0.4606) acc 87.5000 (89.8010) lr 2.7103e-04 eta 0:01:47
epoch [40/50] batch [45/51] time 0.168 (0.203) data 0.000 (0.030) loss 0.4126 (0.4554) acc 93.5000 (90.1477) lr 2.7103e-04 eta 0:01:44
epoch [40/50] batch [50/51] time 0.170 (0.200) data 0.000 (0.027) loss 0.6768 (0.4596) acc 86.2745 (90.0861) lr 2.7103e-04 eta 0:01:42
>>> alpha1: 0.138  alpha2: -0.060 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.15 <<<
epoch [41/50] batch [5/51] time 0.180 (0.473) data 0.000 (0.287) loss 0.2775 (0.3770) acc 95.2830 (92.8482) lr 2.2949e-04 eta 0:03:58
epoch [41/50] batch [10/51] time 0.177 (0.326) data 0.000 (0.144) loss 0.4254 (0.4327) acc 91.5094 (92.4912) lr 2.2949e-04 eta 0:02:43
epoch [41/50] batch [15/51] time 0.165 (0.276) data 0.000 (0.096) loss 0.5078 (0.4263) acc 89.3617 (92.1129) lr 2.2949e-04 eta 0:02:16
epoch [41/50] batch [20/51] time 0.179 (0.252) data 0.000 (0.072) loss 0.5341 (0.4327) acc 86.7021 (91.5393) lr 2.2949e-04 eta 0:02:03
epoch [41/50] batch [25/51] time 0.162 (0.235) data 0.000 (0.058) loss 0.3061 (0.4530) acc 92.5532 (90.9809) lr 2.2949e-04 eta 0:01:53
epoch [41/50] batch [30/51] time 0.175 (0.225) data 0.000 (0.048) loss 0.4715 (0.4492) acc 89.7059 (91.1011) lr 2.2949e-04 eta 0:01:48
epoch [41/50] batch [35/51] time 0.171 (0.219) data 0.000 (0.041) loss 0.4681 (0.4523) acc 88.5000 (90.6968) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [40/51] time 0.168 (0.213) data 0.000 (0.036) loss 0.4044 (0.4528) acc 92.6471 (90.7885) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [45/51] time 0.174 (0.208) data 0.000 (0.032) loss 0.3982 (0.4578) acc 88.9423 (90.5801) lr 2.2949e-04 eta 0:01:36
epoch [41/50] batch [50/51] time 0.192 (0.205) data 0.000 (0.029) loss 0.3943 (0.4610) acc 92.4528 (90.4929) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.136  alpha2: -0.053 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [42/50] batch [5/51] time 0.169 (0.454) data 0.000 (0.269) loss 0.3484 (0.4478) acc 94.0000 (90.6435) lr 1.9098e-04 eta 0:03:26
epoch [42/50] batch [10/51] time 0.174 (0.317) data 0.000 (0.135) loss 0.3377 (0.4405) acc 91.3462 (91.3506) lr 1.9098e-04 eta 0:02:22
epoch [42/50] batch [15/51] time 0.172 (0.269) data 0.001 (0.090) loss 0.4194 (0.4471) acc 91.1765 (90.9244) lr 1.9098e-04 eta 0:01:59
epoch [42/50] batch [20/51] time 0.181 (0.248) data 0.000 (0.067) loss 0.2946 (0.4278) acc 93.8679 (91.3503) lr 1.9098e-04 eta 0:01:48
epoch [42/50] batch [25/51] time 0.173 (0.234) data 0.000 (0.054) loss 0.2813 (0.4368) acc 95.7447 (91.2041) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [30/51] time 0.183 (0.225) data 0.000 (0.045) loss 0.3485 (0.4461) acc 93.2692 (90.9896) lr 1.9098e-04 eta 0:01:36
epoch [42/50] batch [35/51] time 0.174 (0.218) data 0.000 (0.039) loss 0.3315 (0.4385) acc 92.0000 (90.9837) lr 1.9098e-04 eta 0:01:32
epoch [42/50] batch [40/51] time 0.162 (0.212) data 0.000 (0.034) loss 0.4213 (0.4334) acc 90.4255 (91.2007) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [45/51] time 0.173 (0.208) data 0.000 (0.030) loss 0.3576 (0.4319) acc 93.7500 (91.0681) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [50/51] time 0.169 (0.204) data 0.000 (0.027) loss 0.2754 (0.4364) acc 95.5000 (91.0099) lr 1.9098e-04 eta 0:01:23
>>> alpha1: 0.134  alpha2: -0.054 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [43/50] batch [5/51] time 0.182 (0.448) data 0.000 (0.265) loss 0.4364 (0.4017) acc 90.6250 (91.4970) lr 1.5567e-04 eta 0:03:00
epoch [43/50] batch [10/51] time 0.174 (0.311) data 0.000 (0.132) loss 0.3690 (0.3631) acc 93.2692 (92.4326) lr 1.5567e-04 eta 0:02:03
epoch [43/50] batch [15/51] time 0.169 (0.263) data 0.000 (0.088) loss 0.4175 (0.3923) acc 91.0000 (91.9148) lr 1.5567e-04 eta 0:01:43
epoch [43/50] batch [20/51] time 0.169 (0.241) data 0.000 (0.066) loss 0.8312 (0.4337) acc 83.8542 (91.2600) lr 1.5567e-04 eta 0:01:33
epoch [43/50] batch [25/51] time 0.160 (0.228) data 0.000 (0.053) loss 0.4117 (0.4356) acc 90.2174 (91.1416) lr 1.5567e-04 eta 0:01:27
epoch [43/50] batch [30/51] time 0.179 (0.220) data 0.000 (0.045) loss 0.4194 (0.4441) acc 91.1458 (90.9626) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [35/51] time 0.170 (0.213) data 0.000 (0.038) loss 0.5208 (0.4535) acc 84.5000 (90.7658) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [40/51] time 0.176 (0.208) data 0.000 (0.034) loss 0.4162 (0.4450) acc 90.0943 (90.7409) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [45/51] time 0.172 (0.204) data 0.000 (0.030) loss 0.6233 (0.4436) acc 83.3333 (90.6611) lr 1.5567e-04 eta 0:01:14
epoch [43/50] batch [50/51] time 0.170 (0.201) data 0.000 (0.027) loss 0.3382 (0.4444) acc 94.6078 (90.6293) lr 1.5567e-04 eta 0:01:11
>>> alpha1: 0.133  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [44/50] batch [5/51] time 0.196 (0.447) data 0.000 (0.255) loss 0.3096 (0.3932) acc 94.2308 (91.8251) lr 1.2369e-04 eta 0:02:37
epoch [44/50] batch [10/51] time 0.184 (0.312) data 0.000 (0.128) loss 0.4886 (0.4125) acc 88.5000 (90.8540) lr 1.2369e-04 eta 0:01:48
epoch [44/50] batch [15/51] time 0.173 (0.266) data 0.000 (0.085) loss 0.5910 (0.4156) acc 87.9808 (91.1513) lr 1.2369e-04 eta 0:01:30
epoch [44/50] batch [20/51] time 0.178 (0.242) data 0.000 (0.064) loss 0.3545 (0.4266) acc 94.3396 (91.0445) lr 1.2369e-04 eta 0:01:21
epoch [44/50] batch [25/51] time 0.169 (0.228) data 0.000 (0.051) loss 0.4057 (0.4335) acc 92.5000 (90.8700) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [30/51] time 0.165 (0.219) data 0.000 (0.043) loss 0.3855 (0.4348) acc 91.1458 (90.7416) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [35/51] time 0.189 (0.213) data 0.000 (0.037) loss 0.4328 (0.4371) acc 89.5000 (90.6030) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [40/51] time 0.171 (0.208) data 0.000 (0.032) loss 0.3262 (0.4317) acc 92.7885 (90.6708) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [45/51] time 0.166 (0.204) data 0.000 (0.029) loss 0.4286 (0.4425) acc 91.8367 (90.4851) lr 1.2369e-04 eta 0:01:03
epoch [44/50] batch [50/51] time 0.176 (0.200) data 0.000 (0.026) loss 0.3658 (0.4390) acc 91.5094 (90.6561) lr 1.2369e-04 eta 0:01:01
>>> alpha1: 0.132  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [45/50] batch [5/51] time 0.174 (0.445) data 0.000 (0.272) loss 0.3637 (0.4289) acc 94.2308 (91.4433) lr 9.5173e-05 eta 0:02:13
epoch [45/50] batch [10/51] time 0.171 (0.311) data 0.000 (0.136) loss 0.5871 (0.4408) acc 83.8235 (90.1136) lr 9.5173e-05 eta 0:01:32
epoch [45/50] batch [15/51] time 0.160 (0.266) data 0.000 (0.091) loss 0.5469 (0.4551) acc 89.6739 (89.9413) lr 9.5173e-05 eta 0:01:17
epoch [45/50] batch [20/51] time 0.189 (0.243) data 0.000 (0.068) loss 0.5235 (0.4466) acc 93.1034 (90.5755) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [25/51] time 0.178 (0.231) data 0.000 (0.055) loss 0.4574 (0.4438) acc 88.4259 (90.5388) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [30/51] time 0.176 (0.222) data 0.001 (0.046) loss 0.5703 (0.4412) acc 87.7660 (90.7669) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [35/51] time 0.174 (0.215) data 0.000 (0.039) loss 0.4443 (0.4395) acc 87.5000 (90.6979) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [40/51] time 0.172 (0.210) data 0.000 (0.034) loss 0.4255 (0.4411) acc 93.2692 (90.6260) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.030) loss 0.3380 (0.4480) acc 94.5000 (90.4597) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [50/51] time 0.166 (0.202) data 0.000 (0.027) loss 0.5630 (0.4407) acc 89.0625 (90.6476) lr 9.5173e-05 eta 0:00:51
>>> alpha1: 0.130  alpha2: -0.051 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [46/50] batch [5/51] time 0.175 (0.455) data 0.000 (0.272) loss 0.3963 (0.4094) acc 92.7885 (91.9873) lr 7.0224e-05 eta 0:01:53
epoch [46/50] batch [10/51] time 0.168 (0.315) data 0.000 (0.136) loss 0.4234 (0.4200) acc 90.5000 (90.9581) lr 7.0224e-05 eta 0:01:17
epoch [46/50] batch [15/51] time 0.161 (0.268) data 0.000 (0.091) loss 0.5093 (0.4213) acc 92.9348 (91.1180) lr 7.0224e-05 eta 0:01:04
epoch [46/50] batch [20/51] time 0.172 (0.244) data 0.000 (0.068) loss 0.3845 (0.4172) acc 92.3469 (90.9181) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [25/51] time 0.201 (0.229) data 0.019 (0.055) loss 0.4246 (0.4309) acc 91.3462 (90.3602) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [30/51] time 0.173 (0.221) data 0.000 (0.046) loss 0.3986 (0.4352) acc 91.0000 (90.4395) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [35/51] time 0.187 (0.215) data 0.001 (0.040) loss 0.5020 (0.4401) acc 90.0943 (90.4989) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [40/51] time 0.159 (0.209) data 0.000 (0.035) loss 0.3673 (0.4312) acc 89.1304 (90.6878) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.166 (0.204) data 0.000 (0.031) loss 0.3962 (0.4269) acc 89.2857 (90.8419) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [50/51] time 0.159 (0.201) data 0.000 (0.028) loss 0.4139 (0.4318) acc 91.3043 (90.6457) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.132  alpha2: -0.054 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [47/50] batch [5/51] time 0.186 (0.573) data 0.000 (0.390) loss 0.4339 (0.4923) acc 89.7059 (88.6869) lr 4.8943e-05 eta 0:01:53
epoch [47/50] batch [10/51] time 0.188 (0.374) data 0.001 (0.195) loss 0.3073 (0.4141) acc 93.0000 (91.3263) lr 4.8943e-05 eta 0:01:12
epoch [47/50] batch [15/51] time 0.168 (0.307) data 0.000 (0.130) loss 0.4661 (0.3805) acc 93.7500 (92.7077) lr 4.8943e-05 eta 0:00:58
epoch [47/50] batch [20/51] time 0.187 (0.274) data 0.000 (0.098) loss 0.5726 (0.3952) acc 86.5741 (92.1541) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [25/51] time 0.169 (0.254) data 0.000 (0.078) loss 0.2750 (0.3955) acc 97.0000 (92.1284) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [30/51] time 0.174 (0.240) data 0.000 (0.065) loss 0.3451 (0.4000) acc 90.6863 (91.9524) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [35/51] time 0.160 (0.230) data 0.000 (0.056) loss 0.3755 (0.3986) acc 91.3043 (91.8235) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [40/51] time 0.166 (0.224) data 0.000 (0.049) loss 0.5905 (0.4056) acc 85.2041 (91.5933) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [45/51] time 0.165 (0.218) data 0.000 (0.044) loss 0.2851 (0.4121) acc 93.7500 (91.4160) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [50/51] time 0.172 (0.213) data 0.000 (0.039) loss 0.4768 (0.4166) acc 89.4231 (91.3261) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.132  alpha2: -0.056 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.07 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.13 <<<
epoch [48/50] batch [5/51] time 0.165 (0.464) data 0.000 (0.282) loss 0.4458 (0.4736) acc 91.1458 (89.3877) lr 3.1417e-05 eta 0:01:08
epoch [48/50] batch [10/51] time 0.169 (0.317) data 0.000 (0.141) loss 0.5499 (0.5041) acc 89.5000 (89.0675) lr 3.1417e-05 eta 0:00:45
epoch [48/50] batch [15/51] time 0.178 (0.271) data 0.000 (0.095) loss 0.4494 (0.4734) acc 93.1373 (89.9511) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [20/51] time 0.169 (0.245) data 0.000 (0.071) loss 0.3405 (0.4522) acc 93.0000 (90.8583) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [25/51] time 0.173 (0.230) data 0.000 (0.057) loss 0.3532 (0.4435) acc 92.7885 (91.0308) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [30/51] time 0.179 (0.220) data 0.000 (0.048) loss 0.4440 (0.4454) acc 93.8679 (90.8583) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.170 (0.213) data 0.000 (0.041) loss 0.5375 (0.4454) acc 87.0000 (90.8104) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.167 (0.208) data 0.000 (0.036) loss 0.5290 (0.4413) acc 89.7959 (90.8079) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [45/51] time 0.169 (0.203) data 0.000 (0.032) loss 0.3609 (0.4370) acc 95.8333 (90.9823) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [50/51] time 0.198 (0.200) data 0.000 (0.029) loss 0.5468 (0.4326) acc 88.3621 (91.0188) lr 3.1417e-05 eta 0:00:20
>>> alpha1: 0.132  alpha2: -0.051 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [49/50] batch [5/51] time 0.178 (0.445) data 0.000 (0.264) loss 0.3805 (0.4919) acc 91.5094 (89.8663) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [10/51] time 0.167 (0.307) data 0.000 (0.132) loss 0.3989 (0.4576) acc 90.6250 (90.4221) lr 1.7713e-05 eta 0:00:28
epoch [49/50] batch [15/51] time 0.165 (0.261) data 0.000 (0.088) loss 0.3679 (0.4397) acc 91.6667 (90.8532) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [20/51] time 0.182 (0.240) data 0.000 (0.066) loss 0.3721 (0.4250) acc 90.2778 (91.1099) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [25/51] time 0.194 (0.229) data 0.000 (0.054) loss 0.3158 (0.4186) acc 93.8775 (91.0896) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.179 (0.222) data 0.000 (0.045) loss 0.3321 (0.4265) acc 93.8889 (90.9451) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [35/51] time 0.169 (0.215) data 0.000 (0.038) loss 0.4771 (0.4326) acc 90.4255 (90.5835) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.164 (0.210) data 0.000 (0.034) loss 0.4793 (0.4369) acc 89.5833 (90.4888) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [45/51] time 0.170 (0.205) data 0.000 (0.030) loss 0.2899 (0.4312) acc 92.6471 (90.5714) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.177 (0.202) data 0.000 (0.027) loss 0.3412 (0.4297) acc 94.8113 (90.5655) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.130  alpha2: -0.052 <<<
>>> noisy rate: 0.25 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.14 <<<
epoch [50/50] batch [5/51] time 0.179 (0.452) data 0.001 (0.271) loss 0.4848 (0.4662) acc 89.3617 (89.0797) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [10/51] time 0.182 (0.318) data 0.000 (0.136) loss 0.4754 (0.4449) acc 85.4167 (90.5438) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.174 (0.273) data 0.000 (0.091) loss 0.4463 (0.4292) acc 91.8269 (90.7760) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.167 (0.246) data 0.000 (0.068) loss 0.4855 (0.4282) acc 87.7660 (90.8630) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.177 (0.232) data 0.000 (0.054) loss 0.4655 (0.4216) acc 89.2857 (91.0953) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.163 (0.223) data 0.000 (0.045) loss 0.5866 (0.4189) acc 86.1702 (91.2807) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.174 (0.216) data 0.000 (0.039) loss 0.4528 (0.4198) acc 90.3846 (91.0813) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.180 (0.210) data 0.000 (0.034) loss 0.5121 (0.4298) acc 91.8182 (90.7820) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.170 (0.205) data 0.000 (0.030) loss 0.5407 (0.4362) acc 88.7255 (90.7455) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.165 (0.202) data 0.000 (0.027) loss 0.5610 (0.4358) acc 87.7551 (90.7969) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_4FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.09, 0.09, 0.09, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.07, 0.08, 0.08, 0.07, 0.08, 0.08]
* matched noise rate: [0.04, 0.03, 0.03, 0.03, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.04, 0.03, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03]
* unmatched noise rate: [0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16, 0.15, 0.15, 0.14, 0.15, 0.15, 0.14, 0.14, 0.14, 0.13, 0.13, 0.15, 0.14, 0.13, 0.15, 0.14, 0.15, 0.14, 0.13, 0.14, 0.13, 0.14, 0.15, 0.14, 0.13, 0.14, 0.14, 0.14, 0.14, 0.13, 0.14, 0.14]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:07,  2.80s/it] 12%|█▏        | 3/25 [00:03<00:17,  1.25it/s] 20%|██        | 5/25 [00:03<00:08,  2.32it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.52it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.81it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.12it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.36it/s] 60%|██████    | 15/25 [00:03<00:01,  8.46it/s] 68%|██████▊   | 17/25 [00:04<00:01,  6.94it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.03it/s] 84%|████████▍ | 21/25 [00:04<00:00,  8.99it/s] 92%|█████████▏| 23/25 [00:04<00:00,  9.78it/s]100%|██████████| 25/25 [00:05<00:00,  6.35it/s]100%|██████████| 25/25 [00:05<00:00,  4.43it/s]
=> result
* total: 2,463
* correct: 2,196
* accuracy: 89.2%
* error: 10.8%
* macro_f1: 87.7%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 4	acc: 33.3%
* class: 3 (sweet pea)	total: 17	correct: 11	acc: 64.7%
* class: 4 (english marigold)	total: 20	correct: 19	acc: 95.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 24	acc: 92.3%
* class: 11 (colt's foot)	total: 26	correct: 24	acc: 92.3%
* class: 12 (king protea)	total: 15	correct: 14	acc: 93.3%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 3	acc: 23.1%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 22	acc: 88.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 25	acc: 92.6%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 23	acc: 88.5%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 13	acc: 92.9%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 14	acc: 82.4%
* class: 38 (siam tulip)	total: 13	correct: 10	acc: 76.9%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 17	acc: 44.7%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 2	acc: 5.1%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 9	acc: 75.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 19	acc: 95.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 25	acc: 89.3%
* class: 50 (petunia)	total: 77	correct: 66	acc: 85.7%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 15	acc: 93.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 30	acc: 96.8%
* class: 65 (osteospermum)	total: 19	correct: 18	acc: 94.7%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 15	acc: 93.8%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 33	acc: 91.7%
* class: 75 (morning glory)	total: 32	correct: 29	acc: 90.6%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 32	acc: 94.1%
* class: 82 (hibiscus)	total: 39	correct: 33	acc: 84.6%
* class: 83 (columbine)	total: 26	correct: 23	acc: 88.5%
* class: 84 (desert-rose)	total: 18	correct: 17	acc: 94.4%
* class: 85 (tree mallow)	total: 17	correct: 13	acc: 76.5%
* class: 86 (magnolia)	total: 18	correct: 15	acc: 83.3%
* class: 87 (cyclamen)	total: 46	correct: 37	acc: 80.4%
* class: 88 (watercress)	total: 55	correct: 28	acc: 50.9%
* class: 89 (canna lily)	total: 25	correct: 23	acc: 92.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 18	acc: 90.0%
* class: 92 (ball moss)	total: 14	correct: 1	acc: 7.1%
* class: 93 (foxglove)	total: 49	correct: 48	acc: 98.0%
* class: 94 (bougainvillea)	total: 38	correct: 28	acc: 73.7%
* class: 95 (camellia)	total: 27	correct: 23	acc: 85.2%
* class: 96 (mallow)	total: 20	correct: 19	acc: 95.0%
* class: 97 (mexican petunia)	total: 25	correct: 17	acc: 68.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 15	acc: 88.2%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 89.7%
Elapsed: 0:28:09
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_6-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.266 (1.015) data 0.000 (0.258) loss 4.4890 (4.5552) acc 6.2500 (10.0000) lr 1.0000e-05 eta 0:43:03
epoch [1/50] batch [10/51] time 0.274 (0.642) data 0.000 (0.129) loss 4.3617 (4.4522) acc 6.2500 (10.6250) lr 1.0000e-05 eta 0:27:09
epoch [1/50] batch [15/51] time 0.262 (0.516) data 0.000 (0.086) loss 4.5097 (4.4357) acc 12.5000 (10.8333) lr 1.0000e-05 eta 0:21:49
epoch [1/50] batch [20/51] time 0.267 (0.454) data 0.000 (0.065) loss 4.1590 (4.3669) acc 15.6250 (11.4062) lr 1.0000e-05 eta 0:19:09
epoch [1/50] batch [25/51] time 0.268 (0.417) data 0.000 (0.052) loss 4.1327 (4.3000) acc 18.7500 (13.2500) lr 1.0000e-05 eta 0:17:34
epoch [1/50] batch [30/51] time 0.266 (0.392) data 0.000 (0.043) loss 4.0283 (4.2240) acc 31.2500 (15.1042) lr 1.0000e-05 eta 0:16:27
epoch [1/50] batch [35/51] time 0.262 (0.375) data 0.000 (0.037) loss 3.9306 (4.1656) acc 40.6250 (17.5893) lr 1.0000e-05 eta 0:15:42
epoch [1/50] batch [40/51] time 0.263 (0.361) data 0.000 (0.032) loss 3.4928 (4.1080) acc 31.2500 (19.1406) lr 1.0000e-05 eta 0:15:06
epoch [1/50] batch [45/51] time 0.261 (0.350) data 0.000 (0.029) loss 3.8937 (4.0970) acc 31.2500 (20.0694) lr 1.0000e-05 eta 0:14:36
epoch [1/50] batch [50/51] time 0.262 (0.341) data 0.000 (0.026) loss 4.1323 (4.0636) acc 25.0000 (21.1875) lr 1.0000e-05 eta 0:14:13
epoch [2/50] batch [5/51] time 0.283 (0.556) data 0.000 (0.281) loss 3.2496 (3.9690) acc 40.6250 (26.2500) lr 2.0000e-03 eta 0:23:05
epoch [2/50] batch [10/51] time 0.274 (0.411) data 0.000 (0.141) loss 3.2625 (3.7715) acc 46.8750 (31.2500) lr 2.0000e-03 eta 0:17:03
epoch [2/50] batch [15/51] time 0.273 (0.364) data 0.000 (0.094) loss 3.1292 (3.7291) acc 46.8750 (33.1250) lr 2.0000e-03 eta 0:15:04
epoch [2/50] batch [20/51] time 0.270 (0.342) data 0.000 (0.070) loss 3.4079 (3.6967) acc 40.6250 (33.4375) lr 2.0000e-03 eta 0:14:07
epoch [2/50] batch [25/51] time 0.272 (0.327) data 0.000 (0.056) loss 3.7016 (3.7313) acc 28.1250 (32.3750) lr 2.0000e-03 eta 0:13:30
epoch [2/50] batch [30/51] time 0.283 (0.318) data 0.000 (0.047) loss 3.6842 (3.6962) acc 50.0000 (33.3333) lr 2.0000e-03 eta 0:13:04
epoch [2/50] batch [35/51] time 0.274 (0.311) data 0.000 (0.040) loss 3.3909 (3.7211) acc 50.0000 (33.6607) lr 2.0000e-03 eta 0:12:46
epoch [2/50] batch [40/51] time 0.263 (0.306) data 0.000 (0.035) loss 3.8092 (3.7202) acc 31.2500 (33.2031) lr 2.0000e-03 eta 0:12:31
epoch [2/50] batch [45/51] time 0.262 (0.301) data 0.000 (0.031) loss 3.5163 (3.6970) acc 18.7500 (33.2639) lr 2.0000e-03 eta 0:12:18
epoch [2/50] batch [50/51] time 0.265 (0.297) data 0.000 (0.028) loss 3.9480 (3.6683) acc 34.3750 (34.1875) lr 2.0000e-03 eta 0:12:07
epoch [3/50] batch [5/51] time 0.302 (0.582) data 0.000 (0.290) loss 3.0035 (3.3393) acc 43.7500 (42.5000) lr 1.9980e-03 eta 0:23:41
epoch [3/50] batch [10/51] time 0.267 (0.424) data 0.000 (0.145) loss 3.7818 (3.4179) acc 28.1250 (39.0625) lr 1.9980e-03 eta 0:17:14
epoch [3/50] batch [15/51] time 0.274 (0.374) data 0.000 (0.097) loss 3.6876 (3.4587) acc 34.3750 (38.7500) lr 1.9980e-03 eta 0:15:09
epoch [3/50] batch [20/51] time 0.264 (0.347) data 0.000 (0.073) loss 3.2958 (3.5002) acc 46.8750 (38.1250) lr 1.9980e-03 eta 0:14:02
epoch [3/50] batch [25/51] time 0.263 (0.331) data 0.000 (0.058) loss 3.3752 (3.5355) acc 43.7500 (37.5000) lr 1.9980e-03 eta 0:13:21
epoch [3/50] batch [30/51] time 0.268 (0.321) data 0.000 (0.049) loss 3.9564 (3.5147) acc 25.0000 (37.1875) lr 1.9980e-03 eta 0:12:55
epoch [3/50] batch [35/51] time 0.263 (0.313) data 0.000 (0.042) loss 3.3247 (3.5376) acc 40.6250 (37.4107) lr 1.9980e-03 eta 0:12:36
epoch [3/50] batch [40/51] time 0.262 (0.307) data 0.000 (0.037) loss 3.6022 (3.5416) acc 34.3750 (37.3438) lr 1.9980e-03 eta 0:12:20
epoch [3/50] batch [45/51] time 0.261 (0.302) data 0.000 (0.033) loss 4.3041 (3.5367) acc 28.1250 (37.3611) lr 1.9980e-03 eta 0:12:06
epoch [3/50] batch [50/51] time 0.263 (0.298) data 0.000 (0.029) loss 4.6512 (3.5129) acc 15.6250 (38.1875) lr 1.9980e-03 eta 0:11:55
epoch [4/50] batch [5/51] time 0.286 (0.545) data 0.000 (0.270) loss 3.0331 (3.5951) acc 50.0000 (38.7500) lr 1.9921e-03 eta 0:21:42
epoch [4/50] batch [10/51] time 0.280 (0.405) data 0.000 (0.135) loss 3.8160 (3.4892) acc 43.7500 (39.6875) lr 1.9921e-03 eta 0:16:07
epoch [4/50] batch [15/51] time 0.268 (0.359) data 0.000 (0.090) loss 3.2103 (3.4805) acc 56.2500 (41.4583) lr 1.9921e-03 eta 0:14:15
epoch [4/50] batch [20/51] time 0.275 (0.337) data 0.000 (0.068) loss 3.1601 (3.4398) acc 40.6250 (41.2500) lr 1.9921e-03 eta 0:13:21
epoch [4/50] batch [25/51] time 0.272 (0.324) data 0.000 (0.054) loss 3.3957 (3.4835) acc 40.6250 (40.6250) lr 1.9921e-03 eta 0:12:48
epoch [4/50] batch [30/51] time 0.276 (0.315) data 0.000 (0.045) loss 3.7986 (3.5075) acc 34.3750 (39.5833) lr 1.9921e-03 eta 0:12:25
epoch [4/50] batch [35/51] time 0.262 (0.308) data 0.000 (0.039) loss 3.0914 (3.4877) acc 43.7500 (40.0893) lr 1.9921e-03 eta 0:12:07
epoch [4/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.034) loss 3.4611 (3.4745) acc 34.3750 (39.9219) lr 1.9921e-03 eta 0:11:52
epoch [4/50] batch [45/51] time 0.262 (0.298) data 0.000 (0.030) loss 4.0768 (3.4592) acc 40.6250 (40.6944) lr 1.9921e-03 eta 0:11:40
epoch [4/50] batch [50/51] time 0.262 (0.294) data 0.000 (0.027) loss 3.7018 (3.4663) acc 34.3750 (40.1250) lr 1.9921e-03 eta 0:11:30
epoch [5/50] batch [5/51] time 0.282 (0.515) data 0.000 (0.217) loss 2.8028 (3.1251) acc 46.8750 (46.8750) lr 1.9823e-03 eta 0:20:06
epoch [5/50] batch [10/51] time 0.265 (0.391) data 0.000 (0.109) loss 3.7167 (3.3191) acc 43.7500 (42.8125) lr 1.9823e-03 eta 0:15:13
epoch [5/50] batch [15/51] time 0.263 (0.349) data 0.000 (0.072) loss 2.9339 (3.2386) acc 50.0000 (43.5417) lr 1.9823e-03 eta 0:13:33
epoch [5/50] batch [20/51] time 0.263 (0.328) data 0.000 (0.054) loss 3.8413 (3.3427) acc 37.5000 (41.7188) lr 1.9823e-03 eta 0:12:44
epoch [5/50] batch [25/51] time 0.273 (0.316) data 0.000 (0.044) loss 3.2641 (3.3257) acc 37.5000 (41.6250) lr 1.9823e-03 eta 0:12:14
epoch [5/50] batch [30/51] time 0.270 (0.309) data 0.000 (0.036) loss 4.2457 (3.3356) acc 31.2500 (41.5625) lr 1.9823e-03 eta 0:11:55
epoch [5/50] batch [35/51] time 0.265 (0.303) data 0.000 (0.031) loss 3.2071 (3.3589) acc 43.7500 (40.8929) lr 1.9823e-03 eta 0:11:41
epoch [5/50] batch [40/51] time 0.261 (0.298) data 0.000 (0.027) loss 4.0462 (3.3940) acc 34.3750 (40.1562) lr 1.9823e-03 eta 0:11:27
epoch [5/50] batch [45/51] time 0.263 (0.294) data 0.000 (0.024) loss 4.1259 (3.3850) acc 28.1250 (40.4167) lr 1.9823e-03 eta 0:11:17
epoch [5/50] batch [50/51] time 0.261 (0.291) data 0.000 (0.022) loss 3.1677 (3.3955) acc 50.0000 (41.0000) lr 1.9823e-03 eta 0:11:08
epoch [6/50] batch [5/51] time 0.264 (0.487) data 0.000 (0.211) loss 3.3937 (3.0539) acc 37.5000 (47.5000) lr 1.9686e-03 eta 0:18:36
epoch [6/50] batch [10/51] time 0.272 (0.382) data 0.000 (0.106) loss 4.5720 (3.3395) acc 25.0000 (41.8750) lr 1.9686e-03 eta 0:14:31
epoch [6/50] batch [15/51] time 0.277 (0.345) data 0.000 (0.071) loss 3.0574 (3.3295) acc 43.7500 (39.5833) lr 1.9686e-03 eta 0:13:06
epoch [6/50] batch [20/51] time 0.270 (0.326) data 0.000 (0.053) loss 3.4170 (3.2942) acc 37.5000 (40.9375) lr 1.9686e-03 eta 0:12:20
epoch [6/50] batch [25/51] time 0.268 (0.314) data 0.000 (0.042) loss 3.8282 (3.3344) acc 31.2500 (41.8750) lr 1.9686e-03 eta 0:11:52
epoch [6/50] batch [30/51] time 0.272 (0.306) data 0.000 (0.035) loss 3.4653 (3.3687) acc 37.5000 (41.1458) lr 1.9686e-03 eta 0:11:33
epoch [6/50] batch [35/51] time 0.263 (0.302) data 0.000 (0.030) loss 3.3697 (3.3981) acc 59.3750 (40.8036) lr 1.9686e-03 eta 0:11:21
epoch [6/50] batch [40/51] time 0.263 (0.297) data 0.000 (0.027) loss 2.8617 (3.3777) acc 46.8750 (41.7969) lr 1.9686e-03 eta 0:11:08
epoch [6/50] batch [45/51] time 0.263 (0.293) data 0.000 (0.024) loss 3.3042 (3.3890) acc 46.8750 (41.8750) lr 1.9686e-03 eta 0:10:58
epoch [6/50] batch [50/51] time 0.265 (0.290) data 0.000 (0.021) loss 3.1971 (3.3850) acc 40.6250 (42.1875) lr 1.9686e-03 eta 0:10:51
epoch [7/50] batch [5/51] time 0.275 (0.487) data 0.000 (0.206) loss 3.6706 (3.3088) acc 40.6250 (46.2500) lr 1.9511e-03 eta 0:18:09
epoch [7/50] batch [10/51] time 0.263 (0.377) data 0.000 (0.103) loss 3.3094 (3.3908) acc 40.6250 (43.1250) lr 1.9511e-03 eta 0:14:02
epoch [7/50] batch [15/51] time 0.269 (0.340) data 0.000 (0.069) loss 3.4483 (3.2842) acc 37.5000 (45.0000) lr 1.9511e-03 eta 0:12:38
epoch [7/50] batch [20/51] time 0.268 (0.322) data 0.000 (0.052) loss 3.0234 (3.3252) acc 50.0000 (44.6875) lr 1.9511e-03 eta 0:11:55
epoch [7/50] batch [25/51] time 0.275 (0.312) data 0.000 (0.041) loss 3.0571 (3.2772) acc 53.1250 (45.6250) lr 1.9511e-03 eta 0:11:31
epoch [7/50] batch [30/51] time 0.283 (0.306) data 0.000 (0.035) loss 3.4271 (3.2824) acc 43.7500 (45.5208) lr 1.9511e-03 eta 0:11:16
epoch [7/50] batch [35/51] time 0.263 (0.300) data 0.000 (0.030) loss 2.7354 (3.3204) acc 40.6250 (44.2857) lr 1.9511e-03 eta 0:11:03
epoch [7/50] batch [40/51] time 0.262 (0.296) data 0.000 (0.026) loss 3.5877 (3.3114) acc 40.6250 (44.7656) lr 1.9511e-03 eta 0:10:52
epoch [7/50] batch [45/51] time 0.265 (0.292) data 0.000 (0.023) loss 3.5714 (3.3240) acc 37.5000 (43.9583) lr 1.9511e-03 eta 0:10:42
epoch [7/50] batch [50/51] time 0.263 (0.289) data 0.000 (0.021) loss 4.9192 (3.3306) acc 21.8750 (43.5000) lr 1.9511e-03 eta 0:10:34
epoch [8/50] batch [5/51] time 0.299 (0.560) data 0.000 (0.280) loss 2.8368 (3.1805) acc 50.0000 (45.0000) lr 1.9298e-03 eta 0:20:24
epoch [8/50] batch [10/51] time 0.267 (0.413) data 0.000 (0.140) loss 3.5816 (3.0300) acc 43.7500 (48.1250) lr 1.9298e-03 eta 0:15:01
epoch [8/50] batch [15/51] time 0.265 (0.365) data 0.000 (0.093) loss 2.8521 (3.1564) acc 43.7500 (46.4583) lr 1.9298e-03 eta 0:13:14
epoch [8/50] batch [20/51] time 0.269 (0.341) data 0.000 (0.070) loss 3.7335 (3.2123) acc 34.3750 (45.6250) lr 1.9298e-03 eta 0:12:21
epoch [8/50] batch [25/51] time 0.263 (0.326) data 0.000 (0.057) loss 3.0323 (3.2584) acc 43.7500 (44.6250) lr 1.9298e-03 eta 0:11:47
epoch [8/50] batch [30/51] time 0.263 (0.316) data 0.000 (0.047) loss 3.2040 (3.2361) acc 43.7500 (45.0000) lr 1.9298e-03 eta 0:11:24
epoch [8/50] batch [35/51] time 0.264 (0.309) data 0.000 (0.040) loss 2.4395 (3.2218) acc 65.6250 (45.2679) lr 1.9298e-03 eta 0:11:06
epoch [8/50] batch [40/51] time 0.262 (0.303) data 0.000 (0.035) loss 4.2191 (3.2573) acc 21.8750 (44.8438) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [45/51] time 0.266 (0.299) data 0.000 (0.032) loss 3.0752 (3.2490) acc 37.5000 (45.0000) lr 1.9298e-03 eta 0:10:42
epoch [8/50] batch [50/51] time 0.262 (0.296) data 0.000 (0.028) loss 3.5787 (3.2684) acc 43.7500 (44.6875) lr 1.9298e-03 eta 0:10:33
epoch [9/50] batch [5/51] time 0.279 (0.548) data 0.000 (0.262) loss 3.1882 (2.9508) acc 50.0000 (56.2500) lr 1.9048e-03 eta 0:19:31
epoch [9/50] batch [10/51] time 0.276 (0.409) data 0.000 (0.131) loss 3.5158 (3.0344) acc 34.3750 (51.8750) lr 1.9048e-03 eta 0:14:31
epoch [9/50] batch [15/51] time 0.268 (0.362) data 0.000 (0.087) loss 2.9655 (3.1000) acc 46.8750 (49.5833) lr 1.9048e-03 eta 0:12:50
epoch [9/50] batch [20/51] time 0.270 (0.339) data 0.000 (0.066) loss 3.1261 (3.1085) acc 46.8750 (48.4375) lr 1.9048e-03 eta 0:11:58
epoch [9/50] batch [25/51] time 0.271 (0.325) data 0.000 (0.053) loss 3.0542 (3.1540) acc 50.0000 (47.8750) lr 1.9048e-03 eta 0:11:27
epoch [9/50] batch [30/51] time 0.277 (0.316) data 0.000 (0.044) loss 2.7292 (3.1118) acc 56.2500 (48.6458) lr 1.9048e-03 eta 0:11:07
epoch [9/50] batch [35/51] time 0.271 (0.309) data 0.000 (0.038) loss 3.7168 (3.1220) acc 43.7500 (48.1250) lr 1.9048e-03 eta 0:10:50
epoch [9/50] batch [40/51] time 0.261 (0.303) data 0.000 (0.033) loss 3.2405 (3.1392) acc 50.0000 (48.1250) lr 1.9048e-03 eta 0:10:37
epoch [9/50] batch [45/51] time 0.261 (0.298) data 0.000 (0.029) loss 2.9708 (3.1696) acc 53.1250 (47.9861) lr 1.9048e-03 eta 0:10:25
epoch [9/50] batch [50/51] time 0.261 (0.295) data 0.000 (0.026) loss 3.7746 (3.2009) acc 40.6250 (47.5000) lr 1.9048e-03 eta 0:10:16
epoch [10/50] batch [5/51] time 0.267 (0.555) data 0.000 (0.279) loss 3.0574 (3.2628) acc 50.0000 (44.3750) lr 1.8763e-03 eta 0:19:16
epoch [10/50] batch [10/51] time 0.263 (0.411) data 0.000 (0.140) loss 3.9223 (3.2795) acc 31.2500 (45.3125) lr 1.8763e-03 eta 0:14:14
epoch [10/50] batch [15/51] time 0.273 (0.363) data 0.000 (0.093) loss 2.5534 (3.2276) acc 59.3750 (45.4167) lr 1.8763e-03 eta 0:12:32
epoch [10/50] batch [20/51] time 0.267 (0.339) data 0.000 (0.070) loss 2.1634 (3.2017) acc 59.3750 (46.4062) lr 1.8763e-03 eta 0:11:41
epoch [10/50] batch [25/51] time 0.269 (0.325) data 0.000 (0.056) loss 3.1172 (3.1982) acc 56.2500 (46.7500) lr 1.8763e-03 eta 0:11:11
epoch [10/50] batch [30/51] time 0.263 (0.315) data 0.000 (0.047) loss 2.4141 (3.1803) acc 53.1250 (46.6667) lr 1.8763e-03 eta 0:10:49
epoch [10/50] batch [35/51] time 0.268 (0.308) data 0.000 (0.040) loss 2.5843 (3.1730) acc 56.2500 (46.5179) lr 1.8763e-03 eta 0:10:34
epoch [10/50] batch [40/51] time 0.264 (0.303) data 0.000 (0.035) loss 2.8333 (3.1279) acc 53.1250 (47.5781) lr 1.8763e-03 eta 0:10:21
epoch [10/50] batch [45/51] time 0.265 (0.299) data 0.000 (0.031) loss 3.3122 (3.1418) acc 50.0000 (47.5694) lr 1.8763e-03 eta 0:10:11
epoch [10/50] batch [50/51] time 0.265 (0.295) data 0.000 (0.028) loss 2.9094 (3.1557) acc 59.3750 (47.5625) lr 1.8763e-03 eta 0:10:02
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.630  alpha2: 0.285 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.24 <<<
epoch [11/50] batch [5/51] time 0.164 (0.872) data 0.000 (0.308) loss 1.8204 (1.9183) acc 64.3617 (69.5388) lr 1.8443e-03 eta 0:29:33
epoch [11/50] batch [10/51] time 0.696 (0.643) data 0.000 (0.154) loss 1.4336 (1.7647) acc 77.9762 (70.6011) lr 1.8443e-03 eta 0:21:44
epoch [11/50] batch [15/51] time 0.755 (0.594) data 0.000 (0.103) loss 1.6050 (1.6744) acc 73.9583 (71.4975) lr 1.8443e-03 eta 0:20:01
epoch [11/50] batch [20/51] time 0.161 (0.518) data 0.000 (0.077) loss 1.3349 (1.6534) acc 73.8889 (71.2697) lr 1.8443e-03 eta 0:17:25
epoch [11/50] batch [25/51] time 0.164 (0.467) data 0.000 (0.062) loss 1.5851 (1.6434) acc 69.7917 (70.4595) lr 1.8443e-03 eta 0:15:40
epoch [11/50] batch [30/51] time 0.171 (0.417) data 0.000 (0.052) loss 1.4263 (1.6091) acc 76.0870 (70.8196) lr 1.8443e-03 eta 0:13:57
epoch [11/50] batch [35/51] time 0.163 (0.396) data 0.000 (0.044) loss 1.6025 (1.5933) acc 64.8936 (70.5680) lr 1.8443e-03 eta 0:13:13
epoch [11/50] batch [40/51] time 0.167 (0.367) data 0.000 (0.039) loss 1.3010 (1.5731) acc 70.9184 (70.8492) lr 1.8443e-03 eta 0:12:14
epoch [11/50] batch [45/51] time 0.159 (0.345) data 0.000 (0.034) loss 1.2060 (1.5742) acc 68.3333 (70.2779) lr 1.8443e-03 eta 0:11:27
epoch [11/50] batch [50/51] time 0.649 (0.348) data 0.000 (0.031) loss 1.6615 (1.5671) acc 63.7500 (70.5025) lr 1.8443e-03 eta 0:11:33
>>> alpha1: 0.491  alpha2: 0.202 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.23 <<<
epoch [12/50] batch [5/51] time 0.165 (0.446) data 0.000 (0.279) loss 1.2158 (1.1912) acc 67.7083 (72.0871) lr 1.8090e-03 eta 0:14:45
epoch [12/50] batch [10/51] time 0.172 (0.310) data 0.000 (0.140) loss 1.0950 (1.1409) acc 73.9583 (74.0196) lr 1.8090e-03 eta 0:10:12
epoch [12/50] batch [15/51] time 0.175 (0.304) data 0.000 (0.093) loss 0.9499 (1.0994) acc 80.5000 (75.7925) lr 1.8090e-03 eta 0:09:59
epoch [12/50] batch [20/51] time 0.171 (0.271) data 0.000 (0.070) loss 1.0416 (1.1117) acc 74.0196 (75.0137) lr 1.8090e-03 eta 0:08:52
epoch [12/50] batch [25/51] time 0.178 (0.252) data 0.000 (0.056) loss 0.8419 (1.0975) acc 80.6122 (75.3865) lr 1.8090e-03 eta 0:08:15
epoch [12/50] batch [30/51] time 0.166 (0.239) data 0.000 (0.047) loss 1.1595 (1.0909) acc 73.4694 (75.5399) lr 1.8090e-03 eta 0:07:47
epoch [12/50] batch [35/51] time 0.170 (0.229) data 0.000 (0.040) loss 1.0897 (1.0918) acc 73.0000 (75.2544) lr 1.8090e-03 eta 0:07:27
epoch [12/50] batch [40/51] time 0.163 (0.222) data 0.000 (0.035) loss 0.9650 (1.0768) acc 80.2083 (75.5516) lr 1.8090e-03 eta 0:07:12
epoch [12/50] batch [45/51] time 0.165 (0.216) data 0.000 (0.031) loss 0.7899 (1.0652) acc 79.6875 (75.7540) lr 1.8090e-03 eta 0:06:58
epoch [12/50] batch [50/51] time 0.162 (0.210) data 0.000 (0.028) loss 1.3554 (1.0725) acc 65.4255 (75.4688) lr 1.8090e-03 eta 0:06:48
>>> alpha1: 0.401  alpha2: 0.124 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.24 <<<
epoch [13/50] batch [5/51] time 0.183 (0.474) data 0.000 (0.297) loss 0.7764 (0.9224) acc 80.7292 (80.3352) lr 1.7705e-03 eta 0:15:17
epoch [13/50] batch [10/51] time 0.174 (0.324) data 0.000 (0.148) loss 0.9215 (0.9339) acc 77.8846 (78.1566) lr 1.7705e-03 eta 0:10:25
epoch [13/50] batch [15/51] time 0.166 (0.273) data 0.000 (0.099) loss 1.1220 (0.9304) acc 71.8085 (78.5585) lr 1.7705e-03 eta 0:08:44
epoch [13/50] batch [20/51] time 0.168 (0.247) data 0.000 (0.074) loss 0.8813 (0.9392) acc 75.5102 (78.1313) lr 1.7705e-03 eta 0:07:53
epoch [13/50] batch [25/51] time 0.167 (0.257) data 0.000 (0.060) loss 1.0804 (0.9154) acc 75.5102 (78.3407) lr 1.7705e-03 eta 0:08:10
epoch [13/50] batch [30/51] time 0.195 (0.244) data 0.000 (0.050) loss 0.7189 (0.9216) acc 77.6596 (78.1303) lr 1.7705e-03 eta 0:07:44
epoch [13/50] batch [35/51] time 0.167 (0.253) data 0.000 (0.043) loss 1.0646 (0.9257) acc 76.0638 (78.0905) lr 1.7705e-03 eta 0:08:01
epoch [13/50] batch [40/51] time 0.161 (0.242) data 0.000 (0.037) loss 0.9733 (0.9240) acc 70.7447 (78.1180) lr 1.7705e-03 eta 0:07:39
epoch [13/50] batch [45/51] time 0.150 (0.233) data 0.000 (0.033) loss 1.4352 (0.9423) acc 58.9286 (77.2304) lr 1.7705e-03 eta 0:07:20
epoch [13/50] batch [50/51] time 0.164 (0.227) data 0.000 (0.030) loss 0.8019 (0.9313) acc 78.6458 (77.6044) lr 1.7705e-03 eta 0:07:08
>>> alpha1: 0.355  alpha2: 0.091 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.24 <<<
epoch [14/50] batch [5/51] time 0.174 (0.572) data 0.000 (0.265) loss 0.7996 (0.7833) acc 80.0000 (81.8838) lr 1.7290e-03 eta 0:17:56
epoch [14/50] batch [10/51] time 0.165 (0.372) data 0.000 (0.133) loss 0.7602 (0.8150) acc 80.8511 (80.4644) lr 1.7290e-03 eta 0:11:37
epoch [14/50] batch [15/51] time 0.186 (0.308) data 0.000 (0.089) loss 1.0100 (0.8396) acc 73.4375 (79.2424) lr 1.7290e-03 eta 0:09:35
epoch [14/50] batch [20/51] time 0.184 (0.276) data 0.000 (0.067) loss 0.9379 (0.8499) acc 82.1429 (79.1380) lr 1.7290e-03 eta 0:08:35
epoch [14/50] batch [25/51] time 0.167 (0.256) data 0.000 (0.053) loss 1.0672 (0.8286) acc 75.0000 (80.0642) lr 1.7290e-03 eta 0:07:57
epoch [14/50] batch [30/51] time 0.179 (0.243) data 0.000 (0.044) loss 0.8596 (0.8363) acc 80.8824 (79.6974) lr 1.7290e-03 eta 0:07:31
epoch [14/50] batch [35/51] time 0.175 (0.234) data 0.000 (0.038) loss 0.8354 (0.8208) acc 83.1731 (80.3049) lr 1.7290e-03 eta 0:07:13
epoch [14/50] batch [40/51] time 0.162 (0.226) data 0.001 (0.033) loss 0.8700 (0.8219) acc 81.5217 (80.1422) lr 1.7290e-03 eta 0:06:57
epoch [14/50] batch [45/51] time 0.152 (0.219) data 0.000 (0.030) loss 1.1315 (0.8307) acc 74.4048 (80.0961) lr 1.7290e-03 eta 0:06:43
epoch [14/50] batch [50/51] time 0.164 (0.214) data 0.000 (0.027) loss 0.7450 (0.8384) acc 81.2500 (79.9728) lr 1.7290e-03 eta 0:06:32
>>> alpha1: 0.326  alpha2: 0.075 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.24 <<<
epoch [15/50] batch [5/51] time 0.168 (0.492) data 0.000 (0.311) loss 0.8218 (0.6875) acc 85.4167 (84.6117) lr 1.6845e-03 eta 0:15:00
epoch [15/50] batch [10/51] time 0.171 (0.333) data 0.000 (0.156) loss 0.6991 (0.7346) acc 83.8235 (82.5665) lr 1.6845e-03 eta 0:10:08
epoch [15/50] batch [15/51] time 0.173 (0.323) data 0.000 (0.104) loss 0.7188 (0.7418) acc 78.8043 (82.2545) lr 1.6845e-03 eta 0:09:47
epoch [15/50] batch [20/51] time 0.186 (0.286) data 0.000 (0.078) loss 0.5314 (0.7323) acc 87.9808 (83.1494) lr 1.6845e-03 eta 0:08:39
epoch [15/50] batch [25/51] time 0.176 (0.265) data 0.000 (0.062) loss 0.6910 (0.7404) acc 85.4167 (82.9369) lr 1.6845e-03 eta 0:07:59
epoch [15/50] batch [30/51] time 0.175 (0.250) data 0.000 (0.052) loss 1.1032 (0.7716) acc 77.1739 (82.0343) lr 1.6845e-03 eta 0:07:30
epoch [15/50] batch [35/51] time 0.168 (0.237) data 0.000 (0.045) loss 0.6651 (0.7835) acc 84.1837 (81.6097) lr 1.6845e-03 eta 0:07:07
epoch [15/50] batch [40/51] time 0.166 (0.229) data 0.000 (0.039) loss 0.6834 (0.7895) acc 82.1429 (81.1597) lr 1.6845e-03 eta 0:06:50
epoch [15/50] batch [45/51] time 0.160 (0.221) data 0.000 (0.035) loss 0.8179 (0.7999) acc 80.8511 (80.8394) lr 1.6845e-03 eta 0:06:36
epoch [15/50] batch [50/51] time 0.156 (0.215) data 0.000 (0.031) loss 0.8017 (0.7991) acc 81.1111 (80.8738) lr 1.6845e-03 eta 0:06:24
>>> alpha1: 0.249  alpha2: -0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.23 <<<
epoch [16/50] batch [5/51] time 0.175 (0.442) data 0.000 (0.267) loss 0.7231 (0.7839) acc 79.6875 (82.2111) lr 1.6374e-03 eta 0:13:07
epoch [16/50] batch [10/51] time 0.192 (0.312) data 0.000 (0.134) loss 0.7938 (0.8231) acc 76.9231 (80.1973) lr 1.6374e-03 eta 0:09:13
epoch [16/50] batch [15/51] time 0.189 (0.266) data 0.000 (0.089) loss 0.7288 (0.8032) acc 86.3208 (81.2827) lr 1.6374e-03 eta 0:07:51
epoch [16/50] batch [20/51] time 0.170 (0.242) data 0.000 (0.067) loss 0.6219 (0.7567) acc 85.5000 (82.6798) lr 1.6374e-03 eta 0:07:06
epoch [16/50] batch [25/51] time 0.170 (0.228) data 0.001 (0.054) loss 1.0538 (0.7614) acc 73.9583 (82.2264) lr 1.6374e-03 eta 0:06:41
epoch [16/50] batch [30/51] time 0.168 (0.218) data 0.000 (0.045) loss 0.8974 (0.7643) acc 73.4043 (81.9777) lr 1.6374e-03 eta 0:06:22
epoch [16/50] batch [35/51] time 0.159 (0.211) data 0.000 (0.038) loss 0.7938 (0.7604) acc 85.5556 (82.2382) lr 1.6374e-03 eta 0:06:09
epoch [16/50] batch [40/51] time 0.170 (0.206) data 0.000 (0.034) loss 0.7425 (0.7477) acc 79.4118 (82.4808) lr 1.6374e-03 eta 0:05:59
epoch [16/50] batch [45/51] time 0.166 (0.202) data 0.000 (0.030) loss 0.6281 (0.7470) acc 85.7143 (82.5365) lr 1.6374e-03 eta 0:05:51
epoch [16/50] batch [50/51] time 0.170 (0.199) data 0.000 (0.027) loss 0.5417 (0.7452) acc 87.7451 (82.6636) lr 1.6374e-03 eta 0:05:45
>>> alpha1: 0.218  alpha2: -0.019 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.24 <<<
epoch [17/50] batch [5/51] time 0.158 (0.502) data 0.000 (0.322) loss 0.8602 (0.7623) acc 81.2500 (82.0446) lr 1.5878e-03 eta 0:14:28
epoch [17/50] batch [10/51] time 0.172 (0.340) data 0.000 (0.161) loss 0.6958 (0.7620) acc 86.2745 (81.1783) lr 1.5878e-03 eta 0:09:45
epoch [17/50] batch [15/51] time 0.171 (0.281) data 0.000 (0.108) loss 0.9421 (0.7666) acc 73.5294 (80.7803) lr 1.5878e-03 eta 0:08:02
epoch [17/50] batch [20/51] time 0.172 (0.255) data 0.000 (0.081) loss 0.5990 (0.7225) acc 87.0000 (82.2544) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [25/51] time 0.167 (0.239) data 0.000 (0.065) loss 0.6234 (0.7287) acc 83.1522 (81.9824) lr 1.5878e-03 eta 0:06:48
epoch [17/50] batch [30/51] time 0.182 (0.228) data 0.000 (0.054) loss 0.6661 (0.7189) acc 83.1731 (82.4077) lr 1.5878e-03 eta 0:06:29
epoch [17/50] batch [35/51] time 0.189 (0.221) data 0.000 (0.046) loss 0.4765 (0.7043) acc 87.2642 (82.6578) lr 1.5878e-03 eta 0:06:14
epoch [17/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.041) loss 1.0210 (0.7233) acc 68.1373 (82.2493) lr 1.5878e-03 eta 0:06:03
epoch [17/50] batch [45/51] time 0.172 (0.210) data 0.000 (0.037) loss 0.9940 (0.7383) acc 74.0385 (81.7652) lr 1.5878e-03 eta 0:05:54
epoch [17/50] batch [50/51] time 0.174 (0.205) data 0.000 (0.033) loss 0.6515 (0.7444) acc 84.9057 (81.8156) lr 1.5878e-03 eta 0:05:45
>>> alpha1: 0.207  alpha2: -0.014 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.22 <<<
epoch [18/50] batch [5/51] time 0.180 (0.862) data 0.000 (0.530) loss 0.5018 (0.6628) acc 91.8182 (86.4161) lr 1.5358e-03 eta 0:24:06
epoch [18/50] batch [10/51] time 0.169 (0.518) data 0.000 (0.265) loss 0.8109 (0.6851) acc 74.5000 (84.4713) lr 1.5358e-03 eta 0:14:27
epoch [18/50] batch [15/51] time 0.167 (0.401) data 0.000 (0.177) loss 0.6701 (0.6832) acc 80.6122 (84.2128) lr 1.5358e-03 eta 0:11:09
epoch [18/50] batch [20/51] time 0.184 (0.345) data 0.000 (0.133) loss 0.5679 (0.6872) acc 86.0577 (84.1434) lr 1.5358e-03 eta 0:09:33
epoch [18/50] batch [25/51] time 0.168 (0.311) data 0.000 (0.106) loss 0.7306 (0.6757) acc 82.0000 (84.5137) lr 1.5358e-03 eta 0:08:35
epoch [18/50] batch [30/51] time 0.171 (0.288) data 0.000 (0.088) loss 0.5577 (0.6909) acc 86.2745 (83.8829) lr 1.5358e-03 eta 0:07:55
epoch [18/50] batch [35/51] time 0.172 (0.271) data 0.000 (0.076) loss 0.8052 (0.7076) acc 81.0000 (83.2736) lr 1.5358e-03 eta 0:07:26
epoch [18/50] batch [40/51] time 0.179 (0.258) data 0.000 (0.066) loss 0.7262 (0.7120) acc 83.6364 (83.1632) lr 1.5358e-03 eta 0:07:04
epoch [18/50] batch [45/51] time 0.178 (0.249) data 0.000 (0.059) loss 0.7126 (0.7142) acc 83.1731 (83.2739) lr 1.5358e-03 eta 0:06:47
epoch [18/50] batch [50/51] time 0.163 (0.240) data 0.000 (0.053) loss 0.8137 (0.7119) acc 79.1667 (83.3697) lr 1.5358e-03 eta 0:06:32
>>> alpha1: 0.199  alpha2: -0.005 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.21 <<<
epoch [19/50] batch [5/51] time 0.171 (0.428) data 0.001 (0.253) loss 0.8319 (0.6941) acc 82.0000 (83.9847) lr 1.4818e-03 eta 0:11:36
epoch [19/50] batch [10/51] time 0.184 (0.302) data 0.000 (0.127) loss 0.6438 (0.7044) acc 87.5000 (84.9564) lr 1.4818e-03 eta 0:08:10
epoch [19/50] batch [15/51] time 0.171 (0.259) data 0.000 (0.085) loss 1.0092 (0.7155) acc 74.4318 (84.2152) lr 1.4818e-03 eta 0:06:59
epoch [19/50] batch [20/51] time 0.165 (0.272) data 0.000 (0.064) loss 0.9800 (0.7050) acc 79.7872 (84.6667) lr 1.4818e-03 eta 0:07:18
epoch [19/50] batch [25/51] time 0.181 (0.253) data 0.000 (0.051) loss 0.6689 (0.7012) acc 78.9216 (84.1983) lr 1.4818e-03 eta 0:06:47
epoch [19/50] batch [30/51] time 0.177 (0.242) data 0.000 (0.042) loss 0.5759 (0.7132) acc 88.7755 (83.8158) lr 1.4818e-03 eta 0:06:26
epoch [19/50] batch [35/51] time 0.175 (0.233) data 0.000 (0.036) loss 0.8077 (0.7080) acc 77.6596 (83.6661) lr 1.4818e-03 eta 0:06:11
epoch [19/50] batch [40/51] time 0.161 (0.225) data 0.000 (0.032) loss 0.8177 (0.7162) acc 78.8043 (83.4784) lr 1.4818e-03 eta 0:05:58
epoch [19/50] batch [45/51] time 0.163 (0.219) data 0.000 (0.028) loss 0.6995 (0.7024) acc 87.2340 (84.0096) lr 1.4818e-03 eta 0:05:47
epoch [19/50] batch [50/51] time 0.165 (0.213) data 0.000 (0.026) loss 0.8017 (0.7072) acc 79.5918 (83.7689) lr 1.4818e-03 eta 0:05:37
>>> alpha1: 0.191  alpha2: -0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [20/50] batch [5/51] time 0.173 (0.492) data 0.000 (0.287) loss 0.7531 (0.6856) acc 83.1522 (84.9414) lr 1.4258e-03 eta 0:12:55
epoch [20/50] batch [10/51] time 0.178 (0.334) data 0.000 (0.144) loss 0.6835 (0.6552) acc 80.0926 (84.7501) lr 1.4258e-03 eta 0:08:44
epoch [20/50] batch [15/51] time 0.187 (0.282) data 0.000 (0.096) loss 0.4487 (0.6703) acc 86.2745 (84.6935) lr 1.4258e-03 eta 0:07:21
epoch [20/50] batch [20/51] time 0.187 (0.258) data 0.001 (0.072) loss 0.8878 (0.6853) acc 77.9412 (84.3909) lr 1.4258e-03 eta 0:06:42
epoch [20/50] batch [25/51] time 0.184 (0.245) data 0.001 (0.058) loss 0.8276 (0.6780) acc 80.0000 (84.4913) lr 1.4258e-03 eta 0:06:20
epoch [20/50] batch [30/51] time 0.183 (0.235) data 0.000 (0.048) loss 0.7150 (0.6899) acc 81.2500 (84.1028) lr 1.4258e-03 eta 0:06:04
epoch [20/50] batch [35/51] time 0.204 (0.228) data 0.000 (0.041) loss 0.8271 (0.6924) acc 80.3191 (84.2444) lr 1.4258e-03 eta 0:05:51
epoch [20/50] batch [40/51] time 0.176 (0.222) data 0.000 (0.036) loss 0.5986 (0.6881) acc 85.7843 (84.2500) lr 1.4258e-03 eta 0:05:41
epoch [20/50] batch [45/51] time 0.165 (0.217) data 0.000 (0.032) loss 0.4662 (0.6799) acc 89.2857 (84.1478) lr 1.4258e-03 eta 0:05:33
epoch [20/50] batch [50/51] time 0.186 (0.212) data 0.000 (0.029) loss 0.4196 (0.6820) acc 90.0000 (83.8656) lr 1.4258e-03 eta 0:05:25
>>> alpha1: 0.186  alpha2: -0.000 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [21/50] batch [5/51] time 0.172 (0.468) data 0.000 (0.293) loss 0.6842 (0.7179) acc 79.9020 (82.1775) lr 1.3681e-03 eta 0:11:53
epoch [21/50] batch [10/51] time 0.171 (0.320) data 0.000 (0.147) loss 0.5678 (0.6777) acc 89.7059 (84.5113) lr 1.3681e-03 eta 0:08:06
epoch [21/50] batch [15/51] time 0.174 (0.273) data 0.000 (0.098) loss 0.7205 (0.6578) acc 85.1064 (84.9918) lr 1.3681e-03 eta 0:06:52
epoch [21/50] batch [20/51] time 0.172 (0.249) data 0.000 (0.074) loss 0.5380 (0.6494) acc 86.7647 (85.1733) lr 1.3681e-03 eta 0:06:15
epoch [21/50] batch [25/51] time 0.165 (0.233) data 0.000 (0.059) loss 0.7301 (0.6518) acc 83.3333 (85.4463) lr 1.3681e-03 eta 0:05:49
epoch [21/50] batch [30/51] time 0.168 (0.223) data 0.000 (0.049) loss 0.6453 (0.6420) acc 79.2553 (85.6372) lr 1.3681e-03 eta 0:05:34
epoch [21/50] batch [35/51] time 0.157 (0.215) data 0.000 (0.042) loss 0.7043 (0.6442) acc 84.4445 (85.5570) lr 1.3681e-03 eta 0:05:21
epoch [21/50] batch [40/51] time 0.161 (0.210) data 0.000 (0.037) loss 0.8116 (0.6468) acc 80.8511 (85.3216) lr 1.3681e-03 eta 0:05:13
epoch [21/50] batch [45/51] time 0.165 (0.206) data 0.000 (0.033) loss 0.8910 (0.6563) acc 78.1250 (84.9649) lr 1.3681e-03 eta 0:05:05
epoch [21/50] batch [50/51] time 0.162 (0.201) data 0.000 (0.030) loss 0.8230 (0.6674) acc 78.2609 (84.4945) lr 1.3681e-03 eta 0:04:57
>>> alpha1: 0.182  alpha2: 0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [22/50] batch [5/51] time 0.180 (0.484) data 0.000 (0.296) loss 0.7379 (0.6632) acc 90.7407 (87.0530) lr 1.3090e-03 eta 0:11:53
epoch [22/50] batch [10/51] time 0.177 (0.327) data 0.000 (0.148) loss 1.0231 (0.7224) acc 78.0000 (85.1937) lr 1.3090e-03 eta 0:08:00
epoch [22/50] batch [15/51] time 0.169 (0.278) data 0.000 (0.099) loss 0.5694 (0.6919) acc 89.0000 (85.2358) lr 1.3090e-03 eta 0:06:47
epoch [22/50] batch [20/51] time 0.174 (0.253) data 0.000 (0.074) loss 0.7751 (0.6850) acc 79.6875 (85.1105) lr 1.3090e-03 eta 0:06:09
epoch [22/50] batch [25/51] time 0.170 (0.238) data 0.000 (0.059) loss 0.7381 (0.6739) acc 81.5000 (85.3265) lr 1.3090e-03 eta 0:05:46
epoch [22/50] batch [30/51] time 0.187 (0.228) data 0.000 (0.049) loss 0.6796 (0.6521) acc 84.3750 (85.8315) lr 1.3090e-03 eta 0:05:30
epoch [22/50] batch [35/51] time 0.180 (0.220) data 0.000 (0.042) loss 0.6383 (0.6519) acc 83.7963 (85.6774) lr 1.3090e-03 eta 0:05:17
epoch [22/50] batch [40/51] time 0.175 (0.215) data 0.000 (0.037) loss 0.5340 (0.6448) acc 90.4255 (85.9186) lr 1.3090e-03 eta 0:05:08
epoch [22/50] batch [45/51] time 0.154 (0.210) data 0.000 (0.033) loss 0.7697 (0.6512) acc 75.0000 (85.4954) lr 1.3090e-03 eta 0:05:00
epoch [22/50] batch [50/51] time 0.171 (0.206) data 0.000 (0.030) loss 0.6603 (0.6444) acc 82.8431 (85.6771) lr 1.3090e-03 eta 0:04:54
>>> alpha1: 0.174  alpha2: 0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [23/50] batch [5/51] time 0.168 (0.473) data 0.000 (0.293) loss 0.6705 (1.0143) acc 85.7143 (82.4238) lr 1.2487e-03 eta 0:11:12
epoch [23/50] batch [10/51] time 0.165 (0.323) data 0.000 (0.147) loss 0.5874 (0.8330) acc 89.0625 (84.8630) lr 1.2487e-03 eta 0:07:38
epoch [23/50] batch [15/51] time 0.180 (0.275) data 0.000 (0.098) loss 0.7261 (0.7627) acc 86.7347 (85.6086) lr 1.2487e-03 eta 0:06:28
epoch [23/50] batch [20/51] time 0.190 (0.251) data 0.000 (0.073) loss 0.6480 (0.7032) acc 81.1321 (85.8027) lr 1.2487e-03 eta 0:05:53
epoch [23/50] batch [25/51] time 0.170 (0.236) data 0.000 (0.059) loss 0.6438 (0.6865) acc 86.0000 (85.7594) lr 1.2487e-03 eta 0:05:30
epoch [23/50] batch [30/51] time 0.170 (0.226) data 0.000 (0.049) loss 0.5220 (0.6769) acc 87.2449 (85.8489) lr 1.2487e-03 eta 0:05:16
epoch [23/50] batch [35/51] time 0.182 (0.219) data 0.001 (0.042) loss 0.5886 (0.6731) acc 83.3333 (85.7188) lr 1.2487e-03 eta 0:05:05
epoch [23/50] batch [40/51] time 0.161 (0.213) data 0.001 (0.037) loss 0.6538 (0.6685) acc 86.4130 (85.5614) lr 1.2487e-03 eta 0:04:56
epoch [23/50] batch [45/51] time 0.159 (0.208) data 0.000 (0.033) loss 0.8146 (0.6578) acc 78.3333 (85.5524) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.163 (0.204) data 0.000 (0.030) loss 0.7257 (0.6646) acc 80.8511 (85.2888) lr 1.2487e-03 eta 0:04:41
>>> alpha1: 0.170  alpha2: -0.003 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [24/50] batch [5/51] time 0.191 (0.515) data 0.001 (0.333) loss 0.6524 (0.7225) acc 85.4167 (87.5228) lr 1.1874e-03 eta 0:11:46
epoch [24/50] batch [10/51] time 0.184 (0.350) data 0.000 (0.167) loss 0.6690 (0.6759) acc 80.8511 (86.6016) lr 1.1874e-03 eta 0:07:58
epoch [24/50] batch [15/51] time 0.177 (0.292) data 0.000 (0.111) loss 0.8413 (0.6643) acc 82.8125 (86.3857) lr 1.1874e-03 eta 0:06:38
epoch [24/50] batch [20/51] time 0.172 (0.264) data 0.000 (0.083) loss 0.7618 (0.6467) acc 86.0000 (86.6320) lr 1.1874e-03 eta 0:05:58
epoch [24/50] batch [25/51] time 0.193 (0.251) data 0.001 (0.067) loss 0.3987 (0.6196) acc 90.6250 (87.0116) lr 1.1874e-03 eta 0:05:38
epoch [24/50] batch [30/51] time 0.191 (0.239) data 0.001 (0.056) loss 0.7254 (0.6177) acc 83.0000 (86.8221) lr 1.1874e-03 eta 0:05:22
epoch [24/50] batch [35/51] time 0.175 (0.232) data 0.001 (0.048) loss 0.5585 (0.6186) acc 87.5000 (86.4642) lr 1.1874e-03 eta 0:05:11
epoch [24/50] batch [40/51] time 0.171 (0.224) data 0.000 (0.042) loss 0.5334 (0.6345) acc 85.7843 (86.1442) lr 1.1874e-03 eta 0:05:00
epoch [24/50] batch [45/51] time 0.165 (0.218) data 0.000 (0.037) loss 0.7433 (0.6299) acc 83.3333 (86.0760) lr 1.1874e-03 eta 0:04:50
epoch [24/50] batch [50/51] time 0.179 (0.213) data 0.000 (0.034) loss 0.5011 (0.6287) acc 90.5660 (86.0505) lr 1.1874e-03 eta 0:04:42
>>> alpha1: 0.166  alpha2: -0.005 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.19 <<<
epoch [25/50] batch [5/51] time 0.158 (0.469) data 0.000 (0.298) loss 0.7442 (0.5899) acc 86.3636 (85.8437) lr 1.1253e-03 eta 0:10:19
epoch [25/50] batch [10/51] time 0.170 (0.326) data 0.000 (0.151) loss 0.6821 (0.6057) acc 83.5000 (86.3541) lr 1.1253e-03 eta 0:07:09
epoch [25/50] batch [15/51] time 0.172 (0.276) data 0.000 (0.100) loss 0.6027 (0.6211) acc 90.1961 (86.0794) lr 1.1253e-03 eta 0:06:01
epoch [25/50] batch [20/51] time 0.180 (0.250) data 0.000 (0.075) loss 0.4102 (0.6017) acc 93.8679 (86.5309) lr 1.1253e-03 eta 0:05:26
epoch [25/50] batch [25/51] time 0.182 (0.235) data 0.000 (0.061) loss 0.6717 (0.5929) acc 86.1111 (87.0842) lr 1.1253e-03 eta 0:05:05
epoch [25/50] batch [30/51] time 0.171 (0.225) data 0.000 (0.051) loss 0.6089 (0.5966) acc 86.4583 (86.7418) lr 1.1253e-03 eta 0:04:51
epoch [25/50] batch [35/51] time 0.174 (0.218) data 0.000 (0.044) loss 0.4291 (0.5922) acc 87.5000 (86.7545) lr 1.1253e-03 eta 0:04:40
epoch [25/50] batch [40/51] time 0.171 (0.212) data 0.000 (0.038) loss 0.5378 (0.5901) acc 87.7451 (86.8449) lr 1.1253e-03 eta 0:04:32
epoch [25/50] batch [45/51] time 0.158 (0.207) data 0.000 (0.034) loss 0.6161 (0.5872) acc 84.4445 (86.8788) lr 1.1253e-03 eta 0:04:25
epoch [25/50] batch [50/51] time 0.173 (0.203) data 0.000 (0.031) loss 0.6777 (0.5929) acc 85.0962 (86.9817) lr 1.1253e-03 eta 0:04:19
>>> alpha1: 0.164  alpha2: -0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.20 <<<
epoch [26/50] batch [5/51] time 0.174 (0.535) data 0.000 (0.355) loss 0.5928 (0.5827) acc 85.5769 (87.1098) lr 1.0628e-03 eta 0:11:19
epoch [26/50] batch [10/51] time 0.174 (0.355) data 0.000 (0.178) loss 0.6029 (0.5883) acc 86.0577 (87.1479) lr 1.0628e-03 eta 0:07:29
epoch [26/50] batch [15/51] time 0.162 (0.294) data 0.000 (0.119) loss 0.8138 (0.5977) acc 81.3830 (86.5955) lr 1.0628e-03 eta 0:06:10
epoch [26/50] batch [20/51] time 0.180 (0.263) data 0.000 (0.089) loss 0.4509 (0.5892) acc 88.8889 (86.3860) lr 1.0628e-03 eta 0:05:30
epoch [26/50] batch [25/51] time 0.186 (0.246) data 0.000 (0.071) loss 0.7288 (0.6760) acc 79.8246 (85.2642) lr 1.0628e-03 eta 0:05:07
epoch [26/50] batch [30/51] time 0.171 (0.234) data 0.000 (0.059) loss 0.4592 (0.6539) acc 88.5000 (85.7601) lr 1.0628e-03 eta 0:04:50
epoch [26/50] batch [35/51] time 0.167 (0.226) data 0.000 (0.051) loss 0.4934 (0.6447) acc 88.0208 (85.9155) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [40/51] time 0.175 (0.220) data 0.000 (0.045) loss 0.6847 (0.6434) acc 84.8039 (85.9526) lr 1.0628e-03 eta 0:04:31
epoch [26/50] batch [45/51] time 0.164 (0.214) data 0.000 (0.040) loss 0.6141 (0.6337) acc 89.5833 (86.1669) lr 1.0628e-03 eta 0:04:23
epoch [26/50] batch [50/51] time 0.161 (0.210) data 0.000 (0.036) loss 0.6707 (0.6493) acc 81.9149 (86.1396) lr 1.0628e-03 eta 0:04:17
>>> alpha1: 0.161  alpha2: -0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [27/50] batch [5/51] time 0.165 (0.489) data 0.000 (0.311) loss 0.5962 (0.5357) acc 87.2340 (88.2924) lr 1.0000e-03 eta 0:09:56
epoch [27/50] batch [10/51] time 0.187 (0.331) data 0.000 (0.155) loss 0.3937 (0.5450) acc 91.0377 (87.6928) lr 1.0000e-03 eta 0:06:41
epoch [27/50] batch [15/51] time 0.193 (0.281) data 0.000 (0.104) loss 0.6067 (0.5590) acc 87.2642 (87.3033) lr 1.0000e-03 eta 0:05:39
epoch [27/50] batch [20/51] time 0.183 (0.254) data 0.000 (0.078) loss 0.3339 (0.5606) acc 94.5455 (87.4234) lr 1.0000e-03 eta 0:05:06
epoch [27/50] batch [25/51] time 0.175 (0.239) data 0.000 (0.062) loss 0.5341 (0.5594) acc 88.9423 (87.7585) lr 1.0000e-03 eta 0:04:46
epoch [27/50] batch [30/51] time 0.173 (0.228) data 0.000 (0.052) loss 0.6916 (0.5751) acc 82.4468 (87.1639) lr 1.0000e-03 eta 0:04:31
epoch [27/50] batch [35/51] time 0.170 (0.220) data 0.000 (0.045) loss 0.6923 (0.5796) acc 81.0000 (86.7405) lr 1.0000e-03 eta 0:04:21
epoch [27/50] batch [40/51] time 0.172 (0.214) data 0.000 (0.039) loss 0.5543 (0.5690) acc 87.7451 (86.9505) lr 1.0000e-03 eta 0:04:13
epoch [27/50] batch [45/51] time 0.176 (0.210) data 0.000 (0.035) loss 0.4870 (0.5639) acc 89.6226 (87.1749) lr 1.0000e-03 eta 0:04:08
epoch [27/50] batch [50/51] time 0.173 (0.207) data 0.000 (0.031) loss 0.4673 (0.5597) acc 89.9038 (87.2099) lr 1.0000e-03 eta 0:04:02
>>> alpha1: 0.161  alpha2: 0.003 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [28/50] batch [5/51] time 0.177 (0.488) data 0.000 (0.302) loss 0.7718 (0.6147) acc 84.6154 (86.8719) lr 9.3721e-04 eta 0:09:29
epoch [28/50] batch [10/51] time 0.168 (0.332) data 0.000 (0.151) loss 0.5584 (0.5838) acc 88.7755 (87.4037) lr 9.3721e-04 eta 0:06:25
epoch [28/50] batch [15/51] time 0.175 (0.279) data 0.000 (0.101) loss 0.8321 (0.5972) acc 78.9216 (86.1171) lr 9.3721e-04 eta 0:05:23
epoch [28/50] batch [20/51] time 0.171 (0.253) data 0.000 (0.076) loss 0.3328 (0.5866) acc 91.1765 (86.2222) lr 9.3721e-04 eta 0:04:51
epoch [28/50] batch [25/51] time 0.185 (0.239) data 0.000 (0.061) loss 0.4342 (0.5720) acc 95.2830 (86.7808) lr 9.3721e-04 eta 0:04:34
epoch [28/50] batch [30/51] time 0.196 (0.230) data 0.000 (0.051) loss 0.4977 (0.5767) acc 85.2941 (86.9070) lr 9.3721e-04 eta 0:04:23
epoch [28/50] batch [35/51] time 0.182 (0.224) data 0.000 (0.043) loss 0.4171 (0.5651) acc 92.7273 (87.3199) lr 9.3721e-04 eta 0:04:15
epoch [28/50] batch [40/51] time 0.166 (0.217) data 0.000 (0.038) loss 0.9097 (0.5729) acc 76.0417 (86.9849) lr 9.3721e-04 eta 0:04:06
epoch [28/50] batch [45/51] time 0.174 (0.213) data 0.001 (0.034) loss 0.5177 (0.5674) acc 88.5000 (87.1940) lr 9.3721e-04 eta 0:04:00
epoch [28/50] batch [50/51] time 0.166 (0.208) data 0.000 (0.030) loss 0.5726 (0.5639) acc 85.7143 (87.2424) lr 9.3721e-04 eta 0:03:54
>>> alpha1: 0.157  alpha2: 0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.18 <<<
epoch [29/50] batch [5/51] time 0.169 (0.473) data 0.000 (0.290) loss 0.4361 (0.4527) acc 90.8163 (90.0896) lr 8.7467e-04 eta 0:08:48
epoch [29/50] batch [10/51] time 0.188 (0.329) data 0.000 (0.145) loss 0.5722 (0.5108) acc 88.4615 (89.0087) lr 8.7467e-04 eta 0:06:05
epoch [29/50] batch [15/51] time 0.169 (0.277) data 0.000 (0.097) loss 0.6374 (0.5169) acc 88.7755 (88.8982) lr 8.7467e-04 eta 0:05:06
epoch [29/50] batch [20/51] time 0.167 (0.252) data 0.000 (0.073) loss 0.5846 (0.5230) acc 86.9792 (88.4582) lr 8.7467e-04 eta 0:04:37
epoch [29/50] batch [25/51] time 0.173 (0.237) data 0.000 (0.058) loss 0.4080 (0.5468) acc 90.6863 (87.7954) lr 8.7467e-04 eta 0:04:19
epoch [29/50] batch [30/51] time 0.175 (0.227) data 0.000 (0.049) loss 0.7077 (0.5463) acc 81.3726 (87.7663) lr 8.7467e-04 eta 0:04:07
epoch [29/50] batch [35/51] time 0.170 (0.219) data 0.000 (0.042) loss 0.5465 (0.5463) acc 89.0625 (87.8698) lr 8.7467e-04 eta 0:03:58
epoch [29/50] batch [40/51] time 0.159 (0.214) data 0.000 (0.037) loss 0.6740 (0.5507) acc 83.6956 (87.7140) lr 8.7467e-04 eta 0:03:51
epoch [29/50] batch [45/51] time 0.180 (0.208) data 0.000 (0.033) loss 0.3054 (0.5423) acc 93.6364 (87.8135) lr 8.7467e-04 eta 0:03:44
epoch [29/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.029) loss 0.8219 (0.5483) acc 76.6304 (87.7172) lr 8.7467e-04 eta 0:03:39
>>> alpha1: 0.152  alpha2: 0.007 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [30/50] batch [5/51] time 0.157 (0.455) data 0.000 (0.282) loss 0.6606 (0.5659) acc 81.2500 (85.6325) lr 8.1262e-04 eta 0:08:05
epoch [30/50] batch [10/51] time 0.165 (0.311) data 0.001 (0.141) loss 0.4569 (0.5569) acc 89.1304 (86.9027) lr 8.1262e-04 eta 0:05:30
epoch [30/50] batch [15/51] time 0.189 (0.267) data 0.000 (0.094) loss 0.3983 (0.5205) acc 94.4444 (88.1340) lr 8.1262e-04 eta 0:04:41
epoch [30/50] batch [20/51] time 0.179 (0.244) data 0.000 (0.071) loss 0.6177 (0.5202) acc 87.5000 (88.3098) lr 8.1262e-04 eta 0:04:16
epoch [30/50] batch [25/51] time 0.179 (0.231) data 0.000 (0.057) loss 0.5558 (0.5263) acc 88.8889 (88.1284) lr 8.1262e-04 eta 0:04:01
epoch [30/50] batch [30/51] time 0.186 (0.222) data 0.000 (0.047) loss 0.4390 (0.5259) acc 94.1964 (88.3139) lr 8.1262e-04 eta 0:03:50
epoch [30/50] batch [35/51] time 0.181 (0.215) data 0.001 (0.041) loss 0.5906 (0.5381) acc 86.9792 (88.0644) lr 8.1262e-04 eta 0:03:42
epoch [30/50] batch [40/51] time 0.185 (0.210) data 0.000 (0.036) loss 0.4807 (0.5484) acc 87.2807 (87.6803) lr 8.1262e-04 eta 0:03:36
epoch [30/50] batch [45/51] time 0.183 (0.206) data 0.000 (0.032) loss 0.4931 (0.5471) acc 87.2727 (87.7207) lr 8.1262e-04 eta 0:03:31
epoch [30/50] batch [50/51] time 0.161 (0.202) data 0.000 (0.028) loss 0.3942 (0.5508) acc 92.3913 (87.5092) lr 8.1262e-04 eta 0:03:26
>>> alpha1: 0.148  alpha2: 0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [31/50] batch [5/51] time 0.172 (0.442) data 0.000 (0.270) loss 0.5255 (0.4840) acc 88.7255 (91.4686) lr 7.5131e-04 eta 0:07:28
epoch [31/50] batch [10/51] time 0.161 (0.307) data 0.000 (0.135) loss 0.6946 (0.5144) acc 80.4348 (89.2378) lr 7.5131e-04 eta 0:05:10
epoch [31/50] batch [15/51] time 0.175 (0.263) data 0.000 (0.090) loss 0.4691 (0.5170) acc 89.9038 (88.6495) lr 7.5131e-04 eta 0:04:24
epoch [31/50] batch [20/51] time 0.177 (0.242) data 0.000 (0.068) loss 0.5265 (0.5177) acc 85.8696 (88.4600) lr 7.5131e-04 eta 0:04:01
epoch [31/50] batch [25/51] time 0.195 (0.229) data 0.000 (0.054) loss 0.5836 (0.5320) acc 91.3265 (88.2398) lr 7.5131e-04 eta 0:03:48
epoch [31/50] batch [30/51] time 0.161 (0.218) data 0.000 (0.045) loss 0.6137 (0.5397) acc 86.9565 (88.0259) lr 7.5131e-04 eta 0:03:36
epoch [31/50] batch [35/51] time 0.167 (0.211) data 0.000 (0.039) loss 0.5466 (0.5479) acc 87.2449 (87.6839) lr 7.5131e-04 eta 0:03:28
epoch [31/50] batch [40/51] time 0.179 (0.207) data 0.000 (0.034) loss 0.3675 (0.5431) acc 91.5094 (87.8636) lr 7.5131e-04 eta 0:03:23
epoch [31/50] batch [45/51] time 0.175 (0.204) data 0.000 (0.030) loss 0.6702 (0.5409) acc 86.5385 (88.0445) lr 7.5131e-04 eta 0:03:18
epoch [31/50] batch [50/51] time 0.177 (0.201) data 0.000 (0.027) loss 0.6372 (0.5534) acc 84.4340 (87.6613) lr 7.5131e-04 eta 0:03:15
>>> alpha1: 0.146  alpha2: 0.011 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.17 <<<
epoch [32/50] batch [5/51] time 0.174 (0.481) data 0.000 (0.298) loss 0.4334 (0.4668) acc 88.9423 (90.3548) lr 6.9098e-04 eta 0:07:44
epoch [32/50] batch [10/51] time 0.164 (0.328) data 0.000 (0.149) loss 0.6414 (0.5182) acc 84.5745 (88.3557) lr 6.9098e-04 eta 0:05:14
epoch [32/50] batch [15/51] time 0.175 (0.275) data 0.000 (0.100) loss 0.4237 (0.5330) acc 90.8654 (87.7594) lr 6.9098e-04 eta 0:04:22
epoch [32/50] batch [20/51] time 0.172 (0.251) data 0.000 (0.075) loss 0.5502 (0.5306) acc 90.0000 (87.8489) lr 6.9098e-04 eta 0:03:58
epoch [32/50] batch [25/51] time 0.179 (0.237) data 0.000 (0.060) loss 0.4441 (0.5203) acc 88.8889 (88.2426) lr 6.9098e-04 eta 0:03:43
epoch [32/50] batch [30/51] time 0.168 (0.226) data 0.000 (0.050) loss 0.5647 (0.5199) acc 91.8367 (88.4223) lr 6.9098e-04 eta 0:03:32
epoch [32/50] batch [35/51] time 0.168 (0.219) data 0.000 (0.043) loss 0.5672 (0.5232) acc 88.7755 (88.5304) lr 6.9098e-04 eta 0:03:24
epoch [32/50] batch [40/51] time 0.174 (0.213) data 0.001 (0.038) loss 0.4349 (0.5197) acc 88.2353 (88.4650) lr 6.9098e-04 eta 0:03:18
epoch [32/50] batch [45/51] time 0.178 (0.209) data 0.000 (0.033) loss 0.4266 (0.5223) acc 91.9811 (88.3685) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [50/51] time 0.164 (0.205) data 0.000 (0.030) loss 0.4788 (0.5163) acc 90.1042 (88.4523) lr 6.9098e-04 eta 0:03:08
>>> alpha1: 0.144  alpha2: 0.012 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [33/50] batch [5/51] time 0.168 (0.538) data 0.000 (0.357) loss 0.3877 (0.4722) acc 89.7959 (89.8871) lr 6.3188e-04 eta 0:08:11
epoch [33/50] batch [10/51] time 0.172 (0.359) data 0.001 (0.179) loss 0.4290 (0.4827) acc 89.0000 (89.0761) lr 6.3188e-04 eta 0:05:26
epoch [33/50] batch [15/51] time 0.185 (0.299) data 0.000 (0.119) loss 0.5969 (0.4928) acc 83.5106 (88.5411) lr 6.3188e-04 eta 0:04:30
epoch [33/50] batch [20/51] time 0.159 (0.269) data 0.000 (0.090) loss 0.4916 (0.4999) acc 86.9318 (88.1890) lr 6.3188e-04 eta 0:04:01
epoch [33/50] batch [25/51] time 0.191 (0.251) data 0.000 (0.072) loss 0.5844 (0.4987) acc 84.4340 (88.2751) lr 6.3188e-04 eta 0:03:43
epoch [33/50] batch [30/51] time 0.180 (0.238) data 0.000 (0.060) loss 0.4534 (0.4998) acc 90.3061 (88.3987) lr 6.3188e-04 eta 0:03:31
epoch [33/50] batch [35/51] time 0.190 (0.229) data 0.001 (0.051) loss 0.5720 (0.5090) acc 87.0192 (88.1364) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [40/51] time 0.167 (0.222) data 0.000 (0.045) loss 0.5824 (0.5120) acc 88.7755 (88.3519) lr 6.3188e-04 eta 0:03:15
epoch [33/50] batch [45/51] time 0.171 (0.217) data 0.000 (0.040) loss 0.4247 (0.4972) acc 88.7255 (88.7796) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [50/51] time 0.168 (0.212) data 0.000 (0.036) loss 0.6685 (0.5062) acc 81.5000 (88.4245) lr 6.3188e-04 eta 0:03:04
>>> alpha1: 0.143  alpha2: 0.014 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [34/50] batch [5/51] time 0.194 (0.462) data 0.000 (0.275) loss 0.4652 (0.5004) acc 88.0000 (89.0421) lr 5.7422e-04 eta 0:06:37
epoch [34/50] batch [10/51] time 0.163 (0.318) data 0.000 (0.137) loss 0.5554 (0.5220) acc 89.6739 (88.9616) lr 5.7422e-04 eta 0:04:32
epoch [34/50] batch [15/51] time 0.186 (0.273) data 0.000 (0.092) loss 0.5183 (0.5127) acc 87.5000 (88.7614) lr 5.7422e-04 eta 0:03:52
epoch [34/50] batch [20/51] time 0.162 (0.248) data 0.000 (0.069) loss 0.7212 (0.5178) acc 86.9565 (88.4774) lr 5.7422e-04 eta 0:03:30
epoch [34/50] batch [25/51] time 0.160 (0.234) data 0.000 (0.055) loss 0.6116 (0.5069) acc 89.6739 (89.1054) lr 5.7422e-04 eta 0:03:16
epoch [34/50] batch [30/51] time 0.177 (0.224) data 0.000 (0.046) loss 0.5045 (0.5201) acc 90.5660 (88.7094) lr 5.7422e-04 eta 0:03:07
epoch [34/50] batch [35/51] time 0.171 (0.217) data 0.000 (0.039) loss 0.3572 (0.5054) acc 92.1569 (89.0386) lr 5.7422e-04 eta 0:03:00
epoch [34/50] batch [40/51] time 0.166 (0.211) data 0.000 (0.035) loss 0.3769 (0.5028) acc 92.8571 (89.1130) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [45/51] time 0.171 (0.206) data 0.000 (0.031) loss 0.3540 (0.5040) acc 91.6667 (89.0626) lr 5.7422e-04 eta 0:02:49
epoch [34/50] batch [50/51] time 0.176 (0.203) data 0.000 (0.028) loss 0.5248 (0.5070) acc 88.4615 (88.9031) lr 5.7422e-04 eta 0:02:45
>>> alpha1: 0.143  alpha2: 0.015 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [35/50] batch [5/51] time 0.182 (0.424) data 0.000 (0.242) loss 0.4819 (0.4654) acc 89.5000 (89.7555) lr 5.1825e-04 eta 0:05:44
epoch [35/50] batch [10/51] time 0.183 (0.301) data 0.000 (0.121) loss 0.4131 (0.5159) acc 88.8393 (89.7047) lr 5.1825e-04 eta 0:04:02
epoch [35/50] batch [15/51] time 0.172 (0.261) data 0.000 (0.081) loss 0.4602 (0.4836) acc 90.0000 (90.3598) lr 5.1825e-04 eta 0:03:29
epoch [35/50] batch [20/51] time 0.182 (0.240) data 0.001 (0.061) loss 0.3679 (0.4915) acc 92.1296 (89.8272) lr 5.1825e-04 eta 0:03:10
epoch [35/50] batch [25/51] time 0.165 (0.226) data 0.000 (0.049) loss 0.5527 (0.5325) acc 90.1042 (89.2906) lr 5.1825e-04 eta 0:02:58
epoch [35/50] batch [30/51] time 0.182 (0.217) data 0.000 (0.041) loss 0.4226 (0.5340) acc 88.7255 (89.0756) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [35/51] time 0.173 (0.211) data 0.000 (0.035) loss 0.6253 (0.5280) acc 82.6087 (89.1248) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [40/51] time 0.173 (0.206) data 0.000 (0.031) loss 0.3768 (0.5228) acc 91.3462 (89.0477) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [45/51] time 0.172 (0.202) data 0.000 (0.027) loss 0.4849 (0.5293) acc 89.7059 (88.7514) lr 5.1825e-04 eta 0:02:35
epoch [35/50] batch [50/51] time 0.177 (0.199) data 0.000 (0.025) loss 0.5216 (0.5303) acc 89.1509 (88.7097) lr 5.1825e-04 eta 0:02:32
>>> alpha1: 0.140  alpha2: 0.013 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [36/50] batch [5/51] time 0.170 (0.433) data 0.000 (0.260) loss 0.6554 (0.5101) acc 87.7660 (89.6051) lr 4.6417e-04 eta 0:05:29
epoch [36/50] batch [10/51] time 0.194 (0.306) data 0.000 (0.130) loss 0.4528 (0.4639) acc 92.7273 (90.8629) lr 4.6417e-04 eta 0:03:50
epoch [36/50] batch [15/51] time 0.172 (0.263) data 0.001 (0.087) loss 0.4794 (0.4877) acc 90.0000 (90.1942) lr 4.6417e-04 eta 0:03:17
epoch [36/50] batch [20/51] time 0.180 (0.244) data 0.000 (0.065) loss 0.4120 (0.4899) acc 92.9245 (90.1576) lr 4.6417e-04 eta 0:03:01
epoch [36/50] batch [25/51] time 0.165 (0.231) data 0.000 (0.052) loss 0.4110 (0.4909) acc 90.1042 (89.6746) lr 4.6417e-04 eta 0:02:50
epoch [36/50] batch [30/51] time 0.164 (0.222) data 0.000 (0.044) loss 0.5326 (0.5002) acc 92.0213 (89.4865) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [35/51] time 0.176 (0.216) data 0.000 (0.038) loss 0.5265 (0.4988) acc 85.5000 (89.3738) lr 4.6417e-04 eta 0:02:37
epoch [36/50] batch [40/51] time 0.165 (0.211) data 0.000 (0.033) loss 0.4172 (0.4909) acc 89.5833 (89.5289) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [45/51] time 0.168 (0.207) data 0.000 (0.029) loss 0.5718 (0.5032) acc 84.1837 (88.9732) lr 4.6417e-04 eta 0:02:28
epoch [36/50] batch [50/51] time 0.178 (0.204) data 0.001 (0.026) loss 0.4579 (0.5060) acc 88.2075 (88.7571) lr 4.6417e-04 eta 0:02:25
>>> alpha1: 0.139  alpha2: 0.015 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [37/50] batch [5/51] time 0.171 (0.471) data 0.000 (0.295) loss 0.3813 (0.4566) acc 92.5000 (90.0646) lr 4.1221e-04 eta 0:05:34
epoch [37/50] batch [10/51] time 0.171 (0.325) data 0.000 (0.148) loss 0.3370 (0.4742) acc 93.1373 (89.4964) lr 4.1221e-04 eta 0:03:48
epoch [37/50] batch [15/51] time 0.186 (0.276) data 0.000 (0.098) loss 0.5062 (0.5394) acc 91.8269 (88.1801) lr 4.1221e-04 eta 0:03:13
epoch [37/50] batch [20/51] time 0.194 (0.251) data 0.000 (0.074) loss 0.4379 (0.5439) acc 90.0943 (88.1706) lr 4.1221e-04 eta 0:02:54
epoch [37/50] batch [25/51] time 0.177 (0.237) data 0.000 (0.059) loss 0.4735 (0.5306) acc 91.1458 (88.6316) lr 4.1221e-04 eta 0:02:43
epoch [37/50] batch [30/51] time 0.170 (0.227) data 0.000 (0.049) loss 0.5933 (0.5290) acc 86.0000 (88.4148) lr 4.1221e-04 eta 0:02:35
epoch [37/50] batch [35/51] time 0.183 (0.219) data 0.000 (0.042) loss 0.4360 (0.5278) acc 91.3462 (88.4363) lr 4.1221e-04 eta 0:02:29
epoch [37/50] batch [40/51] time 0.177 (0.215) data 0.000 (0.037) loss 0.2846 (0.5170) acc 95.7547 (88.6029) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [45/51] time 0.165 (0.209) data 0.000 (0.033) loss 0.5316 (0.5152) acc 91.6667 (88.6895) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [50/51] time 0.179 (0.206) data 0.000 (0.030) loss 0.6168 (0.5168) acc 85.6481 (88.6413) lr 4.1221e-04 eta 0:02:16
>>> alpha1: 0.140  alpha2: 0.019 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.16 <<<
epoch [38/50] batch [5/51] time 0.173 (0.458) data 0.000 (0.277) loss 0.5969 (0.4982) acc 83.6735 (89.4350) lr 3.6258e-04 eta 0:05:01
epoch [38/50] batch [10/51] time 0.182 (0.320) data 0.000 (0.139) loss 0.3869 (0.5065) acc 93.1818 (89.1660) lr 3.6258e-04 eta 0:03:28
epoch [38/50] batch [15/51] time 0.175 (0.269) data 0.000 (0.093) loss 0.3829 (0.4971) acc 93.7500 (89.0475) lr 3.6258e-04 eta 0:02:54
epoch [38/50] batch [20/51] time 0.169 (0.246) data 0.000 (0.070) loss 0.5421 (0.4922) acc 89.0000 (89.4547) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [25/51] time 0.181 (0.232) data 0.000 (0.056) loss 0.4980 (0.5005) acc 88.1818 (88.7023) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.181 (0.223) data 0.000 (0.047) loss 0.4024 (0.4869) acc 90.9091 (89.2412) lr 3.6258e-04 eta 0:02:20
epoch [38/50] batch [35/51] time 0.167 (0.215) data 0.001 (0.040) loss 0.4782 (0.4932) acc 91.8367 (89.2570) lr 3.6258e-04 eta 0:02:14
epoch [38/50] batch [40/51] time 0.171 (0.210) data 0.000 (0.035) loss 0.5457 (0.4945) acc 89.2157 (89.1667) lr 3.6258e-04 eta 0:02:10
epoch [38/50] batch [45/51] time 0.182 (0.206) data 0.000 (0.031) loss 0.5158 (0.5011) acc 89.7321 (88.9113) lr 3.6258e-04 eta 0:02:07
epoch [38/50] batch [50/51] time 0.188 (0.203) data 0.000 (0.028) loss 0.3787 (0.5060) acc 94.2308 (88.9281) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.139  alpha2: 0.023 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [39/50] batch [5/51] time 0.210 (0.422) data 0.000 (0.244) loss 0.4946 (0.6081) acc 87.7451 (86.2339) lr 3.1545e-04 eta 0:04:16
epoch [39/50] batch [10/51] time 0.166 (0.299) data 0.000 (0.122) loss 0.3809 (0.5438) acc 92.3469 (88.3949) lr 3.1545e-04 eta 0:03:00
epoch [39/50] batch [15/51] time 0.174 (0.257) data 0.000 (0.081) loss 0.3895 (0.5342) acc 93.1373 (88.5556) lr 3.1545e-04 eta 0:02:33
epoch [39/50] batch [20/51] time 0.173 (0.239) data 0.000 (0.061) loss 0.5259 (0.5222) acc 86.7647 (88.7428) lr 3.1545e-04 eta 0:02:21
epoch [39/50] batch [25/51] time 0.182 (0.228) data 0.001 (0.049) loss 0.6550 (0.5222) acc 87.2642 (88.6028) lr 3.1545e-04 eta 0:02:13
epoch [39/50] batch [30/51] time 0.173 (0.220) data 0.000 (0.041) loss 0.6462 (0.5181) acc 85.7843 (88.8630) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [35/51] time 0.196 (0.214) data 0.000 (0.035) loss 0.4925 (0.5095) acc 91.5179 (89.1563) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [40/51] time 0.164 (0.209) data 0.000 (0.031) loss 0.4452 (0.5116) acc 93.7500 (89.1436) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [45/51] time 0.175 (0.205) data 0.001 (0.027) loss 0.4423 (0.5069) acc 89.4231 (89.1331) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [50/51] time 0.171 (0.202) data 0.000 (0.025) loss 0.3683 (0.5023) acc 94.1176 (89.1886) lr 3.1545e-04 eta 0:01:53
>>> alpha1: 0.139  alpha2: 0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [40/50] batch [5/51] time 0.175 (0.484) data 0.000 (0.306) loss 0.5892 (0.5888) acc 88.9423 (87.9675) lr 2.7103e-04 eta 0:04:29
epoch [40/50] batch [10/51] time 0.168 (0.335) data 0.000 (0.153) loss 0.6490 (0.5465) acc 85.2041 (88.2283) lr 2.7103e-04 eta 0:03:04
epoch [40/50] batch [15/51] time 0.187 (0.282) data 0.000 (0.102) loss 0.5078 (0.5342) acc 89.0351 (88.8131) lr 2.7103e-04 eta 0:02:33
epoch [40/50] batch [20/51] time 0.188 (0.256) data 0.000 (0.077) loss 0.3415 (0.4962) acc 96.2963 (89.8362) lr 2.7103e-04 eta 0:02:18
epoch [40/50] batch [25/51] time 0.174 (0.240) data 0.000 (0.061) loss 0.4822 (0.4953) acc 87.5000 (89.9331) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [30/51] time 0.172 (0.229) data 0.001 (0.051) loss 0.3825 (0.4941) acc 87.2449 (89.5577) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [35/51] time 0.186 (0.221) data 0.000 (0.044) loss 0.3846 (0.4889) acc 91.8269 (89.7543) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [40/51] time 0.168 (0.216) data 0.000 (0.038) loss 0.5407 (0.4834) acc 91.0000 (89.9796) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [45/51] time 0.172 (0.210) data 0.000 (0.034) loss 0.5534 (0.4832) acc 87.2549 (89.8527) lr 2.7103e-04 eta 0:01:48
epoch [40/50] batch [50/51] time 0.168 (0.207) data 0.000 (0.031) loss 0.6304 (0.4854) acc 87.5000 (89.7883) lr 2.7103e-04 eta 0:01:45
>>> alpha1: 0.138  alpha2: 0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [41/50] batch [5/51] time 0.175 (0.481) data 0.000 (0.301) loss 0.4792 (0.5251) acc 87.7451 (87.9738) lr 2.2949e-04 eta 0:04:02
epoch [41/50] batch [10/51] time 0.172 (0.328) data 0.001 (0.151) loss 0.6004 (0.5179) acc 86.2745 (88.4974) lr 2.2949e-04 eta 0:02:43
epoch [41/50] batch [15/51] time 0.176 (0.278) data 0.000 (0.100) loss 0.6367 (0.4818) acc 86.2245 (89.2861) lr 2.2949e-04 eta 0:02:17
epoch [41/50] batch [20/51] time 0.177 (0.255) data 0.000 (0.075) loss 0.5043 (0.4734) acc 87.2449 (89.3528) lr 2.2949e-04 eta 0:02:04
epoch [41/50] batch [25/51] time 0.176 (0.240) data 0.001 (0.060) loss 0.3775 (0.4674) acc 93.1373 (89.5652) lr 2.2949e-04 eta 0:01:56
epoch [41/50] batch [30/51] time 0.170 (0.229) data 0.000 (0.050) loss 0.3819 (0.4674) acc 92.1875 (89.7555) lr 2.2949e-04 eta 0:01:50
epoch [41/50] batch [35/51] time 0.186 (0.221) data 0.000 (0.043) loss 0.4505 (0.4718) acc 94.6078 (89.9633) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [40/51] time 0.185 (0.216) data 0.000 (0.038) loss 0.3220 (0.4688) acc 94.7368 (89.9909) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [45/51] time 0.185 (0.211) data 0.001 (0.034) loss 0.5565 (0.4785) acc 87.7273 (89.7336) lr 2.2949e-04 eta 0:01:38
epoch [41/50] batch [50/51] time 0.156 (0.207) data 0.001 (0.030) loss 0.5742 (0.4823) acc 86.0465 (89.5398) lr 2.2949e-04 eta 0:01:35
>>> alpha1: 0.137  alpha2: 0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [42/50] batch [5/51] time 0.170 (0.471) data 0.000 (0.290) loss 0.4943 (0.4713) acc 88.5000 (90.0224) lr 1.9098e-04 eta 0:03:34
epoch [42/50] batch [10/51] time 0.184 (0.329) data 0.001 (0.145) loss 0.5627 (0.4676) acc 87.7358 (90.5857) lr 1.9098e-04 eta 0:02:27
epoch [42/50] batch [15/51] time 0.199 (0.281) data 0.000 (0.097) loss 0.2690 (0.4949) acc 95.0000 (91.1113) lr 1.9098e-04 eta 0:02:04
epoch [42/50] batch [20/51] time 0.181 (0.256) data 0.000 (0.073) loss 0.2713 (0.4813) acc 94.5455 (91.2160) lr 1.9098e-04 eta 0:01:52
epoch [42/50] batch [25/51] time 0.183 (0.239) data 0.000 (0.058) loss 0.4814 (0.4846) acc 88.2353 (90.7054) lr 1.9098e-04 eta 0:01:43
epoch [42/50] batch [30/51] time 0.179 (0.229) data 0.000 (0.049) loss 0.4840 (0.4882) acc 89.7059 (90.1657) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [35/51] time 0.174 (0.221) data 0.000 (0.042) loss 0.4306 (0.4890) acc 91.1765 (90.1240) lr 1.9098e-04 eta 0:01:33
epoch [42/50] batch [40/51] time 0.168 (0.215) data 0.000 (0.037) loss 0.4199 (0.4893) acc 90.5000 (90.1253) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [45/51] time 0.162 (0.210) data 0.000 (0.033) loss 0.5058 (0.4874) acc 88.2979 (89.8690) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [50/51] time 0.170 (0.206) data 0.000 (0.029) loss 0.4627 (0.4909) acc 86.2745 (89.7601) lr 1.9098e-04 eta 0:01:24
>>> alpha1: 0.136  alpha2: 0.022 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [43/50] batch [5/51] time 0.168 (0.490) data 0.000 (0.305) loss 0.5187 (0.4452) acc 86.2245 (89.1653) lr 1.5567e-04 eta 0:03:17
epoch [43/50] batch [10/51] time 0.173 (0.333) data 0.001 (0.153) loss 0.4355 (0.4747) acc 91.0000 (89.0409) lr 1.5567e-04 eta 0:02:12
epoch [43/50] batch [15/51] time 0.178 (0.281) data 0.000 (0.102) loss 0.4294 (0.4844) acc 93.7500 (89.4870) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [20/51] time 0.169 (0.255) data 0.000 (0.076) loss 0.5795 (0.4886) acc 83.0000 (89.2481) lr 1.5567e-04 eta 0:01:38
epoch [43/50] batch [25/51] time 0.181 (0.239) data 0.000 (0.061) loss 0.5040 (0.5016) acc 88.2075 (88.7756) lr 1.5567e-04 eta 0:01:31
epoch [43/50] batch [30/51] time 0.205 (0.230) data 0.000 (0.051) loss 0.3127 (0.4943) acc 93.3036 (89.3581) lr 1.5567e-04 eta 0:01:27
epoch [43/50] batch [35/51] time 0.176 (0.223) data 0.000 (0.044) loss 0.5485 (0.4920) acc 86.4130 (89.3467) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [40/51] time 0.168 (0.217) data 0.000 (0.038) loss 0.4775 (0.4824) acc 88.7755 (89.5685) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.174 (0.212) data 0.000 (0.034) loss 0.3519 (0.4762) acc 94.2308 (89.7418) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [50/51] time 0.169 (0.209) data 0.000 (0.031) loss 0.6063 (0.4766) acc 86.0000 (89.6013) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.134  alpha2: 0.019 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [44/50] batch [5/51] time 0.189 (0.455) data 0.001 (0.270) loss 0.4286 (0.4345) acc 91.6667 (91.6050) lr 1.2369e-04 eta 0:02:40
epoch [44/50] batch [10/51] time 0.190 (0.317) data 0.000 (0.135) loss 0.5261 (0.4826) acc 88.0208 (89.7922) lr 1.2369e-04 eta 0:01:49
epoch [44/50] batch [15/51] time 0.175 (0.268) data 0.000 (0.090) loss 0.4364 (0.4723) acc 89.5000 (89.8724) lr 1.2369e-04 eta 0:01:31
epoch [44/50] batch [20/51] time 0.171 (0.245) data 0.001 (0.068) loss 0.4063 (0.4646) acc 93.0000 (90.3604) lr 1.2369e-04 eta 0:01:22
epoch [44/50] batch [25/51] time 0.182 (0.233) data 0.001 (0.054) loss 0.5971 (0.4630) acc 86.0000 (90.5246) lr 1.2369e-04 eta 0:01:17
epoch [44/50] batch [30/51] time 0.190 (0.224) data 0.000 (0.045) loss 0.5378 (0.4553) acc 90.5172 (90.7220) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [35/51] time 0.177 (0.218) data 0.001 (0.039) loss 0.4553 (0.4568) acc 88.4615 (90.8206) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [40/51] time 0.173 (0.212) data 0.000 (0.034) loss 0.3355 (0.4574) acc 95.6731 (90.6837) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [45/51] time 0.170 (0.207) data 0.000 (0.030) loss 0.4242 (0.4634) acc 95.0980 (90.5484) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.158 (0.203) data 0.000 (0.027) loss 0.4962 (0.4714) acc 86.4130 (90.2942) lr 1.2369e-04 eta 0:01:02
>>> alpha1: 0.134  alpha2: 0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [45/50] batch [5/51] time 0.168 (0.540) data 0.000 (0.366) loss 0.4875 (0.4748) acc 90.3061 (88.7743) lr 9.5173e-05 eta 0:02:42
epoch [45/50] batch [10/51] time 0.178 (0.360) data 0.000 (0.183) loss 0.4017 (0.4640) acc 87.7358 (89.1314) lr 9.5173e-05 eta 0:01:46
epoch [45/50] batch [15/51] time 0.164 (0.297) data 0.001 (0.122) loss 0.5817 (0.4804) acc 88.2979 (88.5053) lr 9.5173e-05 eta 0:01:26
epoch [45/50] batch [20/51] time 0.187 (0.267) data 0.000 (0.092) loss 0.4426 (0.4659) acc 89.5000 (88.8613) lr 9.5173e-05 eta 0:01:16
epoch [45/50] batch [25/51] time 0.168 (0.249) data 0.000 (0.073) loss 0.5532 (0.4663) acc 86.1702 (88.8574) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [30/51] time 0.195 (0.236) data 0.000 (0.061) loss 0.6645 (0.4734) acc 83.4821 (88.8038) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [35/51] time 0.179 (0.228) data 0.000 (0.053) loss 0.3443 (0.4635) acc 91.3462 (89.2634) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [40/51] time 0.160 (0.221) data 0.000 (0.046) loss 0.5207 (0.4669) acc 85.3261 (89.1351) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.191 (0.216) data 0.000 (0.041) loss 0.3974 (0.4646) acc 95.3704 (89.3019) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.173 (0.211) data 0.000 (0.037) loss 0.4870 (0.4655) acc 86.2745 (89.3780) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.135  alpha2: 0.029 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [46/50] batch [5/51] time 0.171 (0.467) data 0.001 (0.292) loss 0.4092 (0.4528) acc 92.0000 (90.9074) lr 7.0224e-05 eta 0:01:56
epoch [46/50] batch [10/51] time 0.163 (0.321) data 0.000 (0.146) loss 0.7088 (0.4512) acc 88.2979 (91.0204) lr 7.0224e-05 eta 0:01:18
epoch [46/50] batch [15/51] time 0.172 (0.273) data 0.000 (0.098) loss 0.3625 (0.4489) acc 91.3265 (90.5750) lr 7.0224e-05 eta 0:01:05
epoch [46/50] batch [20/51] time 0.184 (0.248) data 0.000 (0.073) loss 0.3133 (0.4521) acc 94.3396 (90.2575) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [25/51] time 0.184 (0.236) data 0.001 (0.059) loss 0.3827 (0.4700) acc 92.5926 (89.8325) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [30/51] time 0.178 (0.227) data 0.001 (0.049) loss 0.4631 (0.4786) acc 88.5000 (89.6122) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.190 (0.221) data 0.000 (0.042) loss 0.3702 (0.4748) acc 92.6724 (89.7544) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [40/51] time 0.173 (0.216) data 0.000 (0.037) loss 0.4045 (0.4749) acc 91.3462 (89.8592) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.168 (0.211) data 0.000 (0.033) loss 0.7010 (0.4758) acc 85.5000 (89.8376) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.161 (0.207) data 0.000 (0.030) loss 0.5376 (0.4698) acc 90.4255 (89.9981) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.136  alpha2: 0.032 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.15 <<<
epoch [47/50] batch [5/51] time 0.193 (0.463) data 0.000 (0.274) loss 0.4189 (0.5470) acc 91.6667 (88.1616) lr 4.8943e-05 eta 0:01:32
epoch [47/50] batch [10/51] time 0.198 (0.325) data 0.000 (0.137) loss 0.3284 (0.4772) acc 95.3125 (90.2301) lr 4.8943e-05 eta 0:01:03
epoch [47/50] batch [15/51] time 0.165 (0.274) data 0.000 (0.092) loss 0.3918 (0.4806) acc 92.7083 (90.0579) lr 4.8943e-05 eta 0:00:51
epoch [47/50] batch [20/51] time 0.200 (0.250) data 0.000 (0.069) loss 0.3304 (0.4686) acc 91.5179 (90.2023) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.186 (0.236) data 0.000 (0.055) loss 0.4406 (0.4646) acc 97.0000 (90.4314) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [30/51] time 0.183 (0.226) data 0.000 (0.046) loss 0.2450 (0.4680) acc 95.0000 (89.9931) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.210 (0.220) data 0.000 (0.039) loss 0.4167 (0.4680) acc 91.9811 (89.8300) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.169 (0.214) data 0.000 (0.035) loss 0.3754 (0.4713) acc 93.0000 (89.7344) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.171 (0.209) data 0.000 (0.031) loss 0.5107 (0.4819) acc 87.7451 (89.3754) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.162 (0.205) data 0.000 (0.028) loss 0.3861 (0.4720) acc 90.7609 (89.5134) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.135  alpha2: 0.032 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [48/50] batch [5/51] time 0.180 (0.432) data 0.000 (0.254) loss 0.4895 (0.4285) acc 89.8148 (91.5873) lr 3.1417e-05 eta 0:01:04
epoch [48/50] batch [10/51] time 0.181 (0.304) data 0.000 (0.127) loss 0.6694 (0.4499) acc 83.0000 (90.3549) lr 3.1417e-05 eta 0:00:43
epoch [48/50] batch [15/51] time 0.166 (0.262) data 0.000 (0.085) loss 0.4430 (0.4354) acc 92.7083 (90.9703) lr 3.1417e-05 eta 0:00:36
epoch [48/50] batch [20/51] time 0.172 (0.242) data 0.000 (0.064) loss 0.5170 (0.4401) acc 88.2653 (90.8192) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [25/51] time 0.189 (0.231) data 0.000 (0.051) loss 0.3369 (0.4437) acc 95.1923 (90.4687) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [30/51] time 0.178 (0.221) data 0.000 (0.043) loss 0.3851 (0.4478) acc 91.9811 (90.4594) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.189 (0.216) data 0.000 (0.037) loss 0.6449 (0.4583) acc 80.8824 (90.1540) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.163 (0.211) data 0.000 (0.032) loss 0.3795 (0.4584) acc 93.0851 (90.1865) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [45/51] time 0.163 (0.206) data 0.000 (0.028) loss 0.5364 (0.4565) acc 84.0425 (90.0634) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.183 (0.204) data 0.000 (0.026) loss 0.3942 (0.4521) acc 90.6250 (90.2391) lr 3.1417e-05 eta 0:00:20
>>> alpha1: 0.136  alpha2: 0.033 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [49/50] batch [5/51] time 0.173 (0.435) data 0.000 (0.255) loss 0.6724 (0.5351) acc 83.8235 (87.8664) lr 1.7713e-05 eta 0:00:42
epoch [49/50] batch [10/51] time 0.170 (0.305) data 0.000 (0.128) loss 0.5834 (0.5069) acc 87.0000 (89.1193) lr 1.7713e-05 eta 0:00:28
epoch [49/50] batch [15/51] time 0.178 (0.264) data 0.000 (0.085) loss 0.4408 (0.4812) acc 91.5094 (89.9863) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [20/51] time 0.167 (0.242) data 0.000 (0.064) loss 0.6189 (0.4763) acc 87.2340 (90.0616) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [25/51] time 0.179 (0.229) data 0.000 (0.051) loss 0.4385 (0.4624) acc 85.9375 (90.3660) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.156 (0.220) data 0.000 (0.043) loss 0.6610 (0.4810) acc 81.2500 (89.8354) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [35/51] time 0.169 (0.215) data 0.000 (0.037) loss 0.3951 (0.4757) acc 92.5000 (90.0014) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.166 (0.210) data 0.000 (0.032) loss 0.5932 (0.4798) acc 90.8163 (90.0375) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.029) loss 0.3505 (0.4749) acc 95.0000 (90.2204) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.180 (0.203) data 0.000 (0.026) loss 0.3630 (0.4613) acc 92.7273 (90.5378) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.135  alpha2: 0.027 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.16 <<<
epoch [50/50] batch [5/51] time 0.179 (0.450) data 0.000 (0.266) loss 0.3204 (0.4088) acc 94.3396 (91.0176) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [10/51] time 0.191 (0.316) data 0.001 (0.133) loss 0.6473 (0.4612) acc 85.4546 (89.8496) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [15/51] time 0.181 (0.271) data 0.001 (0.089) loss 0.6037 (0.4602) acc 85.6481 (89.5144) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.163 (0.247) data 0.001 (0.067) loss 0.4335 (0.4528) acc 88.8298 (89.6364) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.171 (0.233) data 0.000 (0.054) loss 0.3609 (0.4507) acc 92.0000 (90.0555) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.166 (0.223) data 0.000 (0.045) loss 0.7914 (0.4565) acc 84.8958 (90.0889) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.172 (0.218) data 0.000 (0.038) loss 0.5026 (0.4649) acc 91.1765 (90.0638) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.173 (0.213) data 0.000 (0.034) loss 0.3808 (0.4662) acc 94.6078 (90.0652) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.176 (0.209) data 0.000 (0.030) loss 0.4200 (0.4659) acc 91.3462 (90.0224) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.176 (0.205) data 0.000 (0.027) loss 0.5377 (0.4730) acc 88.6792 (89.7755) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.16, 0.14, 0.14, 0.14, 0.13, 0.13, 0.13, 0.12, 0.12, 0.12, 0.11, 0.12, 0.12, 0.12, 0.11, 0.11, 0.11, 0.11, 0.11, 0.1, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09, 0.1]
* matched noise rate: [0.05, 0.05, 0.04, 0.05, 0.04, 0.05, 0.05, 0.05, 0.05, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06]
* unmatched noise rate: [0.24, 0.23, 0.24, 0.24, 0.24, 0.23, 0.24, 0.22, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.19, 0.2, 0.18, 0.18, 0.18, 0.17, 0.17, 0.17, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.15, 0.16, 0.16, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:07,  2.80s/it] 12%|█▏        | 3/25 [00:02<00:17,  1.26it/s] 16%|█▌        | 4/25 [00:03<00:11,  1.79it/s] 24%|██▍       | 6/25 [00:03<00:06,  3.07it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.45it/s] 40%|████      | 10/25 [00:03<00:02,  5.84it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.14it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.30it/s] 64%|██████▍   | 16/25 [00:04<00:00,  9.26it/s] 72%|███████▏  | 18/25 [00:04<00:00,  8.76it/s] 80%|████████  | 20/25 [00:04<00:00,  9.62it/s] 88%|████████▊ | 22/25 [00:04<00:00, 10.17it/s] 96%|█████████▌| 24/25 [00:04<00:00, 10.75it/s]100%|██████████| 25/25 [00:05<00:00,  4.60it/s]
=> result
* total: 2,463
* correct: 2,158
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 7	acc: 58.3%
* class: 3 (sweet pea)	total: 17	correct: 13	acc: 76.5%
* class: 4 (english marigold)	total: 20	correct: 14	acc: 70.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 23	acc: 88.5%
* class: 11 (colt's foot)	total: 26	correct: 22	acc: 84.6%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 14	acc: 93.3%
* class: 15 (globe-flower)	total: 13	correct: 10	acc: 76.9%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 20	acc: 80.0%
* class: 18 (balloon flower)	total: 15	correct: 14	acc: 93.3%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 15	acc: 88.2%
* class: 22 (fritillary)	total: 27	correct: 23	acc: 85.2%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 22	acc: 84.6%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 31	acc: 96.9%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 17	acc: 44.7%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 6	acc: 50.0%
* class: 45 (wallflower)	total: 59	correct: 55	acc: 93.2%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 26	acc: 92.9%
* class: 50 (petunia)	total: 77	correct: 66	acc: 85.7%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 24	acc: 85.7%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 15	acc: 71.4%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 14	acc: 87.5%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 15	acc: 93.8%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 21	acc: 91.3%
* class: 71 (azalea)	total: 29	correct: 25	acc: 86.2%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 36	acc: 100.0%
* class: 75 (morning glory)	total: 32	correct: 26	acc: 81.2%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 40	acc: 95.2%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 36	acc: 92.3%
* class: 83 (columbine)	total: 26	correct: 22	acc: 84.6%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 14	acc: 77.8%
* class: 87 (cyclamen)	total: 46	correct: 36	acc: 78.3%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 18	acc: 78.3%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 49	acc: 100.0%
* class: 94 (bougainvillea)	total: 38	correct: 36	acc: 94.7%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 17	acc: 85.0%
* class: 97 (mexican petunia)	total: 25	correct: 19	acc: 76.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 10	acc: 58.8%
* class: 101 (blackberry lily)	total: 14	correct: 13	acc: 92.9%
* average: 88.9%
Elapsed: 0:28:34
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_6-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.261 (1.077) data 0.000 (0.319) loss 4.3478 (4.5323) acc 9.3750 (4.3750) lr 1.0000e-05 eta 0:45:41
epoch [1/50] batch [10/51] time 0.256 (0.670) data 0.000 (0.160) loss 4.3742 (4.5012) acc 12.5000 (8.1250) lr 1.0000e-05 eta 0:28:21
epoch [1/50] batch [15/51] time 0.263 (0.533) data 0.000 (0.107) loss 4.0498 (4.4522) acc 15.6250 (8.9583) lr 1.0000e-05 eta 0:22:32
epoch [1/50] batch [20/51] time 0.258 (0.467) data 0.000 (0.080) loss 4.2000 (4.3968) acc 15.6250 (9.8438) lr 1.0000e-05 eta 0:19:40
epoch [1/50] batch [25/51] time 0.270 (0.427) data 0.000 (0.064) loss 4.2578 (4.3050) acc 18.7500 (12.3750) lr 1.0000e-05 eta 0:17:59
epoch [1/50] batch [30/51] time 0.257 (0.400) data 0.000 (0.053) loss 3.8947 (4.2785) acc 28.1250 (13.3333) lr 1.0000e-05 eta 0:16:48
epoch [1/50] batch [35/51] time 0.267 (0.381) data 0.000 (0.046) loss 3.9598 (4.2776) acc 21.8750 (14.1071) lr 1.0000e-05 eta 0:15:58
epoch [1/50] batch [40/51] time 0.260 (0.366) data 0.000 (0.040) loss 4.4523 (4.2375) acc 12.5000 (14.9219) lr 1.0000e-05 eta 0:15:18
epoch [1/50] batch [45/51] time 0.261 (0.354) data 0.000 (0.036) loss 4.0701 (4.2108) acc 28.1250 (16.1806) lr 1.0000e-05 eta 0:14:47
epoch [1/50] batch [50/51] time 0.255 (0.344) data 0.000 (0.032) loss 4.5109 (4.1939) acc 15.6250 (16.8125) lr 1.0000e-05 eta 0:14:21
epoch [2/50] batch [5/51] time 0.291 (0.557) data 0.000 (0.260) loss 3.9507 (4.0752) acc 37.5000 (29.3750) lr 2.0000e-03 eta 0:23:10
epoch [2/50] batch [10/51] time 0.260 (0.413) data 0.000 (0.130) loss 3.8608 (3.9479) acc 37.5000 (29.0625) lr 2.0000e-03 eta 0:17:08
epoch [2/50] batch [15/51] time 0.258 (0.363) data 0.000 (0.087) loss 3.5612 (3.9246) acc 37.5000 (28.9583) lr 2.0000e-03 eta 0:15:02
epoch [2/50] batch [20/51] time 0.258 (0.338) data 0.000 (0.065) loss 3.1597 (3.8457) acc 34.3750 (30.0000) lr 2.0000e-03 eta 0:13:57
epoch [2/50] batch [25/51] time 0.261 (0.322) data 0.000 (0.052) loss 2.8187 (3.7779) acc 50.0000 (31.0000) lr 2.0000e-03 eta 0:13:17
epoch [2/50] batch [30/51] time 0.273 (0.313) data 0.000 (0.044) loss 3.3514 (3.7486) acc 40.6250 (31.9792) lr 2.0000e-03 eta 0:12:51
epoch [2/50] batch [35/51] time 0.260 (0.306) data 0.000 (0.038) loss 3.2004 (3.7133) acc 53.1250 (33.2143) lr 2.0000e-03 eta 0:12:33
epoch [2/50] batch [40/51] time 0.261 (0.300) data 0.000 (0.033) loss 3.5694 (3.7396) acc 37.5000 (33.1250) lr 2.0000e-03 eta 0:12:18
epoch [2/50] batch [45/51] time 0.257 (0.296) data 0.000 (0.030) loss 3.7412 (3.7115) acc 37.5000 (33.5417) lr 2.0000e-03 eta 0:12:05
epoch [2/50] batch [50/51] time 0.257 (0.292) data 0.000 (0.027) loss 3.4758 (3.7011) acc 37.5000 (34.0000) lr 2.0000e-03 eta 0:11:54
epoch [3/50] batch [5/51] time 0.274 (0.612) data 0.000 (0.314) loss 2.9731 (3.4648) acc 40.6250 (39.3750) lr 1.9980e-03 eta 0:24:55
epoch [3/50] batch [10/51] time 0.261 (0.439) data 0.000 (0.157) loss 3.7513 (3.6504) acc 37.5000 (35.3125) lr 1.9980e-03 eta 0:17:50
epoch [3/50] batch [15/51] time 0.266 (0.381) data 0.000 (0.105) loss 3.3065 (3.6392) acc 43.7500 (36.2500) lr 1.9980e-03 eta 0:15:26
epoch [3/50] batch [20/51] time 0.277 (0.354) data 0.001 (0.079) loss 3.8201 (3.6517) acc 34.3750 (37.5000) lr 1.9980e-03 eta 0:14:18
epoch [3/50] batch [25/51] time 0.259 (0.336) data 0.000 (0.063) loss 4.0225 (3.6746) acc 28.1250 (36.2500) lr 1.9980e-03 eta 0:13:35
epoch [3/50] batch [30/51] time 0.259 (0.324) data 0.000 (0.053) loss 3.2695 (3.6275) acc 46.8750 (37.9167) lr 1.9980e-03 eta 0:13:03
epoch [3/50] batch [35/51] time 0.259 (0.316) data 0.000 (0.045) loss 3.6850 (3.5879) acc 37.5000 (38.7500) lr 1.9980e-03 eta 0:12:41
epoch [3/50] batch [40/51] time 0.257 (0.309) data 0.000 (0.040) loss 3.9218 (3.5912) acc 28.1250 (38.9062) lr 1.9980e-03 eta 0:12:23
epoch [3/50] batch [45/51] time 0.257 (0.303) data 0.000 (0.035) loss 3.3100 (3.5781) acc 43.7500 (38.8194) lr 1.9980e-03 eta 0:12:08
epoch [3/50] batch [50/51] time 0.259 (0.299) data 0.000 (0.032) loss 3.2783 (3.5789) acc 50.0000 (38.8750) lr 1.9980e-03 eta 0:11:55
epoch [4/50] batch [5/51] time 0.270 (0.568) data 0.000 (0.285) loss 3.6753 (3.5351) acc 40.6250 (43.1250) lr 1.9921e-03 eta 0:22:38
epoch [4/50] batch [10/51] time 0.275 (0.420) data 0.000 (0.143) loss 3.3641 (3.4824) acc 50.0000 (43.4375) lr 1.9921e-03 eta 0:16:42
epoch [4/50] batch [15/51] time 0.274 (0.372) data 0.000 (0.095) loss 3.3642 (3.5880) acc 34.3750 (38.7500) lr 1.9921e-03 eta 0:14:46
epoch [4/50] batch [20/51] time 0.260 (0.345) data 0.000 (0.071) loss 2.8473 (3.5265) acc 56.2500 (40.6250) lr 1.9921e-03 eta 0:13:41
epoch [4/50] batch [25/51] time 0.263 (0.329) data 0.000 (0.057) loss 2.7277 (3.4410) acc 43.7500 (41.7500) lr 1.9921e-03 eta 0:13:00
epoch [4/50] batch [30/51] time 0.266 (0.319) data 0.000 (0.048) loss 3.8040 (3.5124) acc 40.6250 (41.4583) lr 1.9921e-03 eta 0:12:35
epoch [4/50] batch [35/51] time 0.262 (0.311) data 0.000 (0.041) loss 3.5037 (3.5091) acc 37.5000 (41.7857) lr 1.9921e-03 eta 0:12:15
epoch [4/50] batch [40/51] time 0.259 (0.305) data 0.000 (0.036) loss 3.3316 (3.5383) acc 46.8750 (41.4844) lr 1.9921e-03 eta 0:11:58
epoch [4/50] batch [45/51] time 0.258 (0.300) data 0.000 (0.032) loss 3.1079 (3.5340) acc 53.1250 (41.0417) lr 1.9921e-03 eta 0:11:45
epoch [4/50] batch [50/51] time 0.260 (0.296) data 0.000 (0.029) loss 2.8374 (3.5094) acc 46.8750 (41.1250) lr 1.9921e-03 eta 0:11:33
epoch [5/50] batch [5/51] time 0.287 (0.520) data 0.001 (0.246) loss 3.4370 (3.2654) acc 43.7500 (45.6250) lr 1.9823e-03 eta 0:20:16
epoch [5/50] batch [10/51] time 0.265 (0.394) data 0.000 (0.123) loss 2.8640 (3.4546) acc 46.8750 (41.8750) lr 1.9823e-03 eta 0:15:21
epoch [5/50] batch [15/51] time 0.271 (0.352) data 0.000 (0.082) loss 3.2627 (3.3744) acc 46.8750 (43.1250) lr 1.9823e-03 eta 0:13:41
epoch [5/50] batch [20/51] time 0.273 (0.330) data 0.000 (0.062) loss 2.8566 (3.3953) acc 56.2500 (41.5625) lr 1.9823e-03 eta 0:12:48
epoch [5/50] batch [25/51] time 0.262 (0.317) data 0.000 (0.049) loss 2.8092 (3.3800) acc 53.1250 (42.1250) lr 1.9823e-03 eta 0:12:15
epoch [5/50] batch [30/51] time 0.266 (0.309) data 0.000 (0.041) loss 3.8842 (3.4270) acc 31.2500 (41.7708) lr 1.9823e-03 eta 0:11:55
epoch [5/50] batch [35/51] time 0.261 (0.302) data 0.000 (0.035) loss 3.1317 (3.3894) acc 50.0000 (43.3036) lr 1.9823e-03 eta 0:11:38
epoch [5/50] batch [40/51] time 0.258 (0.297) data 0.000 (0.031) loss 2.5254 (3.3873) acc 53.1250 (43.2031) lr 1.9823e-03 eta 0:11:24
epoch [5/50] batch [45/51] time 0.262 (0.293) data 0.000 (0.028) loss 3.7789 (3.4054) acc 28.1250 (42.7778) lr 1.9823e-03 eta 0:11:13
epoch [5/50] batch [50/51] time 0.259 (0.289) data 0.000 (0.025) loss 3.8579 (3.4462) acc 43.7500 (42.5000) lr 1.9823e-03 eta 0:11:04
epoch [6/50] batch [5/51] time 0.259 (0.511) data 0.000 (0.242) loss 3.5893 (3.4974) acc 43.7500 (38.7500) lr 1.9686e-03 eta 0:19:29
epoch [6/50] batch [10/51] time 0.261 (0.387) data 0.000 (0.121) loss 3.5602 (3.3340) acc 43.7500 (43.4375) lr 1.9686e-03 eta 0:14:44
epoch [6/50] batch [15/51] time 0.260 (0.346) data 0.000 (0.081) loss 3.0712 (3.3284) acc 46.8750 (43.7500) lr 1.9686e-03 eta 0:13:08
epoch [6/50] batch [20/51] time 0.262 (0.324) data 0.000 (0.061) loss 3.7756 (3.3719) acc 34.3750 (42.9688) lr 1.9686e-03 eta 0:12:17
epoch [6/50] batch [25/51] time 0.262 (0.312) data 0.001 (0.049) loss 3.3580 (3.4145) acc 46.8750 (42.3750) lr 1.9686e-03 eta 0:11:49
epoch [6/50] batch [30/51] time 0.269 (0.305) data 0.000 (0.040) loss 3.3459 (3.4012) acc 40.6250 (42.2917) lr 1.9686e-03 eta 0:11:30
epoch [6/50] batch [35/51] time 0.264 (0.300) data 0.000 (0.035) loss 2.9722 (3.4346) acc 53.1250 (42.0536) lr 1.9686e-03 eta 0:11:17
epoch [6/50] batch [40/51] time 0.258 (0.295) data 0.000 (0.030) loss 3.2684 (3.4381) acc 40.6250 (41.4844) lr 1.9686e-03 eta 0:11:05
epoch [6/50] batch [45/51] time 0.264 (0.291) data 0.000 (0.027) loss 2.3286 (3.4088) acc 65.6250 (41.8750) lr 1.9686e-03 eta 0:10:55
epoch [6/50] batch [50/51] time 0.256 (0.288) data 0.000 (0.024) loss 3.6024 (3.3904) acc 37.5000 (42.3750) lr 1.9686e-03 eta 0:10:46
epoch [7/50] batch [5/51] time 0.293 (0.555) data 0.001 (0.263) loss 3.4001 (3.4812) acc 43.7500 (43.1250) lr 1.9511e-03 eta 0:20:43
epoch [7/50] batch [10/51] time 0.268 (0.409) data 0.000 (0.131) loss 3.4643 (3.4467) acc 50.0000 (41.8750) lr 1.9511e-03 eta 0:15:13
epoch [7/50] batch [15/51] time 0.271 (0.362) data 0.000 (0.088) loss 4.0677 (3.4115) acc 28.1250 (41.8750) lr 1.9511e-03 eta 0:13:26
epoch [7/50] batch [20/51] time 0.269 (0.338) data 0.000 (0.066) loss 4.3089 (3.4255) acc 34.3750 (42.3438) lr 1.9511e-03 eta 0:12:30
epoch [7/50] batch [25/51] time 0.259 (0.322) data 0.000 (0.053) loss 3.2713 (3.4554) acc 37.5000 (42.2500) lr 1.9511e-03 eta 0:11:55
epoch [7/50] batch [30/51] time 0.259 (0.312) data 0.000 (0.044) loss 3.5541 (3.4113) acc 50.0000 (43.7500) lr 1.9511e-03 eta 0:11:30
epoch [7/50] batch [35/51] time 0.262 (0.305) data 0.000 (0.038) loss 3.2658 (3.4007) acc 50.0000 (43.4821) lr 1.9511e-03 eta 0:11:14
epoch [7/50] batch [40/51] time 0.261 (0.300) data 0.000 (0.033) loss 3.3717 (3.3529) acc 37.5000 (44.0625) lr 1.9511e-03 eta 0:11:00
epoch [7/50] batch [45/51] time 0.259 (0.295) data 0.000 (0.029) loss 3.8779 (3.3433) acc 37.5000 (44.2361) lr 1.9511e-03 eta 0:10:49
epoch [7/50] batch [50/51] time 0.257 (0.292) data 0.000 (0.026) loss 3.1136 (3.3320) acc 46.8750 (44.1875) lr 1.9511e-03 eta 0:10:39
epoch [8/50] batch [5/51] time 0.272 (0.568) data 0.000 (0.259) loss 2.8854 (3.1720) acc 50.0000 (47.5000) lr 1.9298e-03 eta 0:20:42
epoch [8/50] batch [10/51] time 0.260 (0.417) data 0.000 (0.130) loss 3.3460 (3.2418) acc 43.7500 (46.8750) lr 1.9298e-03 eta 0:15:09
epoch [8/50] batch [15/51] time 0.277 (0.368) data 0.000 (0.087) loss 3.1705 (3.2864) acc 46.8750 (46.0417) lr 1.9298e-03 eta 0:13:21
epoch [8/50] batch [20/51] time 0.262 (0.343) data 0.000 (0.065) loss 2.8242 (3.2543) acc 65.6250 (45.9375) lr 1.9298e-03 eta 0:12:25
epoch [8/50] batch [25/51] time 0.270 (0.327) data 0.000 (0.052) loss 3.5224 (3.3003) acc 34.3750 (45.0000) lr 1.9298e-03 eta 0:11:49
epoch [8/50] batch [30/51] time 0.268 (0.316) data 0.000 (0.044) loss 3.2391 (3.2887) acc 53.1250 (45.3125) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [35/51] time 0.269 (0.308) data 0.000 (0.037) loss 3.5449 (3.2711) acc 37.5000 (45.7143) lr 1.9298e-03 eta 0:11:05
epoch [8/50] batch [40/51] time 0.257 (0.302) data 0.000 (0.033) loss 3.0977 (3.2619) acc 46.8750 (45.9375) lr 1.9298e-03 eta 0:10:50
epoch [8/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.029) loss 3.2423 (3.2784) acc 37.5000 (45.2778) lr 1.9298e-03 eta 0:10:38
epoch [8/50] batch [50/51] time 0.259 (0.293) data 0.000 (0.026) loss 3.5358 (3.2792) acc 37.5000 (45.4375) lr 1.9298e-03 eta 0:10:28
epoch [9/50] batch [5/51] time 0.278 (0.577) data 0.000 (0.294) loss 3.6009 (3.1527) acc 40.6250 (47.5000) lr 1.9048e-03 eta 0:20:32
epoch [9/50] batch [10/51] time 0.272 (0.419) data 0.000 (0.147) loss 3.3704 (3.2634) acc 40.6250 (44.6875) lr 1.9048e-03 eta 0:14:54
epoch [9/50] batch [15/51] time 0.274 (0.369) data 0.000 (0.098) loss 3.4575 (3.2307) acc 43.7500 (46.2500) lr 1.9048e-03 eta 0:13:04
epoch [9/50] batch [20/51] time 0.274 (0.344) data 0.000 (0.074) loss 3.3939 (3.2883) acc 40.6250 (43.9062) lr 1.9048e-03 eta 0:12:10
epoch [9/50] batch [25/51] time 0.259 (0.328) data 0.000 (0.059) loss 2.8417 (3.2115) acc 65.6250 (45.8750) lr 1.9048e-03 eta 0:11:34
epoch [9/50] batch [30/51] time 0.273 (0.318) data 0.000 (0.049) loss 2.4260 (3.1931) acc 65.6250 (46.9792) lr 1.9048e-03 eta 0:11:11
epoch [9/50] batch [35/51] time 0.267 (0.311) data 0.000 (0.042) loss 2.5519 (3.1986) acc 68.7500 (47.2321) lr 1.9048e-03 eta 0:10:56
epoch [9/50] batch [40/51] time 0.258 (0.305) data 0.000 (0.037) loss 3.5046 (3.2062) acc 43.7500 (47.0312) lr 1.9048e-03 eta 0:10:41
epoch [9/50] batch [45/51] time 0.257 (0.300) data 0.000 (0.033) loss 3.4173 (3.2252) acc 40.6250 (46.4583) lr 1.9048e-03 eta 0:10:29
epoch [9/50] batch [50/51] time 0.256 (0.296) data 0.000 (0.030) loss 3.9043 (3.2357) acc 37.5000 (46.5625) lr 1.9048e-03 eta 0:10:18
epoch [10/50] batch [5/51] time 0.266 (0.491) data 0.000 (0.213) loss 3.2037 (3.4340) acc 50.0000 (43.1250) lr 1.8763e-03 eta 0:17:03
epoch [10/50] batch [10/51] time 0.261 (0.381) data 0.000 (0.109) loss 3.0648 (3.3085) acc 50.0000 (44.6875) lr 1.8763e-03 eta 0:13:12
epoch [10/50] batch [15/51] time 0.271 (0.344) data 0.000 (0.073) loss 3.0333 (3.2271) acc 46.8750 (44.5833) lr 1.8763e-03 eta 0:11:55
epoch [10/50] batch [20/51] time 0.260 (0.325) data 0.000 (0.055) loss 2.5624 (3.1703) acc 59.3750 (45.9375) lr 1.8763e-03 eta 0:11:12
epoch [10/50] batch [25/51] time 0.265 (0.313) data 0.000 (0.044) loss 3.2294 (3.1550) acc 46.8750 (46.6250) lr 1.8763e-03 eta 0:10:46
epoch [10/50] batch [30/51] time 0.276 (0.306) data 0.000 (0.036) loss 3.8074 (3.1782) acc 43.7500 (46.9792) lr 1.8763e-03 eta 0:10:29
epoch [10/50] batch [35/51] time 0.274 (0.301) data 0.000 (0.031) loss 3.5291 (3.1719) acc 50.0000 (47.8571) lr 1.8763e-03 eta 0:10:19
epoch [10/50] batch [40/51] time 0.257 (0.296) data 0.000 (0.027) loss 2.9890 (3.1842) acc 53.1250 (47.8125) lr 1.8763e-03 eta 0:10:07
epoch [10/50] batch [45/51] time 0.260 (0.292) data 0.000 (0.024) loss 3.0958 (3.2056) acc 43.7500 (47.2917) lr 1.8763e-03 eta 0:09:57
epoch [10/50] batch [50/51] time 0.261 (0.289) data 0.000 (0.022) loss 2.9150 (3.1946) acc 50.0000 (47.1875) lr 1.8763e-03 eta 0:09:50
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.603  alpha2: 0.236 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.33 <<<
epoch [11/50] batch [5/51] time 0.881 (1.012) data 0.001 (0.242) loss 1.4487 (1.7335) acc 75.0000 (70.3877) lr 1.8443e-03 eta 0:34:18
epoch [11/50] batch [10/51] time 0.185 (0.659) data 0.000 (0.121) loss 1.3638 (1.5559) acc 78.0702 (73.6137) lr 1.8443e-03 eta 0:22:17
epoch [11/50] batch [15/51] time 0.182 (0.540) data 0.000 (0.081) loss 1.4266 (1.5612) acc 74.5455 (72.3809) lr 1.8443e-03 eta 0:18:13
epoch [11/50] batch [20/51] time 0.181 (0.514) data 0.000 (0.061) loss 1.5139 (1.5638) acc 75.9615 (71.9500) lr 1.8443e-03 eta 0:17:17
epoch [11/50] batch [25/51] time 0.189 (0.471) data 0.000 (0.049) loss 1.2546 (1.5686) acc 70.6897 (71.0189) lr 1.8443e-03 eta 0:15:48
epoch [11/50] batch [30/51] time 0.177 (0.422) data 0.000 (0.041) loss 1.3698 (1.5435) acc 79.2453 (71.6264) lr 1.8443e-03 eta 0:14:07
epoch [11/50] batch [35/51] time 0.835 (0.406) data 0.000 (0.035) loss 1.1736 (1.5197) acc 74.5763 (71.4258) lr 1.8443e-03 eta 0:13:34
epoch [11/50] batch [40/51] time 0.176 (0.378) data 0.000 (0.031) loss 1.7630 (1.5202) acc 62.5000 (71.5082) lr 1.8443e-03 eta 0:12:36
epoch [11/50] batch [45/51] time 0.174 (0.356) data 0.000 (0.027) loss 1.7890 (1.5097) acc 67.3077 (71.5156) lr 1.8443e-03 eta 0:11:50
epoch [11/50] batch [50/51] time 0.187 (0.339) data 0.001 (0.025) loss 1.2898 (1.5108) acc 81.5789 (71.4073) lr 1.8443e-03 eta 0:11:13
>>> alpha1: 0.477  alpha2: 0.153 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.24 <<<
epoch [12/50] batch [5/51] time 0.168 (0.673) data 0.000 (0.252) loss 1.2107 (1.0461) acc 74.4898 (78.3281) lr 1.8090e-03 eta 0:22:14
epoch [12/50] batch [10/51] time 0.711 (0.532) data 0.000 (0.126) loss 1.0302 (1.0756) acc 82.7778 (77.5417) lr 1.8090e-03 eta 0:17:33
epoch [12/50] batch [15/51] time 0.684 (0.447) data 0.000 (0.085) loss 1.1600 (1.0840) acc 74.4565 (76.3389) lr 1.8090e-03 eta 0:14:41
epoch [12/50] batch [20/51] time 0.160 (0.376) data 0.000 (0.063) loss 1.4216 (1.1084) acc 70.1087 (75.4528) lr 1.8090e-03 eta 0:12:20
epoch [12/50] batch [25/51] time 0.171 (0.336) data 0.000 (0.051) loss 0.9028 (1.0955) acc 80.3922 (75.8534) lr 1.8090e-03 eta 0:10:59
epoch [12/50] batch [30/51] time 0.175 (0.309) data 0.000 (0.042) loss 1.0602 (1.0902) acc 75.0000 (75.8367) lr 1.8090e-03 eta 0:10:04
epoch [12/50] batch [35/51] time 0.189 (0.290) data 0.001 (0.036) loss 0.8332 (1.1328) acc 80.7692 (75.4389) lr 1.8090e-03 eta 0:09:26
epoch [12/50] batch [40/51] time 0.173 (0.276) data 0.000 (0.032) loss 0.7470 (1.1062) acc 83.1731 (76.0038) lr 1.8090e-03 eta 0:08:57
epoch [12/50] batch [45/51] time 0.162 (0.275) data 0.000 (0.028) loss 1.0455 (1.0960) acc 77.1277 (76.0903) lr 1.8090e-03 eta 0:08:53
epoch [12/50] batch [50/51] time 0.165 (0.264) data 0.001 (0.026) loss 0.9112 (1.0787) acc 79.2553 (76.5656) lr 1.8090e-03 eta 0:08:31
>>> alpha1: 0.365  alpha2: 0.050 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.25 <<<
epoch [13/50] batch [5/51] time 0.183 (0.504) data 0.000 (0.329) loss 0.7249 (0.9011) acc 86.3636 (80.0379) lr 1.7705e-03 eta 0:16:14
epoch [13/50] batch [10/51] time 0.163 (0.339) data 0.000 (0.165) loss 0.7509 (0.9128) acc 84.2391 (78.9019) lr 1.7705e-03 eta 0:10:52
epoch [13/50] batch [15/51] time 0.164 (0.282) data 0.000 (0.110) loss 0.8879 (0.8843) acc 79.2553 (79.9683) lr 1.7705e-03 eta 0:09:02
epoch [13/50] batch [20/51] time 0.156 (0.280) data 0.000 (0.083) loss 0.7197 (0.8776) acc 86.3636 (80.2186) lr 1.7705e-03 eta 0:08:56
epoch [13/50] batch [25/51] time 0.158 (0.258) data 0.000 (0.066) loss 1.0341 (0.8758) acc 75.0000 (79.9874) lr 1.7705e-03 eta 0:08:13
epoch [13/50] batch [30/51] time 0.172 (0.244) data 0.000 (0.055) loss 0.8318 (0.8621) acc 85.7843 (80.4583) lr 1.7705e-03 eta 0:07:44
epoch [13/50] batch [35/51] time 0.179 (0.233) data 0.000 (0.047) loss 0.9500 (0.8706) acc 82.1429 (80.4171) lr 1.7705e-03 eta 0:07:24
epoch [13/50] batch [40/51] time 0.160 (0.225) data 0.000 (0.041) loss 1.1013 (0.8861) acc 78.7234 (80.3405) lr 1.7705e-03 eta 0:07:06
epoch [13/50] batch [45/51] time 0.160 (0.218) data 0.000 (0.037) loss 0.7700 (0.8840) acc 79.2553 (80.3697) lr 1.7705e-03 eta 0:06:53
epoch [13/50] batch [50/51] time 0.169 (0.213) data 0.000 (0.033) loss 0.7374 (0.8920) acc 83.3333 (79.9436) lr 1.7705e-03 eta 0:06:41
>>> alpha1: 0.316  alpha2: 0.016 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.26 <<<
epoch [14/50] batch [5/51] time 0.173 (0.496) data 0.014 (0.317) loss 0.8938 (0.8207) acc 84.3023 (82.1309) lr 1.7290e-03 eta 0:15:33
epoch [14/50] batch [10/51] time 0.170 (0.332) data 0.000 (0.159) loss 0.9136 (0.7745) acc 73.4694 (82.5717) lr 1.7290e-03 eta 0:10:23
epoch [14/50] batch [15/51] time 0.184 (0.280) data 0.000 (0.106) loss 0.6662 (0.7637) acc 84.3137 (83.0855) lr 1.7290e-03 eta 0:08:44
epoch [14/50] batch [20/51] time 0.164 (0.251) data 0.000 (0.080) loss 0.8152 (0.7841) acc 83.5106 (82.1946) lr 1.7290e-03 eta 0:07:47
epoch [14/50] batch [25/51] time 0.186 (0.235) data 0.000 (0.064) loss 0.8307 (0.7976) acc 78.8462 (82.0703) lr 1.7290e-03 eta 0:07:16
epoch [14/50] batch [30/51] time 0.176 (0.224) data 0.000 (0.053) loss 0.6833 (0.8101) acc 79.5918 (81.3442) lr 1.7290e-03 eta 0:06:56
epoch [14/50] batch [35/51] time 0.181 (0.216) data 0.000 (0.046) loss 0.8127 (0.8002) acc 83.1633 (81.7415) lr 1.7290e-03 eta 0:06:40
epoch [14/50] batch [40/51] time 0.163 (0.210) data 0.000 (0.040) loss 0.8015 (0.7956) acc 77.0833 (81.5397) lr 1.7290e-03 eta 0:06:27
epoch [14/50] batch [45/51] time 0.155 (0.206) data 0.000 (0.035) loss 1.0640 (0.7900) acc 81.2500 (81.7657) lr 1.7290e-03 eta 0:06:18
epoch [14/50] batch [50/51] time 0.155 (0.202) data 0.000 (0.032) loss 0.9999 (0.7932) acc 77.8409 (81.8974) lr 1.7290e-03 eta 0:06:10
>>> alpha1: 0.289  alpha2: 0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.27 <<<
epoch [15/50] batch [5/51] time 0.168 (0.478) data 0.000 (0.303) loss 0.7936 (0.7069) acc 85.0000 (85.4737) lr 1.6845e-03 eta 0:14:34
epoch [15/50] batch [10/51] time 0.163 (0.326) data 0.000 (0.152) loss 0.7718 (0.7663) acc 83.5106 (83.9720) lr 1.6845e-03 eta 0:09:54
epoch [15/50] batch [15/51] time 0.169 (0.275) data 0.001 (0.101) loss 0.6391 (0.7550) acc 86.1111 (84.0868) lr 1.6845e-03 eta 0:08:21
epoch [15/50] batch [20/51] time 0.181 (0.252) data 0.000 (0.076) loss 0.8828 (0.7616) acc 80.2083 (83.0459) lr 1.6845e-03 eta 0:07:38
epoch [15/50] batch [25/51] time 0.167 (0.238) data 0.001 (0.061) loss 0.9848 (0.7420) acc 76.0417 (83.5168) lr 1.6845e-03 eta 0:07:10
epoch [15/50] batch [30/51] time 0.162 (0.228) data 0.000 (0.051) loss 0.9963 (0.7563) acc 80.0000 (83.0631) lr 1.6845e-03 eta 0:06:51
epoch [15/50] batch [35/51] time 0.175 (0.219) data 0.000 (0.044) loss 0.6067 (0.7535) acc 85.2941 (83.0179) lr 1.6845e-03 eta 0:06:34
epoch [15/50] batch [40/51] time 0.173 (0.213) data 0.000 (0.038) loss 0.7687 (0.7459) acc 82.6923 (83.0726) lr 1.6845e-03 eta 0:06:22
epoch [15/50] batch [45/51] time 0.754 (0.221) data 0.000 (0.034) loss 1.0029 (0.7461) acc 75.0000 (83.1175) lr 1.6845e-03 eta 0:06:35
epoch [15/50] batch [50/51] time 0.167 (0.216) data 0.000 (0.031) loss 0.8279 (0.7444) acc 84.1837 (83.1858) lr 1.6845e-03 eta 0:06:25
>>> alpha1: 0.217  alpha2: -0.052 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.28 <<<
epoch [16/50] batch [5/51] time 0.174 (0.478) data 0.001 (0.294) loss 0.6239 (0.6470) acc 83.5000 (86.6048) lr 1.6374e-03 eta 0:14:11
epoch [16/50] batch [10/51] time 0.172 (0.325) data 0.000 (0.147) loss 0.7276 (0.6835) acc 81.3726 (84.2224) lr 1.6374e-03 eta 0:09:36
epoch [16/50] batch [15/51] time 0.198 (0.276) data 0.000 (0.098) loss 0.7066 (0.6941) acc 85.3774 (84.2392) lr 1.6374e-03 eta 0:08:09
epoch [16/50] batch [20/51] time 0.166 (0.250) data 0.001 (0.074) loss 0.8791 (0.7198) acc 78.1250 (83.7622) lr 1.6374e-03 eta 0:07:20
epoch [16/50] batch [25/51] time 0.168 (0.235) data 0.001 (0.059) loss 0.7782 (0.7056) acc 83.6735 (84.0749) lr 1.6374e-03 eta 0:06:54
epoch [16/50] batch [30/51] time 0.169 (0.225) data 0.000 (0.049) loss 0.5976 (0.7055) acc 87.7660 (84.1771) lr 1.6374e-03 eta 0:06:34
epoch [16/50] batch [35/51] time 0.168 (0.231) data 0.000 (0.042) loss 0.6711 (0.7165) acc 87.0000 (83.8952) lr 1.6374e-03 eta 0:06:44
epoch [16/50] batch [40/51] time 0.168 (0.224) data 0.000 (0.037) loss 0.6012 (0.7160) acc 84.1837 (83.9724) lr 1.6374e-03 eta 0:06:30
epoch [16/50] batch [45/51] time 0.167 (0.218) data 0.000 (0.033) loss 0.6579 (0.7143) acc 85.2041 (83.9793) lr 1.6374e-03 eta 0:06:18
epoch [16/50] batch [50/51] time 0.171 (0.213) data 0.000 (0.030) loss 0.4521 (0.7078) acc 92.5000 (84.1033) lr 1.6374e-03 eta 0:06:09
>>> alpha1: 0.192  alpha2: -0.051 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.27 <<<
epoch [17/50] batch [5/51] time 0.178 (0.469) data 0.001 (0.292) loss 0.6988 (0.7011) acc 83.1522 (85.5131) lr 1.5878e-03 eta 0:13:31
epoch [17/50] batch [10/51] time 0.188 (0.320) data 0.000 (0.146) loss 0.5405 (0.7121) acc 89.6226 (84.8070) lr 1.5878e-03 eta 0:09:11
epoch [17/50] batch [15/51] time 0.181 (0.271) data 0.000 (0.098) loss 0.8407 (0.6748) acc 83.6735 (85.6426) lr 1.5878e-03 eta 0:07:46
epoch [17/50] batch [20/51] time 0.165 (0.247) data 0.000 (0.074) loss 0.5239 (0.6635) acc 89.0625 (85.7619) lr 1.5878e-03 eta 0:07:03
epoch [17/50] batch [25/51] time 0.162 (0.231) data 0.001 (0.059) loss 0.6585 (0.6760) acc 85.1064 (85.4076) lr 1.5878e-03 eta 0:06:35
epoch [17/50] batch [30/51] time 0.161 (0.221) data 0.000 (0.049) loss 0.9670 (0.6956) acc 77.6596 (84.9215) lr 1.5878e-03 eta 0:06:16
epoch [17/50] batch [35/51] time 0.167 (0.215) data 0.001 (0.042) loss 0.5306 (0.6792) acc 87.7551 (85.1164) lr 1.5878e-03 eta 0:06:04
epoch [17/50] batch [40/51] time 0.162 (0.209) data 0.000 (0.037) loss 0.6682 (0.6844) acc 88.0435 (85.2000) lr 1.5878e-03 eta 0:05:53
epoch [17/50] batch [45/51] time 0.172 (0.205) data 0.001 (0.033) loss 0.8663 (0.6876) acc 78.4314 (85.0513) lr 1.5878e-03 eta 0:05:45
epoch [17/50] batch [50/51] time 0.166 (0.201) data 0.000 (0.030) loss 0.7841 (0.6912) acc 78.5714 (84.9358) lr 1.5878e-03 eta 0:05:38
>>> alpha1: 0.182  alpha2: -0.042 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.28 <<<
epoch [18/50] batch [5/51] time 0.182 (0.455) data 0.000 (0.276) loss 0.6399 (0.6408) acc 87.7273 (85.9732) lr 1.5358e-03 eta 0:12:43
epoch [18/50] batch [10/51] time 0.179 (0.317) data 0.000 (0.138) loss 0.4786 (0.5964) acc 90.2778 (86.2135) lr 1.5358e-03 eta 0:08:49
epoch [18/50] batch [15/51] time 0.167 (0.270) data 0.001 (0.092) loss 0.7875 (0.6168) acc 77.6042 (85.8821) lr 1.5358e-03 eta 0:07:30
epoch [18/50] batch [20/51] time 0.182 (0.245) data 0.001 (0.069) loss 0.6308 (0.6421) acc 85.4546 (85.2581) lr 1.5358e-03 eta 0:06:47
epoch [18/50] batch [25/51] time 0.179 (0.232) data 0.000 (0.056) loss 0.5620 (0.6324) acc 89.9038 (85.6529) lr 1.5358e-03 eta 0:06:24
epoch [18/50] batch [30/51] time 0.181 (0.223) data 0.000 (0.046) loss 0.6601 (0.6404) acc 87.5000 (85.7284) lr 1.5358e-03 eta 0:06:07
epoch [18/50] batch [35/51] time 0.164 (0.215) data 0.001 (0.040) loss 0.9341 (0.6490) acc 72.7273 (85.1207) lr 1.5358e-03 eta 0:05:53
epoch [18/50] batch [40/51] time 0.169 (0.209) data 0.000 (0.035) loss 0.7738 (0.6550) acc 85.7143 (84.9675) lr 1.5358e-03 eta 0:05:43
epoch [18/50] batch [45/51] time 0.175 (0.205) data 0.000 (0.031) loss 0.7418 (0.6564) acc 82.2115 (84.8926) lr 1.5358e-03 eta 0:05:36
epoch [18/50] batch [50/51] time 0.168 (0.202) data 0.000 (0.028) loss 0.7146 (0.6563) acc 87.0000 (84.9543) lr 1.5358e-03 eta 0:05:29
>>> alpha1: 0.174  alpha2: -0.032 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [19/50] batch [5/51] time 0.178 (0.506) data 0.000 (0.313) loss 0.8158 (0.6485) acc 77.0000 (84.7511) lr 1.4818e-03 eta 0:13:43
epoch [19/50] batch [10/51] time 0.166 (0.341) data 0.000 (0.157) loss 0.7324 (0.6425) acc 80.7292 (85.4484) lr 1.4818e-03 eta 0:09:13
epoch [19/50] batch [15/51] time 0.169 (0.286) data 0.000 (0.105) loss 0.7477 (0.6333) acc 84.5000 (86.2266) lr 1.4818e-03 eta 0:07:41
epoch [19/50] batch [20/51] time 0.160 (0.257) data 0.001 (0.079) loss 0.7101 (0.6453) acc 83.5227 (85.4747) lr 1.4818e-03 eta 0:06:53
epoch [19/50] batch [25/51] time 0.171 (0.240) data 0.000 (0.063) loss 0.5320 (0.6474) acc 84.5000 (85.0679) lr 1.4818e-03 eta 0:06:26
epoch [19/50] batch [30/51] time 0.173 (0.231) data 0.000 (0.052) loss 0.7924 (0.6590) acc 78.9216 (84.8964) lr 1.4818e-03 eta 0:06:09
epoch [19/50] batch [35/51] time 0.187 (0.223) data 0.001 (0.045) loss 0.3974 (0.6466) acc 92.3077 (85.2103) lr 1.4818e-03 eta 0:05:55
epoch [19/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.039) loss 0.5678 (0.6557) acc 88.5000 (85.0504) lr 1.4818e-03 eta 0:05:44
epoch [19/50] batch [45/51] time 0.161 (0.211) data 0.000 (0.035) loss 0.5253 (0.6504) acc 88.2979 (85.0551) lr 1.4818e-03 eta 0:05:34
epoch [19/50] batch [50/51] time 0.163 (0.206) data 0.000 (0.032) loss 0.4798 (0.6518) acc 86.7021 (84.9993) lr 1.4818e-03 eta 0:05:26
>>> alpha1: 0.166  alpha2: -0.029 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [20/50] batch [5/51] time 0.185 (0.447) data 0.001 (0.266) loss 0.4440 (0.6273) acc 88.3929 (84.7664) lr 1.4258e-03 eta 0:11:45
epoch [20/50] batch [10/51] time 0.173 (0.310) data 0.000 (0.133) loss 0.6192 (0.6171) acc 82.8431 (85.2345) lr 1.4258e-03 eta 0:08:07
epoch [20/50] batch [15/51] time 0.186 (0.266) data 0.000 (0.089) loss 0.4993 (0.6099) acc 91.8269 (86.3070) lr 1.4258e-03 eta 0:06:57
epoch [20/50] batch [20/51] time 0.172 (0.243) data 0.000 (0.067) loss 0.6619 (0.6194) acc 91.1765 (86.3161) lr 1.4258e-03 eta 0:06:19
epoch [20/50] batch [25/51] time 0.162 (0.230) data 0.000 (0.053) loss 0.9686 (0.6341) acc 77.1277 (85.7745) lr 1.4258e-03 eta 0:05:58
epoch [20/50] batch [30/51] time 0.195 (0.222) data 0.000 (0.045) loss 0.7431 (0.6465) acc 79.0000 (85.3548) lr 1.4258e-03 eta 0:05:44
epoch [20/50] batch [35/51] time 0.162 (0.215) data 0.000 (0.038) loss 0.8916 (0.6493) acc 82.6087 (85.0804) lr 1.4258e-03 eta 0:05:33
epoch [20/50] batch [40/51] time 0.163 (0.211) data 0.001 (0.034) loss 0.7844 (0.6548) acc 78.1915 (85.1156) lr 1.4258e-03 eta 0:05:24
epoch [20/50] batch [45/51] time 0.165 (0.205) data 0.000 (0.030) loss 0.5504 (0.6557) acc 87.5000 (85.0309) lr 1.4258e-03 eta 0:05:15
epoch [20/50] batch [50/51] time 0.181 (0.202) data 0.000 (0.027) loss 0.6542 (0.6561) acc 85.9091 (85.0969) lr 1.4258e-03 eta 0:05:09
>>> alpha1: 0.159  alpha2: -0.030 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [21/50] batch [5/51] time 0.221 (0.498) data 0.000 (0.299) loss 0.5575 (0.5631) acc 86.5741 (87.2313) lr 1.3681e-03 eta 0:12:39
epoch [21/50] batch [10/51] time 0.200 (0.336) data 0.001 (0.150) loss 0.4045 (0.5645) acc 93.0556 (86.9192) lr 1.3681e-03 eta 0:08:30
epoch [21/50] batch [15/51] time 0.187 (0.284) data 0.001 (0.101) loss 0.4520 (0.5610) acc 91.0377 (87.3136) lr 1.3681e-03 eta 0:07:09
epoch [21/50] batch [20/51] time 0.171 (0.257) data 0.000 (0.076) loss 0.5896 (0.5817) acc 86.5000 (86.6667) lr 1.3681e-03 eta 0:06:27
epoch [21/50] batch [25/51] time 0.169 (0.241) data 0.000 (0.061) loss 0.9597 (0.6008) acc 78.5000 (86.3016) lr 1.3681e-03 eta 0:06:02
epoch [21/50] batch [30/51] time 0.165 (0.230) data 0.000 (0.051) loss 0.6566 (0.5976) acc 88.5417 (86.0551) lr 1.3681e-03 eta 0:05:45
epoch [21/50] batch [35/51] time 0.185 (0.223) data 0.000 (0.043) loss 0.6634 (0.6056) acc 84.5000 (86.0415) lr 1.3681e-03 eta 0:05:33
epoch [21/50] batch [40/51] time 0.160 (0.217) data 0.000 (0.038) loss 0.7058 (0.6091) acc 83.1522 (86.0085) lr 1.3681e-03 eta 0:05:23
epoch [21/50] batch [45/51] time 0.162 (0.212) data 0.000 (0.034) loss 0.4926 (0.6139) acc 87.2340 (86.0987) lr 1.3681e-03 eta 0:05:14
epoch [21/50] batch [50/51] time 0.179 (0.207) data 0.000 (0.030) loss 0.6667 (0.6115) acc 81.4815 (86.1121) lr 1.3681e-03 eta 0:05:06
>>> alpha1: 0.155  alpha2: -0.032 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [22/50] batch [5/51] time 0.193 (0.474) data 0.000 (0.299) loss 0.5645 (0.5403) acc 89.1509 (87.6721) lr 1.3090e-03 eta 0:11:38
epoch [22/50] batch [10/51] time 0.175 (0.325) data 0.000 (0.150) loss 0.5988 (0.5504) acc 83.0000 (86.6555) lr 1.3090e-03 eta 0:07:57
epoch [22/50] batch [15/51] time 0.169 (0.276) data 0.000 (0.100) loss 0.6113 (0.5679) acc 84.6939 (85.5693) lr 1.3090e-03 eta 0:06:43
epoch [22/50] batch [20/51] time 0.167 (0.251) data 0.000 (0.075) loss 0.5493 (0.5692) acc 82.1429 (86.0747) lr 1.3090e-03 eta 0:06:05
epoch [22/50] batch [25/51] time 0.189 (0.237) data 0.000 (0.060) loss 0.6817 (0.5750) acc 84.1346 (85.9389) lr 1.3090e-03 eta 0:05:44
epoch [22/50] batch [30/51] time 0.182 (0.226) data 0.000 (0.050) loss 0.7708 (0.5802) acc 77.3256 (85.8390) lr 1.3090e-03 eta 0:05:27
epoch [22/50] batch [35/51] time 0.190 (0.219) data 0.000 (0.043) loss 0.5795 (0.5914) acc 86.2245 (85.7155) lr 1.3090e-03 eta 0:05:16
epoch [22/50] batch [40/51] time 0.175 (0.214) data 0.000 (0.038) loss 0.4445 (0.5863) acc 90.0943 (85.8966) lr 1.3090e-03 eta 0:05:08
epoch [22/50] batch [45/51] time 0.163 (0.209) data 0.000 (0.033) loss 0.4677 (0.5882) acc 86.9792 (86.0954) lr 1.3090e-03 eta 0:04:59
epoch [22/50] batch [50/51] time 0.167 (0.204) data 0.000 (0.030) loss 0.4530 (0.5881) acc 88.5000 (86.1410) lr 1.3090e-03 eta 0:04:52
>>> alpha1: 0.154  alpha2: -0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [23/50] batch [5/51] time 0.184 (0.466) data 0.000 (0.284) loss 0.5438 (0.5963) acc 89.2857 (88.1885) lr 1.2487e-03 eta 0:11:03
epoch [23/50] batch [10/51] time 0.173 (0.321) data 0.000 (0.142) loss 0.4705 (0.5966) acc 87.9808 (87.3133) lr 1.2487e-03 eta 0:07:34
epoch [23/50] batch [15/51] time 0.165 (0.272) data 0.000 (0.095) loss 0.8158 (0.7236) acc 80.2083 (85.1506) lr 1.2487e-03 eta 0:06:24
epoch [23/50] batch [20/51] time 0.183 (0.248) data 0.000 (0.071) loss 0.3007 (0.6684) acc 91.9643 (86.1341) lr 1.2487e-03 eta 0:05:49
epoch [23/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.057) loss 0.4195 (0.6300) acc 90.6863 (86.8678) lr 1.2487e-03 eta 0:05:27
epoch [23/50] batch [30/51] time 0.196 (0.224) data 0.000 (0.047) loss 0.6401 (0.6229) acc 84.2391 (86.8155) lr 1.2487e-03 eta 0:05:13
epoch [23/50] batch [35/51] time 0.173 (0.217) data 0.000 (0.041) loss 0.8124 (0.6228) acc 81.5000 (86.7913) lr 1.2487e-03 eta 0:05:02
epoch [23/50] batch [40/51] time 0.178 (0.212) data 0.000 (0.036) loss 0.4727 (0.6113) acc 87.2642 (86.8326) lr 1.2487e-03 eta 0:04:54
epoch [23/50] batch [45/51] time 0.163 (0.208) data 0.000 (0.032) loss 0.6327 (0.6226) acc 88.0435 (86.3380) lr 1.2487e-03 eta 0:04:47
epoch [23/50] batch [50/51] time 0.161 (0.204) data 0.000 (0.029) loss 0.6506 (0.6243) acc 87.7660 (86.2869) lr 1.2487e-03 eta 0:04:40
>>> alpha1: 0.151  alpha2: -0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [24/50] batch [5/51] time 0.189 (0.451) data 0.000 (0.270) loss 0.5761 (0.6111) acc 87.2807 (86.2758) lr 1.1874e-03 eta 0:10:19
epoch [24/50] batch [10/51] time 0.178 (0.317) data 0.000 (0.135) loss 0.5599 (0.5673) acc 84.4340 (87.8994) lr 1.1874e-03 eta 0:07:12
epoch [24/50] batch [15/51] time 0.168 (0.269) data 0.000 (0.090) loss 0.5686 (0.5844) acc 86.7347 (87.3572) lr 1.1874e-03 eta 0:06:06
epoch [24/50] batch [20/51] time 0.189 (0.247) data 0.001 (0.068) loss 0.5379 (0.5806) acc 90.0000 (87.5069) lr 1.1874e-03 eta 0:05:35
epoch [24/50] batch [25/51] time 0.181 (0.233) data 0.000 (0.054) loss 0.4546 (0.5627) acc 87.5000 (87.7407) lr 1.1874e-03 eta 0:05:15
epoch [24/50] batch [30/51] time 0.160 (0.223) data 0.000 (0.045) loss 0.7202 (0.5661) acc 79.3478 (87.2709) lr 1.1874e-03 eta 0:05:00
epoch [24/50] batch [35/51] time 0.167 (0.216) data 0.000 (0.039) loss 0.5967 (0.5712) acc 84.1837 (86.9572) lr 1.1874e-03 eta 0:04:49
epoch [24/50] batch [40/51] time 0.177 (0.211) data 0.000 (0.034) loss 0.5750 (0.5697) acc 86.7924 (86.8672) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [45/51] time 0.174 (0.207) data 0.001 (0.030) loss 0.6920 (0.5778) acc 85.2941 (86.6475) lr 1.1874e-03 eta 0:04:35
epoch [24/50] batch [50/51] time 0.172 (0.204) data 0.000 (0.027) loss 0.5613 (0.5800) acc 89.7059 (86.5855) lr 1.1874e-03 eta 0:04:30
>>> alpha1: 0.150  alpha2: -0.025 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [25/50] batch [5/51] time 0.168 (0.481) data 0.001 (0.303) loss 0.6248 (0.6087) acc 87.2449 (86.2481) lr 1.1253e-03 eta 0:10:35
epoch [25/50] batch [10/51] time 0.175 (0.328) data 0.000 (0.152) loss 0.5858 (0.5820) acc 85.0962 (86.8794) lr 1.1253e-03 eta 0:07:11
epoch [25/50] batch [15/51] time 0.176 (0.276) data 0.000 (0.101) loss 0.5145 (0.5845) acc 91.3462 (87.0348) lr 1.1253e-03 eta 0:06:02
epoch [25/50] batch [20/51] time 0.179 (0.252) data 0.000 (0.076) loss 0.3161 (0.5649) acc 95.1923 (87.6838) lr 1.1253e-03 eta 0:05:29
epoch [25/50] batch [25/51] time 0.162 (0.236) data 0.000 (0.061) loss 0.7184 (0.5783) acc 82.0652 (87.7436) lr 1.1253e-03 eta 0:05:07
epoch [25/50] batch [30/51] time 0.187 (0.227) data 0.000 (0.051) loss 0.5843 (0.5693) acc 84.8039 (87.6890) lr 1.1253e-03 eta 0:04:53
epoch [25/50] batch [35/51] time 0.169 (0.219) data 0.000 (0.044) loss 0.4783 (0.5644) acc 89.0000 (87.7352) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [40/51] time 0.162 (0.214) data 0.000 (0.038) loss 0.6373 (0.5568) acc 81.3830 (87.7799) lr 1.1253e-03 eta 0:04:34
epoch [25/50] batch [45/51] time 0.170 (0.209) data 0.000 (0.034) loss 0.5787 (0.5646) acc 85.7843 (87.4606) lr 1.1253e-03 eta 0:04:28
epoch [25/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.031) loss 0.6356 (0.5583) acc 87.5000 (87.5126) lr 1.1253e-03 eta 0:04:22
>>> alpha1: 0.147  alpha2: -0.025 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.26 <<<
epoch [26/50] batch [5/51] time 0.181 (0.453) data 0.000 (0.274) loss 0.7157 (0.5740) acc 86.0577 (86.9583) lr 1.0628e-03 eta 0:09:34
epoch [26/50] batch [10/51] time 0.171 (0.316) data 0.000 (0.137) loss 0.6106 (0.5747) acc 87.7451 (87.7893) lr 1.0628e-03 eta 0:06:39
epoch [26/50] batch [15/51] time 0.183 (0.270) data 0.000 (0.092) loss 0.5425 (0.5972) acc 88.9423 (86.7378) lr 1.0628e-03 eta 0:05:40
epoch [26/50] batch [20/51] time 0.174 (0.249) data 0.000 (0.069) loss 0.6766 (0.5739) acc 84.8039 (87.3769) lr 1.0628e-03 eta 0:05:11
epoch [26/50] batch [25/51] time 0.181 (0.233) data 0.000 (0.055) loss 0.4443 (0.5629) acc 89.4231 (87.7011) lr 1.0628e-03 eta 0:04:51
epoch [26/50] batch [30/51] time 0.198 (0.224) data 0.000 (0.046) loss 0.5043 (0.5527) acc 83.1818 (87.4049) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [35/51] time 0.178 (0.218) data 0.001 (0.039) loss 0.5385 (0.5570) acc 86.2745 (87.1906) lr 1.0628e-03 eta 0:04:30
epoch [26/50] batch [40/51] time 0.167 (0.212) data 0.001 (0.035) loss 0.4493 (0.5529) acc 88.2653 (87.4255) lr 1.0628e-03 eta 0:04:21
epoch [26/50] batch [45/51] time 0.174 (0.207) data 0.000 (0.031) loss 0.6444 (0.5593) acc 83.6538 (87.4030) lr 1.0628e-03 eta 0:04:14
epoch [26/50] batch [50/51] time 0.174 (0.204) data 0.000 (0.028) loss 0.4808 (0.5476) acc 89.9038 (87.6958) lr 1.0628e-03 eta 0:04:09
>>> alpha1: 0.147  alpha2: -0.014 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [27/50] batch [5/51] time 0.198 (0.480) data 0.000 (0.299) loss 0.5372 (0.9519) acc 86.7647 (81.3866) lr 1.0000e-03 eta 0:09:44
epoch [27/50] batch [10/51] time 0.185 (0.332) data 0.000 (0.151) loss 0.4382 (0.7428) acc 89.0000 (84.6645) lr 1.0000e-03 eta 0:06:43
epoch [27/50] batch [15/51] time 0.175 (0.278) data 0.000 (0.101) loss 0.5466 (0.6926) acc 89.9038 (85.6260) lr 1.0000e-03 eta 0:05:36
epoch [27/50] batch [20/51] time 0.199 (0.254) data 0.001 (0.076) loss 0.5751 (0.6442) acc 90.3846 (86.6852) lr 1.0000e-03 eta 0:05:06
epoch [27/50] batch [25/51] time 0.184 (0.239) data 0.000 (0.061) loss 0.4352 (0.6209) acc 91.6667 (86.9581) lr 1.0000e-03 eta 0:04:46
epoch [27/50] batch [30/51] time 0.171 (0.229) data 0.000 (0.051) loss 0.6806 (0.6071) acc 87.5000 (87.2885) lr 1.0000e-03 eta 0:04:33
epoch [27/50] batch [35/51] time 0.176 (0.222) data 0.000 (0.044) loss 0.5194 (0.6047) acc 90.3846 (87.3292) lr 1.0000e-03 eta 0:04:23
epoch [27/50] batch [40/51] time 0.165 (0.216) data 0.000 (0.038) loss 0.5823 (0.5944) acc 90.3061 (87.5645) lr 1.0000e-03 eta 0:04:15
epoch [27/50] batch [45/51] time 0.166 (0.210) data 0.000 (0.034) loss 0.6368 (0.5916) acc 83.6735 (87.6080) lr 1.0000e-03 eta 0:04:07
epoch [27/50] batch [50/51] time 0.168 (0.206) data 0.000 (0.031) loss 0.5239 (0.5872) acc 88.0000 (87.5233) lr 1.0000e-03 eta 0:04:01
>>> alpha1: 0.145  alpha2: -0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [28/50] batch [5/51] time 0.172 (0.466) data 0.000 (0.288) loss 0.4908 (0.5462) acc 91.6667 (87.1912) lr 9.3721e-04 eta 0:09:04
epoch [28/50] batch [10/51] time 0.170 (0.322) data 0.000 (0.144) loss 0.5056 (0.5242) acc 90.5000 (88.2181) lr 9.3721e-04 eta 0:06:14
epoch [28/50] batch [15/51] time 0.174 (0.272) data 0.000 (0.096) loss 0.3338 (0.5157) acc 90.3846 (88.6968) lr 9.3721e-04 eta 0:05:15
epoch [28/50] batch [20/51] time 0.165 (0.247) data 0.000 (0.072) loss 0.6105 (0.5280) acc 84.3750 (88.2358) lr 9.3721e-04 eta 0:04:45
epoch [28/50] batch [25/51] time 0.183 (0.232) data 0.000 (0.058) loss 0.4516 (0.5211) acc 88.2653 (88.2639) lr 9.3721e-04 eta 0:04:26
epoch [28/50] batch [30/51] time 0.180 (0.223) data 0.000 (0.048) loss 0.4697 (0.5203) acc 88.6792 (88.1548) lr 9.3721e-04 eta 0:04:15
epoch [28/50] batch [35/51] time 0.168 (0.217) data 0.001 (0.041) loss 0.4874 (0.5155) acc 90.8163 (88.4362) lr 9.3721e-04 eta 0:04:06
epoch [28/50] batch [40/51] time 0.182 (0.212) data 0.000 (0.036) loss 0.4319 (0.5147) acc 92.5926 (88.5027) lr 9.3721e-04 eta 0:04:00
epoch [28/50] batch [45/51] time 0.169 (0.208) data 0.000 (0.032) loss 0.7879 (0.5238) acc 87.5000 (88.3362) lr 9.3721e-04 eta 0:03:54
epoch [28/50] batch [50/51] time 0.187 (0.204) data 0.000 (0.029) loss 0.3093 (0.5268) acc 94.8276 (88.2943) lr 9.3721e-04 eta 0:03:49
>>> alpha1: 0.143  alpha2: -0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.26 <<<
epoch [29/50] batch [5/51] time 0.180 (0.469) data 0.001 (0.286) loss 0.3780 (0.5065) acc 93.3962 (89.1923) lr 8.7467e-04 eta 0:08:43
epoch [29/50] batch [10/51] time 0.152 (0.320) data 0.000 (0.143) loss 0.5649 (0.4994) acc 89.8810 (89.3389) lr 8.7467e-04 eta 0:05:55
epoch [29/50] batch [15/51] time 0.174 (0.273) data 0.000 (0.096) loss 0.5257 (0.5286) acc 87.2549 (88.6341) lr 8.7467e-04 eta 0:05:02
epoch [29/50] batch [20/51] time 0.178 (0.248) data 0.001 (0.072) loss 0.4478 (0.5251) acc 86.2245 (88.4808) lr 8.7467e-04 eta 0:04:33
epoch [29/50] batch [25/51] time 0.165 (0.236) data 0.001 (0.058) loss 0.7123 (0.5359) acc 82.6087 (88.1193) lr 8.7467e-04 eta 0:04:18
epoch [29/50] batch [30/51] time 0.174 (0.226) data 0.001 (0.048) loss 0.4389 (0.5441) acc 92.3469 (88.3632) lr 8.7467e-04 eta 0:04:06
epoch [29/50] batch [35/51] time 0.182 (0.219) data 0.000 (0.041) loss 0.4427 (0.5372) acc 91.0000 (88.5535) lr 8.7467e-04 eta 0:03:57
epoch [29/50] batch [40/51] time 0.162 (0.212) data 0.001 (0.036) loss 0.6571 (0.5411) acc 89.1304 (88.4556) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.032) loss 0.3618 (0.5358) acc 94.3878 (88.6261) lr 8.7467e-04 eta 0:03:43
epoch [29/50] batch [50/51] time 0.186 (0.204) data 0.000 (0.029) loss 0.3374 (0.5215) acc 91.2281 (88.9619) lr 8.7467e-04 eta 0:03:39
>>> alpha1: 0.141  alpha2: -0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.26 <<<
epoch [30/50] batch [5/51] time 0.184 (0.474) data 0.001 (0.293) loss 0.2872 (0.4507) acc 95.0980 (89.3092) lr 8.1262e-04 eta 0:08:25
epoch [30/50] batch [10/51] time 0.170 (0.325) data 0.000 (0.147) loss 0.4459 (0.4903) acc 87.5000 (88.9508) lr 8.1262e-04 eta 0:05:44
epoch [30/50] batch [15/51] time 0.174 (0.274) data 0.000 (0.098) loss 0.6193 (0.4864) acc 86.2745 (89.5246) lr 8.1262e-04 eta 0:04:49
epoch [30/50] batch [20/51] time 0.167 (0.247) data 0.000 (0.073) loss 0.6864 (0.5015) acc 84.3750 (89.3519) lr 8.1262e-04 eta 0:04:19
epoch [30/50] batch [25/51] time 0.169 (0.232) data 0.001 (0.059) loss 0.6001 (0.5146) acc 84.8958 (88.6196) lr 8.1262e-04 eta 0:04:03
epoch [30/50] batch [30/51] time 0.170 (0.222) data 0.000 (0.049) loss 0.5490 (0.5204) acc 87.0000 (88.5045) lr 8.1262e-04 eta 0:03:51
epoch [30/50] batch [35/51] time 0.193 (0.216) data 0.000 (0.042) loss 0.6230 (0.5268) acc 81.3726 (88.3085) lr 8.1262e-04 eta 0:03:43
epoch [30/50] batch [40/51] time 0.156 (0.210) data 0.000 (0.037) loss 0.6397 (0.5357) acc 87.5000 (88.3430) lr 8.1262e-04 eta 0:03:36
epoch [30/50] batch [45/51] time 0.167 (0.205) data 0.000 (0.033) loss 0.4716 (0.5320) acc 90.0000 (88.4582) lr 8.1262e-04 eta 0:03:30
epoch [30/50] batch [50/51] time 0.173 (0.201) data 0.000 (0.030) loss 0.3487 (0.5264) acc 94.3878 (88.5299) lr 8.1262e-04 eta 0:03:25
>>> alpha1: 0.139  alpha2: -0.003 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [31/50] batch [5/51] time 0.188 (0.462) data 0.000 (0.283) loss 0.2900 (0.4721) acc 95.8333 (89.6356) lr 7.5131e-04 eta 0:07:49
epoch [31/50] batch [10/51] time 0.185 (0.319) data 0.000 (0.142) loss 0.2855 (0.4991) acc 94.7115 (89.3921) lr 7.5131e-04 eta 0:05:22
epoch [31/50] batch [15/51] time 0.169 (0.270) data 0.001 (0.095) loss 0.5446 (0.5028) acc 90.4255 (89.2959) lr 7.5131e-04 eta 0:04:30
epoch [31/50] batch [20/51] time 0.168 (0.245) data 0.000 (0.071) loss 0.5759 (0.5195) acc 84.1837 (88.9643) lr 7.5131e-04 eta 0:04:04
epoch [31/50] batch [25/51] time 0.182 (0.231) data 0.000 (0.057) loss 0.4362 (0.5046) acc 85.5000 (89.2185) lr 7.5131e-04 eta 0:03:49
epoch [31/50] batch [30/51] time 0.158 (0.221) data 0.000 (0.048) loss 0.6108 (0.4997) acc 88.8889 (89.4060) lr 7.5131e-04 eta 0:03:38
epoch [31/50] batch [35/51] time 0.168 (0.216) data 0.000 (0.041) loss 0.5564 (0.4981) acc 89.2857 (89.4226) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [40/51] time 0.170 (0.211) data 0.000 (0.036) loss 0.3835 (0.4912) acc 92.5000 (89.6987) lr 7.5131e-04 eta 0:03:26
epoch [31/50] batch [45/51] time 0.164 (0.206) data 0.000 (0.032) loss 0.3927 (0.4940) acc 90.6250 (89.5082) lr 7.5131e-04 eta 0:03:21
epoch [31/50] batch [50/51] time 0.170 (0.203) data 0.000 (0.029) loss 0.7252 (0.4967) acc 85.0000 (89.4581) lr 7.5131e-04 eta 0:03:16
>>> alpha1: 0.137  alpha2: -0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [32/50] batch [5/51] time 0.201 (0.480) data 0.002 (0.291) loss 0.5914 (0.4862) acc 87.5000 (89.2104) lr 6.9098e-04 eta 0:07:42
epoch [32/50] batch [10/51] time 0.191 (0.333) data 0.000 (0.146) loss 0.4344 (0.4619) acc 89.7321 (90.1639) lr 6.9098e-04 eta 0:05:18
epoch [32/50] batch [15/51] time 0.180 (0.282) data 0.000 (0.097) loss 0.3887 (0.4352) acc 91.9811 (90.9672) lr 6.9098e-04 eta 0:04:29
epoch [32/50] batch [20/51] time 0.174 (0.256) data 0.001 (0.073) loss 0.6896 (0.4665) acc 84.3750 (90.2046) lr 6.9098e-04 eta 0:04:02
epoch [32/50] batch [25/51] time 0.183 (0.240) data 0.000 (0.059) loss 0.3887 (0.4829) acc 95.0000 (89.7873) lr 6.9098e-04 eta 0:03:46
epoch [32/50] batch [30/51] time 0.170 (0.229) data 0.000 (0.049) loss 0.4703 (0.4812) acc 92.0000 (89.8176) lr 6.9098e-04 eta 0:03:34
epoch [32/50] batch [35/51] time 0.179 (0.221) data 0.000 (0.042) loss 0.4858 (0.4810) acc 91.5094 (89.7546) lr 6.9098e-04 eta 0:03:26
epoch [32/50] batch [40/51] time 0.164 (0.214) data 0.000 (0.037) loss 0.4768 (0.4809) acc 88.5417 (89.5577) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [45/51] time 0.161 (0.209) data 0.000 (0.033) loss 0.5496 (0.4836) acc 85.1064 (89.3723) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.029) loss 0.3795 (0.4811) acc 91.8478 (89.3961) lr 6.9098e-04 eta 0:03:07
>>> alpha1: 0.136  alpha2: -0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [33/50] batch [5/51] time 0.176 (0.466) data 0.000 (0.286) loss 0.6736 (0.5349) acc 86.9565 (89.6150) lr 6.3188e-04 eta 0:07:05
epoch [33/50] batch [10/51] time 0.176 (0.321) data 0.000 (0.143) loss 0.3797 (0.4982) acc 90.3061 (89.1993) lr 6.3188e-04 eta 0:04:51
epoch [33/50] batch [15/51] time 0.174 (0.272) data 0.000 (0.096) loss 0.5308 (0.4996) acc 88.9423 (89.1181) lr 6.3188e-04 eta 0:04:05
epoch [33/50] batch [20/51] time 0.170 (0.246) data 0.000 (0.072) loss 0.4340 (0.5025) acc 87.5000 (88.9094) lr 6.3188e-04 eta 0:03:40
epoch [33/50] batch [25/51] time 0.177 (0.232) data 0.000 (0.057) loss 0.6465 (0.4944) acc 86.3208 (89.4762) lr 6.3188e-04 eta 0:03:26
epoch [33/50] batch [30/51] time 0.166 (0.222) data 0.000 (0.048) loss 0.5901 (0.4962) acc 86.4583 (89.1935) lr 6.3188e-04 eta 0:03:16
epoch [33/50] batch [35/51] time 0.188 (0.216) data 0.000 (0.041) loss 0.3759 (0.4845) acc 91.2037 (89.4912) lr 6.3188e-04 eta 0:03:11
epoch [33/50] batch [40/51] time 0.165 (0.211) data 0.000 (0.036) loss 0.5405 (0.4765) acc 86.9792 (89.7139) lr 6.3188e-04 eta 0:03:05
epoch [33/50] batch [45/51] time 0.164 (0.206) data 0.000 (0.032) loss 0.6994 (0.4830) acc 83.8542 (89.5317) lr 6.3188e-04 eta 0:02:59
epoch [33/50] batch [50/51] time 0.178 (0.203) data 0.000 (0.029) loss 0.3571 (0.4856) acc 93.9815 (89.4746) lr 6.3188e-04 eta 0:02:55
>>> alpha1: 0.136  alpha2: -0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [34/50] batch [5/51] time 0.167 (0.438) data 0.000 (0.263) loss 0.5509 (0.4920) acc 89.2857 (88.8599) lr 5.7422e-04 eta 0:06:17
epoch [34/50] batch [10/51] time 0.201 (0.309) data 0.000 (0.132) loss 0.6526 (0.5281) acc 85.3774 (88.1739) lr 5.7422e-04 eta 0:04:24
epoch [34/50] batch [15/51] time 0.180 (0.266) data 0.000 (0.088) loss 0.3284 (0.4749) acc 96.2963 (89.8392) lr 5.7422e-04 eta 0:03:46
epoch [34/50] batch [20/51] time 0.180 (0.244) data 0.000 (0.066) loss 0.4965 (0.4893) acc 85.4167 (89.4153) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [25/51] time 0.173 (0.231) data 0.000 (0.053) loss 0.5023 (0.4884) acc 89.2157 (89.6362) lr 5.7422e-04 eta 0:03:14
epoch [34/50] batch [30/51] time 0.182 (0.222) data 0.000 (0.044) loss 0.3688 (0.4834) acc 92.5000 (89.5444) lr 5.7422e-04 eta 0:03:05
epoch [34/50] batch [35/51] time 0.186 (0.215) data 0.000 (0.038) loss 0.6112 (0.4818) acc 84.8039 (89.4201) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [40/51] time 0.173 (0.210) data 0.000 (0.033) loss 0.3234 (0.4784) acc 93.7500 (89.6403) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [45/51] time 0.174 (0.205) data 0.000 (0.030) loss 0.4664 (0.4821) acc 87.5000 (89.5773) lr 5.7422e-04 eta 0:02:48
epoch [34/50] batch [50/51] time 0.164 (0.201) data 0.000 (0.027) loss 0.5520 (0.4885) acc 88.5417 (89.3340) lr 5.7422e-04 eta 0:02:44
>>> alpha1: 0.134  alpha2: -0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [35/50] batch [5/51] time 0.178 (0.481) data 0.000 (0.302) loss 0.4315 (0.4556) acc 89.6226 (91.6230) lr 5.1825e-04 eta 0:06:30
epoch [35/50] batch [10/51] time 0.188 (0.328) data 0.000 (0.151) loss 0.4741 (0.4827) acc 89.5000 (90.1701) lr 5.1825e-04 eta 0:04:24
epoch [35/50] batch [15/51] time 0.181 (0.278) data 0.000 (0.101) loss 0.3912 (0.4799) acc 89.0909 (90.4794) lr 5.1825e-04 eta 0:03:42
epoch [35/50] batch [20/51] time 0.179 (0.251) data 0.000 (0.076) loss 0.4689 (0.4751) acc 89.6226 (90.6220) lr 5.1825e-04 eta 0:03:19
epoch [35/50] batch [25/51] time 0.174 (0.235) data 0.000 (0.061) loss 0.4688 (0.5700) acc 88.7255 (89.4552) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.051) loss 0.4999 (0.6107) acc 91.5094 (88.8755) lr 5.1825e-04 eta 0:02:57
epoch [35/50] batch [35/51] time 0.168 (0.219) data 0.000 (0.043) loss 0.4698 (0.5908) acc 91.3265 (89.1368) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.038) loss 0.4755 (0.5697) acc 90.1961 (89.3397) lr 5.1825e-04 eta 0:02:46
epoch [35/50] batch [45/51] time 0.165 (0.209) data 0.000 (0.034) loss 0.5336 (0.5556) acc 90.1042 (89.4802) lr 5.1825e-04 eta 0:02:41
epoch [35/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.030) loss 0.5193 (0.5509) acc 89.0625 (89.5195) lr 5.1825e-04 eta 0:02:37
>>> alpha1: 0.133  alpha2: -0.003 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [36/50] batch [5/51] time 0.188 (0.446) data 0.000 (0.261) loss 0.5982 (0.4874) acc 87.7451 (88.6661) lr 4.6417e-04 eta 0:05:38
epoch [36/50] batch [10/51] time 0.164 (0.312) data 0.000 (0.131) loss 0.4997 (0.5005) acc 88.2979 (88.9026) lr 4.6417e-04 eta 0:03:55
epoch [36/50] batch [15/51] time 0.189 (0.267) data 0.000 (0.087) loss 0.4590 (0.4867) acc 91.2037 (89.4594) lr 4.6417e-04 eta 0:03:20
epoch [36/50] batch [20/51] time 0.169 (0.244) data 0.000 (0.066) loss 0.3258 (0.4395) acc 92.3469 (90.5172) lr 4.6417e-04 eta 0:03:02
epoch [36/50] batch [25/51] time 0.182 (0.232) data 0.000 (0.052) loss 0.4076 (0.4554) acc 90.4546 (89.8204) lr 4.6417e-04 eta 0:02:51
epoch [36/50] batch [30/51] time 0.158 (0.222) data 0.000 (0.044) loss 0.5263 (0.4517) acc 87.7778 (90.0542) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [35/51] time 0.182 (0.215) data 0.000 (0.038) loss 0.5183 (0.4666) acc 87.7273 (89.8397) lr 4.6417e-04 eta 0:02:37
epoch [36/50] batch [40/51] time 0.176 (0.210) data 0.000 (0.033) loss 0.4706 (0.4600) acc 91.5094 (89.7719) lr 4.6417e-04 eta 0:02:32
epoch [36/50] batch [45/51] time 0.169 (0.205) data 0.000 (0.029) loss 0.6151 (0.4696) acc 84.0000 (89.6546) lr 4.6417e-04 eta 0:02:27
epoch [36/50] batch [50/51] time 0.162 (0.202) data 0.000 (0.026) loss 0.4634 (0.4676) acc 88.2979 (89.6313) lr 4.6417e-04 eta 0:02:24
>>> alpha1: 0.134  alpha2: -0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [37/50] batch [5/51] time 0.170 (0.426) data 0.000 (0.235) loss 0.3042 (0.4165) acc 96.0000 (92.2080) lr 4.1221e-04 eta 0:05:02
epoch [37/50] batch [10/51] time 0.185 (0.303) data 0.000 (0.118) loss 0.4214 (0.4278) acc 90.3846 (91.1652) lr 4.1221e-04 eta 0:03:33
epoch [37/50] batch [15/51] time 0.169 (0.260) data 0.000 (0.079) loss 0.5970 (0.4643) acc 89.0625 (90.2823) lr 4.1221e-04 eta 0:03:01
epoch [37/50] batch [20/51] time 0.200 (0.240) data 0.001 (0.059) loss 0.3909 (0.4547) acc 93.1034 (90.5903) lr 4.1221e-04 eta 0:02:46
epoch [37/50] batch [25/51] time 0.167 (0.227) data 0.000 (0.047) loss 0.5509 (0.4530) acc 86.2245 (90.4405) lr 4.1221e-04 eta 0:02:36
epoch [37/50] batch [30/51] time 0.179 (0.218) data 0.000 (0.039) loss 0.4087 (0.4555) acc 91.8367 (90.4602) lr 4.1221e-04 eta 0:02:29
epoch [37/50] batch [35/51] time 0.174 (0.213) data 0.000 (0.034) loss 0.2747 (0.4591) acc 96.6346 (90.3705) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [40/51] time 0.168 (0.208) data 0.000 (0.030) loss 0.3926 (0.4630) acc 90.8163 (90.2536) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [45/51] time 0.184 (0.204) data 0.000 (0.026) loss 0.4297 (0.4665) acc 91.5179 (90.2322) lr 4.1221e-04 eta 0:02:16
epoch [37/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.024) loss 0.3568 (0.4617) acc 95.9184 (90.3527) lr 4.1221e-04 eta 0:02:13
>>> alpha1: 0.132  alpha2: 0.000 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [38/50] batch [5/51] time 0.185 (0.439) data 0.000 (0.244) loss 0.3074 (0.4639) acc 95.3704 (89.2024) lr 3.6258e-04 eta 0:04:48
epoch [38/50] batch [10/51] time 0.170 (0.310) data 0.000 (0.122) loss 0.4901 (0.4189) acc 88.0000 (90.3318) lr 3.6258e-04 eta 0:03:22
epoch [38/50] batch [15/51] time 0.200 (0.266) data 0.000 (0.082) loss 0.4270 (0.4408) acc 90.8654 (89.9413) lr 3.6258e-04 eta 0:02:52
epoch [38/50] batch [20/51] time 0.167 (0.245) data 0.000 (0.061) loss 0.3921 (0.4318) acc 88.7755 (90.3777) lr 3.6258e-04 eta 0:02:37
epoch [38/50] batch [25/51] time 0.189 (0.233) data 0.000 (0.049) loss 0.3030 (0.4427) acc 92.3729 (90.2151) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.173 (0.224) data 0.000 (0.041) loss 0.4271 (0.4388) acc 94.2308 (90.3070) lr 3.6258e-04 eta 0:02:21
epoch [38/50] batch [35/51] time 0.201 (0.217) data 0.000 (0.035) loss 0.3011 (0.4366) acc 94.1964 (90.3496) lr 3.6258e-04 eta 0:02:16
epoch [38/50] batch [40/51] time 0.159 (0.212) data 0.000 (0.031) loss 0.5435 (0.4415) acc 87.5000 (90.2550) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [45/51] time 0.154 (0.207) data 0.000 (0.027) loss 0.5546 (0.4529) acc 86.9318 (90.0417) lr 3.6258e-04 eta 0:02:07
epoch [38/50] batch [50/51] time 0.165 (0.203) data 0.000 (0.025) loss 0.4317 (0.4503) acc 92.3469 (90.2921) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.131  alpha2: 0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [39/50] batch [5/51] time 0.174 (0.494) data 0.000 (0.319) loss 0.4307 (0.4559) acc 87.2549 (88.6052) lr 3.1545e-04 eta 0:05:00
epoch [39/50] batch [10/51] time 0.179 (0.335) data 0.000 (0.160) loss 0.2576 (0.4670) acc 93.3962 (89.1220) lr 3.1545e-04 eta 0:03:21
epoch [39/50] batch [15/51] time 0.186 (0.283) data 0.001 (0.107) loss 0.4315 (0.4649) acc 93.3036 (89.7471) lr 3.1545e-04 eta 0:02:49
epoch [39/50] batch [20/51] time 0.173 (0.256) data 0.000 (0.080) loss 0.3658 (0.4512) acc 92.6471 (90.1019) lr 3.1545e-04 eta 0:02:31
epoch [39/50] batch [25/51] time 0.170 (0.241) data 0.000 (0.064) loss 0.3307 (0.4369) acc 90.6250 (90.1384) lr 3.1545e-04 eta 0:02:21
epoch [39/50] batch [30/51] time 0.204 (0.231) data 0.000 (0.053) loss 0.3174 (0.4492) acc 92.1296 (90.0227) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [35/51] time 0.178 (0.223) data 0.001 (0.046) loss 0.4161 (0.4406) acc 91.9811 (90.2961) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [40/51] time 0.192 (0.218) data 0.000 (0.040) loss 0.4663 (0.4429) acc 88.3621 (90.1899) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [45/51] time 0.180 (0.213) data 0.000 (0.036) loss 0.7377 (0.4590) acc 82.4074 (89.8511) lr 3.1545e-04 eta 0:02:00
epoch [39/50] batch [50/51] time 0.167 (0.210) data 0.000 (0.032) loss 0.4861 (0.4649) acc 89.7959 (89.8165) lr 3.1545e-04 eta 0:01:57
>>> alpha1: 0.132  alpha2: 0.007 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [40/50] batch [5/51] time 0.165 (0.492) data 0.000 (0.315) loss 0.4733 (0.4748) acc 89.3617 (89.0862) lr 2.7103e-04 eta 0:04:33
epoch [40/50] batch [10/51] time 0.173 (0.336) data 0.000 (0.158) loss 0.5181 (0.5096) acc 90.1961 (88.0960) lr 2.7103e-04 eta 0:03:04
epoch [40/50] batch [15/51] time 0.180 (0.284) data 0.000 (0.105) loss 0.4180 (0.4854) acc 91.0000 (88.9877) lr 2.7103e-04 eta 0:02:35
epoch [40/50] batch [20/51] time 0.173 (0.257) data 0.000 (0.079) loss 0.6291 (0.4962) acc 87.2549 (88.7220) lr 2.7103e-04 eta 0:02:19
epoch [40/50] batch [25/51] time 0.174 (0.241) data 0.001 (0.063) loss 0.3471 (0.4769) acc 92.0000 (89.2796) lr 2.7103e-04 eta 0:02:09
epoch [40/50] batch [30/51] time 0.173 (0.231) data 0.001 (0.053) loss 0.2331 (0.4653) acc 99.5098 (89.5956) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [35/51] time 0.166 (0.224) data 0.000 (0.045) loss 0.4970 (0.4561) acc 85.9375 (89.7232) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [40/51] time 0.173 (0.218) data 0.000 (0.040) loss 0.4100 (0.4559) acc 91.1765 (89.7876) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [45/51] time 0.183 (0.213) data 0.000 (0.035) loss 0.2635 (0.4419) acc 95.9821 (90.1690) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.179 (0.209) data 0.000 (0.032) loss 0.3128 (0.4481) acc 93.0556 (90.1387) lr 2.7103e-04 eta 0:01:46
>>> alpha1: 0.132  alpha2: 0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [41/50] batch [5/51] time 0.170 (0.536) data 0.000 (0.354) loss 0.3666 (0.4451) acc 93.6274 (90.8917) lr 2.2949e-04 eta 0:04:30
epoch [41/50] batch [10/51] time 0.162 (0.355) data 0.000 (0.177) loss 0.4389 (0.4610) acc 93.0851 (90.3001) lr 2.2949e-04 eta 0:02:57
epoch [41/50] batch [15/51] time 0.166 (0.297) data 0.000 (0.118) loss 0.4450 (0.4598) acc 88.0208 (89.7217) lr 2.2949e-04 eta 0:02:27
epoch [41/50] batch [20/51] time 0.174 (0.269) data 0.000 (0.089) loss 0.4919 (0.4674) acc 91.1765 (89.6417) lr 2.2949e-04 eta 0:02:11
epoch [41/50] batch [25/51] time 0.181 (0.251) data 0.000 (0.071) loss 0.3373 (0.4587) acc 89.1509 (89.6933) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [30/51] time 0.188 (0.240) data 0.000 (0.059) loss 0.3347 (0.4417) acc 95.0893 (90.1853) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [35/51] time 0.205 (0.231) data 0.000 (0.051) loss 0.4771 (0.4406) acc 92.9245 (90.4115) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [40/51] time 0.165 (0.224) data 0.000 (0.045) loss 0.4566 (0.4375) acc 92.1875 (90.6839) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [45/51] time 0.170 (0.218) data 0.000 (0.040) loss 0.5385 (0.4426) acc 88.5000 (90.6000) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [50/51] time 0.168 (0.214) data 0.000 (0.036) loss 0.4641 (0.4419) acc 91.0000 (90.6190) lr 2.2949e-04 eta 0:01:38
>>> alpha1: 0.130  alpha2: 0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [42/50] batch [5/51] time 0.173 (0.487) data 0.001 (0.296) loss 0.4455 (0.4502) acc 89.2157 (90.5285) lr 1.9098e-04 eta 0:03:41
epoch [42/50] batch [10/51] time 0.162 (0.328) data 0.000 (0.148) loss 0.5675 (0.4355) acc 90.4255 (90.5709) lr 1.9098e-04 eta 0:02:27
epoch [42/50] batch [15/51] time 0.160 (0.277) data 0.000 (0.099) loss 0.3923 (0.4324) acc 91.1111 (90.7378) lr 1.9098e-04 eta 0:02:03
epoch [42/50] batch [20/51] time 0.182 (0.254) data 0.000 (0.074) loss 0.4606 (0.4412) acc 90.5660 (90.7382) lr 1.9098e-04 eta 0:01:51
epoch [42/50] batch [25/51] time 0.171 (0.238) data 0.000 (0.059) loss 0.6329 (0.4489) acc 87.7660 (90.7519) lr 1.9098e-04 eta 0:01:43
epoch [42/50] batch [30/51] time 0.177 (0.229) data 0.000 (0.050) loss 0.4484 (0.4504) acc 92.9245 (90.7987) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [35/51] time 0.182 (0.224) data 0.000 (0.042) loss 0.4640 (0.4487) acc 89.0625 (90.6155) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [40/51] time 0.218 (0.221) data 0.001 (0.037) loss 0.3845 (0.4465) acc 88.9423 (90.5352) lr 1.9098e-04 eta 0:01:32
epoch [42/50] batch [45/51] time 0.193 (0.217) data 0.000 (0.033) loss 0.3975 (0.4449) acc 90.3846 (90.5181) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [50/51] time 0.214 (0.214) data 0.000 (0.030) loss 0.6297 (0.4502) acc 86.7924 (90.3627) lr 1.9098e-04 eta 0:01:27
>>> alpha1: 0.130  alpha2: 0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [43/50] batch [5/51] time 0.172 (0.467) data 0.000 (0.286) loss 0.4579 (0.3828) acc 88.2353 (91.5732) lr 1.5567e-04 eta 0:03:08
epoch [43/50] batch [10/51] time 0.170 (0.319) data 0.000 (0.143) loss 0.4800 (0.3978) acc 89.7059 (91.9250) lr 1.5567e-04 eta 0:02:07
epoch [43/50] batch [15/51] time 0.173 (0.271) data 0.001 (0.096) loss 0.4313 (0.4043) acc 89.7959 (92.0518) lr 1.5567e-04 eta 0:01:46
epoch [43/50] batch [20/51] time 0.160 (0.246) data 0.000 (0.072) loss 0.6299 (0.4273) acc 90.7609 (91.3284) lr 1.5567e-04 eta 0:01:35
epoch [43/50] batch [25/51] time 0.183 (0.233) data 0.000 (0.057) loss 0.4088 (0.4439) acc 91.3636 (90.8212) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [30/51] time 0.167 (0.222) data 0.000 (0.048) loss 0.4154 (0.4611) acc 89.2857 (90.2083) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [35/51] time 0.178 (0.216) data 0.001 (0.041) loss 0.4529 (0.4486) acc 91.3265 (90.5860) lr 1.5567e-04 eta 0:01:20
epoch [43/50] batch [40/51] time 0.171 (0.211) data 0.000 (0.036) loss 0.5350 (0.4501) acc 85.2941 (90.4261) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [45/51] time 0.167 (0.207) data 0.000 (0.032) loss 0.3601 (0.4442) acc 90.8163 (90.5528) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [50/51] time 0.167 (0.203) data 0.000 (0.029) loss 0.6122 (0.4408) acc 89.2857 (90.6176) lr 1.5567e-04 eta 0:01:12
>>> alpha1: 0.129  alpha2: 0.008 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [44/50] batch [5/51] time 0.162 (0.470) data 0.000 (0.292) loss 0.6188 (0.5347) acc 91.4894 (90.6387) lr 1.2369e-04 eta 0:02:45
epoch [44/50] batch [10/51] time 0.170 (0.321) data 0.000 (0.146) loss 0.3945 (0.4984) acc 90.5000 (89.8121) lr 1.2369e-04 eta 0:01:51
epoch [44/50] batch [15/51] time 0.171 (0.271) data 0.000 (0.098) loss 0.4456 (0.4699) acc 91.6667 (90.6465) lr 1.2369e-04 eta 0:01:32
epoch [44/50] batch [20/51] time 0.182 (0.248) data 0.000 (0.073) loss 0.3804 (0.4762) acc 92.1296 (90.1206) lr 1.2369e-04 eta 0:01:23
epoch [44/50] batch [25/51] time 0.197 (0.236) data 0.016 (0.059) loss 0.3844 (0.4756) acc 92.1296 (90.0943) lr 1.2369e-04 eta 0:01:18
epoch [44/50] batch [30/51] time 0.182 (0.227) data 0.001 (0.049) loss 0.4778 (0.4633) acc 90.4546 (90.3857) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [35/51] time 0.164 (0.220) data 0.000 (0.042) loss 0.7775 (0.4647) acc 82.2222 (90.3255) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [40/51] time 0.167 (0.213) data 0.000 (0.037) loss 0.5706 (0.4706) acc 89.7959 (90.2028) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [45/51] time 0.165 (0.208) data 0.000 (0.033) loss 0.4133 (0.4704) acc 92.7083 (90.1695) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.174 (0.204) data 0.000 (0.030) loss 0.2066 (0.4618) acc 96.1538 (90.3055) lr 1.2369e-04 eta 0:01:02
>>> alpha1: 0.127  alpha2: 0.007 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.25 <<<
epoch [45/50] batch [5/51] time 0.194 (0.444) data 0.001 (0.258) loss 0.3455 (0.4208) acc 93.7500 (93.6274) lr 9.5173e-05 eta 0:02:13
epoch [45/50] batch [10/51] time 0.169 (0.313) data 0.001 (0.129) loss 0.4392 (0.4445) acc 92.5532 (91.9285) lr 9.5173e-05 eta 0:01:32
epoch [45/50] batch [15/51] time 0.165 (0.268) data 0.000 (0.086) loss 0.2088 (0.4457) acc 96.3542 (91.2570) lr 9.5173e-05 eta 0:01:17
epoch [45/50] batch [20/51] time 0.167 (0.246) data 0.000 (0.065) loss 0.5149 (0.4313) acc 90.2174 (91.5152) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [25/51] time 0.176 (0.233) data 0.000 (0.052) loss 0.4138 (0.4272) acc 93.5000 (91.4071) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [30/51] time 0.172 (0.223) data 0.000 (0.043) loss 0.6105 (0.4419) acc 88.0208 (91.0531) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [35/51] time 0.167 (0.216) data 0.000 (0.037) loss 0.4119 (0.4366) acc 90.2174 (91.2097) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [40/51] time 0.180 (0.212) data 0.000 (0.033) loss 0.5036 (0.4402) acc 86.9792 (91.0062) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [45/51] time 0.189 (0.209) data 0.000 (0.029) loss 0.3630 (0.4316) acc 91.3462 (91.1344) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [50/51] time 0.187 (0.207) data 0.000 (0.026) loss 0.4347 (0.4750) acc 95.5000 (90.6486) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.126  alpha2: 0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.27 <<<
epoch [46/50] batch [5/51] time 0.177 (0.486) data 0.000 (0.287) loss 0.4077 (0.3529) acc 91.6667 (92.9531) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [10/51] time 0.162 (0.334) data 0.000 (0.144) loss 0.3811 (0.3836) acc 90.9091 (91.4419) lr 7.0224e-05 eta 0:01:21
epoch [46/50] batch [15/51] time 0.189 (0.283) data 0.000 (0.096) loss 0.6973 (0.4035) acc 83.6538 (91.0519) lr 7.0224e-05 eta 0:01:07
epoch [46/50] batch [20/51] time 0.181 (0.257) data 0.000 (0.072) loss 0.3756 (0.4139) acc 93.7500 (90.9535) lr 7.0224e-05 eta 0:01:00
epoch [46/50] batch [25/51] time 0.178 (0.239) data 0.000 (0.058) loss 0.3391 (0.4283) acc 95.4082 (90.6859) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.168 (0.228) data 0.000 (0.048) loss 0.4175 (0.4255) acc 95.1087 (91.0641) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.172 (0.221) data 0.000 (0.041) loss 0.3581 (0.4233) acc 94.7917 (91.3517) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.036) loss 0.4915 (0.4291) acc 92.0000 (91.2907) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.164 (0.210) data 0.000 (0.032) loss 0.5736 (0.4331) acc 89.5833 (91.5320) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.175 (0.206) data 0.000 (0.029) loss 0.4365 (0.4306) acc 92.0000 (91.4681) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.127  alpha2: 0.009 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [47/50] batch [5/51] time 0.167 (0.456) data 0.000 (0.276) loss 0.4089 (0.3454) acc 89.8936 (92.5111) lr 4.8943e-05 eta 0:01:30
epoch [47/50] batch [10/51] time 0.174 (0.318) data 0.000 (0.138) loss 0.4985 (0.4176) acc 89.9038 (90.6486) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [15/51] time 0.170 (0.270) data 0.000 (0.092) loss 0.4598 (0.4311) acc 87.0000 (90.8993) lr 4.8943e-05 eta 0:00:51
epoch [47/50] batch [20/51] time 0.164 (0.245) data 0.000 (0.069) loss 0.3782 (0.4319) acc 92.3913 (90.6762) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [25/51] time 0.166 (0.230) data 0.000 (0.055) loss 0.4815 (0.4231) acc 92.1875 (91.0080) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.176 (0.220) data 0.000 (0.046) loss 0.6020 (0.4229) acc 82.6087 (90.7790) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [35/51] time 0.194 (0.215) data 0.016 (0.040) loss 0.3960 (0.4722) acc 90.5660 (90.1574) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.168 (0.210) data 0.000 (0.035) loss 0.4911 (0.4686) acc 90.5000 (89.9800) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.163 (0.205) data 0.000 (0.031) loss 0.4248 (0.4671) acc 90.1042 (89.9439) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.028) loss 0.4095 (0.4679) acc 89.5000 (89.8930) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.126  alpha2: 0.009 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.25 <<<
epoch [48/50] batch [5/51] time 0.155 (0.492) data 0.000 (0.311) loss 0.5599 (0.4546) acc 88.6364 (91.4681) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [10/51] time 0.185 (0.335) data 0.001 (0.156) loss 0.3320 (0.4118) acc 97.4490 (92.5122) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.180 (0.282) data 0.000 (0.104) loss 0.2994 (0.4242) acc 91.6667 (91.6272) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [20/51] time 0.176 (0.255) data 0.000 (0.078) loss 0.4312 (0.4124) acc 90.6863 (91.6222) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.187 (0.239) data 0.000 (0.062) loss 0.3407 (0.4081) acc 93.7500 (91.7555) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.175 (0.229) data 0.000 (0.052) loss 0.4228 (0.4156) acc 87.7551 (91.5776) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.179 (0.222) data 0.001 (0.045) loss 0.4481 (0.4155) acc 90.0000 (91.4067) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.164 (0.215) data 0.000 (0.039) loss 0.3119 (0.4203) acc 95.3125 (91.2187) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.155 (0.209) data 0.000 (0.035) loss 0.4631 (0.4202) acc 89.7727 (91.1520) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.031) loss 0.4496 (0.4263) acc 90.6863 (91.0399) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.126  alpha2: 0.011 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.26 <<<
epoch [49/50] batch [5/51] time 0.195 (0.468) data 0.001 (0.273) loss 0.4037 (0.4316) acc 90.1961 (89.8713) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.171 (0.323) data 0.000 (0.137) loss 0.3847 (0.4159) acc 90.6863 (90.8350) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.172 (0.270) data 0.000 (0.091) loss 0.4165 (0.4440) acc 87.7451 (89.9644) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.177 (0.249) data 0.000 (0.068) loss 0.4792 (0.4210) acc 90.0000 (90.6546) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.174 (0.233) data 0.000 (0.055) loss 0.4100 (0.4261) acc 93.0851 (90.7445) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [30/51] time 0.159 (0.223) data 0.000 (0.046) loss 0.5108 (0.4341) acc 90.2174 (90.5709) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.163 (0.215) data 0.000 (0.039) loss 0.6175 (0.4363) acc 89.3617 (90.7361) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.166 (0.210) data 0.000 (0.034) loss 0.5091 (0.4395) acc 86.1702 (90.6320) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.167 (0.205) data 0.000 (0.031) loss 0.4142 (0.4440) acc 91.8367 (90.5760) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.027) loss 0.5392 (0.4578) acc 84.5000 (90.2738) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.126  alpha2: 0.011 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [50/50] batch [5/51] time 0.178 (0.466) data 0.000 (0.295) loss 0.3953 (0.4361) acc 91.5000 (90.7660) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.176 (0.319) data 0.000 (0.148) loss 0.4440 (0.4346) acc 92.7083 (91.4045) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.182 (0.271) data 0.000 (0.098) loss 0.4754 (0.4296) acc 91.6667 (91.5469) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.170 (0.248) data 0.000 (0.074) loss 0.5549 (0.4198) acc 87.2549 (91.7634) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.168 (0.233) data 0.000 (0.059) loss 0.4562 (0.4245) acc 91.0000 (91.6831) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.166 (0.224) data 0.000 (0.049) loss 0.3086 (0.4337) acc 91.8367 (91.3693) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.179 (0.217) data 0.000 (0.042) loss 0.4041 (0.4324) acc 88.5000 (91.2434) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.177 (0.210) data 0.000 (0.037) loss 0.3024 (0.4291) acc 93.0000 (91.1349) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.168 (0.205) data 0.000 (0.033) loss 0.1838 (0.4300) acc 96.5000 (91.0282) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.170 (0.201) data 0.000 (0.030) loss 0.4252 (0.4329) acc 89.2157 (91.0928) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.15, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15]
* matched noise rate: [0.08, 0.05, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.05, 0.06, 0.06, 0.05, 0.05, 0.06, 0.06, 0.07, 0.06, 0.06, 0.07, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.06, 0.06, 0.05, 0.06, 0.06, 0.05, 0.06]
* unmatched noise rate: [0.33, 0.24, 0.25, 0.26, 0.27, 0.28, 0.27, 0.28, 0.27, 0.27, 0.26, 0.27, 0.28, 0.27, 0.25, 0.26, 0.27, 0.27, 0.26, 0.26, 0.26, 0.27, 0.27, 0.28, 0.27, 0.28, 0.28, 0.27, 0.27, 0.26, 0.27, 0.26, 0.26, 0.26, 0.25, 0.27, 0.26, 0.25, 0.26, 0.26]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:06,  2.77s/it] 12%|█▏        | 3/25 [00:02<00:17,  1.29it/s] 20%|██        | 5/25 [00:03<00:08,  2.35it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.56it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.87it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.18it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.41it/s] 60%|██████    | 15/25 [00:03<00:01,  8.51it/s] 68%|██████▊   | 17/25 [00:04<00:01,  7.92it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.90it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.75it/s] 92%|█████████▏| 23/25 [00:04<00:00, 10.43it/s]100%|██████████| 25/25 [00:05<00:00,  7.27it/s]100%|██████████| 25/25 [00:05<00:00,  4.65it/s]
=> result
* total: 2,463
* correct: 2,037
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 80.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 17	acc: 94.4%
* class: 2 (canterbury bells)	total: 12	correct: 2	acc: 16.7%
* class: 3 (sweet pea)	total: 17	correct: 6	acc: 35.3%
* class: 4 (english marigold)	total: 20	correct: 14	acc: 70.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 12	acc: 85.7%
* class: 10 (snapdragon)	total: 26	correct: 24	acc: 92.3%
* class: 11 (colt's foot)	total: 26	correct: 20	acc: 76.9%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 18	acc: 72.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 11	acc: 91.7%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 25	acc: 92.6%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 12	acc: 92.3%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 21	acc: 80.8%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 12	acc: 85.7%
* class: 33 (mexican aster)	total: 12	correct: 11	acc: 91.7%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 19	acc: 86.4%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 15	acc: 88.2%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 53	acc: 89.8%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 27	acc: 96.4%
* class: 50 (petunia)	total: 77	correct: 70	acc: 90.9%
* class: 51 (wild pansy)	total: 26	correct: 24	acc: 92.3%
* class: 52 (primula)	total: 28	correct: 20	acc: 71.4%
* class: 53 (sunflower)	total: 19	correct: 18	acc: 94.7%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 13	acc: 81.2%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 15	acc: 93.8%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 14	acc: 87.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 49	acc: 96.1%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 28	acc: 87.5%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 38	acc: 90.5%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 49	acc: 98.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 33	acc: 84.6%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 17	acc: 100.0%
* class: 86 (magnolia)	total: 18	correct: 16	acc: 88.9%
* class: 87 (cyclamen)	total: 46	correct: 35	acc: 76.1%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 9	acc: 64.3%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 34	acc: 89.5%
* class: 95 (camellia)	total: 27	correct: 23	acc: 85.2%
* class: 96 (mallow)	total: 20	correct: 2	acc: 10.0%
* class: 97 (mexican petunia)	total: 25	correct: 22	acc: 88.0%
* class: 98 (bromelia)	total: 18	correct: 15	acc: 83.3%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 12	acc: 70.6%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 83.4%
Elapsed: 0:28:20
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '6', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_6-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 6
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.259 (1.099) data 0.000 (0.288) loss 4.5568 (4.7350) acc 3.1250 (3.7500) lr 1.0000e-05 eta 0:46:36
epoch [1/50] batch [10/51] time 0.267 (0.682) data 0.000 (0.144) loss 4.3043 (4.5955) acc 6.2500 (4.6875) lr 1.0000e-05 eta 0:28:51
epoch [1/50] batch [15/51] time 0.258 (0.542) data 0.000 (0.096) loss 4.5414 (4.5342) acc 3.1250 (4.7917) lr 1.0000e-05 eta 0:22:53
epoch [1/50] batch [20/51] time 0.262 (0.471) data 0.000 (0.072) loss 4.2726 (4.4772) acc 12.5000 (5.6250) lr 1.0000e-05 eta 0:19:52
epoch [1/50] batch [25/51] time 0.264 (0.430) data 0.000 (0.058) loss 4.1787 (4.4782) acc 6.2500 (5.5000) lr 1.0000e-05 eta 0:18:05
epoch [1/50] batch [30/51] time 0.269 (0.402) data 0.000 (0.048) loss 4.2418 (4.4590) acc 12.5000 (6.0417) lr 1.0000e-05 eta 0:16:52
epoch [1/50] batch [35/51] time 0.258 (0.382) data 0.000 (0.041) loss 4.1185 (4.4313) acc 9.3750 (6.6071) lr 1.0000e-05 eta 0:16:00
epoch [1/50] batch [40/51] time 0.257 (0.367) data 0.000 (0.036) loss 4.0613 (4.3913) acc 18.7500 (7.5000) lr 1.0000e-05 eta 0:15:20
epoch [1/50] batch [45/51] time 0.259 (0.355) data 0.000 (0.032) loss 4.1559 (4.3538) acc 6.2500 (8.2639) lr 1.0000e-05 eta 0:14:48
epoch [1/50] batch [50/51] time 0.261 (0.345) data 0.000 (0.029) loss 4.3164 (4.3458) acc 15.6250 (9.0625) lr 1.0000e-05 eta 0:14:23
epoch [2/50] batch [5/51] time 0.304 (0.571) data 0.000 (0.280) loss 4.1517 (4.0739) acc 28.1250 (23.1250) lr 2.0000e-03 eta 0:23:44
epoch [2/50] batch [10/51] time 0.258 (0.418) data 0.000 (0.140) loss 3.6390 (4.0874) acc 28.1250 (22.1875) lr 2.0000e-03 eta 0:17:20
epoch [2/50] batch [15/51] time 0.261 (0.366) data 0.000 (0.094) loss 3.7569 (3.9361) acc 28.1250 (26.0417) lr 2.0000e-03 eta 0:15:09
epoch [2/50] batch [20/51] time 0.260 (0.340) data 0.000 (0.070) loss 3.9935 (3.9454) acc 28.1250 (26.0938) lr 2.0000e-03 eta 0:14:02
epoch [2/50] batch [25/51] time 0.270 (0.325) data 0.000 (0.056) loss 3.4375 (3.8947) acc 40.6250 (26.5000) lr 2.0000e-03 eta 0:13:25
epoch [2/50] batch [30/51] time 0.261 (0.315) data 0.000 (0.047) loss 3.5419 (3.8461) acc 37.5000 (27.6042) lr 2.0000e-03 eta 0:12:57
epoch [2/50] batch [35/51] time 0.263 (0.308) data 0.000 (0.040) loss 3.8083 (3.8076) acc 37.5000 (28.8393) lr 2.0000e-03 eta 0:12:37
epoch [2/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.035) loss 3.8214 (3.7997) acc 37.5000 (29.4531) lr 2.0000e-03 eta 0:12:22
epoch [2/50] batch [45/51] time 0.260 (0.297) data 0.000 (0.031) loss 3.6077 (3.7736) acc 34.3750 (30.0694) lr 2.0000e-03 eta 0:12:09
epoch [2/50] batch [50/51] time 0.256 (0.293) data 0.000 (0.028) loss 3.5091 (3.7213) acc 40.6250 (31.3750) lr 2.0000e-03 eta 0:11:58
epoch [3/50] batch [5/51] time 0.263 (0.534) data 0.000 (0.252) loss 3.8318 (3.3472) acc 37.5000 (45.0000) lr 1.9980e-03 eta 0:21:43
epoch [3/50] batch [10/51] time 0.272 (0.400) data 0.000 (0.126) loss 3.7682 (3.5606) acc 34.3750 (39.3750) lr 1.9980e-03 eta 0:16:14
epoch [3/50] batch [15/51] time 0.266 (0.354) data 0.000 (0.084) loss 2.9101 (3.4478) acc 53.1250 (40.4167) lr 1.9980e-03 eta 0:14:20
epoch [3/50] batch [20/51] time 0.266 (0.331) data 0.000 (0.063) loss 3.1016 (3.4781) acc 40.6250 (39.8438) lr 1.9980e-03 eta 0:13:24
epoch [3/50] batch [25/51] time 0.267 (0.318) data 0.000 (0.051) loss 3.0816 (3.4699) acc 50.0000 (40.3750) lr 1.9980e-03 eta 0:12:50
epoch [3/50] batch [30/51] time 0.268 (0.309) data 0.000 (0.042) loss 3.8597 (3.5047) acc 28.1250 (39.2708) lr 1.9980e-03 eta 0:12:27
epoch [3/50] batch [35/51] time 0.269 (0.302) data 0.000 (0.036) loss 2.9882 (3.5361) acc 46.8750 (39.1964) lr 1.9980e-03 eta 0:12:09
epoch [3/50] batch [40/51] time 0.257 (0.297) data 0.000 (0.032) loss 3.2818 (3.5042) acc 43.7500 (39.6094) lr 1.9980e-03 eta 0:11:54
epoch [3/50] batch [45/51] time 0.257 (0.292) data 0.000 (0.028) loss 4.1410 (3.5205) acc 28.1250 (39.3750) lr 1.9980e-03 eta 0:11:42
epoch [3/50] batch [50/51] time 0.256 (0.289) data 0.000 (0.025) loss 4.3924 (3.5527) acc 21.8750 (38.9375) lr 1.9980e-03 eta 0:11:32
epoch [4/50] batch [5/51] time 0.273 (0.550) data 0.000 (0.275) loss 3.6103 (3.4735) acc 40.6250 (41.8750) lr 1.9921e-03 eta 0:21:56
epoch [4/50] batch [10/51] time 0.260 (0.407) data 0.000 (0.138) loss 3.7824 (3.4783) acc 28.1250 (40.0000) lr 1.9921e-03 eta 0:16:11
epoch [4/50] batch [15/51] time 0.259 (0.359) data 0.000 (0.092) loss 2.5535 (3.4213) acc 43.7500 (39.1667) lr 1.9921e-03 eta 0:14:15
epoch [4/50] batch [20/51] time 0.268 (0.336) data 0.000 (0.069) loss 3.4683 (3.4554) acc 40.6250 (40.0000) lr 1.9921e-03 eta 0:13:18
epoch [4/50] batch [25/51] time 0.262 (0.322) data 0.000 (0.055) loss 3.6653 (3.4702) acc 40.6250 (39.0000) lr 1.9921e-03 eta 0:12:43
epoch [4/50] batch [30/51] time 0.261 (0.312) data 0.000 (0.046) loss 2.8753 (3.4434) acc 50.0000 (40.1042) lr 1.9921e-03 eta 0:12:17
epoch [4/50] batch [35/51] time 0.272 (0.305) data 0.000 (0.039) loss 3.4686 (3.4695) acc 40.6250 (40.3571) lr 1.9921e-03 eta 0:12:00
epoch [4/50] batch [40/51] time 0.259 (0.300) data 0.000 (0.035) loss 3.7214 (3.4938) acc 37.5000 (39.9219) lr 1.9921e-03 eta 0:11:47
epoch [4/50] batch [45/51] time 0.259 (0.295) data 0.000 (0.031) loss 3.0195 (3.4860) acc 53.1250 (40.0000) lr 1.9921e-03 eta 0:11:34
epoch [4/50] batch [50/51] time 0.259 (0.292) data 0.000 (0.028) loss 3.3956 (3.4685) acc 46.8750 (40.3750) lr 1.9921e-03 eta 0:11:24
epoch [5/50] batch [5/51] time 0.260 (0.560) data 0.000 (0.290) loss 3.2980 (3.0808) acc 53.1250 (45.6250) lr 1.9823e-03 eta 0:21:50
epoch [5/50] batch [10/51] time 0.258 (0.411) data 0.000 (0.145) loss 2.8941 (3.0452) acc 50.0000 (47.5000) lr 1.9823e-03 eta 0:15:59
epoch [5/50] batch [15/51] time 0.284 (0.363) data 0.000 (0.097) loss 4.1271 (3.2868) acc 31.2500 (43.7500) lr 1.9823e-03 eta 0:14:06
epoch [5/50] batch [20/51] time 0.261 (0.339) data 0.000 (0.073) loss 4.5574 (3.3993) acc 28.1250 (42.0312) lr 1.9823e-03 eta 0:13:08
epoch [5/50] batch [25/51] time 0.258 (0.324) data 0.000 (0.058) loss 3.1736 (3.4069) acc 40.6250 (41.3750) lr 1.9823e-03 eta 0:12:31
epoch [5/50] batch [30/51] time 0.259 (0.313) data 0.000 (0.048) loss 3.1420 (3.3521) acc 53.1250 (42.7083) lr 1.9823e-03 eta 0:12:05
epoch [5/50] batch [35/51] time 0.276 (0.307) data 0.000 (0.042) loss 3.1916 (3.3758) acc 56.2500 (42.7679) lr 1.9823e-03 eta 0:11:48
epoch [5/50] batch [40/51] time 0.258 (0.301) data 0.000 (0.036) loss 2.7960 (3.4068) acc 50.0000 (42.5781) lr 1.9823e-03 eta 0:11:33
epoch [5/50] batch [45/51] time 0.257 (0.296) data 0.000 (0.032) loss 3.6176 (3.4113) acc 40.6250 (42.7083) lr 1.9823e-03 eta 0:11:21
epoch [5/50] batch [50/51] time 0.258 (0.292) data 0.000 (0.029) loss 3.6039 (3.4127) acc 31.2500 (42.3125) lr 1.9823e-03 eta 0:11:10
epoch [6/50] batch [5/51] time 0.279 (0.567) data 0.000 (0.281) loss 2.5382 (3.2284) acc 62.5000 (48.1250) lr 1.9686e-03 eta 0:21:38
epoch [6/50] batch [10/51] time 0.259 (0.416) data 0.000 (0.140) loss 3.1931 (3.5134) acc 56.2500 (42.1875) lr 1.9686e-03 eta 0:15:50
epoch [6/50] batch [15/51] time 0.264 (0.366) data 0.000 (0.094) loss 3.5527 (3.4707) acc 50.0000 (43.3333) lr 1.9686e-03 eta 0:13:53
epoch [6/50] batch [20/51] time 0.258 (0.340) data 0.000 (0.070) loss 3.4561 (3.4450) acc 40.6250 (42.8125) lr 1.9686e-03 eta 0:12:52
epoch [6/50] batch [25/51] time 0.267 (0.326) data 0.000 (0.056) loss 3.6448 (3.4495) acc 37.5000 (42.6250) lr 1.9686e-03 eta 0:12:18
epoch [6/50] batch [30/51] time 0.261 (0.316) data 0.000 (0.047) loss 4.0237 (3.4437) acc 37.5000 (42.2917) lr 1.9686e-03 eta 0:11:55
epoch [6/50] batch [35/51] time 0.272 (0.309) data 0.000 (0.040) loss 3.1630 (3.4165) acc 50.0000 (42.9464) lr 1.9686e-03 eta 0:11:37
epoch [6/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.035) loss 3.2981 (3.4084) acc 43.7500 (43.0469) lr 1.9686e-03 eta 0:11:22
epoch [6/50] batch [45/51] time 0.259 (0.298) data 0.000 (0.031) loss 3.7509 (3.3879) acc 43.7500 (43.6111) lr 1.9686e-03 eta 0:11:10
epoch [6/50] batch [50/51] time 0.259 (0.294) data 0.000 (0.028) loss 3.3018 (3.3886) acc 40.6250 (43.6250) lr 1.9686e-03 eta 0:11:00
epoch [7/50] batch [5/51] time 0.262 (0.561) data 0.000 (0.280) loss 3.6181 (3.6673) acc 37.5000 (36.2500) lr 1.9511e-03 eta 0:20:56
epoch [7/50] batch [10/51] time 0.274 (0.415) data 0.000 (0.140) loss 2.9868 (3.4853) acc 53.1250 (41.2500) lr 1.9511e-03 eta 0:15:28
epoch [7/50] batch [15/51] time 0.270 (0.365) data 0.000 (0.094) loss 3.2870 (3.4259) acc 43.7500 (41.8750) lr 1.9511e-03 eta 0:13:34
epoch [7/50] batch [20/51] time 0.268 (0.340) data 0.000 (0.070) loss 3.2179 (3.3484) acc 40.6250 (43.1250) lr 1.9511e-03 eta 0:12:37
epoch [7/50] batch [25/51] time 0.259 (0.325) data 0.000 (0.057) loss 3.2300 (3.3886) acc 50.0000 (42.2500) lr 1.9511e-03 eta 0:12:02
epoch [7/50] batch [30/51] time 0.270 (0.316) data 0.000 (0.047) loss 3.3631 (3.3800) acc 40.6250 (42.2917) lr 1.9511e-03 eta 0:11:38
epoch [7/50] batch [35/51] time 0.271 (0.309) data 0.000 (0.041) loss 3.2917 (3.3507) acc 53.1250 (42.5893) lr 1.9511e-03 eta 0:11:22
epoch [7/50] batch [40/51] time 0.258 (0.303) data 0.000 (0.036) loss 3.4385 (3.3537) acc 34.3750 (42.7344) lr 1.9511e-03 eta 0:11:07
epoch [7/50] batch [45/51] time 0.260 (0.298) data 0.000 (0.032) loss 3.1783 (3.3548) acc 40.6250 (43.0556) lr 1.9511e-03 eta 0:10:55
epoch [7/50] batch [50/51] time 0.258 (0.294) data 0.000 (0.028) loss 2.7137 (3.3297) acc 56.2500 (43.5000) lr 1.9511e-03 eta 0:10:45
epoch [8/50] batch [5/51] time 0.293 (0.567) data 0.000 (0.278) loss 2.7525 (3.1035) acc 46.8750 (45.0000) lr 1.9298e-03 eta 0:20:40
epoch [8/50] batch [10/51] time 0.273 (0.420) data 0.000 (0.139) loss 3.4947 (3.2681) acc 43.7500 (45.0000) lr 1.9298e-03 eta 0:15:16
epoch [8/50] batch [15/51] time 0.272 (0.371) data 0.000 (0.093) loss 2.8584 (3.2568) acc 46.8750 (45.0000) lr 1.9298e-03 eta 0:13:27
epoch [8/50] batch [20/51] time 0.273 (0.345) data 0.000 (0.070) loss 2.8899 (3.2855) acc 56.2500 (45.1562) lr 1.9298e-03 eta 0:12:30
epoch [8/50] batch [25/51] time 0.270 (0.330) data 0.000 (0.056) loss 2.8290 (3.2483) acc 56.2500 (46.0000) lr 1.9298e-03 eta 0:11:55
epoch [8/50] batch [30/51] time 0.261 (0.320) data 0.000 (0.047) loss 3.5415 (3.2691) acc 34.3750 (45.3125) lr 1.9298e-03 eta 0:11:31
epoch [8/50] batch [35/51] time 0.262 (0.312) data 0.000 (0.040) loss 3.2699 (3.2755) acc 40.6250 (45.0893) lr 1.9298e-03 eta 0:11:13
epoch [8/50] batch [40/51] time 0.258 (0.306) data 0.000 (0.035) loss 3.3606 (3.2613) acc 34.3750 (45.4688) lr 1.9298e-03 eta 0:10:58
epoch [8/50] batch [45/51] time 0.258 (0.301) data 0.000 (0.031) loss 3.1692 (3.2444) acc 53.1250 (45.9028) lr 1.9298e-03 eta 0:10:45
epoch [8/50] batch [50/51] time 0.256 (0.296) data 0.000 (0.028) loss 2.8767 (3.2523) acc 46.8750 (45.3750) lr 1.9298e-03 eta 0:10:34
epoch [9/50] batch [5/51] time 0.272 (0.609) data 0.000 (0.331) loss 2.8950 (3.0746) acc 53.1250 (49.3750) lr 1.9048e-03 eta 0:21:40
epoch [9/50] batch [10/51] time 0.259 (0.436) data 0.000 (0.166) loss 3.2454 (3.1130) acc 46.8750 (47.5000) lr 1.9048e-03 eta 0:15:29
epoch [9/50] batch [15/51] time 0.260 (0.378) data 0.000 (0.111) loss 3.1007 (3.1633) acc 46.8750 (47.5000) lr 1.9048e-03 eta 0:13:24
epoch [9/50] batch [20/51] time 0.260 (0.349) data 0.000 (0.083) loss 2.9981 (3.1739) acc 53.1250 (46.5625) lr 1.9048e-03 eta 0:12:20
epoch [9/50] batch [25/51] time 0.259 (0.332) data 0.000 (0.066) loss 2.9823 (3.1364) acc 50.0000 (47.1250) lr 1.9048e-03 eta 0:11:42
epoch [9/50] batch [30/51] time 0.268 (0.322) data 0.000 (0.055) loss 2.8126 (3.1544) acc 46.8750 (46.5625) lr 1.9048e-03 eta 0:11:19
epoch [9/50] batch [35/51] time 0.258 (0.314) data 0.000 (0.047) loss 3.2263 (3.2150) acc 40.6250 (45.6250) lr 1.9048e-03 eta 0:11:00
epoch [9/50] batch [40/51] time 0.256 (0.307) data 0.000 (0.042) loss 3.1992 (3.2374) acc 46.8750 (45.3906) lr 1.9048e-03 eta 0:10:44
epoch [9/50] batch [45/51] time 0.259 (0.301) data 0.000 (0.037) loss 3.4516 (3.2585) acc 37.5000 (45.0000) lr 1.9048e-03 eta 0:10:31
epoch [9/50] batch [50/51] time 0.257 (0.297) data 0.000 (0.033) loss 2.4567 (3.2445) acc 62.5000 (45.2500) lr 1.9048e-03 eta 0:10:21
epoch [10/50] batch [5/51] time 0.267 (0.555) data 0.000 (0.277) loss 2.8836 (2.9298) acc 56.2500 (51.2500) lr 1.8763e-03 eta 0:19:17
epoch [10/50] batch [10/51] time 0.270 (0.411) data 0.000 (0.139) loss 2.8650 (2.9929) acc 50.0000 (51.8750) lr 1.8763e-03 eta 0:14:15
epoch [10/50] batch [15/51] time 0.259 (0.362) data 0.000 (0.093) loss 4.1004 (3.1522) acc 28.1250 (49.3750) lr 1.8763e-03 eta 0:12:31
epoch [10/50] batch [20/51] time 0.282 (0.340) data 0.000 (0.069) loss 3.0035 (3.0775) acc 56.2500 (50.4688) lr 1.8763e-03 eta 0:11:44
epoch [10/50] batch [25/51] time 0.262 (0.326) data 0.000 (0.056) loss 3.2542 (3.0638) acc 46.8750 (50.6250) lr 1.8763e-03 eta 0:11:12
epoch [10/50] batch [30/51] time 0.273 (0.315) data 0.000 (0.046) loss 3.2763 (3.1030) acc 37.5000 (49.6875) lr 1.8763e-03 eta 0:10:50
epoch [10/50] batch [35/51] time 0.260 (0.308) data 0.000 (0.040) loss 2.9479 (3.1358) acc 46.8750 (48.7500) lr 1.8763e-03 eta 0:10:33
epoch [10/50] batch [40/51] time 0.258 (0.302) data 0.000 (0.035) loss 4.3429 (3.1618) acc 31.2500 (48.2812) lr 1.8763e-03 eta 0:10:19
epoch [10/50] batch [45/51] time 0.261 (0.297) data 0.000 (0.031) loss 3.2489 (3.1670) acc 43.7500 (48.4028) lr 1.8763e-03 eta 0:10:07
epoch [10/50] batch [50/51] time 0.256 (0.293) data 0.000 (0.028) loss 4.0861 (3.1763) acc 37.5000 (47.7500) lr 1.8763e-03 eta 0:09:58
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.607  alpha2: 0.232 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.25 <<<
epoch [11/50] batch [5/51] time 0.739 (0.813) data 0.000 (0.303) loss 1.6548 (1.8132) acc 71.6667 (68.2386) lr 1.8443e-03 eta 0:27:34
epoch [11/50] batch [10/51] time 0.809 (0.748) data 0.000 (0.152) loss 1.8381 (1.7438) acc 68.0851 (67.7639) lr 1.8443e-03 eta 0:25:18
epoch [11/50] batch [15/51] time 0.747 (0.680) data 0.000 (0.101) loss 1.5585 (1.7499) acc 71.0227 (66.6542) lr 1.8443e-03 eta 0:22:56
epoch [11/50] batch [20/51] time 0.173 (0.550) data 0.001 (0.076) loss 1.5233 (1.6881) acc 67.2222 (67.7099) lr 1.8443e-03 eta 0:18:31
epoch [11/50] batch [25/51] time 0.159 (0.502) data 0.000 (0.061) loss 1.4381 (1.6611) acc 74.4445 (68.1431) lr 1.8443e-03 eta 0:16:51
epoch [11/50] batch [30/51] time 0.154 (0.445) data 0.000 (0.051) loss 1.6515 (1.6625) acc 63.3721 (68.2524) lr 1.8443e-03 eta 0:14:54
epoch [11/50] batch [35/51] time 0.162 (0.404) data 0.000 (0.044) loss 1.5763 (1.6423) acc 74.4681 (68.3215) lr 1.8443e-03 eta 0:13:30
epoch [11/50] batch [40/51] time 0.974 (0.415) data 0.000 (0.038) loss 1.5851 (1.6296) acc 70.6731 (68.7128) lr 1.8443e-03 eta 0:13:49
epoch [11/50] batch [45/51] time 0.157 (0.387) data 0.000 (0.034) loss 1.4470 (1.6209) acc 73.8889 (68.6273) lr 1.8443e-03 eta 0:12:52
epoch [11/50] batch [50/51] time 0.162 (0.381) data 0.000 (0.031) loss 1.4097 (1.6079) acc 72.7273 (68.8483) lr 1.8443e-03 eta 0:12:37
>>> alpha1: 0.482  alpha2: 0.174 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.13 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.21 <<<
epoch [12/50] batch [5/51] time 0.165 (0.447) data 0.000 (0.279) loss 0.9313 (1.0654) acc 80.7292 (77.3462) lr 1.8090e-03 eta 0:14:47
epoch [12/50] batch [10/51] time 0.179 (0.310) data 0.000 (0.139) loss 0.8398 (1.0327) acc 85.8491 (77.5877) lr 1.8090e-03 eta 0:10:13
epoch [12/50] batch [15/51] time 0.172 (0.263) data 0.000 (0.093) loss 1.0412 (1.0467) acc 80.3922 (77.2962) lr 1.8090e-03 eta 0:08:38
epoch [12/50] batch [20/51] time 0.174 (0.281) data 0.000 (0.070) loss 0.8971 (1.0393) acc 78.8462 (76.9292) lr 1.8090e-03 eta 0:09:13
epoch [12/50] batch [25/51] time 0.163 (0.257) data 0.000 (0.056) loss 0.9444 (1.0696) acc 80.8511 (75.7202) lr 1.8090e-03 eta 0:08:25
epoch [12/50] batch [30/51] time 0.164 (0.264) data 0.000 (0.047) loss 1.1506 (1.0956) acc 70.7447 (74.9098) lr 1.8090e-03 eta 0:08:37
epoch [12/50] batch [35/51] time 0.156 (0.251) data 0.000 (0.040) loss 1.1612 (1.0797) acc 69.0476 (75.2320) lr 1.8090e-03 eta 0:08:11
epoch [12/50] batch [40/51] time 0.178 (0.241) data 0.000 (0.035) loss 1.0207 (1.0711) acc 74.5283 (75.5286) lr 1.8090e-03 eta 0:07:49
epoch [12/50] batch [45/51] time 0.171 (0.233) data 0.000 (0.031) loss 1.0790 (1.0748) acc 75.9804 (75.4635) lr 1.8090e-03 eta 0:07:32
epoch [12/50] batch [50/51] time 0.156 (0.226) data 0.000 (0.028) loss 1.0865 (1.0703) acc 72.7273 (75.5751) lr 1.8090e-03 eta 0:07:17
>>> alpha1: 0.392  alpha2: 0.099 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.12 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.19 <<<
epoch [13/50] batch [5/51] time 0.192 (0.416) data 0.000 (0.240) loss 0.9048 (0.8287) acc 77.2727 (79.5082) lr 1.7705e-03 eta 0:13:24
epoch [13/50] batch [10/51] time 0.160 (0.295) data 0.000 (0.120) loss 1.1267 (0.8935) acc 73.9130 (78.5617) lr 1.7705e-03 eta 0:09:27
epoch [13/50] batch [15/51] time 0.164 (0.253) data 0.000 (0.080) loss 1.1096 (0.9124) acc 72.3404 (78.3969) lr 1.7705e-03 eta 0:08:06
epoch [13/50] batch [20/51] time 0.176 (0.233) data 0.001 (0.060) loss 0.8210 (0.9078) acc 79.7872 (78.5328) lr 1.7705e-03 eta 0:07:26
epoch [13/50] batch [25/51] time 0.154 (0.253) data 0.000 (0.048) loss 1.1566 (0.9099) acc 70.9302 (78.4605) lr 1.7705e-03 eta 0:08:03
epoch [13/50] batch [30/51] time 0.169 (0.240) data 0.000 (0.040) loss 0.8922 (0.8975) acc 77.7778 (78.9987) lr 1.7705e-03 eta 0:07:37
epoch [13/50] batch [35/51] time 0.171 (0.231) data 0.000 (0.035) loss 0.9810 (0.8915) acc 74.4898 (79.1037) lr 1.7705e-03 eta 0:07:18
epoch [13/50] batch [40/51] time 0.153 (0.223) data 0.000 (0.030) loss 1.1857 (0.9024) acc 73.8372 (79.1439) lr 1.7705e-03 eta 0:07:02
epoch [13/50] batch [45/51] time 0.161 (0.216) data 0.000 (0.027) loss 0.7120 (0.9028) acc 80.8511 (78.9130) lr 1.7705e-03 eta 0:06:48
epoch [13/50] batch [50/51] time 0.163 (0.210) data 0.000 (0.024) loss 0.8659 (0.9052) acc 80.7292 (78.7611) lr 1.7705e-03 eta 0:06:36
>>> alpha1: 0.347  alpha2: 0.072 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.18 <<<
epoch [14/50] batch [5/51] time 0.168 (0.448) data 0.000 (0.278) loss 1.1507 (0.9232) acc 76.0417 (79.2346) lr 1.7290e-03 eta 0:14:03
epoch [14/50] batch [10/51] time 0.168 (0.309) data 0.001 (0.139) loss 0.8850 (0.8748) acc 76.5625 (80.1087) lr 1.7290e-03 eta 0:09:39
epoch [14/50] batch [15/51] time 0.174 (0.262) data 0.000 (0.093) loss 0.7133 (0.8430) acc 81.7308 (81.1086) lr 1.7290e-03 eta 0:08:11
epoch [14/50] batch [20/51] time 0.167 (0.240) data 0.000 (0.070) loss 0.7808 (0.8765) acc 84.6591 (79.9496) lr 1.7290e-03 eta 0:07:28
epoch [14/50] batch [25/51] time 0.171 (0.226) data 0.000 (0.056) loss 0.8411 (0.9288) acc 80.8511 (79.6449) lr 1.7290e-03 eta 0:07:01
epoch [14/50] batch [30/51] time 0.173 (0.218) data 0.000 (0.047) loss 0.6756 (0.8947) acc 86.5385 (79.8599) lr 1.7290e-03 eta 0:06:45
epoch [14/50] batch [35/51] time 0.171 (0.212) data 0.000 (0.040) loss 0.9979 (0.8739) acc 76.0638 (80.2665) lr 1.7290e-03 eta 0:06:31
epoch [14/50] batch [40/51] time 0.147 (0.206) data 0.000 (0.035) loss 1.0029 (0.8659) acc 78.6585 (80.6071) lr 1.7290e-03 eta 0:06:21
epoch [14/50] batch [45/51] time 0.157 (0.202) data 0.000 (0.031) loss 0.9411 (0.8707) acc 73.3333 (80.2427) lr 1.7290e-03 eta 0:06:11
epoch [14/50] batch [50/51] time 0.164 (0.197) data 0.000 (0.028) loss 0.9110 (0.8775) acc 84.3750 (79.9347) lr 1.7290e-03 eta 0:06:01
>>> alpha1: 0.319  alpha2: 0.058 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.17 <<<
epoch [15/50] batch [5/51] time 0.157 (0.456) data 0.000 (0.279) loss 0.7628 (0.7549) acc 85.4651 (82.7796) lr 1.6845e-03 eta 0:13:55
epoch [15/50] batch [10/51] time 0.182 (0.312) data 0.000 (0.140) loss 0.8688 (0.7891) acc 83.0000 (81.6543) lr 1.6845e-03 eta 0:09:29
epoch [15/50] batch [15/51] time 0.173 (0.268) data 0.000 (0.093) loss 0.7844 (0.8030) acc 83.8235 (81.0459) lr 1.6845e-03 eta 0:08:08
epoch [15/50] batch [20/51] time 0.157 (0.242) data 0.000 (0.070) loss 0.9234 (0.8013) acc 73.3333 (80.8315) lr 1.6845e-03 eta 0:07:19
epoch [15/50] batch [25/51] time 0.167 (0.228) data 0.000 (0.056) loss 1.0513 (0.7972) acc 73.4694 (80.9462) lr 1.6845e-03 eta 0:06:52
epoch [15/50] batch [30/51] time 0.185 (0.220) data 0.000 (0.047) loss 0.5945 (0.7774) acc 87.5000 (81.4560) lr 1.6845e-03 eta 0:06:37
epoch [15/50] batch [35/51] time 0.160 (0.212) data 0.001 (0.040) loss 0.8544 (0.7868) acc 80.9783 (81.2621) lr 1.6845e-03 eta 0:06:22
epoch [15/50] batch [40/51] time 0.156 (0.207) data 0.000 (0.035) loss 0.8077 (0.7734) acc 80.0000 (81.7622) lr 1.6845e-03 eta 0:06:11
epoch [15/50] batch [45/51] time 0.163 (0.202) data 0.000 (0.031) loss 0.8919 (0.7761) acc 81.2500 (81.6275) lr 1.6845e-03 eta 0:06:01
epoch [15/50] batch [50/51] time 0.163 (0.198) data 0.000 (0.028) loss 0.8382 (0.7785) acc 83.8542 (81.6399) lr 1.6845e-03 eta 0:05:53
>>> alpha1: 0.244  alpha2: -0.005 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.11 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.18 <<<
epoch [16/50] batch [5/51] time 0.171 (0.475) data 0.000 (0.292) loss 0.6403 (0.6727) acc 87.7551 (84.5016) lr 1.6374e-03 eta 0:14:05
epoch [16/50] batch [10/51] time 0.178 (0.324) data 0.000 (0.146) loss 0.8363 (0.7075) acc 78.1250 (83.8839) lr 1.6374e-03 eta 0:09:34
epoch [16/50] batch [15/51] time 0.175 (0.273) data 0.000 (0.097) loss 1.1981 (0.7310) acc 77.1277 (84.2280) lr 1.6374e-03 eta 0:08:04
epoch [16/50] batch [20/51] time 0.185 (0.250) data 0.000 (0.073) loss 0.7105 (0.7245) acc 81.4815 (83.8764) lr 1.6374e-03 eta 0:07:21
epoch [16/50] batch [25/51] time 0.178 (0.236) data 0.000 (0.059) loss 0.9827 (0.7521) acc 73.3696 (82.9481) lr 1.6374e-03 eta 0:06:55
epoch [16/50] batch [30/51] time 0.198 (0.229) data 0.000 (0.049) loss 0.6695 (0.7442) acc 85.3774 (83.2436) lr 1.6374e-03 eta 0:06:41
epoch [16/50] batch [35/51] time 0.174 (0.223) data 0.000 (0.042) loss 0.9587 (0.7457) acc 75.4808 (82.7899) lr 1.6374e-03 eta 0:06:29
epoch [16/50] batch [40/51] time 0.168 (0.216) data 0.000 (0.037) loss 0.7354 (0.7563) acc 86.5000 (82.5504) lr 1.6374e-03 eta 0:06:16
epoch [16/50] batch [45/51] time 0.165 (0.210) data 0.000 (0.033) loss 0.6894 (0.7478) acc 83.6735 (82.8920) lr 1.6374e-03 eta 0:06:05
epoch [16/50] batch [50/51] time 0.166 (0.205) data 0.000 (0.029) loss 0.6525 (0.7415) acc 85.2041 (82.9087) lr 1.6374e-03 eta 0:05:56
>>> alpha1: 0.213  alpha2: -0.014 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.19 <<<
epoch [17/50] batch [5/51] time 0.172 (0.519) data 0.000 (0.347) loss 0.6625 (0.7680) acc 80.2885 (80.8053) lr 1.5878e-03 eta 0:14:56
epoch [17/50] batch [10/51] time 0.190 (0.347) data 0.000 (0.173) loss 0.4574 (0.7066) acc 90.6863 (83.5688) lr 1.5878e-03 eta 0:09:58
epoch [17/50] batch [15/51] time 0.178 (0.288) data 0.000 (0.116) loss 0.4692 (0.6915) acc 89.3519 (84.1036) lr 1.5878e-03 eta 0:08:15
epoch [17/50] batch [20/51] time 0.191 (0.260) data 0.000 (0.087) loss 0.4801 (0.6897) acc 88.6364 (84.2607) lr 1.5878e-03 eta 0:07:25
epoch [17/50] batch [25/51] time 0.164 (0.243) data 0.000 (0.069) loss 0.5009 (0.6823) acc 90.6250 (84.5753) lr 1.5878e-03 eta 0:06:55
epoch [17/50] batch [30/51] time 0.171 (0.232) data 0.000 (0.058) loss 0.7982 (0.6921) acc 79.9020 (84.0102) lr 1.5878e-03 eta 0:06:35
epoch [17/50] batch [35/51] time 0.167 (0.224) data 0.000 (0.050) loss 0.6631 (0.6855) acc 86.7347 (84.1405) lr 1.5878e-03 eta 0:06:19
epoch [17/50] batch [40/51] time 0.164 (0.218) data 0.000 (0.043) loss 0.6459 (0.6906) acc 90.1042 (84.2449) lr 1.5878e-03 eta 0:06:09
epoch [17/50] batch [45/51] time 0.168 (0.212) data 0.000 (0.039) loss 0.9245 (0.7047) acc 81.5000 (83.7076) lr 1.5878e-03 eta 0:05:58
epoch [17/50] batch [50/51] time 0.170 (0.208) data 0.000 (0.035) loss 0.7464 (0.7043) acc 85.7843 (83.8209) lr 1.5878e-03 eta 0:05:49
>>> alpha1: 0.198  alpha2: -0.010 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.19 <<<
epoch [18/50] batch [5/51] time 0.170 (0.486) data 0.000 (0.316) loss 0.8842 (0.7360) acc 79.4118 (83.8370) lr 1.5358e-03 eta 0:13:35
epoch [18/50] batch [10/51] time 0.172 (0.329) data 0.000 (0.158) loss 0.7281 (0.7502) acc 79.6875 (83.3541) lr 1.5358e-03 eta 0:09:10
epoch [18/50] batch [15/51] time 0.180 (0.281) data 0.000 (0.106) loss 0.5966 (0.6612) acc 87.7451 (85.3946) lr 1.5358e-03 eta 0:07:48
epoch [18/50] batch [20/51] time 0.157 (0.254) data 0.000 (0.079) loss 0.8300 (0.6798) acc 81.8182 (84.7761) lr 1.5358e-03 eta 0:07:01
epoch [18/50] batch [25/51] time 0.169 (0.239) data 0.000 (0.063) loss 0.4956 (0.6625) acc 88.0000 (85.1105) lr 1.5358e-03 eta 0:06:35
epoch [18/50] batch [30/51] time 0.181 (0.228) data 0.000 (0.053) loss 0.9323 (0.6656) acc 76.6667 (85.0728) lr 1.5358e-03 eta 0:06:17
epoch [18/50] batch [35/51] time 0.185 (0.221) data 0.000 (0.045) loss 0.4722 (0.6684) acc 91.9643 (85.1132) lr 1.5358e-03 eta 0:06:03
epoch [18/50] batch [40/51] time 0.170 (0.215) data 0.000 (0.040) loss 0.8188 (0.6808) acc 81.3726 (84.5877) lr 1.5358e-03 eta 0:05:52
epoch [18/50] batch [45/51] time 0.160 (0.209) data 0.000 (0.035) loss 0.6369 (0.6812) acc 86.1702 (84.8700) lr 1.5358e-03 eta 0:05:42
epoch [18/50] batch [50/51] time 0.159 (0.221) data 0.000 (0.032) loss 0.7372 (0.6824) acc 82.6087 (84.6272) lr 1.5358e-03 eta 0:06:01
>>> alpha1: 0.188  alpha2: -0.009 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.18 <<<
epoch [19/50] batch [5/51] time 0.170 (0.547) data 0.000 (0.352) loss 0.5716 (0.5853) acc 86.5000 (87.6325) lr 1.4818e-03 eta 0:14:49
epoch [19/50] batch [10/51] time 0.183 (0.363) data 0.000 (0.176) loss 0.7308 (0.6766) acc 84.8958 (86.8579) lr 1.4818e-03 eta 0:09:48
epoch [19/50] batch [15/51] time 0.174 (0.299) data 0.000 (0.118) loss 2.8390 (0.8521) acc 56.7308 (84.0569) lr 1.4818e-03 eta 0:08:03
epoch [19/50] batch [20/51] time 0.172 (0.267) data 0.000 (0.088) loss 0.6222 (0.8614) acc 83.3333 (83.6035) lr 1.4818e-03 eta 0:07:10
epoch [19/50] batch [25/51] time 0.170 (0.249) data 0.000 (0.071) loss 0.6297 (0.8151) acc 82.1429 (83.8313) lr 1.4818e-03 eta 0:06:39
epoch [19/50] batch [30/51] time 0.193 (0.237) data 0.000 (0.059) loss 0.5672 (0.7909) acc 83.1818 (84.1106) lr 1.4818e-03 eta 0:06:18
epoch [19/50] batch [35/51] time 0.174 (0.227) data 0.001 (0.051) loss 0.7167 (0.7657) acc 82.6923 (84.3609) lr 1.4818e-03 eta 0:06:02
epoch [19/50] batch [40/51] time 0.152 (0.219) data 0.000 (0.044) loss 0.8104 (0.7533) acc 81.9767 (84.2226) lr 1.4818e-03 eta 0:05:49
epoch [19/50] batch [45/51] time 0.165 (0.213) data 0.000 (0.039) loss 0.7754 (0.7499) acc 82.1429 (84.2833) lr 1.4818e-03 eta 0:05:38
epoch [19/50] batch [50/51] time 0.163 (0.209) data 0.000 (0.035) loss 0.4909 (0.7357) acc 90.1042 (84.4849) lr 1.4818e-03 eta 0:05:30
>>> alpha1: 0.180  alpha2: -0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.18 <<<
epoch [20/50] batch [5/51] time 0.178 (0.500) data 0.000 (0.328) loss 0.6732 (0.5633) acc 84.9057 (86.4662) lr 1.4258e-03 eta 0:13:07
epoch [20/50] batch [10/51] time 0.170 (0.336) data 0.000 (0.164) loss 0.6228 (0.6113) acc 85.7843 (86.4541) lr 1.4258e-03 eta 0:08:47
epoch [20/50] batch [15/51] time 0.167 (0.281) data 0.000 (0.110) loss 0.5604 (0.6413) acc 89.7959 (85.7876) lr 1.4258e-03 eta 0:07:20
epoch [20/50] batch [20/51] time 0.165 (0.253) data 0.000 (0.083) loss 0.6651 (0.6452) acc 84.8958 (85.8475) lr 1.4258e-03 eta 0:06:34
epoch [20/50] batch [25/51] time 0.171 (0.236) data 0.000 (0.066) loss 0.6644 (0.6495) acc 85.8696 (85.7473) lr 1.4258e-03 eta 0:06:07
epoch [20/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.055) loss 0.4937 (0.6423) acc 89.8936 (85.6430) lr 1.4258e-03 eta 0:05:50
epoch [20/50] batch [35/51] time 0.179 (0.219) data 0.000 (0.047) loss 0.5272 (0.6313) acc 83.7963 (85.8913) lr 1.4258e-03 eta 0:05:38
epoch [20/50] batch [40/51] time 0.166 (0.213) data 0.000 (0.042) loss 0.7617 (0.6281) acc 84.6939 (85.9353) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [45/51] time 0.169 (0.208) data 0.000 (0.037) loss 0.3608 (0.6166) acc 92.1569 (86.1360) lr 1.4258e-03 eta 0:05:19
epoch [20/50] batch [50/51] time 0.177 (0.204) data 0.000 (0.034) loss 0.4645 (0.6205) acc 89.3519 (85.9359) lr 1.4258e-03 eta 0:05:12
>>> alpha1: 0.171  alpha2: -0.002 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.17 <<<
epoch [21/50] batch [5/51] time 0.174 (0.500) data 0.000 (0.324) loss 0.5872 (0.6114) acc 86.5385 (87.1443) lr 1.3681e-03 eta 0:12:42
epoch [21/50] batch [10/51] time 0.173 (0.335) data 0.000 (0.162) loss 0.9617 (0.6081) acc 73.0769 (87.1904) lr 1.3681e-03 eta 0:08:29
epoch [21/50] batch [15/51] time 0.179 (0.282) data 0.000 (0.108) loss 0.4627 (0.5945) acc 90.4255 (87.0042) lr 1.3681e-03 eta 0:07:06
epoch [21/50] batch [20/51] time 0.169 (0.254) data 0.000 (0.081) loss 0.8784 (0.6099) acc 75.0000 (86.2082) lr 1.3681e-03 eta 0:06:23
epoch [21/50] batch [25/51] time 1.101 (0.277) data 0.000 (0.065) loss 0.4661 (0.6008) acc 88.3333 (86.1408) lr 1.3681e-03 eta 0:06:57
epoch [21/50] batch [30/51] time 0.188 (0.260) data 0.000 (0.054) loss 0.5332 (0.5920) acc 87.0192 (86.5790) lr 1.3681e-03 eta 0:06:29
epoch [21/50] batch [35/51] time 0.168 (0.248) data 0.000 (0.046) loss 0.5819 (0.5983) acc 87.7551 (86.4657) lr 1.3681e-03 eta 0:06:10
epoch [21/50] batch [40/51] time 0.182 (0.239) data 0.000 (0.041) loss 0.3432 (0.5906) acc 92.8571 (86.6561) lr 1.3681e-03 eta 0:05:56
epoch [21/50] batch [45/51] time 0.166 (0.231) data 0.000 (0.036) loss 0.6822 (0.5978) acc 83.1633 (86.4603) lr 1.3681e-03 eta 0:05:43
epoch [21/50] batch [50/51] time 0.179 (0.225) data 0.000 (0.033) loss 0.3829 (0.5974) acc 90.7407 (86.5398) lr 1.3681e-03 eta 0:05:32
>>> alpha1: 0.167  alpha2: -0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.18 <<<
epoch [22/50] batch [5/51] time 0.184 (0.513) data 0.000 (0.333) loss 0.4184 (0.4933) acc 90.0000 (89.9349) lr 1.3090e-03 eta 0:12:36
epoch [22/50] batch [10/51] time 0.167 (0.340) data 0.000 (0.166) loss 0.4811 (0.5672) acc 88.2653 (88.6871) lr 1.3090e-03 eta 0:08:19
epoch [22/50] batch [15/51] time 0.162 (0.283) data 0.000 (0.111) loss 0.2999 (0.5770) acc 95.7447 (88.1208) lr 1.3090e-03 eta 0:06:54
epoch [22/50] batch [20/51] time 0.192 (0.257) data 0.000 (0.083) loss 0.5171 (0.5827) acc 89.8148 (88.0175) lr 1.3090e-03 eta 0:06:15
epoch [22/50] batch [25/51] time 0.175 (0.241) data 0.000 (0.067) loss 0.5707 (0.5883) acc 86.7021 (87.6071) lr 1.3090e-03 eta 0:05:49
epoch [22/50] batch [30/51] time 0.160 (0.229) data 0.000 (0.056) loss 0.4910 (0.5881) acc 90.1163 (87.1560) lr 1.3090e-03 eta 0:05:31
epoch [22/50] batch [35/51] time 0.179 (0.223) data 0.000 (0.048) loss 0.6570 (0.5879) acc 85.5556 (86.9060) lr 1.3090e-03 eta 0:05:22
epoch [22/50] batch [40/51] time 0.159 (0.216) data 0.000 (0.042) loss 0.6045 (0.6028) acc 82.0652 (86.3437) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [45/51] time 0.169 (0.211) data 0.000 (0.037) loss 0.3775 (0.6079) acc 91.0000 (86.0782) lr 1.3090e-03 eta 0:05:02
epoch [22/50] batch [50/51] time 0.178 (0.207) data 0.000 (0.033) loss 0.4642 (0.6034) acc 89.3519 (86.1771) lr 1.3090e-03 eta 0:04:55
>>> alpha1: 0.163  alpha2: 0.001 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.17 <<<
epoch [23/50] batch [5/51] time 0.165 (0.517) data 0.000 (0.330) loss 0.5395 (0.5665) acc 86.7021 (87.2458) lr 1.2487e-03 eta 0:12:15
epoch [23/50] batch [10/51] time 0.181 (0.343) data 0.000 (0.165) loss 0.5583 (0.5852) acc 84.6154 (85.9302) lr 1.2487e-03 eta 0:08:06
epoch [23/50] batch [15/51] time 0.169 (0.285) data 0.000 (0.110) loss 0.6259 (0.7365) acc 82.5000 (84.3181) lr 1.2487e-03 eta 0:06:42
epoch [23/50] batch [20/51] time 0.166 (0.256) data 0.000 (0.083) loss 0.6651 (0.6921) acc 82.6531 (84.9443) lr 1.2487e-03 eta 0:06:00
epoch [23/50] batch [25/51] time 0.179 (0.241) data 0.001 (0.066) loss 0.4869 (0.6705) acc 91.1765 (85.0619) lr 1.2487e-03 eta 0:05:37
epoch [23/50] batch [30/51] time 0.182 (0.229) data 0.000 (0.055) loss 0.4995 (0.6544) acc 84.5000 (85.1939) lr 1.2487e-03 eta 0:05:20
epoch [23/50] batch [35/51] time 0.174 (0.221) data 0.000 (0.047) loss 0.5563 (0.6440) acc 85.0962 (85.3144) lr 1.2487e-03 eta 0:05:07
epoch [23/50] batch [40/51] time 0.160 (0.214) data 0.000 (0.041) loss 0.6186 (0.6398) acc 85.1064 (85.6395) lr 1.2487e-03 eta 0:04:57
epoch [23/50] batch [45/51] time 0.162 (0.208) data 0.000 (0.037) loss 0.5995 (0.6388) acc 85.4167 (85.7226) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.164 (0.204) data 0.000 (0.033) loss 0.5496 (0.6308) acc 86.7347 (85.9031) lr 1.2487e-03 eta 0:04:41
>>> alpha1: 0.160  alpha2: 0.007 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.17 <<<
epoch [24/50] batch [5/51] time 0.201 (0.472) data 0.000 (0.286) loss 0.5111 (0.5624) acc 90.2778 (88.7164) lr 1.1874e-03 eta 0:10:47
epoch [24/50] batch [10/51] time 0.174 (0.321) data 0.000 (0.143) loss 0.3393 (0.5137) acc 95.6731 (89.3358) lr 1.1874e-03 eta 0:07:19
epoch [24/50] batch [15/51] time 0.173 (0.271) data 0.001 (0.095) loss 0.6202 (0.5426) acc 86.7021 (88.6482) lr 1.1874e-03 eta 0:06:09
epoch [24/50] batch [20/51] time 0.180 (0.246) data 0.000 (0.072) loss 0.4702 (0.5357) acc 90.0000 (88.9997) lr 1.1874e-03 eta 0:05:33
epoch [24/50] batch [25/51] time 0.174 (0.232) data 0.000 (0.057) loss 0.6205 (0.5538) acc 87.7451 (88.2212) lr 1.1874e-03 eta 0:05:13
epoch [24/50] batch [30/51] time 0.177 (0.222) data 0.000 (0.048) loss 0.5138 (0.5500) acc 86.0577 (88.1246) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [35/51] time 0.169 (0.215) data 0.000 (0.041) loss 0.5942 (0.5461) acc 87.5000 (88.1623) lr 1.1874e-03 eta 0:04:48
epoch [24/50] batch [40/51] time 0.171 (0.210) data 0.000 (0.036) loss 0.3402 (0.5598) acc 92.1569 (87.7865) lr 1.1874e-03 eta 0:04:40
epoch [24/50] batch [45/51] time 0.180 (0.205) data 0.000 (0.032) loss 0.5387 (0.5657) acc 85.0000 (87.3410) lr 1.1874e-03 eta 0:04:33
epoch [24/50] batch [50/51] time 0.154 (0.201) data 0.000 (0.029) loss 0.7464 (0.5768) acc 83.5227 (87.1160) lr 1.1874e-03 eta 0:04:26
>>> alpha1: 0.155  alpha2: 0.003 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.16 <<<
epoch [25/50] batch [5/51] time 0.187 (0.536) data 0.000 (0.337) loss 0.4710 (0.5167) acc 91.3265 (88.2039) lr 1.1253e-03 eta 0:11:48
epoch [25/50] batch [10/51] time 0.199 (0.366) data 0.000 (0.169) loss 0.6493 (0.4921) acc 79.3269 (88.6883) lr 1.1253e-03 eta 0:08:01
epoch [25/50] batch [15/51] time 0.216 (0.307) data 0.000 (0.113) loss 0.3868 (0.5035) acc 93.5000 (88.6279) lr 1.1253e-03 eta 0:06:42
epoch [25/50] batch [20/51] time 0.173 (0.274) data 0.001 (0.085) loss 0.4553 (0.5214) acc 90.1042 (88.2679) lr 1.1253e-03 eta 0:05:57
epoch [25/50] batch [25/51] time 0.162 (0.255) data 0.000 (0.068) loss 0.7358 (0.5285) acc 85.6383 (88.5769) lr 1.1253e-03 eta 0:05:31
epoch [25/50] batch [30/51] time 0.187 (0.240) data 0.000 (0.056) loss 0.6971 (0.5493) acc 86.7924 (88.1910) lr 1.1253e-03 eta 0:05:11
epoch [25/50] batch [35/51] time 0.171 (0.232) data 0.000 (0.048) loss 0.4522 (0.5515) acc 91.1765 (87.9951) lr 1.1253e-03 eta 0:04:59
epoch [25/50] batch [40/51] time 0.179 (0.224) data 0.000 (0.042) loss 0.4807 (0.5833) acc 90.7609 (87.6992) lr 1.1253e-03 eta 0:04:48
epoch [25/50] batch [45/51] time 0.186 (0.220) data 0.000 (0.038) loss 0.6699 (0.5784) acc 85.2041 (87.8516) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [50/51] time 0.194 (0.218) data 0.000 (0.034) loss 0.5669 (0.5730) acc 86.2745 (87.9837) lr 1.1253e-03 eta 0:04:37
>>> alpha1: 0.151  alpha2: 0.006 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.17 <<<
epoch [26/50] batch [5/51] time 0.178 (0.481) data 0.000 (0.299) loss 0.6233 (0.5222) acc 86.9792 (88.7411) lr 1.0628e-03 eta 0:10:10
epoch [26/50] batch [10/51] time 0.155 (0.325) data 0.000 (0.149) loss 0.5718 (0.5529) acc 84.6591 (87.8156) lr 1.0628e-03 eta 0:06:51
epoch [26/50] batch [15/51] time 0.161 (0.274) data 0.000 (0.100) loss 0.6069 (0.5295) acc 88.5870 (88.7012) lr 1.0628e-03 eta 0:05:44
epoch [26/50] batch [20/51] time 0.167 (0.249) data 0.000 (0.075) loss 0.4774 (0.5505) acc 89.7959 (88.3924) lr 1.0628e-03 eta 0:05:12
epoch [26/50] batch [25/51] time 0.170 (0.233) data 0.000 (0.060) loss 0.5811 (0.5546) acc 87.0000 (88.3327) lr 1.0628e-03 eta 0:04:51
epoch [26/50] batch [30/51] time 0.168 (0.224) data 0.000 (0.050) loss 0.5791 (0.5435) acc 85.7143 (88.3008) lr 1.0628e-03 eta 0:04:38
epoch [26/50] batch [35/51] time 0.169 (0.216) data 0.000 (0.043) loss 0.5247 (0.5307) acc 91.1458 (88.7660) lr 1.0628e-03 eta 0:04:28
epoch [26/50] batch [40/51] time 0.174 (0.211) data 0.000 (0.038) loss 0.6907 (0.5338) acc 84.6154 (88.6000) lr 1.0628e-03 eta 0:04:20
epoch [26/50] batch [45/51] time 0.169 (0.206) data 0.000 (0.033) loss 0.5452 (0.5319) acc 86.0000 (88.5685) lr 1.0628e-03 eta 0:04:13
epoch [26/50] batch [50/51] time 0.169 (0.202) data 0.000 (0.030) loss 0.5091 (0.5308) acc 84.8039 (88.4972) lr 1.0628e-03 eta 0:04:07
>>> alpha1: 0.147  alpha2: 0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.03 & unmatched refined noisy rate: 0.17 <<<
epoch [27/50] batch [5/51] time 0.192 (0.429) data 0.000 (0.251) loss 0.6155 (0.7005) acc 85.7143 (88.3084) lr 1.0000e-03 eta 0:08:43
epoch [27/50] batch [10/51] time 0.159 (0.298) data 0.001 (0.126) loss 0.4911 (0.6241) acc 88.3333 (87.4114) lr 1.0000e-03 eta 0:06:02
epoch [27/50] batch [15/51] time 0.166 (0.258) data 0.000 (0.084) loss 0.5405 (0.5877) acc 90.8163 (88.1990) lr 1.0000e-03 eta 0:05:11
epoch [27/50] batch [20/51] time 0.174 (0.236) data 0.000 (0.063) loss 0.6139 (0.5748) acc 87.0192 (88.0827) lr 1.0000e-03 eta 0:04:43
epoch [27/50] batch [25/51] time 0.185 (0.223) data 0.000 (0.050) loss 0.4296 (0.5772) acc 89.9123 (87.9799) lr 1.0000e-03 eta 0:04:27
epoch [27/50] batch [30/51] time 0.170 (0.216) data 0.000 (0.042) loss 0.4751 (0.5683) acc 89.6739 (88.2564) lr 1.0000e-03 eta 0:04:17
epoch [27/50] batch [35/51] time 0.185 (0.211) data 0.001 (0.036) loss 0.4340 (0.5550) acc 91.3462 (88.3234) lr 1.0000e-03 eta 0:04:11
epoch [27/50] batch [40/51] time 0.168 (0.206) data 0.000 (0.032) loss 0.5436 (0.5512) acc 90.5000 (88.2373) lr 1.0000e-03 eta 0:04:04
epoch [27/50] batch [45/51] time 0.168 (0.202) data 0.000 (0.028) loss 0.6373 (0.5498) acc 87.5000 (88.1509) lr 1.0000e-03 eta 0:03:58
epoch [27/50] batch [50/51] time 0.171 (0.199) data 0.000 (0.025) loss 0.5227 (0.5431) acc 88.7255 (88.2976) lr 1.0000e-03 eta 0:03:53
>>> alpha1: 0.146  alpha2: 0.010 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.16 <<<
epoch [28/50] batch [5/51] time 0.174 (0.514) data 0.000 (0.336) loss 0.6997 (0.6172) acc 85.2941 (86.1531) lr 9.3721e-04 eta 0:09:59
epoch [28/50] batch [10/51] time 0.190 (0.345) data 0.000 (0.168) loss 0.5050 (0.5699) acc 91.0377 (87.8572) lr 9.3721e-04 eta 0:06:41
epoch [28/50] batch [15/51] time 0.190 (0.290) data 0.000 (0.112) loss 0.4985 (0.5619) acc 91.0377 (88.4935) lr 9.3721e-04 eta 0:05:36
epoch [28/50] batch [20/51] time 0.174 (0.261) data 0.000 (0.084) loss 0.7437 (0.5530) acc 84.0425 (88.1648) lr 9.3721e-04 eta 0:05:00
epoch [28/50] batch [25/51] time 0.188 (0.244) data 0.000 (0.067) loss 0.3963 (0.5490) acc 92.3077 (87.9873) lr 9.3721e-04 eta 0:04:40
epoch [28/50] batch [30/51] time 0.178 (0.234) data 0.000 (0.056) loss 0.5803 (0.5577) acc 90.0943 (88.0772) lr 9.3721e-04 eta 0:04:26
epoch [28/50] batch [35/51] time 0.172 (0.225) data 0.000 (0.048) loss 0.2895 (0.5501) acc 93.6274 (88.1617) lr 9.3721e-04 eta 0:04:15
epoch [28/50] batch [40/51] time 0.172 (0.217) data 0.000 (0.042) loss 0.3342 (0.5362) acc 95.1923 (88.4809) lr 9.3721e-04 eta 0:04:06
epoch [28/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.038) loss 0.5498 (0.5278) acc 87.7451 (88.7084) lr 9.3721e-04 eta 0:03:59
epoch [28/50] batch [50/51] time 0.178 (0.209) data 0.000 (0.034) loss 0.5001 (0.5231) acc 93.0556 (88.7928) lr 9.3721e-04 eta 0:03:54
>>> alpha1: 0.141  alpha2: 0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.10 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.17 <<<
epoch [29/50] batch [5/51] time 0.195 (0.479) data 0.000 (0.291) loss 0.5135 (0.5222) acc 89.3519 (88.8377) lr 8.7467e-04 eta 0:08:55
epoch [29/50] batch [10/51] time 0.158 (0.324) data 0.000 (0.146) loss 0.6262 (0.4677) acc 85.2273 (90.3001) lr 8.7467e-04 eta 0:06:00
epoch [29/50] batch [15/51] time 0.168 (0.276) data 0.000 (0.097) loss 0.5012 (0.4915) acc 92.8571 (90.2108) lr 8.7467e-04 eta 0:05:05
epoch [29/50] batch [20/51] time 0.171 (0.254) data 0.000 (0.074) loss 0.6171 (0.4805) acc 87.2549 (90.0582) lr 8.7467e-04 eta 0:04:39
epoch [29/50] batch [25/51] time 0.180 (0.239) data 0.000 (0.059) loss 0.4025 (0.4969) acc 88.7255 (89.4815) lr 8.7467e-04 eta 0:04:21
epoch [29/50] batch [30/51] time 0.201 (0.229) data 0.000 (0.049) loss 0.5041 (0.5018) acc 89.0909 (89.2631) lr 8.7467e-04 eta 0:04:09
epoch [29/50] batch [35/51] time 0.192 (0.221) data 0.000 (0.042) loss 0.4815 (0.5038) acc 90.3846 (89.2128) lr 8.7467e-04 eta 0:04:00
epoch [29/50] batch [40/51] time 0.157 (0.234) data 0.000 (0.037) loss 0.5801 (0.4992) acc 88.8889 (89.4612) lr 8.7467e-04 eta 0:04:13
epoch [29/50] batch [45/51] time 0.161 (0.227) data 0.000 (0.033) loss 0.6954 (0.5182) acc 79.2553 (88.7734) lr 8.7467e-04 eta 0:04:04
epoch [29/50] batch [50/51] time 0.169 (0.221) data 0.000 (0.030) loss 0.4009 (0.5150) acc 95.5882 (89.0156) lr 8.7467e-04 eta 0:03:57
>>> alpha1: 0.140  alpha2: 0.004 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [30/50] batch [5/51] time 0.202 (0.450) data 0.001 (0.252) loss 0.5692 (0.5510) acc 86.7647 (88.6220) lr 8.1262e-04 eta 0:07:59
epoch [30/50] batch [10/51] time 0.161 (0.313) data 0.000 (0.126) loss 0.4465 (0.5402) acc 88.8298 (88.6538) lr 8.1262e-04 eta 0:05:31
epoch [30/50] batch [15/51] time 0.180 (0.267) data 0.000 (0.084) loss 0.4396 (0.5215) acc 90.3846 (88.6494) lr 8.1262e-04 eta 0:04:42
epoch [30/50] batch [20/51] time 0.179 (0.244) data 0.000 (0.063) loss 0.4645 (0.5058) acc 90.1042 (88.8834) lr 8.1262e-04 eta 0:04:16
epoch [30/50] batch [25/51] time 0.166 (0.230) data 0.000 (0.051) loss 0.4736 (0.5055) acc 93.8775 (88.7784) lr 8.1262e-04 eta 0:04:00
epoch [30/50] batch [30/51] time 0.194 (0.222) data 0.000 (0.042) loss 0.5073 (0.5032) acc 85.9091 (88.8013) lr 8.1262e-04 eta 0:03:50
epoch [30/50] batch [35/51] time 0.168 (0.215) data 0.000 (0.036) loss 0.3013 (0.4904) acc 96.5000 (89.2417) lr 8.1262e-04 eta 0:03:43
epoch [30/50] batch [40/51] time 0.181 (0.209) data 0.000 (0.032) loss 0.7195 (0.4940) acc 82.1429 (89.2244) lr 8.1262e-04 eta 0:03:35
epoch [30/50] batch [45/51] time 0.176 (0.205) data 0.000 (0.028) loss 0.3986 (0.4959) acc 93.3962 (89.4919) lr 8.1262e-04 eta 0:03:30
epoch [30/50] batch [50/51] time 0.161 (0.201) data 0.000 (0.025) loss 0.7550 (0.4990) acc 78.1915 (89.4241) lr 8.1262e-04 eta 0:03:25
>>> alpha1: 0.141  alpha2: 0.010 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.16 <<<
epoch [31/50] batch [5/51] time 0.175 (0.501) data 0.000 (0.318) loss 0.4635 (0.4773) acc 91.1765 (89.5101) lr 7.5131e-04 eta 0:08:28
epoch [31/50] batch [10/51] time 0.170 (0.338) data 0.000 (0.159) loss 0.5267 (0.4830) acc 88.2353 (89.6299) lr 7.5131e-04 eta 0:05:41
epoch [31/50] batch [15/51] time 0.176 (0.283) data 0.000 (0.106) loss 0.6409 (0.4688) acc 82.6923 (89.9819) lr 7.5131e-04 eta 0:04:44
epoch [31/50] batch [20/51] time 0.166 (0.254) data 0.000 (0.080) loss 0.6067 (0.4856) acc 85.7955 (89.3312) lr 7.5131e-04 eta 0:04:14
epoch [31/50] batch [25/51] time 0.186 (0.239) data 0.000 (0.064) loss 0.6277 (0.4803) acc 88.2353 (89.6670) lr 7.5131e-04 eta 0:03:57
epoch [31/50] batch [30/51] time 0.181 (0.227) data 0.000 (0.053) loss 0.7523 (0.4961) acc 86.0000 (89.4036) lr 7.5131e-04 eta 0:03:45
epoch [31/50] batch [35/51] time 0.189 (0.221) data 0.000 (0.046) loss 0.4670 (0.5035) acc 92.7885 (89.2530) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.040) loss 0.2120 (0.4995) acc 96.5686 (89.2128) lr 7.5131e-04 eta 0:03:29
epoch [31/50] batch [45/51] time 0.174 (0.209) data 0.000 (0.036) loss 0.6595 (0.4940) acc 84.1346 (89.3905) lr 7.5131e-04 eta 0:03:24
epoch [31/50] batch [50/51] time 0.166 (0.205) data 0.000 (0.032) loss 0.5499 (0.4966) acc 85.7143 (89.6392) lr 7.5131e-04 eta 0:03:19
>>> alpha1: 0.141  alpha2: 0.016 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [32/50] batch [5/51] time 0.170 (0.510) data 0.000 (0.333) loss 0.4209 (0.4487) acc 90.5000 (90.4978) lr 6.9098e-04 eta 0:08:11
epoch [32/50] batch [10/51] time 0.195 (0.345) data 0.000 (0.167) loss 0.4369 (0.4589) acc 90.6863 (89.8972) lr 6.9098e-04 eta 0:05:30
epoch [32/50] batch [15/51] time 0.175 (0.290) data 0.000 (0.111) loss 0.4328 (0.4749) acc 92.2222 (89.7743) lr 6.9098e-04 eta 0:04:36
epoch [32/50] batch [20/51] time 0.187 (0.264) data 0.000 (0.083) loss 0.4725 (0.4635) acc 90.6250 (90.6099) lr 6.9098e-04 eta 0:04:10
epoch [32/50] batch [25/51] time 0.175 (0.247) data 0.000 (0.067) loss 0.4011 (0.4665) acc 92.1569 (90.6826) lr 6.9098e-04 eta 0:03:53
epoch [32/50] batch [30/51] time 0.167 (0.235) data 0.000 (0.056) loss 0.6064 (0.4795) acc 88.7755 (90.2780) lr 6.9098e-04 eta 0:03:40
epoch [32/50] batch [35/51] time 0.180 (0.227) data 0.000 (0.048) loss 0.5150 (0.4746) acc 90.0943 (90.2784) lr 6.9098e-04 eta 0:03:31
epoch [32/50] batch [40/51] time 0.165 (0.220) data 0.000 (0.042) loss 0.4082 (0.4709) acc 95.4082 (90.4981) lr 6.9098e-04 eta 0:03:24
epoch [32/50] batch [45/51] time 0.158 (0.214) data 0.000 (0.037) loss 0.6273 (0.4811) acc 86.4130 (90.3068) lr 6.9098e-04 eta 0:03:18
epoch [32/50] batch [50/51] time 0.165 (0.210) data 0.000 (0.034) loss 0.4554 (0.4809) acc 89.2857 (90.2062) lr 6.9098e-04 eta 0:03:12
>>> alpha1: 0.139  alpha2: 0.015 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.15 <<<
epoch [33/50] batch [5/51] time 0.155 (0.473) data 0.001 (0.291) loss 0.5633 (0.5027) acc 88.3721 (88.7023) lr 6.3188e-04 eta 0:07:12
epoch [33/50] batch [10/51] time 0.171 (0.322) data 0.000 (0.146) loss 0.5218 (0.4904) acc 89.8936 (89.2380) lr 6.3188e-04 eta 0:04:52
epoch [33/50] batch [15/51] time 0.169 (0.274) data 0.000 (0.098) loss 0.4506 (0.4880) acc 89.0000 (89.1764) lr 6.3188e-04 eta 0:04:07
epoch [33/50] batch [20/51] time 0.180 (0.249) data 0.000 (0.074) loss 0.4974 (0.4915) acc 86.7347 (89.2315) lr 6.3188e-04 eta 0:03:43
epoch [33/50] batch [25/51] time 0.173 (0.233) data 0.000 (0.059) loss 0.5194 (0.4805) acc 90.3061 (89.7754) lr 6.3188e-04 eta 0:03:28
epoch [33/50] batch [30/51] time 0.191 (0.224) data 0.000 (0.049) loss 0.4525 (0.4760) acc 88.2075 (89.8355) lr 6.3188e-04 eta 0:03:18
epoch [33/50] batch [35/51] time 0.166 (0.216) data 0.000 (0.042) loss 0.4936 (0.4730) acc 86.7347 (89.7624) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [40/51] time 0.166 (0.210) data 0.000 (0.037) loss 0.4198 (0.4752) acc 88.2653 (89.3474) lr 6.3188e-04 eta 0:03:04
epoch [33/50] batch [45/51] time 0.173 (0.206) data 0.000 (0.033) loss 0.5287 (0.4733) acc 90.3846 (89.5482) lr 6.3188e-04 eta 0:02:59
epoch [33/50] batch [50/51] time 0.162 (0.202) data 0.001 (0.030) loss 0.5491 (0.4794) acc 90.4255 (89.6151) lr 6.3188e-04 eta 0:02:55
>>> alpha1: 0.139  alpha2: 0.021 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [34/50] batch [5/51] time 0.175 (0.515) data 0.001 (0.341) loss 0.3351 (0.4482) acc 93.0000 (89.3409) lr 5.7422e-04 eta 0:07:23
epoch [34/50] batch [10/51] time 0.181 (0.345) data 0.000 (0.171) loss 0.4200 (0.4985) acc 90.0000 (88.4755) lr 5.7422e-04 eta 0:04:55
epoch [34/50] batch [15/51] time 0.182 (0.288) data 0.001 (0.114) loss 0.4979 (0.5042) acc 87.5000 (88.4645) lr 5.7422e-04 eta 0:04:05
epoch [34/50] batch [20/51] time 0.193 (0.260) data 0.000 (0.086) loss 0.5418 (0.4898) acc 86.7924 (88.9730) lr 5.7422e-04 eta 0:03:40
epoch [34/50] batch [25/51] time 0.184 (0.243) data 0.001 (0.068) loss 0.5486 (0.4843) acc 87.0000 (89.1266) lr 5.7422e-04 eta 0:03:24
epoch [34/50] batch [30/51] time 0.170 (0.232) data 0.000 (0.057) loss 0.3461 (0.4766) acc 94.6078 (89.5381) lr 5.7422e-04 eta 0:03:14
epoch [34/50] batch [35/51] time 0.166 (0.223) data 0.000 (0.049) loss 0.5090 (0.4761) acc 92.1875 (89.8009) lr 5.7422e-04 eta 0:03:05
epoch [34/50] batch [40/51] time 0.172 (0.217) data 0.000 (0.043) loss 0.3516 (0.4746) acc 88.9423 (89.6930) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [45/51] time 0.167 (0.211) data 0.000 (0.038) loss 0.4111 (0.4732) acc 91.5000 (89.7814) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [50/51] time 0.158 (0.206) data 0.000 (0.034) loss 0.4437 (0.4683) acc 91.6667 (89.8728) lr 5.7422e-04 eta 0:02:48
>>> alpha1: 0.137  alpha2: 0.023 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.04 & unmatched refined noisy rate: 0.14 <<<
epoch [35/50] batch [5/51] time 0.184 (0.477) data 0.000 (0.295) loss 0.4741 (0.4158) acc 92.8571 (92.8301) lr 5.1825e-04 eta 0:06:27
epoch [35/50] batch [10/51] time 0.174 (0.331) data 0.000 (0.148) loss 0.2648 (0.4096) acc 94.2308 (92.4106) lr 5.1825e-04 eta 0:04:26
epoch [35/50] batch [15/51] time 0.174 (0.278) data 0.000 (0.099) loss 0.3737 (0.4175) acc 94.2308 (92.4028) lr 5.1825e-04 eta 0:03:42
epoch [35/50] batch [20/51] time 0.163 (0.250) data 0.000 (0.074) loss 0.6019 (0.4373) acc 87.7660 (91.6803) lr 5.1825e-04 eta 0:03:18
epoch [35/50] batch [25/51] time 0.205 (0.236) data 0.018 (0.060) loss 0.4208 (0.4396) acc 92.0000 (91.0976) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [30/51] time 0.175 (0.225) data 0.000 (0.050) loss 0.4777 (0.4363) acc 88.5000 (91.0053) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.178 (0.217) data 0.000 (0.043) loss 0.4308 (0.4435) acc 91.5094 (90.9589) lr 5.1825e-04 eta 0:02:49
epoch [35/50] batch [40/51] time 0.158 (0.211) data 0.000 (0.038) loss 0.5340 (0.4430) acc 91.1111 (90.8480) lr 5.1825e-04 eta 0:02:43
epoch [35/50] batch [45/51] time 0.161 (0.206) data 0.000 (0.033) loss 0.5106 (0.4444) acc 89.8936 (90.7768) lr 5.1825e-04 eta 0:02:38
epoch [35/50] batch [50/51] time 0.166 (0.202) data 0.000 (0.030) loss 0.4955 (0.4456) acc 85.7143 (90.6046) lr 5.1825e-04 eta 0:02:34
>>> alpha1: 0.135  alpha2: 0.023 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [36/50] batch [5/51] time 0.184 (0.481) data 0.013 (0.314) loss 0.4485 (0.5024) acc 90.8163 (89.0762) lr 4.6417e-04 eta 0:06:05
epoch [36/50] batch [10/51] time 0.179 (0.329) data 0.000 (0.157) loss 0.4182 (0.4577) acc 92.1296 (90.2366) lr 4.6417e-04 eta 0:04:08
epoch [36/50] batch [15/51] time 0.175 (0.278) data 0.000 (0.105) loss 0.3663 (0.4344) acc 96.1538 (91.2772) lr 4.6417e-04 eta 0:03:28
epoch [36/50] batch [20/51] time 0.162 (0.252) data 0.000 (0.079) loss 0.4326 (0.4165) acc 92.5532 (91.8866) lr 4.6417e-04 eta 0:03:08
epoch [36/50] batch [25/51] time 0.172 (0.236) data 0.000 (0.063) loss 0.3491 (0.4103) acc 89.7059 (91.5945) lr 4.6417e-04 eta 0:02:54
epoch [36/50] batch [30/51] time 0.170 (0.226) data 0.000 (0.053) loss 0.4262 (0.4240) acc 88.7255 (91.1003) lr 4.6417e-04 eta 0:02:46
epoch [36/50] batch [35/51] time 0.184 (0.219) data 0.000 (0.045) loss 0.4962 (0.4361) acc 86.5000 (90.8230) lr 4.6417e-04 eta 0:02:39
epoch [36/50] batch [40/51] time 0.180 (0.213) data 0.000 (0.039) loss 0.2428 (0.4288) acc 94.5455 (91.0728) lr 4.6417e-04 eta 0:02:34
epoch [36/50] batch [45/51] time 0.173 (0.208) data 0.000 (0.035) loss 0.4930 (0.4309) acc 88.4615 (91.0126) lr 4.6417e-04 eta 0:02:29
epoch [36/50] batch [50/51] time 0.169 (0.204) data 0.000 (0.032) loss 0.3950 (0.4439) acc 91.5000 (90.7430) lr 4.6417e-04 eta 0:02:25
>>> alpha1: 0.133  alpha2: 0.019 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [37/50] batch [5/51] time 0.193 (0.462) data 0.000 (0.277) loss 0.5043 (0.4422) acc 93.0556 (91.3001) lr 4.1221e-04 eta 0:05:27
epoch [37/50] batch [10/51] time 0.165 (0.318) data 0.000 (0.139) loss 0.3959 (0.4427) acc 94.2708 (90.7777) lr 4.1221e-04 eta 0:03:43
epoch [37/50] batch [15/51] time 0.167 (0.270) data 0.000 (0.093) loss 0.3817 (0.4287) acc 92.8571 (90.6738) lr 4.1221e-04 eta 0:03:08
epoch [37/50] batch [20/51] time 0.176 (0.248) data 0.000 (0.070) loss 0.6224 (0.4406) acc 84.6154 (90.2689) lr 4.1221e-04 eta 0:02:51
epoch [37/50] batch [25/51] time 0.165 (0.232) data 0.000 (0.056) loss 0.4561 (0.4550) acc 91.6667 (90.1640) lr 4.1221e-04 eta 0:02:40
epoch [37/50] batch [30/51] time 0.167 (0.222) data 0.000 (0.047) loss 0.4312 (0.4396) acc 92.8571 (90.7318) lr 4.1221e-04 eta 0:02:31
epoch [37/50] batch [35/51] time 0.171 (0.215) data 0.000 (0.040) loss 0.6365 (0.4515) acc 83.8235 (90.3891) lr 4.1221e-04 eta 0:02:26
epoch [37/50] batch [40/51] time 0.168 (0.209) data 0.000 (0.035) loss 0.4371 (0.4565) acc 92.0000 (90.3406) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [45/51] time 0.166 (0.205) data 0.000 (0.031) loss 0.3859 (0.4473) acc 91.3265 (90.5726) lr 4.1221e-04 eta 0:02:17
epoch [37/50] batch [50/51] time 0.163 (0.202) data 0.000 (0.028) loss 0.5302 (0.4500) acc 89.0625 (90.5361) lr 4.1221e-04 eta 0:02:14
>>> alpha1: 0.130  alpha2: 0.018 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [38/50] batch [5/51] time 0.169 (0.482) data 0.000 (0.306) loss 0.5102 (0.4409) acc 91.0000 (90.6956) lr 3.6258e-04 eta 0:05:16
epoch [38/50] batch [10/51] time 0.178 (0.330) data 0.000 (0.153) loss 0.3935 (0.4338) acc 91.9811 (90.4553) lr 3.6258e-04 eta 0:03:35
epoch [38/50] batch [15/51] time 0.191 (0.279) data 0.000 (0.102) loss 0.5081 (0.4438) acc 86.0000 (90.2322) lr 3.6258e-04 eta 0:03:00
epoch [38/50] batch [20/51] time 0.171 (0.251) data 0.000 (0.077) loss 0.5426 (0.4668) acc 90.0000 (89.8373) lr 3.6258e-04 eta 0:02:41
epoch [38/50] batch [25/51] time 0.185 (0.235) data 0.000 (0.061) loss 0.5814 (0.4839) acc 86.6071 (89.4049) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [30/51] time 0.193 (0.227) data 0.000 (0.051) loss 0.3321 (0.4634) acc 92.4528 (90.0008) lr 3.6258e-04 eta 0:02:23
epoch [38/50] batch [35/51] time 0.172 (0.220) data 0.000 (0.044) loss 0.4488 (0.4597) acc 93.0851 (90.2205) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [40/51] time 0.169 (0.214) data 0.000 (0.039) loss 0.3467 (0.4533) acc 90.5000 (90.3579) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.034) loss 0.3603 (0.4602) acc 92.0000 (90.2218) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [50/51] time 0.184 (0.206) data 0.000 (0.031) loss 0.4400 (0.4619) acc 90.7895 (90.2576) lr 3.6258e-04 eta 0:02:06
>>> alpha1: 0.131  alpha2: 0.022 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.14 <<<
epoch [39/50] batch [5/51] time 0.194 (0.441) data 0.000 (0.254) loss 0.6208 (0.5160) acc 84.1837 (88.8512) lr 3.1545e-04 eta 0:04:27
epoch [39/50] batch [10/51] time 0.180 (0.308) data 0.000 (0.127) loss 0.4745 (0.4713) acc 88.2075 (89.9289) lr 3.1545e-04 eta 0:03:05
epoch [39/50] batch [15/51] time 0.169 (0.264) data 0.000 (0.085) loss 0.4051 (0.4707) acc 88.7755 (89.7515) lr 3.1545e-04 eta 0:02:37
epoch [39/50] batch [20/51] time 0.172 (0.243) data 0.000 (0.064) loss 0.4794 (0.4682) acc 92.1569 (90.1656) lr 3.1545e-04 eta 0:02:23
epoch [39/50] batch [25/51] time 0.194 (0.229) data 0.000 (0.051) loss 0.3480 (0.4697) acc 94.4444 (89.8732) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [30/51] time 0.175 (0.221) data 0.000 (0.043) loss 0.4055 (0.4866) acc 94.4445 (90.0017) lr 3.1545e-04 eta 0:02:08
epoch [39/50] batch [35/51] time 0.197 (0.217) data 0.001 (0.037) loss 0.3522 (0.4710) acc 93.0851 (90.4052) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [40/51] time 0.190 (0.213) data 0.000 (0.032) loss 0.5135 (0.4701) acc 88.0000 (90.4291) lr 3.1545e-04 eta 0:02:02
epoch [39/50] batch [45/51] time 0.179 (0.210) data 0.000 (0.029) loss 0.3195 (0.4683) acc 92.5926 (90.2987) lr 3.1545e-04 eta 0:01:58
epoch [39/50] batch [50/51] time 0.161 (0.206) data 0.000 (0.026) loss 0.5753 (0.4712) acc 88.0435 (90.1844) lr 3.1545e-04 eta 0:01:55
>>> alpha1: 0.132  alpha2: 0.027 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [40/50] batch [5/51] time 0.180 (0.482) data 0.000 (0.302) loss 0.3964 (0.4490) acc 90.5556 (90.6176) lr 2.7103e-04 eta 0:04:28
epoch [40/50] batch [10/51] time 0.174 (0.334) data 0.000 (0.151) loss 0.5244 (0.4543) acc 87.0192 (90.2453) lr 2.7103e-04 eta 0:03:04
epoch [40/50] batch [15/51] time 0.165 (0.282) data 0.000 (0.101) loss 0.6365 (0.4683) acc 84.8958 (89.8718) lr 2.7103e-04 eta 0:02:33
epoch [40/50] batch [20/51] time 0.190 (0.257) data 0.000 (0.076) loss 0.4583 (0.4518) acc 91.6667 (90.7604) lr 2.7103e-04 eta 0:02:19
epoch [40/50] batch [25/51] time 0.173 (0.241) data 0.001 (0.061) loss 0.5874 (0.4633) acc 86.7647 (90.3136) lr 2.7103e-04 eta 0:02:09
epoch [40/50] batch [30/51] time 0.176 (0.230) data 0.000 (0.051) loss 0.5033 (0.4699) acc 89.5000 (90.1014) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [35/51] time 0.177 (0.222) data 0.000 (0.043) loss 0.3405 (0.4631) acc 94.6808 (90.3124) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [40/51] time 0.172 (0.216) data 0.000 (0.038) loss 0.3888 (0.4584) acc 89.9038 (90.3473) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [45/51] time 0.173 (0.211) data 0.000 (0.034) loss 0.3269 (0.4557) acc 94.2308 (90.2609) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.158 (0.207) data 0.000 (0.030) loss 0.4138 (0.4533) acc 91.3043 (90.2453) lr 2.7103e-04 eta 0:01:45
>>> alpha1: 0.131  alpha2: 0.028 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.14 <<<
epoch [41/50] batch [5/51] time 0.178 (0.582) data 0.000 (0.394) loss 0.4600 (0.4376) acc 87.7358 (91.8272) lr 2.2949e-04 eta 0:04:53
epoch [41/50] batch [10/51] time 0.185 (0.382) data 0.000 (0.197) loss 0.6249 (0.4814) acc 85.2041 (91.2237) lr 2.2949e-04 eta 0:03:10
epoch [41/50] batch [15/51] time 0.182 (0.315) data 0.001 (0.132) loss 0.4810 (0.4679) acc 90.6863 (91.3206) lr 2.2949e-04 eta 0:02:35
epoch [41/50] batch [20/51] time 0.173 (0.282) data 0.000 (0.099) loss 0.4002 (0.4631) acc 93.6274 (91.0617) lr 2.2949e-04 eta 0:02:18
epoch [41/50] batch [25/51] time 0.176 (0.261) data 0.001 (0.079) loss 0.4057 (0.4639) acc 90.8654 (90.9121) lr 2.2949e-04 eta 0:02:06
epoch [41/50] batch [30/51] time 0.174 (0.248) data 0.000 (0.066) loss 0.4441 (0.4581) acc 90.3846 (90.8503) lr 2.2949e-04 eta 0:01:59
epoch [41/50] batch [35/51] time 0.167 (0.237) data 0.000 (0.057) loss 0.5128 (0.4642) acc 90.6250 (90.6137) lr 2.2949e-04 eta 0:01:52
epoch [41/50] batch [40/51] time 0.169 (0.230) data 0.000 (0.050) loss 0.5617 (0.4664) acc 88.5000 (90.5830) lr 2.2949e-04 eta 0:01:47
epoch [41/50] batch [45/51] time 0.179 (0.223) data 0.000 (0.044) loss 0.5475 (0.4673) acc 90.2778 (90.4647) lr 2.2949e-04 eta 0:01:43
epoch [41/50] batch [50/51] time 0.172 (0.217) data 0.000 (0.040) loss 0.3515 (0.4710) acc 90.3846 (90.3235) lr 2.2949e-04 eta 0:01:40
>>> alpha1: 0.130  alpha2: 0.028 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [42/50] batch [5/51] time 0.180 (0.457) data 0.000 (0.273) loss 0.4014 (0.3980) acc 90.2778 (90.8980) lr 1.9098e-04 eta 0:03:27
epoch [42/50] batch [10/51] time 0.163 (0.317) data 0.000 (0.137) loss 0.3812 (0.4021) acc 95.2128 (91.4681) lr 1.9098e-04 eta 0:02:22
epoch [42/50] batch [15/51] time 0.159 (0.327) data 0.001 (0.091) loss 0.3369 (0.4072) acc 95.5556 (91.1962) lr 1.9098e-04 eta 0:02:24
epoch [42/50] batch [20/51] time 0.176 (0.291) data 0.001 (0.069) loss 0.3613 (0.4026) acc 90.1961 (91.2824) lr 1.9098e-04 eta 0:02:07
epoch [42/50] batch [25/51] time 0.193 (0.268) data 0.001 (0.055) loss 0.4647 (0.4188) acc 93.3673 (90.8025) lr 1.9098e-04 eta 0:01:56
epoch [42/50] batch [30/51] time 0.170 (0.253) data 0.000 (0.046) loss 0.5956 (0.4291) acc 88.3333 (90.8643) lr 1.9098e-04 eta 0:01:48
epoch [42/50] batch [35/51] time 0.190 (0.244) data 0.000 (0.039) loss 0.4367 (0.4241) acc 91.6667 (91.0077) lr 1.9098e-04 eta 0:01:43
epoch [42/50] batch [40/51] time 0.173 (0.235) data 0.000 (0.034) loss 0.5274 (0.4297) acc 84.1346 (90.7192) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [45/51] time 0.178 (0.228) data 0.000 (0.031) loss 0.4018 (0.4213) acc 92.1296 (91.0151) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [50/51] time 0.159 (0.222) data 0.000 (0.028) loss 0.4067 (0.4313) acc 89.6739 (90.7761) lr 1.9098e-04 eta 0:01:30
>>> alpha1: 0.131  alpha2: 0.032 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [43/50] batch [5/51] time 0.178 (0.517) data 0.000 (0.330) loss 0.4754 (0.3895) acc 92.0213 (92.9188) lr 1.5567e-04 eta 0:03:28
epoch [43/50] batch [10/51] time 0.178 (0.351) data 0.000 (0.165) loss 0.3473 (0.3849) acc 92.4528 (92.4542) lr 1.5567e-04 eta 0:02:19
epoch [43/50] batch [15/51] time 0.175 (0.291) data 0.000 (0.110) loss 0.4696 (0.4236) acc 88.0000 (91.2251) lr 1.5567e-04 eta 0:01:54
epoch [43/50] batch [20/51] time 0.171 (0.261) data 0.001 (0.083) loss 0.6517 (0.4515) acc 86.2245 (90.6921) lr 1.5567e-04 eta 0:01:41
epoch [43/50] batch [25/51] time 0.158 (0.245) data 0.000 (0.066) loss 0.5458 (0.4460) acc 89.4445 (90.8339) lr 1.5567e-04 eta 0:01:33
epoch [43/50] batch [30/51] time 0.165 (0.233) data 0.000 (0.055) loss 0.5607 (0.4528) acc 89.0625 (90.7571) lr 1.5567e-04 eta 0:01:28
epoch [43/50] batch [35/51] time 0.177 (0.226) data 0.000 (0.048) loss 0.4866 (0.4520) acc 89.3617 (90.8034) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [40/51] time 0.176 (0.220) data 0.000 (0.042) loss 0.3052 (0.4445) acc 95.2830 (91.0223) lr 1.5567e-04 eta 0:01:20
epoch [43/50] batch [45/51] time 0.172 (0.214) data 0.000 (0.037) loss 0.5467 (0.4374) acc 88.9423 (91.1428) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [50/51] time 0.178 (0.210) data 0.000 (0.033) loss 0.3330 (0.4350) acc 92.1296 (91.1483) lr 1.5567e-04 eta 0:01:15
>>> alpha1: 0.129  alpha2: 0.031 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.13 <<<
epoch [44/50] batch [5/51] time 0.169 (0.451) data 0.000 (0.271) loss 0.2587 (0.3674) acc 98.4043 (93.3090) lr 1.2369e-04 eta 0:02:38
epoch [44/50] batch [10/51] time 0.179 (0.319) data 0.000 (0.136) loss 0.3669 (0.3876) acc 91.3265 (92.4006) lr 1.2369e-04 eta 0:01:50
epoch [44/50] batch [15/51] time 0.177 (0.272) data 0.001 (0.091) loss 0.4109 (0.4075) acc 87.9808 (91.8432) lr 1.2369e-04 eta 0:01:33
epoch [44/50] batch [20/51] time 0.181 (0.247) data 0.000 (0.068) loss 0.3161 (0.4157) acc 93.6364 (91.5306) lr 1.2369e-04 eta 0:01:23
epoch [44/50] batch [25/51] time 0.194 (0.235) data 0.000 (0.054) loss 0.1895 (0.4076) acc 97.8448 (91.7201) lr 1.2369e-04 eta 0:01:17
epoch [44/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.045) loss 0.3870 (0.4233) acc 91.8478 (91.0162) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [35/51] time 0.182 (0.220) data 0.000 (0.039) loss 0.4709 (0.4344) acc 90.8163 (90.9951) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [40/51] time 0.158 (0.216) data 0.001 (0.034) loss 0.3420 (0.4300) acc 93.8889 (91.0319) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.166 (0.210) data 0.000 (0.030) loss 0.4115 (0.4354) acc 91.8367 (90.7960) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [50/51] time 0.173 (0.206) data 0.000 (0.027) loss 0.4560 (0.4387) acc 86.5385 (90.6127) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.128  alpha2: 0.029 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [45/50] batch [5/51] time 0.179 (0.494) data 0.001 (0.308) loss 0.5453 (0.4472) acc 91.0377 (91.3806) lr 9.5173e-05 eta 0:02:28
epoch [45/50] batch [10/51] time 0.166 (0.334) data 0.000 (0.154) loss 0.4374 (0.4413) acc 87.7551 (91.1064) lr 9.5173e-05 eta 0:01:38
epoch [45/50] batch [15/51] time 0.172 (0.282) data 0.001 (0.103) loss 0.5082 (0.4468) acc 92.1569 (90.9728) lr 9.5173e-05 eta 0:01:21
epoch [45/50] batch [20/51] time 0.192 (0.254) data 0.000 (0.077) loss 0.5144 (0.4327) acc 90.1961 (91.3134) lr 9.5173e-05 eta 0:01:12
epoch [45/50] batch [25/51] time 0.177 (0.240) data 0.000 (0.062) loss 0.5494 (0.4256) acc 85.3774 (91.2331) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [30/51] time 0.172 (0.230) data 0.000 (0.052) loss 0.5022 (0.4221) acc 88.7255 (91.1433) lr 9.5173e-05 eta 0:01:03
epoch [45/50] batch [35/51] time 0.170 (0.222) data 0.001 (0.044) loss 0.5742 (0.4322) acc 87.5000 (90.7795) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.039) loss 0.5239 (0.4418) acc 92.0000 (90.6239) lr 9.5173e-05 eta 0:00:57
epoch [45/50] batch [45/51] time 0.162 (0.211) data 0.000 (0.034) loss 0.4375 (0.4382) acc 89.8936 (90.5294) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.162 (0.207) data 0.000 (0.031) loss 0.5225 (0.4344) acc 88.8298 (90.5358) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.127  alpha2: 0.028 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [46/50] batch [5/51] time 0.181 (0.521) data 0.001 (0.336) loss 0.4631 (0.4190) acc 88.6792 (90.4541) lr 7.0224e-05 eta 0:02:10
epoch [46/50] batch [10/51] time 0.201 (0.354) data 0.000 (0.168) loss 0.5038 (0.4448) acc 89.3617 (90.5614) lr 7.0224e-05 eta 0:01:26
epoch [46/50] batch [15/51] time 0.217 (0.303) data 0.000 (0.112) loss 0.3111 (0.4184) acc 94.0217 (91.2238) lr 7.0224e-05 eta 0:01:12
epoch [46/50] batch [20/51] time 0.200 (0.274) data 0.000 (0.084) loss 0.6690 (0.4191) acc 85.7143 (91.0477) lr 7.0224e-05 eta 0:01:04
epoch [46/50] batch [25/51] time 0.182 (0.254) data 0.000 (0.067) loss 0.4052 (0.4307) acc 91.6667 (90.8172) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [30/51] time 0.171 (0.242) data 0.000 (0.056) loss 0.6256 (0.4369) acc 83.3333 (90.5128) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [35/51] time 0.178 (0.233) data 0.000 (0.048) loss 0.5571 (0.4334) acc 86.0000 (90.7808) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [40/51] time 0.170 (0.226) data 0.000 (0.042) loss 0.2917 (0.4345) acc 95.5882 (90.7498) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [45/51] time 0.176 (0.219) data 0.000 (0.038) loss 0.3361 (0.4336) acc 95.2830 (90.8020) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [50/51] time 0.161 (0.214) data 0.000 (0.034) loss 0.5126 (0.4354) acc 93.0851 (90.7641) lr 7.0224e-05 eta 0:00:43
>>> alpha1: 0.128  alpha2: 0.028 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.08 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [47/50] batch [5/51] time 0.173 (0.498) data 0.001 (0.318) loss 0.2347 (0.4004) acc 94.1176 (92.2765) lr 4.8943e-05 eta 0:01:39
epoch [47/50] batch [10/51] time 0.174 (0.339) data 0.000 (0.159) loss 0.3251 (0.3832) acc 94.7115 (92.8234) lr 4.8943e-05 eta 0:01:05
epoch [47/50] batch [15/51] time 0.171 (0.284) data 0.000 (0.106) loss 0.4042 (0.3641) acc 92.1569 (93.4082) lr 4.8943e-05 eta 0:00:53
epoch [47/50] batch [20/51] time 0.188 (0.258) data 0.000 (0.080) loss 0.4612 (0.3759) acc 91.6667 (93.0934) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [25/51] time 0.176 (0.242) data 0.000 (0.064) loss 0.2668 (0.3914) acc 95.9184 (92.7227) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [30/51] time 0.170 (0.229) data 0.000 (0.053) loss 0.4388 (0.3971) acc 89.0625 (92.2538) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.157 (0.221) data 0.000 (0.046) loss 0.5492 (0.4101) acc 89.2045 (92.1092) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.169 (0.214) data 0.000 (0.040) loss 0.3889 (0.4156) acc 92.1569 (91.6675) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.036) loss 0.3800 (0.4154) acc 94.6078 (91.6000) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.175 (0.205) data 0.000 (0.032) loss 0.3489 (0.4187) acc 94.3396 (91.5280) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.129  alpha2: 0.027 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.13 <<<
epoch [48/50] batch [5/51] time 0.176 (0.500) data 0.000 (0.316) loss 0.2880 (0.3572) acc 96.1538 (92.9021) lr 3.1417e-05 eta 0:01:13
epoch [48/50] batch [10/51] time 0.187 (0.340) data 0.000 (0.158) loss 0.4773 (0.4145) acc 87.9630 (91.2829) lr 3.1417e-05 eta 0:00:48
epoch [48/50] batch [15/51] time 0.166 (0.286) data 0.000 (0.106) loss 0.6279 (0.4234) acc 85.9375 (90.9537) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.177 (0.260) data 0.000 (0.079) loss 0.3893 (0.4257) acc 92.4528 (91.0047) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.188 (0.242) data 0.000 (0.063) loss 0.3514 (0.4303) acc 91.5094 (90.9227) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.180 (0.231) data 0.000 (0.053) loss 0.2000 (0.4235) acc 96.7593 (91.1019) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.170 (0.224) data 0.000 (0.045) loss 0.5182 (0.4315) acc 83.3333 (90.8924) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.173 (0.217) data 0.000 (0.040) loss 0.3303 (0.4338) acc 93.2692 (90.7818) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.035) loss 0.5527 (0.4428) acc 88.2353 (90.6501) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.184 (0.208) data 0.000 (0.032) loss 0.3298 (0.4385) acc 92.5439 (90.8314) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.129  alpha2: 0.026 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.14 <<<
epoch [49/50] batch [5/51] time 0.174 (0.484) data 0.001 (0.306) loss 0.3821 (0.4900) acc 95.0000 (89.7380) lr 1.7713e-05 eta 0:00:46
epoch [49/50] batch [10/51] time 0.176 (0.331) data 0.000 (0.153) loss 0.5727 (0.4576) acc 85.7843 (90.0982) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.207 (0.282) data 0.018 (0.103) loss 0.2975 (0.4454) acc 94.6078 (90.5083) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [20/51] time 0.192 (0.255) data 0.000 (0.078) loss 0.4081 (0.4261) acc 93.0556 (91.2497) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.160 (0.240) data 0.000 (0.062) loss 0.4997 (0.4388) acc 88.8889 (90.9914) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.168 (0.230) data 0.000 (0.052) loss 0.4902 (0.4268) acc 90.8163 (91.3492) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.173 (0.223) data 0.000 (0.044) loss 0.4335 (0.4144) acc 92.2222 (91.6488) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.039) loss 0.3026 (0.4132) acc 94.2708 (91.7143) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.169 (0.212) data 0.000 (0.035) loss 0.3654 (0.4132) acc 90.6863 (91.6332) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.176 (0.208) data 0.000 (0.031) loss 0.2841 (0.4082) acc 95.2830 (91.7589) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.130  alpha2: 0.025 <<<
>>> noisy rate: 0.38 --> refined noisy rate: 0.09 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.13 <<<
epoch [50/50] batch [5/51] time 0.171 (0.521) data 0.000 (0.337) loss 0.6134 (0.3937) acc 85.2941 (92.1983) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.175 (0.351) data 0.000 (0.168) loss 0.5703 (0.4196) acc 85.2941 (91.2386) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.174 (0.294) data 0.000 (0.112) loss 0.5567 (0.4210) acc 90.0000 (91.3867) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.171 (0.263) data 0.000 (0.084) loss 0.3252 (0.4422) acc 94.1176 (90.6528) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.169 (0.246) data 0.000 (0.068) loss 0.4357 (0.4327) acc 90.0000 (90.7876) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.176 (0.234) data 0.000 (0.056) loss 0.4732 (0.4259) acc 88.7255 (90.9342) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.188 (0.226) data 0.000 (0.048) loss 0.3843 (0.4252) acc 93.2692 (91.0755) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.185 (0.220) data 0.000 (0.042) loss 0.4980 (0.4374) acc 89.4737 (90.7370) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.169 (0.214) data 0.000 (0.038) loss 0.5115 (0.4395) acc 86.5000 (90.6077) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.163 (0.210) data 0.000 (0.034) loss 0.3508 (0.4295) acc 93.2292 (90.9175) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_6FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.17, 0.13, 0.12, 0.11, 0.1, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.08, 0.08, 0.09, 0.09, 0.09]
* matched noise rate: [0.05, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.04, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.06, 0.05, 0.06, 0.05, 0.05, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06]
* unmatched noise rate: [0.25, 0.21, 0.19, 0.18, 0.17, 0.18, 0.19, 0.19, 0.18, 0.18, 0.17, 0.18, 0.17, 0.17, 0.16, 0.17, 0.17, 0.16, 0.17, 0.15, 0.16, 0.15, 0.15, 0.14, 0.14, 0.14, 0.13, 0.14, 0.14, 0.14, 0.14, 0.13, 0.13, 0.13, 0.14, 0.13, 0.13, 0.13, 0.14, 0.13]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:11,  2.97s/it] 12%|█▏        | 3/25 [00:03<00:18,  1.20it/s] 20%|██        | 5/25 [00:03<00:08,  2.24it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.39it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.67it/s] 44%|████▍     | 11/25 [00:03<00:02,  5.97it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.22it/s] 60%|██████    | 15/25 [00:04<00:01,  8.35it/s] 68%|██████▊   | 17/25 [00:04<00:01,  7.39it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.35it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.28it/s] 92%|█████████▏| 23/25 [00:04<00:00, 10.04it/s]100%|██████████| 25/25 [00:05<00:00,  7.27it/s]100%|██████████| 25/25 [00:05<00:00,  4.38it/s]
=> result
* total: 2,463
* correct: 2,134
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 87.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 11	acc: 91.7%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 5	acc: 41.7%
* class: 3 (sweet pea)	total: 17	correct: 12	acc: 70.6%
* class: 4 (english marigold)	total: 20	correct: 14	acc: 70.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 25	acc: 96.2%
* class: 11 (colt's foot)	total: 26	correct: 22	acc: 84.6%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 12	acc: 92.3%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 15	acc: 60.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 25	acc: 92.6%
* class: 23 (red ginger)	total: 13	correct: 12	acc: 92.3%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 18	acc: 90.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 24	acc: 92.3%
* class: 30 (carnation)	total: 16	correct: 13	acc: 81.2%
* class: 31 (garden phlox)	total: 14	correct: 13	acc: 92.9%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 20	acc: 90.9%
* class: 36 (cape flower)	total: 32	correct: 32	acc: 100.0%
* class: 37 (great masterwort)	total: 17	correct: 16	acc: 94.1%
* class: 38 (siam tulip)	total: 13	correct: 4	acc: 30.8%
* class: 39 (lenten rose)	total: 20	correct: 17	acc: 85.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 6	acc: 50.0%
* class: 45 (wallflower)	total: 59	correct: 58	acc: 98.3%
* class: 46 (marigold)	total: 20	correct: 19	acc: 95.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 39	acc: 50.6%
* class: 51 (wild pansy)	total: 26	correct: 25	acc: 96.2%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 19	acc: 95.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 11	acc: 68.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 15	acc: 93.8%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 18	acc: 94.7%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 12	acc: 75.0%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 21	acc: 91.3%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 54	acc: 93.1%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 34	acc: 94.4%
* class: 75 (morning glory)	total: 32	correct: 28	acc: 87.5%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 37	acc: 88.1%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 32	acc: 100.0%
* class: 80 (frangipani)	total: 50	correct: 49	acc: 98.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 39	acc: 100.0%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 17	acc: 94.4%
* class: 85 (tree mallow)	total: 17	correct: 16	acc: 94.1%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 34	acc: 73.9%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 22	acc: 88.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 12	acc: 85.7%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 30	acc: 78.9%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 16	acc: 80.0%
* class: 97 (mexican petunia)	total: 25	correct: 17	acc: 68.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 11	acc: 64.7%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 88.7%
Elapsed: 0:28:25
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_8-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.261 (1.083) data 0.000 (0.269) loss 4.7830 (4.6319) acc 3.1250 (8.7500) lr 1.0000e-05 eta 0:45:55
epoch [1/50] batch [10/51] time 0.260 (0.672) data 0.000 (0.135) loss 4.3687 (4.5473) acc 6.2500 (9.0625) lr 1.0000e-05 eta 0:28:26
epoch [1/50] batch [15/51] time 0.261 (0.536) data 0.000 (0.090) loss 4.6701 (4.5612) acc 6.2500 (8.7500) lr 1.0000e-05 eta 0:22:38
epoch [1/50] batch [20/51] time 0.260 (0.468) data 0.000 (0.067) loss 4.3185 (4.4838) acc 18.7500 (9.3750) lr 1.0000e-05 eta 0:19:44
epoch [1/50] batch [25/51] time 0.268 (0.427) data 0.000 (0.054) loss 4.8051 (4.4878) acc 6.2500 (9.2500) lr 1.0000e-05 eta 0:17:58
epoch [1/50] batch [30/51] time 0.280 (0.401) data 0.000 (0.045) loss 4.8335 (4.4759) acc 9.3750 (9.7917) lr 1.0000e-05 eta 0:16:49
epoch [1/50] batch [35/51] time 0.265 (0.381) data 0.000 (0.039) loss 3.9309 (4.4332) acc 31.2500 (10.8929) lr 1.0000e-05 eta 0:15:57
epoch [1/50] batch [40/51] time 0.259 (0.366) data 0.000 (0.034) loss 4.3695 (4.4030) acc 12.5000 (12.1875) lr 1.0000e-05 eta 0:15:17
epoch [1/50] batch [45/51] time 0.259 (0.354) data 0.000 (0.030) loss 4.4892 (4.3952) acc 18.7500 (12.7778) lr 1.0000e-05 eta 0:14:46
epoch [1/50] batch [50/51] time 0.259 (0.344) data 0.000 (0.027) loss 4.1866 (4.3623) acc 12.5000 (13.6875) lr 1.0000e-05 eta 0:14:20
epoch [2/50] batch [5/51] time 0.281 (0.535) data 0.000 (0.260) loss 4.0468 (4.3474) acc 15.6250 (17.5000) lr 2.0000e-03 eta 0:22:15
epoch [2/50] batch [10/51] time 0.261 (0.398) data 0.000 (0.130) loss 4.1299 (4.2387) acc 28.1250 (20.6250) lr 2.0000e-03 eta 0:16:30
epoch [2/50] batch [15/51] time 0.261 (0.354) data 0.000 (0.087) loss 3.8093 (4.2346) acc 28.1250 (20.2083) lr 2.0000e-03 eta 0:14:39
epoch [2/50] batch [20/51] time 0.262 (0.332) data 0.000 (0.065) loss 4.2092 (4.2268) acc 12.5000 (18.7500) lr 2.0000e-03 eta 0:13:42
epoch [2/50] batch [25/51] time 0.264 (0.318) data 0.000 (0.052) loss 4.1326 (4.2067) acc 9.3750 (18.6250) lr 2.0000e-03 eta 0:13:06
epoch [2/50] batch [30/51] time 0.260 (0.309) data 0.000 (0.044) loss 3.9037 (4.1661) acc 31.2500 (19.7917) lr 2.0000e-03 eta 0:12:43
epoch [2/50] batch [35/51] time 0.271 (0.303) data 0.000 (0.037) loss 3.6818 (4.1667) acc 40.6250 (20.4464) lr 2.0000e-03 eta 0:12:26
epoch [2/50] batch [40/51] time 0.260 (0.298) data 0.000 (0.033) loss 3.9834 (4.1428) acc 18.7500 (20.8594) lr 2.0000e-03 eta 0:12:11
epoch [2/50] batch [45/51] time 0.258 (0.293) data 0.000 (0.029) loss 3.8446 (4.1240) acc 18.7500 (21.0417) lr 2.0000e-03 eta 0:11:59
epoch [2/50] batch [50/51] time 0.257 (0.290) data 0.000 (0.026) loss 4.2933 (4.0939) acc 25.0000 (22.0000) lr 2.0000e-03 eta 0:11:49
epoch [3/50] batch [5/51] time 0.305 (0.559) data 0.000 (0.275) loss 3.5324 (3.5835) acc 37.5000 (35.0000) lr 1.9980e-03 eta 0:22:46
epoch [3/50] batch [10/51] time 0.260 (0.412) data 0.000 (0.138) loss 4.3462 (3.7336) acc 25.0000 (31.2500) lr 1.9980e-03 eta 0:16:44
epoch [3/50] batch [15/51] time 0.259 (0.362) data 0.000 (0.092) loss 3.9251 (3.8019) acc 28.1250 (30.6250) lr 1.9980e-03 eta 0:14:41
epoch [3/50] batch [20/51] time 0.260 (0.337) data 0.000 (0.069) loss 3.6226 (3.8341) acc 37.5000 (29.8438) lr 1.9980e-03 eta 0:13:38
epoch [3/50] batch [25/51] time 0.259 (0.322) data 0.000 (0.055) loss 4.5891 (3.9161) acc 12.5000 (27.8750) lr 1.9980e-03 eta 0:13:01
epoch [3/50] batch [30/51] time 0.260 (0.312) data 0.000 (0.046) loss 4.3971 (3.9074) acc 15.6250 (27.5000) lr 1.9980e-03 eta 0:12:34
epoch [3/50] batch [35/51] time 0.260 (0.305) data 0.000 (0.039) loss 3.9610 (3.9013) acc 37.5000 (27.5893) lr 1.9980e-03 eta 0:12:14
epoch [3/50] batch [40/51] time 0.260 (0.299) data 0.000 (0.035) loss 3.8690 (3.9241) acc 34.3750 (27.5000) lr 1.9980e-03 eta 0:12:00
epoch [3/50] batch [45/51] time 0.260 (0.295) data 0.000 (0.031) loss 4.1849 (3.9414) acc 25.0000 (27.4306) lr 1.9980e-03 eta 0:11:48
epoch [3/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.028) loss 4.1163 (3.9375) acc 18.7500 (27.9375) lr 1.9980e-03 eta 0:11:38
epoch [4/50] batch [5/51] time 0.271 (0.520) data 0.000 (0.231) loss 3.6398 (3.9287) acc 34.3750 (29.3750) lr 1.9921e-03 eta 0:20:44
epoch [4/50] batch [10/51] time 0.270 (0.395) data 0.000 (0.115) loss 3.8925 (3.9239) acc 31.2500 (28.7500) lr 1.9921e-03 eta 0:15:42
epoch [4/50] batch [15/51] time 0.266 (0.351) data 0.000 (0.077) loss 3.6565 (3.9037) acc 28.1250 (29.3750) lr 1.9921e-03 eta 0:13:55
epoch [4/50] batch [20/51] time 0.260 (0.329) data 0.000 (0.058) loss 4.0095 (3.8834) acc 21.8750 (29.0625) lr 1.9921e-03 eta 0:13:01
epoch [4/50] batch [25/51] time 0.259 (0.315) data 0.000 (0.046) loss 4.2732 (3.9105) acc 25.0000 (29.0000) lr 1.9921e-03 eta 0:12:27
epoch [4/50] batch [30/51] time 0.263 (0.306) data 0.000 (0.039) loss 4.2028 (3.9169) acc 15.6250 (28.3333) lr 1.9921e-03 eta 0:12:05
epoch [4/50] batch [35/51] time 0.260 (0.300) data 0.000 (0.033) loss 3.6366 (3.8966) acc 37.5000 (29.2857) lr 1.9921e-03 eta 0:11:48
epoch [4/50] batch [40/51] time 0.257 (0.295) data 0.000 (0.029) loss 4.1622 (3.8997) acc 21.8750 (29.2188) lr 1.9921e-03 eta 0:11:34
epoch [4/50] batch [45/51] time 0.257 (0.291) data 0.000 (0.026) loss 4.3600 (3.8850) acc 21.8750 (29.9306) lr 1.9921e-03 eta 0:11:23
epoch [4/50] batch [50/51] time 0.258 (0.287) data 0.000 (0.023) loss 4.4286 (3.9099) acc 18.7500 (29.1250) lr 1.9921e-03 eta 0:11:14
epoch [5/50] batch [5/51] time 0.274 (0.568) data 0.000 (0.295) loss 3.8819 (3.8194) acc 31.2500 (31.8750) lr 1.9823e-03 eta 0:22:09
epoch [5/50] batch [10/51] time 0.265 (0.416) data 0.000 (0.148) loss 3.5321 (3.8754) acc 37.5000 (30.3125) lr 1.9823e-03 eta 0:16:11
epoch [5/50] batch [15/51] time 0.269 (0.367) data 0.000 (0.099) loss 3.2830 (3.7970) acc 34.3750 (31.0417) lr 1.9823e-03 eta 0:14:15
epoch [5/50] batch [20/51] time 0.259 (0.341) data 0.000 (0.074) loss 3.8353 (3.8407) acc 28.1250 (30.4688) lr 1.9823e-03 eta 0:13:14
epoch [5/50] batch [25/51] time 0.260 (0.326) data 0.000 (0.059) loss 3.8272 (3.8406) acc 21.8750 (30.6250) lr 1.9823e-03 eta 0:12:35
epoch [5/50] batch [30/51] time 0.265 (0.316) data 0.000 (0.049) loss 4.1513 (3.8447) acc 25.0000 (30.7292) lr 1.9823e-03 eta 0:12:11
epoch [5/50] batch [35/51] time 0.276 (0.309) data 0.000 (0.042) loss 3.7409 (3.8531) acc 28.1250 (30.1786) lr 1.9823e-03 eta 0:11:54
epoch [5/50] batch [40/51] time 0.258 (0.303) data 0.000 (0.037) loss 4.2403 (3.8807) acc 15.6250 (29.6094) lr 1.9823e-03 eta 0:11:38
epoch [5/50] batch [45/51] time 0.260 (0.298) data 0.000 (0.033) loss 4.4604 (3.8779) acc 25.0000 (29.9306) lr 1.9823e-03 eta 0:11:25
epoch [5/50] batch [50/51] time 0.259 (0.294) data 0.000 (0.030) loss 3.2655 (3.8725) acc 43.7500 (30.3125) lr 1.9823e-03 eta 0:11:15
epoch [6/50] batch [5/51] time 0.294 (0.599) data 0.000 (0.307) loss 4.7504 (3.8053) acc 18.7500 (35.0000) lr 1.9686e-03 eta 0:22:51
epoch [6/50] batch [10/51] time 0.274 (0.431) data 0.000 (0.154) loss 4.3254 (3.8690) acc 18.7500 (30.9375) lr 1.9686e-03 eta 0:16:25
epoch [6/50] batch [15/51] time 0.259 (0.377) data 0.000 (0.103) loss 3.7893 (3.8173) acc 31.2500 (30.4167) lr 1.9686e-03 eta 0:14:19
epoch [6/50] batch [20/51] time 0.273 (0.349) data 0.000 (0.077) loss 4.1380 (3.8043) acc 28.1250 (31.8750) lr 1.9686e-03 eta 0:13:14
epoch [6/50] batch [25/51] time 0.259 (0.333) data 0.000 (0.062) loss 3.9800 (3.7826) acc 34.3750 (32.8750) lr 1.9686e-03 eta 0:12:36
epoch [6/50] batch [30/51] time 0.291 (0.323) data 0.000 (0.051) loss 3.6715 (3.8281) acc 34.3750 (31.5625) lr 1.9686e-03 eta 0:12:10
epoch [6/50] batch [35/51] time 0.272 (0.314) data 0.000 (0.044) loss 3.8274 (3.8567) acc 34.3750 (30.7143) lr 1.9686e-03 eta 0:11:50
epoch [6/50] batch [40/51] time 0.260 (0.308) data 0.000 (0.039) loss 3.5895 (3.8493) acc 43.7500 (31.1719) lr 1.9686e-03 eta 0:11:33
epoch [6/50] batch [45/51] time 0.259 (0.302) data 0.000 (0.034) loss 3.4995 (3.8334) acc 37.5000 (31.6667) lr 1.9686e-03 eta 0:11:20
epoch [6/50] batch [50/51] time 0.288 (0.300) data 0.000 (0.031) loss 3.7560 (3.8443) acc 34.3750 (31.8750) lr 1.9686e-03 eta 0:11:14
epoch [7/50] batch [5/51] time 0.317 (0.650) data 0.000 (0.334) loss 4.0738 (3.7763) acc 25.0000 (33.7500) lr 1.9511e-03 eta 0:24:14
epoch [7/50] batch [10/51] time 0.308 (0.477) data 0.000 (0.167) loss 3.6483 (3.8486) acc 28.1250 (30.0000) lr 1.9511e-03 eta 0:17:44
epoch [7/50] batch [15/51] time 0.292 (0.418) data 0.000 (0.112) loss 3.8895 (3.7852) acc 31.2500 (32.7083) lr 1.9511e-03 eta 0:15:31
epoch [7/50] batch [20/51] time 0.293 (0.387) data 0.000 (0.084) loss 4.0680 (3.8349) acc 25.0000 (32.1875) lr 1.9511e-03 eta 0:14:21
epoch [7/50] batch [25/51] time 0.299 (0.370) data 0.000 (0.067) loss 3.4331 (3.7938) acc 46.8750 (33.2500) lr 1.9511e-03 eta 0:13:42
epoch [7/50] batch [30/51] time 0.364 (0.363) data 0.000 (0.056) loss 4.0209 (3.8102) acc 31.2500 (33.2292) lr 1.9511e-03 eta 0:13:23
epoch [7/50] batch [35/51] time 0.365 (0.366) data 0.000 (0.048) loss 3.5863 (3.8229) acc 40.6250 (32.8571) lr 1.9511e-03 eta 0:13:28
epoch [7/50] batch [40/51] time 0.259 (0.363) data 0.000 (0.042) loss 4.2524 (3.7985) acc 28.1250 (33.0469) lr 1.9511e-03 eta 0:13:20
epoch [7/50] batch [45/51] time 0.363 (0.358) data 0.000 (0.037) loss 3.4998 (3.7995) acc 34.3750 (32.3611) lr 1.9511e-03 eta 0:13:06
epoch [7/50] batch [50/51] time 0.392 (0.360) data 0.000 (0.034) loss 4.5930 (3.8060) acc 21.8750 (32.5625) lr 1.9511e-03 eta 0:13:10
epoch [8/50] batch [5/51] time 0.387 (0.628) data 0.000 (0.241) loss 3.4823 (3.7240) acc 34.3750 (34.3750) lr 1.9298e-03 eta 0:22:54
epoch [8/50] batch [10/51] time 0.299 (0.492) data 0.000 (0.120) loss 3.7018 (3.6468) acc 28.1250 (36.8750) lr 1.9298e-03 eta 0:17:55
epoch [8/50] batch [15/51] time 0.365 (0.430) data 0.000 (0.080) loss 3.8227 (3.7346) acc 37.5000 (34.7917) lr 1.9298e-03 eta 0:15:35
epoch [8/50] batch [20/51] time 0.370 (0.417) data 0.000 (0.060) loss 4.2762 (3.7649) acc 18.7500 (33.1250) lr 1.9298e-03 eta 0:15:05
epoch [8/50] batch [25/51] time 0.279 (0.399) data 0.000 (0.048) loss 3.4108 (3.7485) acc 40.6250 (33.5000) lr 1.9298e-03 eta 0:14:25
epoch [8/50] batch [30/51] time 0.366 (0.389) data 0.000 (0.040) loss 3.8874 (3.7284) acc 31.2500 (34.4792) lr 1.9298e-03 eta 0:14:00
epoch [8/50] batch [35/51] time 0.387 (0.387) data 0.000 (0.035) loss 3.6624 (3.7428) acc 40.6250 (34.2857) lr 1.9298e-03 eta 0:13:54
epoch [8/50] batch [40/51] time 0.259 (0.376) data 0.000 (0.030) loss 3.8903 (3.7492) acc 31.2500 (33.7500) lr 1.9298e-03 eta 0:13:28
epoch [8/50] batch [45/51] time 0.364 (0.372) data 0.000 (0.027) loss 3.7595 (3.7422) acc 34.3750 (33.4028) lr 1.9298e-03 eta 0:13:18
epoch [8/50] batch [50/51] time 0.365 (0.372) data 0.000 (0.024) loss 3.5815 (3.7507) acc 37.5000 (33.1250) lr 1.9298e-03 eta 0:13:17
epoch [9/50] batch [5/51] time 0.386 (0.795) data 0.000 (0.406) loss 3.5753 (3.5482) acc 34.3750 (36.8750) lr 1.9048e-03 eta 0:28:18
epoch [9/50] batch [10/51] time 0.348 (0.538) data 0.000 (0.203) loss 3.9856 (3.6329) acc 28.1250 (36.8750) lr 1.9048e-03 eta 0:19:07
epoch [9/50] batch [15/51] time 0.392 (0.485) data 0.000 (0.136) loss 3.6662 (3.7065) acc 37.5000 (35.2083) lr 1.9048e-03 eta 0:17:12
epoch [9/50] batch [20/51] time 0.260 (0.450) data 0.000 (0.102) loss 3.8100 (3.7123) acc 37.5000 (34.0625) lr 1.9048e-03 eta 0:15:55
epoch [9/50] batch [25/51] time 0.375 (0.424) data 0.001 (0.081) loss 3.8826 (3.7320) acc 25.0000 (33.1250) lr 1.9048e-03 eta 0:14:57
epoch [9/50] batch [30/51] time 0.376 (0.416) data 0.000 (0.068) loss 3.2130 (3.6963) acc 37.5000 (33.8542) lr 1.9048e-03 eta 0:14:39
epoch [9/50] batch [35/51] time 0.273 (0.402) data 0.000 (0.058) loss 3.6695 (3.7138) acc 40.6250 (33.8393) lr 1.9048e-03 eta 0:14:06
epoch [9/50] batch [40/51] time 0.362 (0.394) data 0.000 (0.051) loss 4.1282 (3.7150) acc 31.2500 (34.6875) lr 1.9048e-03 eta 0:13:49
epoch [9/50] batch [45/51] time 0.380 (0.392) data 0.000 (0.045) loss 3.6453 (3.7255) acc 31.2500 (34.3750) lr 1.9048e-03 eta 0:13:42
epoch [9/50] batch [50/51] time 0.259 (0.382) data 0.000 (0.041) loss 4.1878 (3.7347) acc 25.0000 (34.0625) lr 1.9048e-03 eta 0:13:18
epoch [10/50] batch [5/51] time 0.361 (0.579) data 0.000 (0.263) loss 4.2471 (3.8271) acc 21.8750 (34.3750) lr 1.8763e-03 eta 0:20:08
epoch [10/50] batch [10/51] time 0.366 (0.477) data 0.000 (0.132) loss 4.3898 (3.8089) acc 18.7500 (33.1250) lr 1.8763e-03 eta 0:16:32
epoch [10/50] batch [15/51] time 0.260 (0.430) data 0.000 (0.088) loss 3.3282 (3.7232) acc 46.8750 (33.5417) lr 1.8763e-03 eta 0:14:52
epoch [10/50] batch [20/51] time 0.366 (0.403) data 0.000 (0.066) loss 3.2715 (3.7012) acc 37.5000 (33.7500) lr 1.8763e-03 eta 0:13:55
epoch [10/50] batch [25/51] time 0.367 (0.398) data 0.000 (0.053) loss 3.4460 (3.7368) acc 43.7500 (34.1250) lr 1.8763e-03 eta 0:13:41
epoch [10/50] batch [30/51] time 0.273 (0.383) data 0.000 (0.044) loss 3.4521 (3.7462) acc 43.7500 (33.7500) lr 1.8763e-03 eta 0:13:09
epoch [10/50] batch [35/51] time 0.401 (0.381) data 0.000 (0.038) loss 3.2197 (3.7342) acc 46.8750 (33.7500) lr 1.8763e-03 eta 0:13:03
epoch [10/50] batch [40/51] time 0.383 (0.380) data 0.000 (0.033) loss 2.9869 (3.6985) acc 37.5000 (34.0625) lr 1.8763e-03 eta 0:13:00
epoch [10/50] batch [45/51] time 0.364 (0.370) data 0.000 (0.029) loss 4.1445 (3.7123) acc 34.3750 (34.1667) lr 1.8763e-03 eta 0:12:36
epoch [10/50] batch [50/51] time 0.383 (0.370) data 0.000 (0.026) loss 3.7051 (3.7156) acc 31.2500 (34.1250) lr 1.8763e-03 eta 0:12:34
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.726  alpha2: 0.366 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.52 <<<
epoch [11/50] batch [5/51] time 0.188 (1.017) data 0.000 (0.314) loss 2.3334 (2.7254) acc 70.3704 (59.0538) lr 1.8443e-03 eta 0:34:30
epoch [11/50] batch [10/51] time 0.839 (0.880) data 0.000 (0.157) loss 2.1235 (2.5039) acc 57.2917 (59.2855) lr 1.8443e-03 eta 0:29:46
epoch [11/50] batch [15/51] time 0.164 (0.683) data 0.000 (0.105) loss 2.7102 (2.4939) acc 50.5319 (59.7300) lr 1.8443e-03 eta 0:23:02
epoch [11/50] batch [20/51] time 0.167 (0.594) data 0.000 (0.079) loss 2.0458 (2.4086) acc 60.4167 (59.9234) lr 1.8443e-03 eta 0:19:59
epoch [11/50] batch [25/51] time 0.182 (0.540) data 0.000 (0.063) loss 1.8206 (2.3890) acc 74.5370 (60.4605) lr 1.8443e-03 eta 0:18:08
epoch [11/50] batch [30/51] time 0.172 (0.480) data 0.000 (0.053) loss 2.4694 (2.3652) acc 44.6078 (60.3423) lr 1.8443e-03 eta 0:16:04
epoch [11/50] batch [35/51] time 0.174 (0.461) data 0.000 (0.045) loss 2.1169 (2.3564) acc 65.1961 (60.5206) lr 1.8443e-03 eta 0:15:24
epoch [11/50] batch [40/51] time 0.178 (0.426) data 0.000 (0.040) loss 2.2735 (2.3485) acc 66.9811 (60.6111) lr 1.8443e-03 eta 0:14:11
epoch [11/50] batch [45/51] time 0.166 (0.397) data 0.000 (0.035) loss 2.0650 (2.3303) acc 57.1429 (60.5520) lr 1.8443e-03 eta 0:13:12
epoch [11/50] batch [50/51] time 0.167 (0.374) data 0.000 (0.032) loss 2.4255 (2.3099) acc 64.2857 (61.3342) lr 1.8443e-03 eta 0:12:24
>>> alpha1: 0.601  alpha2: 0.282 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.33 <<<
epoch [12/50] batch [5/51] time 0.180 (0.757) data 0.000 (0.336) loss 1.4330 (1.5514) acc 71.0784 (68.8137) lr 1.8090e-03 eta 0:25:01
epoch [12/50] batch [10/51] time 0.165 (0.461) data 0.000 (0.168) loss 1.3904 (1.4738) acc 74.4681 (70.3450) lr 1.8090e-03 eta 0:15:12
epoch [12/50] batch [15/51] time 0.162 (0.362) data 0.000 (0.112) loss 1.4814 (1.4428) acc 62.2340 (70.0877) lr 1.8090e-03 eta 0:11:54
epoch [12/50] batch [20/51] time 0.155 (0.341) data 0.000 (0.084) loss 1.3502 (1.4817) acc 72.7273 (69.3148) lr 1.8090e-03 eta 0:11:10
epoch [12/50] batch [25/51] time 0.160 (0.304) data 0.000 (0.067) loss 1.2784 (1.4545) acc 70.1087 (69.3282) lr 1.8090e-03 eta 0:09:57
epoch [12/50] batch [30/51] time 0.170 (0.282) data 0.000 (0.056) loss 1.5274 (1.4446) acc 66.3265 (69.6437) lr 1.8090e-03 eta 0:09:12
epoch [12/50] batch [35/51] time 0.160 (0.265) data 0.000 (0.048) loss 1.5449 (1.4469) acc 71.6667 (69.5428) lr 1.8090e-03 eta 0:08:38
epoch [12/50] batch [40/51] time 0.159 (0.253) data 0.000 (0.042) loss 1.5201 (1.4325) acc 67.9348 (69.6673) lr 1.8090e-03 eta 0:08:13
epoch [12/50] batch [45/51] time 0.157 (0.242) data 0.000 (0.038) loss 1.4716 (1.4209) acc 72.2222 (70.0015) lr 1.8090e-03 eta 0:07:50
epoch [12/50] batch [50/51] time 0.166 (0.245) data 0.000 (0.034) loss 1.1314 (1.4171) acc 75.5102 (70.0311) lr 1.8090e-03 eta 0:07:56
>>> alpha1: 0.508  alpha2: 0.219 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.30 <<<
epoch [13/50] batch [5/51] time 0.160 (0.544) data 0.000 (0.377) loss 1.1678 (1.1241) acc 75.5556 (76.6283) lr 1.7705e-03 eta 0:17:32
epoch [13/50] batch [10/51] time 0.171 (0.358) data 0.000 (0.190) loss 1.3346 (1.1731) acc 69.2708 (73.5790) lr 1.7705e-03 eta 0:11:29
epoch [13/50] batch [15/51] time 0.153 (0.292) data 0.000 (0.127) loss 1.0290 (1.1252) acc 79.7619 (74.5694) lr 1.7705e-03 eta 0:09:21
epoch [13/50] batch [20/51] time 0.163 (0.260) data 0.000 (0.095) loss 1.1248 (1.1183) acc 70.7447 (74.6399) lr 1.7705e-03 eta 0:08:18
epoch [13/50] batch [25/51] time 0.168 (0.241) data 0.000 (0.076) loss 1.6415 (1.1346) acc 61.5854 (74.0850) lr 1.7705e-03 eta 0:07:40
epoch [13/50] batch [30/51] time 0.159 (0.228) data 0.000 (0.064) loss 1.1558 (1.1257) acc 68.3333 (74.1812) lr 1.7705e-03 eta 0:07:14
epoch [13/50] batch [35/51] time 0.162 (0.218) data 0.000 (0.055) loss 1.1493 (1.1103) acc 74.4318 (74.7791) lr 1.7705e-03 eta 0:06:55
epoch [13/50] batch [40/51] time 0.152 (0.211) data 0.000 (0.048) loss 1.1933 (1.1092) acc 72.0238 (75.1660) lr 1.7705e-03 eta 0:06:40
epoch [13/50] batch [45/51] time 0.151 (0.205) data 0.001 (0.042) loss 1.2906 (1.1113) acc 75.0000 (74.8699) lr 1.7705e-03 eta 0:06:28
epoch [13/50] batch [50/51] time 0.164 (0.201) data 0.000 (0.038) loss 0.7357 (1.1058) acc 80.2083 (74.8535) lr 1.7705e-03 eta 0:06:19
>>> alpha1: 0.429  alpha2: 0.157 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.05 & unmatched refined noisy rate: 0.28 <<<
epoch [14/50] batch [5/51] time 0.159 (0.472) data 0.000 (0.301) loss 1.1854 (0.9408) acc 77.2222 (80.8445) lr 1.7290e-03 eta 0:14:48
epoch [14/50] batch [10/51] time 0.179 (0.319) data 0.000 (0.151) loss 0.8200 (0.9801) acc 85.7143 (79.9888) lr 1.7290e-03 eta 0:09:59
epoch [14/50] batch [15/51] time 0.165 (0.268) data 0.000 (0.101) loss 1.3172 (1.0309) acc 69.8864 (77.9841) lr 1.7290e-03 eta 0:08:22
epoch [14/50] batch [20/51] time 0.156 (0.241) data 0.000 (0.076) loss 1.2737 (1.0510) acc 69.3182 (76.8971) lr 1.7290e-03 eta 0:07:30
epoch [14/50] batch [25/51] time 0.182 (0.227) data 0.000 (0.060) loss 1.3936 (1.0580) acc 68.7500 (76.5461) lr 1.7290e-03 eta 0:07:02
epoch [14/50] batch [30/51] time 0.194 (0.219) data 0.000 (0.050) loss 1.1043 (1.0331) acc 73.4375 (76.8647) lr 1.7290e-03 eta 0:06:45
epoch [14/50] batch [35/51] time 0.178 (0.212) data 0.000 (0.043) loss 0.8684 (1.0169) acc 79.7872 (77.4106) lr 1.7290e-03 eta 0:06:31
epoch [14/50] batch [40/51] time 0.164 (0.207) data 0.000 (0.038) loss 0.8682 (0.9975) acc 77.6042 (77.4887) lr 1.7290e-03 eta 0:06:21
epoch [14/50] batch [45/51] time 0.157 (0.201) data 0.000 (0.034) loss 0.9594 (0.9872) acc 77.2222 (77.6940) lr 1.7290e-03 eta 0:06:10
epoch [14/50] batch [50/51] time 0.160 (0.197) data 0.000 (0.030) loss 0.9705 (0.9924) acc 76.0870 (77.5742) lr 1.7290e-03 eta 0:06:02
>>> alpha1: 0.386  alpha2: 0.125 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.31 <<<
epoch [15/50] batch [5/51] time 0.186 (0.504) data 0.016 (0.323) loss 1.0718 (0.8654) acc 72.8261 (81.3015) lr 1.6845e-03 eta 0:15:22
epoch [15/50] batch [10/51] time 0.181 (0.342) data 0.000 (0.163) loss 0.7778 (0.9107) acc 89.3519 (80.8755) lr 1.6845e-03 eta 0:10:25
epoch [15/50] batch [15/51] time 0.161 (0.286) data 0.000 (0.109) loss 0.8147 (0.9194) acc 82.2222 (79.4343) lr 1.6845e-03 eta 0:08:40
epoch [15/50] batch [20/51] time 0.169 (0.259) data 0.000 (0.082) loss 0.7580 (0.9076) acc 83.5000 (80.3414) lr 1.6845e-03 eta 0:07:51
epoch [15/50] batch [25/51] time 0.184 (0.244) data 0.001 (0.066) loss 0.7886 (0.8991) acc 81.1225 (80.6226) lr 1.6845e-03 eta 0:07:22
epoch [15/50] batch [30/51] time 0.166 (0.232) data 0.000 (0.055) loss 0.8714 (0.9005) acc 84.0425 (80.6470) lr 1.6845e-03 eta 0:06:59
epoch [15/50] batch [35/51] time 0.186 (0.225) data 0.000 (0.047) loss 0.9180 (0.9051) acc 80.2885 (80.4309) lr 1.6845e-03 eta 0:06:45
epoch [15/50] batch [40/51] time 0.154 (0.218) data 0.000 (0.041) loss 1.0539 (0.9127) acc 75.0000 (79.8791) lr 1.6845e-03 eta 0:06:30
epoch [15/50] batch [45/51] time 0.165 (0.212) data 0.000 (0.037) loss 1.0895 (0.9255) acc 73.4694 (79.7008) lr 1.6845e-03 eta 0:06:19
epoch [15/50] batch [50/51] time 0.166 (0.207) data 0.000 (0.033) loss 1.0149 (0.9207) acc 74.4898 (79.5436) lr 1.6845e-03 eta 0:06:10
>>> alpha1: 0.285  alpha2: 0.031 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [16/50] batch [5/51] time 0.169 (0.528) data 0.001 (0.347) loss 0.7312 (0.8610) acc 82.8125 (81.8480) lr 1.6374e-03 eta 0:15:40
epoch [16/50] batch [10/51] time 0.176 (0.353) data 0.000 (0.174) loss 0.8927 (0.8947) acc 75.9615 (78.9362) lr 1.6374e-03 eta 0:10:26
epoch [16/50] batch [15/51] time 0.169 (0.292) data 0.000 (0.116) loss 0.6622 (0.8805) acc 86.5000 (79.1296) lr 1.6374e-03 eta 0:08:37
epoch [16/50] batch [20/51] time 0.163 (0.263) data 0.000 (0.087) loss 0.8802 (0.8469) acc 80.8511 (79.8656) lr 1.6374e-03 eta 0:07:44
epoch [16/50] batch [25/51] time 0.161 (0.245) data 0.000 (0.070) loss 1.0216 (0.8516) acc 78.8889 (79.9343) lr 1.6374e-03 eta 0:07:11
epoch [16/50] batch [30/51] time 0.175 (0.233) data 0.001 (0.058) loss 0.8037 (0.8277) acc 80.8824 (80.8014) lr 1.6374e-03 eta 0:06:49
epoch [16/50] batch [35/51] time 0.175 (0.225) data 0.000 (0.050) loss 0.6402 (0.8251) acc 84.6154 (80.6962) lr 1.6374e-03 eta 0:06:33
epoch [16/50] batch [40/51] time 0.159 (0.217) data 0.000 (0.044) loss 1.0978 (0.8335) acc 79.3478 (81.0125) lr 1.6374e-03 eta 0:06:19
epoch [16/50] batch [45/51] time 0.167 (0.212) data 0.000 (0.039) loss 0.9088 (0.8474) acc 84.5000 (80.8033) lr 1.6374e-03 eta 0:06:08
epoch [16/50] batch [50/51] time 0.168 (0.208) data 0.000 (0.035) loss 0.8315 (0.8439) acc 81.5000 (80.7747) lr 1.6374e-03 eta 0:06:00
>>> alpha1: 0.227  alpha2: -0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.29 <<<
epoch [17/50] batch [5/51] time 0.154 (0.494) data 0.000 (0.322) loss 1.0930 (0.8754) acc 75.0000 (80.9545) lr 1.5878e-03 eta 0:14:14
epoch [17/50] batch [10/51] time 0.166 (0.334) data 0.001 (0.161) loss 0.7720 (0.8372) acc 82.6087 (81.4641) lr 1.5878e-03 eta 0:09:36
epoch [17/50] batch [15/51] time 0.183 (0.280) data 0.000 (0.108) loss 0.9269 (0.8400) acc 72.5490 (80.3468) lr 1.5878e-03 eta 0:08:01
epoch [17/50] batch [20/51] time 0.170 (0.253) data 0.000 (0.081) loss 0.5863 (0.8150) acc 90.1961 (81.0881) lr 1.5878e-03 eta 0:07:13
epoch [17/50] batch [25/51] time 0.166 (0.237) data 0.000 (0.065) loss 0.5316 (0.8120) acc 89.2857 (81.3773) lr 1.5878e-03 eta 0:06:45
epoch [17/50] batch [30/51] time 0.165 (0.227) data 0.000 (0.054) loss 1.0433 (0.8055) acc 71.8750 (81.7173) lr 1.5878e-03 eta 0:06:26
epoch [17/50] batch [35/51] time 0.187 (0.220) data 0.000 (0.046) loss 0.6511 (0.7955) acc 84.6154 (82.0496) lr 1.5878e-03 eta 0:06:13
epoch [17/50] batch [40/51] time 0.171 (0.214) data 0.000 (0.041) loss 0.8188 (0.8020) acc 82.6923 (81.8134) lr 1.5878e-03 eta 0:06:01
epoch [17/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.036) loss 0.7460 (0.7972) acc 80.5000 (81.7621) lr 1.5878e-03 eta 0:05:51
epoch [17/50] batch [50/51] time 0.166 (0.204) data 0.000 (0.032) loss 0.8218 (0.8104) acc 78.0612 (81.2572) lr 1.5878e-03 eta 0:05:43
>>> alpha1: 0.205  alpha2: -0.022 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [18/50] batch [5/51] time 0.185 (0.554) data 0.000 (0.375) loss 0.6455 (0.7029) acc 88.8393 (85.4396) lr 1.5358e-03 eta 0:15:30
epoch [18/50] batch [10/51] time 0.171 (0.363) data 0.000 (0.188) loss 0.6459 (0.7203) acc 85.0000 (84.5186) lr 1.5358e-03 eta 0:10:07
epoch [18/50] batch [15/51] time 0.169 (0.297) data 0.000 (0.125) loss 0.8747 (0.7294) acc 82.3864 (84.1946) lr 1.5358e-03 eta 0:08:14
epoch [18/50] batch [20/51] time 0.182 (0.266) data 0.000 (0.094) loss 0.7455 (0.7100) acc 83.9623 (84.5220) lr 1.5358e-03 eta 0:07:23
epoch [18/50] batch [25/51] time 0.172 (0.247) data 0.000 (0.075) loss 0.6223 (0.7178) acc 88.7255 (84.4843) lr 1.5358e-03 eta 0:06:49
epoch [18/50] batch [30/51] time 0.188 (0.234) data 0.001 (0.063) loss 0.6536 (0.7370) acc 89.7321 (83.9584) lr 1.5358e-03 eta 0:06:26
epoch [18/50] batch [35/51] time 0.171 (0.226) data 0.000 (0.054) loss 0.6571 (0.7400) acc 86.0000 (83.9393) lr 1.5358e-03 eta 0:06:12
epoch [18/50] batch [40/51] time 0.173 (0.219) data 0.000 (0.047) loss 0.7539 (0.7360) acc 80.7692 (83.9887) lr 1.5358e-03 eta 0:06:00
epoch [18/50] batch [45/51] time 0.159 (0.213) data 0.000 (0.042) loss 0.7612 (0.7473) acc 82.6087 (83.6487) lr 1.5358e-03 eta 0:05:49
epoch [18/50] batch [50/51] time 0.164 (0.209) data 0.000 (0.038) loss 0.5957 (0.7441) acc 88.0208 (83.5650) lr 1.5358e-03 eta 0:05:40
>>> alpha1: 0.189  alpha2: -0.025 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [19/50] batch [5/51] time 0.173 (0.517) data 0.000 (0.332) loss 0.8462 (0.7453) acc 80.0000 (82.1926) lr 1.4818e-03 eta 0:14:01
epoch [19/50] batch [10/51] time 0.181 (0.348) data 0.000 (0.166) loss 0.5694 (0.7165) acc 88.8889 (82.6958) lr 1.4818e-03 eta 0:09:24
epoch [19/50] batch [15/51] time 0.160 (0.291) data 0.000 (0.111) loss 0.9054 (0.7239) acc 79.3478 (83.0922) lr 1.4818e-03 eta 0:07:50
epoch [19/50] batch [20/51] time 0.175 (0.262) data 0.001 (0.083) loss 1.0369 (0.7377) acc 76.5625 (83.1048) lr 1.4818e-03 eta 0:07:01
epoch [19/50] batch [25/51] time 0.178 (0.246) data 0.000 (0.067) loss 0.7848 (0.7370) acc 83.6538 (82.9173) lr 1.4818e-03 eta 0:06:35
epoch [19/50] batch [30/51] time 0.187 (0.235) data 0.014 (0.056) loss 0.7395 (0.7322) acc 83.6735 (83.1964) lr 1.4818e-03 eta 0:06:16
epoch [19/50] batch [35/51] time 0.181 (0.226) data 0.000 (0.048) loss 0.5965 (0.7303) acc 87.7551 (83.2780) lr 1.4818e-03 eta 0:06:00
epoch [19/50] batch [40/51] time 0.155 (0.219) data 0.000 (0.042) loss 0.6130 (0.7308) acc 88.0682 (83.3008) lr 1.4818e-03 eta 0:05:48
epoch [19/50] batch [45/51] time 0.165 (0.213) data 0.001 (0.037) loss 0.5484 (0.7278) acc 86.4583 (83.4378) lr 1.4818e-03 eta 0:05:38
epoch [19/50] batch [50/51] time 0.166 (0.208) data 0.000 (0.034) loss 0.6007 (0.7242) acc 87.7551 (83.5555) lr 1.4818e-03 eta 0:05:29
>>> alpha1: 0.176  alpha2: -0.027 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [20/50] batch [5/51] time 0.168 (0.509) data 0.000 (0.331) loss 0.6677 (0.6387) acc 78.6458 (84.5030) lr 1.4258e-03 eta 0:13:22
epoch [20/50] batch [10/51] time 0.180 (0.342) data 0.000 (0.166) loss 0.7421 (0.6429) acc 83.0189 (85.4748) lr 1.4258e-03 eta 0:08:57
epoch [20/50] batch [15/51] time 0.174 (0.285) data 0.000 (0.110) loss 0.5219 (0.6554) acc 92.3077 (85.2700) lr 1.4258e-03 eta 0:07:26
epoch [20/50] batch [20/51] time 0.181 (0.258) data 0.000 (0.083) loss 0.7326 (0.6590) acc 84.3137 (85.2401) lr 1.4258e-03 eta 0:06:42
epoch [20/50] batch [25/51] time 0.163 (0.241) data 0.000 (0.066) loss 0.7594 (0.6623) acc 79.2553 (84.8971) lr 1.4258e-03 eta 0:06:15
epoch [20/50] batch [30/51] time 0.170 (0.231) data 0.000 (0.055) loss 0.7242 (0.6794) acc 87.7551 (84.5678) lr 1.4258e-03 eta 0:05:58
epoch [20/50] batch [35/51] time 0.164 (0.221) data 0.000 (0.048) loss 0.7468 (0.6900) acc 82.4468 (84.1736) lr 1.4258e-03 eta 0:05:42
epoch [20/50] batch [40/51] time 0.168 (0.216) data 0.000 (0.042) loss 0.7544 (0.6906) acc 83.0000 (84.1643) lr 1.4258e-03 eta 0:05:32
epoch [20/50] batch [45/51] time 0.176 (0.210) data 0.000 (0.037) loss 0.4980 (0.6881) acc 91.9811 (84.4348) lr 1.4258e-03 eta 0:05:23
epoch [20/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.033) loss 0.7818 (0.6897) acc 84.8958 (84.4598) lr 1.4258e-03 eta 0:05:15
>>> alpha1: 0.172  alpha2: -0.020 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [21/50] batch [5/51] time 0.188 (0.500) data 0.000 (0.319) loss 0.5824 (0.6140) acc 90.1042 (85.8022) lr 1.3681e-03 eta 0:12:41
epoch [21/50] batch [10/51] time 0.167 (0.336) data 0.000 (0.160) loss 0.5463 (0.6364) acc 86.0000 (85.0027) lr 1.3681e-03 eta 0:08:29
epoch [21/50] batch [15/51] time 0.177 (0.333) data 0.000 (0.107) loss 0.6651 (0.6227) acc 84.5745 (85.5470) lr 1.3681e-03 eta 0:08:25
epoch [21/50] batch [20/51] time 0.180 (0.294) data 0.000 (0.080) loss 0.5642 (0.6227) acc 87.5000 (85.9102) lr 1.3681e-03 eta 0:07:24
epoch [21/50] batch [25/51] time 0.159 (0.270) data 0.000 (0.064) loss 0.8756 (0.6170) acc 81.3953 (86.3620) lr 1.3681e-03 eta 0:06:45
epoch [21/50] batch [30/51] time 0.208 (0.255) data 0.001 (0.053) loss 0.6683 (0.6192) acc 87.9630 (86.3371) lr 1.3681e-03 eta 0:06:22
epoch [21/50] batch [35/51] time 0.167 (0.243) data 0.000 (0.046) loss 0.8368 (0.6331) acc 82.8125 (86.1744) lr 1.3681e-03 eta 0:06:02
epoch [21/50] batch [40/51] time 0.162 (0.234) data 0.000 (0.040) loss 0.7828 (0.6478) acc 82.4468 (85.6304) lr 1.3681e-03 eta 0:05:48
epoch [21/50] batch [45/51] time 0.166 (0.227) data 0.000 (0.036) loss 0.7463 (0.6501) acc 84.6939 (85.5750) lr 1.3681e-03 eta 0:05:36
epoch [21/50] batch [50/51] time 0.158 (0.220) data 0.000 (0.032) loss 0.6519 (0.6556) acc 84.4445 (85.4060) lr 1.3681e-03 eta 0:05:25
>>> alpha1: 0.165  alpha2: -0.018 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [22/50] batch [5/51] time 0.171 (0.611) data 0.000 (0.436) loss 0.5576 (0.6245) acc 86.2745 (85.2052) lr 1.3090e-03 eta 0:15:00
epoch [22/50] batch [10/51] time 0.157 (0.387) data 0.000 (0.218) loss 0.6486 (0.6423) acc 87.5000 (84.9338) lr 1.3090e-03 eta 0:09:27
epoch [22/50] batch [15/51] time 0.167 (0.315) data 0.000 (0.146) loss 0.5984 (0.6747) acc 84.8958 (84.1691) lr 1.3090e-03 eta 0:07:41
epoch [22/50] batch [20/51] time 0.189 (0.282) data 0.001 (0.109) loss 0.6433 (0.6552) acc 87.2642 (84.9666) lr 1.3090e-03 eta 0:06:51
epoch [22/50] batch [25/51] time 0.173 (0.261) data 0.000 (0.087) loss 0.9073 (0.6765) acc 77.0000 (84.3002) lr 1.3090e-03 eta 0:06:19
epoch [22/50] batch [30/51] time 0.169 (0.247) data 0.000 (0.073) loss 0.7848 (0.6644) acc 81.0000 (84.6415) lr 1.3090e-03 eta 0:05:57
epoch [22/50] batch [35/51] time 0.171 (0.237) data 0.000 (0.063) loss 0.6935 (0.6628) acc 85.1064 (84.8349) lr 1.3090e-03 eta 0:05:41
epoch [22/50] batch [40/51] time 0.161 (0.229) data 0.000 (0.055) loss 0.5336 (0.6523) acc 86.7021 (85.2135) lr 1.3090e-03 eta 0:05:29
epoch [22/50] batch [45/51] time 0.158 (0.222) data 0.000 (0.049) loss 0.8316 (0.6649) acc 78.3333 (84.8097) lr 1.3090e-03 eta 0:05:18
epoch [22/50] batch [50/51] time 0.174 (0.217) data 0.000 (0.044) loss 0.5934 (0.6544) acc 88.4615 (85.1738) lr 1.3090e-03 eta 0:05:10
>>> alpha1: 0.158  alpha2: -0.017 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [23/50] batch [5/51] time 0.169 (0.563) data 0.000 (0.378) loss 0.5715 (0.9442) acc 89.7959 (84.8120) lr 1.2487e-03 eta 0:13:21
epoch [23/50] batch [10/51] time 0.196 (0.373) data 0.001 (0.189) loss 0.5807 (0.7468) acc 85.2941 (85.9360) lr 1.2487e-03 eta 0:08:49
epoch [23/50] batch [15/51] time 0.196 (0.309) data 0.000 (0.126) loss 0.5341 (0.7128) acc 87.9808 (85.9208) lr 1.2487e-03 eta 0:07:16
epoch [23/50] batch [20/51] time 0.184 (0.277) data 0.001 (0.095) loss 0.5490 (0.6722) acc 86.2745 (86.3792) lr 1.2487e-03 eta 0:06:29
epoch [23/50] batch [25/51] time 0.184 (0.256) data 0.001 (0.076) loss 0.6748 (0.6921) acc 83.6364 (85.3482) lr 1.2487e-03 eta 0:05:59
epoch [23/50] batch [30/51] time 0.198 (0.243) data 0.000 (0.063) loss 0.6035 (0.6786) acc 84.9057 (85.1203) lr 1.2487e-03 eta 0:05:40
epoch [23/50] batch [35/51] time 0.173 (0.234) data 0.000 (0.054) loss 0.5419 (0.6753) acc 89.4231 (85.2035) lr 1.2487e-03 eta 0:05:25
epoch [23/50] batch [40/51] time 0.165 (0.226) data 0.000 (0.048) loss 0.8051 (0.6702) acc 83.1633 (85.3338) lr 1.2487e-03 eta 0:05:13
epoch [23/50] batch [45/51] time 0.177 (0.219) data 0.000 (0.042) loss 0.6234 (0.6616) acc 85.6481 (85.7038) lr 1.2487e-03 eta 0:05:03
epoch [23/50] batch [50/51] time 0.169 (0.215) data 0.000 (0.038) loss 0.7749 (0.6575) acc 85.2941 (85.7900) lr 1.2487e-03 eta 0:04:55
>>> alpha1: 0.153  alpha2: -0.017 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [24/50] batch [5/51] time 0.166 (0.502) data 0.000 (0.321) loss 0.9551 (0.7974) acc 82.4468 (85.2061) lr 1.1874e-03 eta 0:11:28
epoch [24/50] batch [10/51] time 0.173 (0.340) data 0.001 (0.161) loss 0.6112 (0.7202) acc 88.2979 (86.4668) lr 1.1874e-03 eta 0:07:44
epoch [24/50] batch [15/51] time 0.169 (0.286) data 0.000 (0.107) loss 0.4838 (0.6294) acc 89.5000 (88.1057) lr 1.1874e-03 eta 0:06:30
epoch [24/50] batch [20/51] time 0.183 (0.259) data 0.001 (0.081) loss 0.4482 (0.5949) acc 92.3469 (88.4684) lr 1.1874e-03 eta 0:05:51
epoch [24/50] batch [25/51] time 0.966 (0.274) data 0.000 (0.065) loss 0.5638 (0.5791) acc 87.0690 (88.2872) lr 1.1874e-03 eta 0:06:10
epoch [24/50] batch [30/51] time 0.180 (0.258) data 0.000 (0.054) loss 0.8176 (0.5962) acc 79.1667 (87.8054) lr 1.1874e-03 eta 0:05:46
epoch [24/50] batch [35/51] time 0.161 (0.245) data 0.000 (0.046) loss 0.5391 (0.6045) acc 93.4783 (87.6391) lr 1.1874e-03 eta 0:05:28
epoch [24/50] batch [40/51] time 0.162 (0.235) data 0.000 (0.040) loss 0.7517 (0.6274) acc 79.7872 (87.0207) lr 1.1874e-03 eta 0:05:14
epoch [24/50] batch [45/51] time 0.164 (0.227) data 0.000 (0.036) loss 0.6085 (0.6306) acc 85.9375 (86.6868) lr 1.1874e-03 eta 0:05:03
epoch [24/50] batch [50/51] time 0.176 (0.222) data 0.000 (0.032) loss 0.7802 (0.6314) acc 81.1321 (86.3938) lr 1.1874e-03 eta 0:04:54
>>> alpha1: 0.150  alpha2: -0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [25/50] batch [5/51] time 0.170 (0.470) data 0.000 (0.293) loss 0.5289 (0.6375) acc 92.0000 (84.8815) lr 1.1253e-03 eta 0:10:21
epoch [25/50] batch [10/51] time 0.171 (0.322) data 0.000 (0.147) loss 0.7555 (0.6003) acc 85.7843 (86.8175) lr 1.1253e-03 eta 0:07:03
epoch [25/50] batch [15/51] time 0.172 (0.273) data 0.000 (0.098) loss 0.8004 (0.6252) acc 77.5000 (86.0632) lr 1.1253e-03 eta 0:05:58
epoch [25/50] batch [20/51] time 0.196 (0.251) data 0.000 (0.073) loss 0.8621 (0.6192) acc 77.5000 (86.1077) lr 1.1253e-03 eta 0:05:27
epoch [25/50] batch [25/51] time 0.181 (0.236) data 0.001 (0.059) loss 0.9514 (0.6334) acc 80.1887 (86.0788) lr 1.1253e-03 eta 0:05:06
epoch [25/50] batch [30/51] time 0.190 (0.226) data 0.000 (0.049) loss 0.4393 (0.6296) acc 92.5926 (86.2012) lr 1.1253e-03 eta 0:04:53
epoch [25/50] batch [35/51] time 0.167 (0.219) data 0.000 (0.042) loss 0.7290 (0.6194) acc 91.3265 (86.5938) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [40/51] time 0.166 (0.214) data 0.000 (0.037) loss 0.4136 (0.6099) acc 92.3469 (86.8878) lr 1.1253e-03 eta 0:04:34
epoch [25/50] batch [45/51] time 0.161 (0.208) data 0.000 (0.033) loss 0.7494 (0.6102) acc 86.7021 (86.9438) lr 1.1253e-03 eta 0:04:26
epoch [25/50] batch [50/51] time 0.179 (0.204) data 0.000 (0.030) loss 0.5788 (0.6029) acc 87.5000 (87.1914) lr 1.1253e-03 eta 0:04:20
>>> alpha1: 0.148  alpha2: -0.010 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [26/50] batch [5/51] time 0.174 (0.520) data 0.001 (0.337) loss 0.4816 (0.6155) acc 88.7255 (85.6356) lr 1.0628e-03 eta 0:11:00
epoch [26/50] batch [10/51] time 0.181 (0.350) data 0.001 (0.169) loss 0.5073 (0.5968) acc 86.1111 (85.6210) lr 1.0628e-03 eta 0:07:23
epoch [26/50] batch [15/51] time 0.159 (0.291) data 0.000 (0.113) loss 0.7694 (0.6104) acc 85.0000 (86.3178) lr 1.0628e-03 eta 0:06:06
epoch [26/50] batch [20/51] time 0.183 (0.262) data 0.000 (0.084) loss 0.4503 (0.5911) acc 90.3846 (86.7652) lr 1.0628e-03 eta 0:05:28
epoch [26/50] batch [25/51] time 0.189 (0.244) data 0.000 (0.068) loss 0.5413 (0.6500) acc 85.9649 (86.2376) lr 1.0628e-03 eta 0:05:05
epoch [26/50] batch [30/51] time 0.169 (0.233) data 0.000 (0.056) loss 0.5434 (0.6477) acc 85.5000 (85.9821) lr 1.0628e-03 eta 0:04:50
epoch [26/50] batch [35/51] time 0.178 (0.225) data 0.000 (0.048) loss 0.4982 (0.6391) acc 89.0625 (86.1465) lr 1.0628e-03 eta 0:04:39
epoch [26/50] batch [40/51] time 0.172 (0.219) data 0.000 (0.042) loss 0.5708 (0.6408) acc 87.0192 (85.7770) lr 1.0628e-03 eta 0:04:30
epoch [26/50] batch [45/51] time 0.164 (0.213) data 0.000 (0.038) loss 0.6832 (0.6383) acc 85.4167 (85.7574) lr 1.0628e-03 eta 0:04:22
epoch [26/50] batch [50/51] time 0.157 (0.209) data 0.000 (0.034) loss 0.6030 (0.6503) acc 86.6667 (85.7645) lr 1.0628e-03 eta 0:04:15
>>> alpha1: 0.146  alpha2: -0.007 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [27/50] batch [5/51] time 0.170 (0.521) data 0.000 (0.336) loss 0.4284 (0.4536) acc 88.2353 (89.5270) lr 1.0000e-03 eta 0:10:35
epoch [27/50] batch [10/51] time 0.171 (0.345) data 0.000 (0.168) loss 0.4493 (0.4989) acc 91.8367 (88.4621) lr 1.0000e-03 eta 0:06:58
epoch [27/50] batch [15/51] time 0.177 (0.288) data 0.000 (0.112) loss 0.8497 (0.5516) acc 80.3922 (87.4692) lr 1.0000e-03 eta 0:05:47
epoch [27/50] batch [20/51] time 0.178 (0.259) data 0.000 (0.084) loss 0.6130 (0.5662) acc 87.5000 (87.4896) lr 1.0000e-03 eta 0:05:11
epoch [27/50] batch [25/51] time 0.173 (0.242) data 0.000 (0.067) loss 0.7486 (0.5817) acc 81.2500 (87.0626) lr 1.0000e-03 eta 0:04:50
epoch [27/50] batch [30/51] time 0.192 (0.232) data 0.000 (0.056) loss 0.5150 (0.5735) acc 88.2075 (87.1311) lr 1.0000e-03 eta 0:04:36
epoch [27/50] batch [35/51] time 0.177 (0.223) data 0.000 (0.048) loss 0.8468 (0.5711) acc 76.9231 (87.1842) lr 1.0000e-03 eta 0:04:25
epoch [27/50] batch [40/51] time 0.173 (0.217) data 0.000 (0.042) loss 0.5621 (0.5671) acc 82.6923 (87.1410) lr 1.0000e-03 eta 0:04:16
epoch [27/50] batch [45/51] time 0.177 (0.212) data 0.000 (0.037) loss 0.5786 (0.5672) acc 85.8491 (87.2552) lr 1.0000e-03 eta 0:04:10
epoch [27/50] batch [50/51] time 0.176 (0.208) data 0.000 (0.034) loss 0.6294 (0.5692) acc 84.9057 (87.1271) lr 1.0000e-03 eta 0:04:04
>>> alpha1: 0.144  alpha2: -0.002 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [28/50] batch [5/51] time 0.171 (0.506) data 0.002 (0.323) loss 0.8464 (0.6182) acc 81.6327 (86.3901) lr 9.3721e-04 eta 0:09:51
epoch [28/50] batch [10/51] time 0.179 (0.343) data 0.000 (0.162) loss 0.4873 (0.5882) acc 92.5000 (87.2456) lr 9.3721e-04 eta 0:06:38
epoch [28/50] batch [15/51] time 0.182 (0.287) data 0.000 (0.108) loss 0.5580 (0.5707) acc 89.7059 (87.6783) lr 9.3721e-04 eta 0:05:32
epoch [28/50] batch [20/51] time 0.176 (0.258) data 0.001 (0.081) loss 0.3632 (0.5564) acc 92.5000 (87.9882) lr 9.3721e-04 eta 0:04:57
epoch [28/50] batch [25/51] time 0.171 (0.243) data 0.000 (0.065) loss 0.7164 (0.5662) acc 83.8235 (87.4962) lr 9.3721e-04 eta 0:04:38
epoch [28/50] batch [30/51] time 0.184 (0.232) data 0.000 (0.054) loss 0.5077 (0.5738) acc 89.7059 (87.1394) lr 9.3721e-04 eta 0:04:25
epoch [28/50] batch [35/51] time 0.181 (0.224) data 0.001 (0.046) loss 0.4432 (0.5691) acc 88.4615 (87.2053) lr 9.3721e-04 eta 0:04:14
epoch [28/50] batch [40/51] time 0.184 (0.218) data 0.000 (0.041) loss 0.5821 (0.5734) acc 87.0536 (87.0366) lr 9.3721e-04 eta 0:04:06
epoch [28/50] batch [45/51] time 0.159 (0.212) data 0.000 (0.036) loss 0.3859 (0.5685) acc 92.9348 (87.3144) lr 9.3721e-04 eta 0:03:59
epoch [28/50] batch [50/51] time 0.176 (0.208) data 0.000 (0.033) loss 0.4770 (0.5654) acc 92.9245 (87.3965) lr 9.3721e-04 eta 0:03:53
>>> alpha1: 0.143  alpha2: 0.003 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [29/50] batch [5/51] time 0.169 (0.511) data 0.000 (0.327) loss 0.5696 (0.5636) acc 87.0000 (88.2436) lr 8.7467e-04 eta 0:09:30
epoch [29/50] batch [10/51] time 0.170 (0.340) data 0.000 (0.164) loss 0.5828 (0.5842) acc 84.0000 (87.3586) lr 8.7467e-04 eta 0:06:17
epoch [29/50] batch [15/51] time 0.178 (0.286) data 0.000 (0.109) loss 0.5638 (0.5936) acc 89.5000 (87.4704) lr 8.7467e-04 eta 0:05:16
epoch [29/50] batch [20/51] time 0.167 (0.259) data 0.000 (0.082) loss 0.5023 (0.5738) acc 88.5417 (87.9114) lr 8.7467e-04 eta 0:04:45
epoch [29/50] batch [25/51] time 0.182 (0.242) data 0.000 (0.066) loss 0.4411 (0.5726) acc 90.0000 (87.8811) lr 8.7467e-04 eta 0:04:25
epoch [29/50] batch [30/51] time 0.167 (0.231) data 0.000 (0.055) loss 0.4964 (0.5684) acc 90.1042 (87.8261) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [35/51] time 0.171 (0.225) data 0.000 (0.047) loss 0.6323 (0.5687) acc 86.0000 (87.7693) lr 8.7467e-04 eta 0:04:04
epoch [29/50] batch [40/51] time 0.164 (0.218) data 0.000 (0.041) loss 0.6794 (0.5714) acc 82.2917 (87.7311) lr 8.7467e-04 eta 0:03:55
epoch [29/50] batch [45/51] time 0.180 (0.213) data 0.000 (0.037) loss 0.5300 (0.5653) acc 86.3636 (87.7167) lr 8.7467e-04 eta 0:03:48
epoch [29/50] batch [50/51] time 0.164 (0.208) data 0.000 (0.033) loss 0.6315 (0.5664) acc 84.3750 (87.7924) lr 8.7467e-04 eta 0:03:43
>>> alpha1: 0.142  alpha2: 0.008 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [30/50] batch [5/51] time 0.162 (0.497) data 0.000 (0.326) loss 0.8215 (0.6426) acc 78.1915 (84.5083) lr 8.1262e-04 eta 0:08:50
epoch [30/50] batch [10/51] time 0.170 (0.333) data 0.001 (0.163) loss 0.5458 (0.6074) acc 85.0000 (86.0976) lr 8.1262e-04 eta 0:05:52
epoch [30/50] batch [15/51] time 0.167 (0.278) data 0.000 (0.109) loss 0.6397 (0.5857) acc 87.2449 (86.5620) lr 8.1262e-04 eta 0:04:53
epoch [30/50] batch [20/51] time 0.185 (0.291) data 0.000 (0.082) loss 0.5829 (0.5917) acc 84.3137 (85.9983) lr 8.1262e-04 eta 0:05:05
epoch [30/50] batch [25/51] time 0.165 (0.268) data 0.000 (0.066) loss 0.4262 (0.5667) acc 93.7500 (86.9942) lr 8.1262e-04 eta 0:04:40
epoch [30/50] batch [30/51] time 0.177 (0.253) data 0.000 (0.055) loss 0.4290 (0.5450) acc 91.3265 (87.7664) lr 8.1262e-04 eta 0:04:23
epoch [30/50] batch [35/51] time 0.177 (0.242) data 0.000 (0.047) loss 0.6908 (0.5478) acc 85.3774 (87.7678) lr 8.1262e-04 eta 0:04:11
epoch [30/50] batch [40/51] time 0.180 (0.234) data 0.000 (0.041) loss 0.4804 (0.5563) acc 90.9091 (87.4301) lr 8.1262e-04 eta 0:04:00
epoch [30/50] batch [45/51] time 0.184 (0.226) data 0.000 (0.037) loss 0.5176 (0.5571) acc 88.5965 (87.3670) lr 8.1262e-04 eta 0:03:52
epoch [30/50] batch [50/51] time 0.166 (0.220) data 0.000 (0.033) loss 0.5202 (0.5606) acc 91.5000 (87.3580) lr 8.1262e-04 eta 0:03:44
>>> alpha1: 0.138  alpha2: 0.008 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [31/50] batch [5/51] time 0.177 (0.555) data 0.000 (0.373) loss 0.4986 (0.4737) acc 87.0192 (88.4803) lr 7.5131e-04 eta 0:09:23
epoch [31/50] batch [10/51] time 0.166 (0.367) data 0.001 (0.187) loss 0.6313 (0.5089) acc 90.1042 (88.9330) lr 7.5131e-04 eta 0:06:10
epoch [31/50] batch [15/51] time 0.179 (0.304) data 0.000 (0.125) loss 0.3238 (0.5084) acc 91.6667 (88.8079) lr 7.5131e-04 eta 0:05:05
epoch [31/50] batch [20/51] time 0.165 (0.270) data 0.000 (0.094) loss 0.3543 (0.5087) acc 92.1875 (88.8623) lr 7.5131e-04 eta 0:04:30
epoch [31/50] batch [25/51] time 0.176 (0.252) data 0.000 (0.075) loss 0.4358 (0.5161) acc 90.1042 (88.5588) lr 7.5131e-04 eta 0:04:10
epoch [31/50] batch [30/51] time 0.169 (0.239) data 0.000 (0.063) loss 0.3619 (0.5199) acc 94.5000 (88.4364) lr 7.5131e-04 eta 0:03:56
epoch [31/50] batch [35/51] time 0.167 (0.229) data 0.000 (0.054) loss 0.7149 (0.5343) acc 79.5918 (87.8976) lr 7.5131e-04 eta 0:03:45
epoch [31/50] batch [40/51] time 0.170 (0.222) data 0.000 (0.047) loss 0.7186 (0.5446) acc 82.3529 (87.5044) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [45/51] time 0.178 (0.216) data 0.000 (0.042) loss 0.3689 (0.5376) acc 92.1296 (87.7541) lr 7.5131e-04 eta 0:03:30
epoch [31/50] batch [50/51] time 0.164 (0.212) data 0.000 (0.038) loss 0.6541 (0.5407) acc 83.3333 (87.5754) lr 7.5131e-04 eta 0:03:25
>>> alpha1: 0.135  alpha2: 0.008 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [32/50] batch [5/51] time 0.181 (0.485) data 0.000 (0.302) loss 0.6562 (0.4826) acc 80.5556 (88.9639) lr 6.9098e-04 eta 0:07:47
epoch [32/50] batch [10/51] time 0.166 (0.333) data 0.001 (0.151) loss 0.3939 (0.4854) acc 93.2292 (89.7582) lr 6.9098e-04 eta 0:05:19
epoch [32/50] batch [15/51] time 0.176 (0.280) data 0.000 (0.101) loss 0.4787 (0.5257) acc 92.8571 (88.6246) lr 6.9098e-04 eta 0:04:27
epoch [32/50] batch [20/51] time 0.187 (0.256) data 0.001 (0.076) loss 0.4261 (0.5059) acc 89.9038 (88.7511) lr 6.9098e-04 eta 0:04:02
epoch [32/50] batch [25/51] time 0.179 (0.240) data 0.000 (0.061) loss 0.5772 (0.5167) acc 86.7021 (88.6707) lr 6.9098e-04 eta 0:03:46
epoch [32/50] batch [30/51] time 0.160 (0.228) data 0.000 (0.051) loss 0.6735 (0.5294) acc 88.5870 (88.4582) lr 6.9098e-04 eta 0:03:34
epoch [32/50] batch [35/51] time 0.172 (0.221) data 0.000 (0.043) loss 0.6604 (0.5351) acc 84.0000 (88.2591) lr 6.9098e-04 eta 0:03:26
epoch [32/50] batch [40/51] time 0.167 (0.215) data 0.000 (0.038) loss 0.5322 (0.5334) acc 85.7143 (88.2137) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [45/51] time 0.182 (0.210) data 0.000 (0.034) loss 0.4567 (0.5322) acc 93.3036 (88.2794) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [50/51] time 0.171 (0.206) data 0.000 (0.030) loss 0.4865 (0.5253) acc 89.2157 (88.5632) lr 6.9098e-04 eta 0:03:09
>>> alpha1: 0.136  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [33/50] batch [5/51] time 0.170 (0.495) data 0.000 (0.313) loss 0.4872 (0.6072) acc 90.5000 (87.4945) lr 6.3188e-04 eta 0:07:31
epoch [33/50] batch [10/51] time 0.188 (0.335) data 0.000 (0.156) loss 0.3484 (0.5674) acc 91.5094 (87.4277) lr 6.3188e-04 eta 0:05:04
epoch [33/50] batch [15/51] time 0.170 (0.283) data 0.000 (0.104) loss 0.5444 (0.5470) acc 89.5000 (87.7967) lr 6.3188e-04 eta 0:04:15
epoch [33/50] batch [20/51] time 0.188 (0.256) data 0.001 (0.078) loss 0.4607 (0.5562) acc 91.4894 (87.8597) lr 6.3188e-04 eta 0:03:49
epoch [33/50] batch [25/51] time 0.171 (0.239) data 0.000 (0.063) loss 0.4214 (0.5517) acc 89.7059 (88.1483) lr 6.3188e-04 eta 0:03:33
epoch [33/50] batch [30/51] time 0.182 (0.230) data 0.000 (0.052) loss 0.5288 (0.5461) acc 88.2353 (88.1917) lr 6.3188e-04 eta 0:03:23
epoch [33/50] batch [35/51] time 0.176 (0.222) data 0.001 (0.045) loss 0.5127 (0.5375) acc 86.4583 (88.5377) lr 6.3188e-04 eta 0:03:16
epoch [33/50] batch [40/51] time 0.163 (0.216) data 0.000 (0.039) loss 0.5712 (0.5424) acc 88.8298 (88.4778) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.176 (0.211) data 0.000 (0.035) loss 0.4240 (0.5272) acc 90.0943 (88.7505) lr 6.3188e-04 eta 0:03:04
epoch [33/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.032) loss 0.6549 (0.5296) acc 82.5472 (88.5132) lr 6.3188e-04 eta 0:02:59
>>> alpha1: 0.134  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [34/50] batch [5/51] time 0.165 (0.551) data 0.000 (0.375) loss 0.5228 (0.4921) acc 86.4583 (90.0797) lr 5.7422e-04 eta 0:07:55
epoch [34/50] batch [10/51] time 0.169 (0.364) data 0.000 (0.188) loss 0.5969 (0.5151) acc 83.6735 (88.7954) lr 5.7422e-04 eta 0:05:11
epoch [34/50] batch [15/51] time 0.179 (0.302) data 0.000 (0.125) loss 0.4282 (0.5158) acc 87.0370 (88.3949) lr 5.7422e-04 eta 0:04:17
epoch [34/50] batch [20/51] time 0.176 (0.271) data 0.000 (0.094) loss 0.7587 (0.5169) acc 85.0000 (88.7817) lr 5.7422e-04 eta 0:03:49
epoch [34/50] batch [25/51] time 0.166 (0.250) data 0.000 (0.075) loss 0.3844 (0.5260) acc 90.8163 (88.5842) lr 5.7422e-04 eta 0:03:30
epoch [34/50] batch [30/51] time 0.177 (0.239) data 0.000 (0.063) loss 0.4549 (0.5174) acc 87.2642 (88.6797) lr 5.7422e-04 eta 0:03:19
epoch [34/50] batch [35/51] time 0.165 (0.230) data 0.000 (0.054) loss 0.4818 (0.5199) acc 89.5833 (88.5554) lr 5.7422e-04 eta 0:03:11
epoch [34/50] batch [40/51] time 0.169 (0.222) data 0.000 (0.047) loss 0.5044 (0.5312) acc 91.0000 (88.4111) lr 5.7422e-04 eta 0:03:03
epoch [34/50] batch [45/51] time 0.176 (0.217) data 0.000 (0.042) loss 0.3434 (0.5259) acc 92.4528 (88.6848) lr 5.7422e-04 eta 0:02:57
epoch [34/50] batch [50/51] time 0.167 (0.211) data 0.000 (0.038) loss 0.5026 (0.5284) acc 88.5000 (88.6861) lr 5.7422e-04 eta 0:02:52
>>> alpha1: 0.135  alpha2: 0.011 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.27 <<<
epoch [35/50] batch [5/51] time 0.169 (0.465) data 0.000 (0.286) loss 0.3777 (0.4669) acc 96.0000 (90.3593) lr 5.1825e-04 eta 0:06:17
epoch [35/50] batch [10/51] time 0.171 (0.320) data 0.000 (0.143) loss 0.4556 (0.5100) acc 91.6667 (90.0969) lr 5.1825e-04 eta 0:04:18
epoch [35/50] batch [15/51] time 0.170 (0.272) data 0.000 (0.096) loss 0.4557 (0.4736) acc 88.2653 (90.5995) lr 5.1825e-04 eta 0:03:38
epoch [35/50] batch [20/51] time 0.189 (0.247) data 0.000 (0.072) loss 0.6792 (0.5068) acc 85.1852 (89.6840) lr 5.1825e-04 eta 0:03:16
epoch [35/50] batch [25/51] time 0.187 (0.232) data 0.011 (0.058) loss 0.5611 (0.5423) acc 88.9423 (89.4697) lr 5.1825e-04 eta 0:03:03
epoch [35/50] batch [30/51] time 0.174 (0.222) data 0.000 (0.048) loss 0.3674 (0.5334) acc 94.5000 (89.5608) lr 5.1825e-04 eta 0:02:54
epoch [35/50] batch [35/51] time 0.164 (0.237) data 0.000 (0.041) loss 0.5879 (0.5328) acc 84.8958 (89.5414) lr 5.1825e-04 eta 0:03:04
epoch [35/50] batch [40/51] time 0.165 (0.229) data 0.000 (0.036) loss 0.3954 (0.5335) acc 92.8571 (89.5085) lr 5.1825e-04 eta 0:02:57
epoch [35/50] batch [45/51] time 0.181 (0.222) data 0.000 (0.032) loss 0.5112 (0.5364) acc 87.2727 (89.3736) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [50/51] time 0.160 (0.217) data 0.000 (0.029) loss 0.7520 (0.5375) acc 78.1915 (89.0311) lr 5.1825e-04 eta 0:02:45
>>> alpha1: 0.135  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [36/50] batch [5/51] time 0.164 (0.505) data 0.000 (0.328) loss 0.5627 (0.4457) acc 85.9375 (90.5539) lr 4.6417e-04 eta 0:06:23
epoch [36/50] batch [10/51] time 0.172 (0.342) data 0.000 (0.164) loss 0.3403 (0.4598) acc 94.7115 (90.2339) lr 4.6417e-04 eta 0:04:18
epoch [36/50] batch [15/51] time 0.182 (0.286) data 0.000 (0.110) loss 0.3388 (0.4777) acc 92.9245 (89.6044) lr 4.6417e-04 eta 0:03:34
epoch [36/50] batch [20/51] time 0.176 (0.258) data 0.000 (0.082) loss 0.3425 (0.4833) acc 93.3962 (89.3656) lr 4.6417e-04 eta 0:03:11
epoch [36/50] batch [25/51] time 0.170 (0.240) data 0.000 (0.066) loss 0.5190 (0.5113) acc 91.1765 (88.7435) lr 4.6417e-04 eta 0:02:57
epoch [36/50] batch [30/51] time 0.158 (0.228) data 0.000 (0.055) loss 0.4130 (0.5312) acc 92.9348 (88.7587) lr 4.6417e-04 eta 0:02:47
epoch [36/50] batch [35/51] time 0.165 (0.221) data 0.000 (0.047) loss 0.5494 (0.5345) acc 88.7755 (88.6866) lr 4.6417e-04 eta 0:02:41
epoch [36/50] batch [40/51] time 0.166 (0.215) data 0.000 (0.041) loss 0.6697 (0.5442) acc 83.0000 (88.5404) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.037) loss 0.3875 (0.5342) acc 90.6863 (88.8355) lr 4.6417e-04 eta 0:02:30
epoch [36/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.033) loss 0.4432 (0.5255) acc 91.8269 (88.9511) lr 4.6417e-04 eta 0:02:26
>>> alpha1: 0.135  alpha2: 0.013 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [37/50] batch [5/51] time 0.186 (0.499) data 0.000 (0.314) loss 0.5029 (0.4831) acc 89.7959 (89.2566) lr 4.1221e-04 eta 0:05:53
epoch [37/50] batch [10/51] time 0.174 (0.338) data 0.000 (0.157) loss 0.4795 (0.5017) acc 86.5385 (89.0437) lr 4.1221e-04 eta 0:03:57
epoch [37/50] batch [15/51] time 0.186 (0.283) data 0.000 (0.105) loss 0.4584 (0.5274) acc 91.8269 (89.3315) lr 4.1221e-04 eta 0:03:18
epoch [37/50] batch [20/51] time 0.181 (0.256) data 0.000 (0.079) loss 0.5648 (0.5402) acc 86.3208 (88.9754) lr 4.1221e-04 eta 0:02:57
epoch [37/50] batch [25/51] time 0.174 (0.240) data 0.000 (0.063) loss 0.5356 (0.5271) acc 88.0208 (89.4400) lr 4.1221e-04 eta 0:02:45
epoch [37/50] batch [30/51] time 0.170 (0.230) data 0.000 (0.053) loss 0.5946 (0.5349) acc 83.8235 (88.8246) lr 4.1221e-04 eta 0:02:37
epoch [37/50] batch [35/51] time 0.182 (0.222) data 0.000 (0.045) loss 0.4668 (0.5340) acc 91.0377 (88.6727) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [40/51] time 0.178 (0.216) data 0.000 (0.040) loss 0.4083 (0.5151) acc 93.6364 (89.1282) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [45/51] time 0.158 (0.210) data 0.000 (0.035) loss 0.5619 (0.5233) acc 86.9565 (88.7214) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [50/51] time 0.169 (0.206) data 0.000 (0.032) loss 0.3274 (0.5213) acc 95.0980 (88.7222) lr 4.1221e-04 eta 0:02:17
>>> alpha1: 0.134  alpha2: 0.017 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [38/50] batch [5/51] time 0.177 (0.461) data 0.000 (0.287) loss 0.3928 (0.4875) acc 92.0000 (89.8293) lr 3.6258e-04 eta 0:05:03
epoch [38/50] batch [10/51] time 0.185 (0.320) data 0.000 (0.144) loss 0.4134 (0.5080) acc 90.1786 (89.2606) lr 3.6258e-04 eta 0:03:28
epoch [38/50] batch [15/51] time 0.180 (0.270) data 0.000 (0.096) loss 0.3277 (0.5122) acc 96.2963 (88.9463) lr 3.6258e-04 eta 0:02:55
epoch [38/50] batch [20/51] time 0.180 (0.246) data 0.000 (0.072) loss 0.4473 (0.4943) acc 91.6667 (89.4270) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [25/51] time 0.171 (0.232) data 0.000 (0.058) loss 0.4768 (0.4974) acc 87.7451 (88.9851) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.177 (0.223) data 0.000 (0.049) loss 0.3936 (0.4961) acc 94.6078 (89.3116) lr 3.6258e-04 eta 0:02:20
epoch [38/50] batch [35/51] time 0.175 (0.216) data 0.000 (0.042) loss 0.3764 (0.5006) acc 89.7959 (89.2965) lr 3.6258e-04 eta 0:02:15
epoch [38/50] batch [40/51] time 0.170 (0.211) data 0.000 (0.037) loss 0.5791 (0.4970) acc 85.2941 (89.3844) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [45/51] time 0.161 (0.206) data 0.000 (0.033) loss 0.7115 (0.5046) acc 80.3191 (89.1202) lr 3.6258e-04 eta 0:02:07
epoch [38/50] batch [50/51] time 0.176 (0.203) data 0.000 (0.029) loss 0.4390 (0.5061) acc 89.6226 (88.9991) lr 3.6258e-04 eta 0:02:04
>>> alpha1: 0.134  alpha2: 0.015 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [39/50] batch [5/51] time 0.174 (0.462) data 0.015 (0.287) loss 0.5282 (0.5749) acc 89.4445 (87.6462) lr 3.1545e-04 eta 0:04:40
epoch [39/50] batch [10/51] time 0.165 (0.318) data 0.000 (0.143) loss 0.4125 (0.5219) acc 92.1875 (89.3524) lr 3.1545e-04 eta 0:03:11
epoch [39/50] batch [15/51] time 0.164 (0.270) data 0.001 (0.096) loss 0.3223 (0.4962) acc 94.1489 (89.8118) lr 3.1545e-04 eta 0:02:41
epoch [39/50] batch [20/51] time 0.162 (0.246) data 0.000 (0.072) loss 0.5028 (0.4876) acc 87.2340 (89.6472) lr 3.1545e-04 eta 0:02:25
epoch [39/50] batch [25/51] time 0.182 (0.231) data 0.000 (0.058) loss 0.4979 (0.5064) acc 87.9630 (89.1595) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [30/51] time 0.180 (0.222) data 0.000 (0.048) loss 0.5883 (0.5123) acc 89.6226 (89.0630) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [35/51] time 0.183 (0.215) data 0.000 (0.041) loss 0.5726 (0.5057) acc 85.5769 (89.1854) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [40/51] time 0.164 (0.210) data 0.000 (0.036) loss 0.4146 (0.4975) acc 94.2708 (89.2814) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [45/51] time 0.173 (0.205) data 0.000 (0.032) loss 0.3828 (0.4987) acc 93.7500 (89.3462) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [50/51] time 0.172 (0.202) data 0.000 (0.029) loss 0.3567 (0.4969) acc 91.8269 (89.4092) lr 3.1545e-04 eta 0:01:53
>>> alpha1: 0.132  alpha2: 0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [40/50] batch [5/51] time 0.169 (0.466) data 0.000 (0.286) loss 0.5384 (0.4662) acc 89.0000 (89.6741) lr 2.7103e-04 eta 0:04:19
epoch [40/50] batch [10/51] time 0.168 (0.320) data 0.000 (0.143) loss 0.6365 (0.5145) acc 84.1837 (89.0427) lr 2.7103e-04 eta 0:02:56
epoch [40/50] batch [15/51] time 0.165 (0.271) data 0.000 (0.095) loss 0.5170 (0.5025) acc 88.5417 (89.2950) lr 2.7103e-04 eta 0:02:28
epoch [40/50] batch [20/51] time 0.172 (0.247) data 0.000 (0.072) loss 0.4708 (0.4756) acc 93.6274 (90.1928) lr 2.7103e-04 eta 0:02:13
epoch [40/50] batch [25/51] time 0.175 (0.234) data 0.000 (0.058) loss 0.5408 (0.4766) acc 87.7451 (90.0475) lr 2.7103e-04 eta 0:02:05
epoch [40/50] batch [30/51] time 0.183 (0.225) data 0.000 (0.049) loss 0.3639 (0.4846) acc 90.8654 (89.8867) lr 2.7103e-04 eta 0:01:59
epoch [40/50] batch [35/51] time 0.189 (0.219) data 0.001 (0.042) loss 0.5404 (0.4878) acc 89.4231 (89.7831) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [40/51] time 0.173 (0.214) data 0.000 (0.037) loss 0.3879 (0.4753) acc 96.6346 (90.3230) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.033) loss 0.6721 (0.4839) acc 85.5000 (90.1403) lr 2.7103e-04 eta 0:01:47
epoch [40/50] batch [50/51] time 0.167 (0.206) data 0.001 (0.029) loss 0.6125 (0.4921) acc 89.7959 (89.8631) lr 2.7103e-04 eta 0:01:45
>>> alpha1: 0.131  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [41/50] batch [5/51] time 0.183 (0.515) data 0.000 (0.336) loss 0.5284 (0.5168) acc 90.1961 (88.8853) lr 2.2949e-04 eta 0:04:20
epoch [41/50] batch [10/51] time 0.184 (0.346) data 0.000 (0.168) loss 0.4318 (0.4964) acc 90.1786 (88.5095) lr 2.2949e-04 eta 0:02:53
epoch [41/50] batch [15/51] time 0.174 (0.290) data 0.001 (0.112) loss 0.7120 (0.4731) acc 86.0000 (89.5382) lr 2.2949e-04 eta 0:02:23
epoch [41/50] batch [20/51] time 0.170 (0.261) data 0.000 (0.084) loss 0.6031 (0.4722) acc 82.1429 (89.5029) lr 2.2949e-04 eta 0:02:07
epoch [41/50] batch [25/51] time 0.195 (0.245) data 0.000 (0.067) loss 0.4344 (0.4700) acc 87.2727 (89.2592) lr 2.2949e-04 eta 0:01:58
epoch [41/50] batch [30/51] time 0.174 (0.233) data 0.000 (0.056) loss 0.4397 (0.4713) acc 92.7885 (89.4692) lr 2.2949e-04 eta 0:01:52
epoch [41/50] batch [35/51] time 0.182 (0.225) data 0.000 (0.048) loss 0.3081 (0.4788) acc 95.0000 (89.2958) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [40/51] time 0.180 (0.218) data 0.000 (0.042) loss 0.4660 (0.4745) acc 91.3636 (89.4352) lr 2.2949e-04 eta 0:01:42
epoch [41/50] batch [45/51] time 0.170 (0.213) data 0.000 (0.038) loss 0.6378 (0.4879) acc 86.2745 (89.2295) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [50/51] time 0.161 (0.209) data 0.000 (0.034) loss 0.6543 (0.4919) acc 86.7021 (89.3037) lr 2.2949e-04 eta 0:01:35
>>> alpha1: 0.133  alpha2: 0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.27 <<<
epoch [42/50] batch [5/51] time 0.185 (0.473) data 0.000 (0.293) loss 0.1771 (0.3956) acc 98.2456 (91.9636) lr 1.9098e-04 eta 0:03:34
epoch [42/50] batch [10/51] time 0.170 (0.324) data 0.000 (0.147) loss 0.4978 (0.4581) acc 89.0000 (90.5643) lr 1.9098e-04 eta 0:02:25
epoch [42/50] batch [15/51] time 0.174 (0.274) data 0.000 (0.098) loss 0.4853 (0.5187) acc 90.8654 (90.0550) lr 1.9098e-04 eta 0:02:01
epoch [42/50] batch [20/51] time 0.194 (0.251) data 0.000 (0.074) loss 0.4161 (0.5195) acc 90.1786 (89.4783) lr 1.9098e-04 eta 0:01:50
epoch [42/50] batch [25/51] time 0.181 (0.236) data 0.000 (0.059) loss 0.3182 (0.5099) acc 93.1818 (89.3949) lr 1.9098e-04 eta 0:01:42
epoch [42/50] batch [30/51] time 0.181 (0.226) data 0.000 (0.049) loss 0.7109 (0.5120) acc 83.6364 (89.2841) lr 1.9098e-04 eta 0:01:37
epoch [42/50] batch [35/51] time 0.184 (0.219) data 0.000 (0.042) loss 0.4512 (0.5039) acc 89.3519 (89.3637) lr 1.9098e-04 eta 0:01:33
epoch [42/50] batch [40/51] time 0.173 (0.214) data 0.000 (0.037) loss 0.4181 (0.5072) acc 91.8269 (89.2521) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [45/51] time 0.170 (0.209) data 0.000 (0.033) loss 0.4841 (0.5104) acc 89.2157 (88.9496) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [50/51] time 0.178 (0.205) data 0.000 (0.030) loss 0.6068 (0.5177) acc 83.3333 (88.6880) lr 1.9098e-04 eta 0:01:23
>>> alpha1: 0.132  alpha2: 0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [43/50] batch [5/51] time 0.179 (0.594) data 0.000 (0.405) loss 0.3746 (0.3983) acc 92.3077 (91.7628) lr 1.5567e-04 eta 0:03:59
epoch [43/50] batch [10/51] time 0.176 (0.385) data 0.000 (0.203) loss 0.5470 (0.4851) acc 86.2245 (90.0790) lr 1.5567e-04 eta 0:02:33
epoch [43/50] batch [15/51] time 0.194 (0.316) data 0.000 (0.135) loss 0.3494 (0.4800) acc 95.3704 (90.1504) lr 1.5567e-04 eta 0:02:04
epoch [43/50] batch [20/51] time 0.172 (0.280) data 0.000 (0.102) loss 0.4824 (0.5140) acc 91.0000 (89.3653) lr 1.5567e-04 eta 0:01:48
epoch [43/50] batch [25/51] time 0.184 (0.260) data 0.000 (0.081) loss 0.4948 (0.5209) acc 87.9630 (89.1710) lr 1.5567e-04 eta 0:01:39
epoch [43/50] batch [30/51] time 0.184 (0.246) data 0.000 (0.068) loss 0.4798 (0.5229) acc 90.6250 (88.7639) lr 1.5567e-04 eta 0:01:33
epoch [43/50] batch [35/51] time 0.174 (0.236) data 0.000 (0.058) loss 0.6192 (0.5279) acc 87.0192 (88.9411) lr 1.5567e-04 eta 0:01:28
epoch [43/50] batch [40/51] time 0.180 (0.229) data 0.000 (0.051) loss 0.3405 (0.5169) acc 92.2727 (89.1582) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [45/51] time 0.168 (0.223) data 0.000 (0.045) loss 0.3533 (0.5099) acc 94.0000 (89.2068) lr 1.5567e-04 eta 0:01:20
epoch [43/50] batch [50/51] time 0.179 (0.218) data 0.000 (0.041) loss 0.6134 (0.5102) acc 84.7222 (89.1631) lr 1.5567e-04 eta 0:01:18
>>> alpha1: 0.131  alpha2: 0.017 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.25 <<<
epoch [44/50] batch [5/51] time 0.172 (0.467) data 0.000 (0.292) loss 0.4435 (0.4567) acc 91.3462 (90.7988) lr 1.2369e-04 eta 0:02:44
epoch [44/50] batch [10/51] time 0.155 (0.321) data 0.000 (0.146) loss 0.5891 (0.5125) acc 87.5000 (89.3393) lr 1.2369e-04 eta 0:01:51
epoch [44/50] batch [15/51] time 0.172 (0.273) data 0.000 (0.098) loss 0.3989 (0.4858) acc 91.8269 (89.6114) lr 1.2369e-04 eta 0:01:33
epoch [44/50] batch [20/51] time 0.190 (0.250) data 0.000 (0.073) loss 0.6288 (0.4713) acc 85.8491 (89.9538) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [25/51] time 0.168 (0.235) data 0.000 (0.059) loss 0.5682 (0.4719) acc 84.3750 (89.7091) lr 1.2369e-04 eta 0:01:17
epoch [44/50] batch [30/51] time 0.173 (0.225) data 0.000 (0.049) loss 0.4992 (0.4722) acc 87.5000 (89.5230) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [35/51] time 0.166 (0.217) data 0.000 (0.042) loss 0.5428 (0.4789) acc 87.7551 (89.4300) lr 1.2369e-04 eta 0:01:09
epoch [44/50] batch [40/51] time 0.176 (0.211) data 0.000 (0.037) loss 0.3107 (0.4652) acc 94.3396 (89.9257) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [45/51] time 0.176 (0.207) data 0.000 (0.033) loss 0.4401 (0.4638) acc 91.5094 (89.9447) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.180 (0.204) data 0.000 (0.029) loss 0.3779 (0.4639) acc 90.0000 (89.9409) lr 1.2369e-04 eta 0:01:02
>>> alpha1: 0.132  alpha2: 0.019 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.26 <<<
epoch [45/50] batch [5/51] time 0.187 (0.515) data 0.001 (0.328) loss 0.5353 (0.5062) acc 90.3846 (88.1978) lr 9.5173e-05 eta 0:02:34
epoch [45/50] batch [10/51] time 0.176 (0.346) data 0.000 (0.164) loss 0.4274 (0.4934) acc 90.8654 (88.2835) lr 9.5173e-05 eta 0:01:42
epoch [45/50] batch [15/51] time 0.169 (0.290) data 0.001 (0.110) loss 0.6383 (0.4967) acc 87.5000 (88.7217) lr 9.5173e-05 eta 0:01:24
epoch [45/50] batch [20/51] time 0.185 (0.262) data 0.000 (0.082) loss 0.5915 (0.4856) acc 86.6071 (88.8714) lr 9.5173e-05 eta 0:01:15
epoch [45/50] batch [25/51] time 0.168 (0.245) data 0.000 (0.066) loss 0.4848 (0.4782) acc 90.3061 (89.3309) lr 9.5173e-05 eta 0:01:08
epoch [45/50] batch [30/51] time 0.184 (0.235) data 0.001 (0.056) loss 0.5593 (0.4749) acc 87.7273 (89.4866) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [35/51] time 0.181 (0.226) data 0.000 (0.048) loss 0.5126 (0.4755) acc 87.9630 (89.5394) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [40/51] time 0.168 (0.219) data 0.000 (0.042) loss 0.5291 (0.4824) acc 88.5000 (89.3692) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.170 (0.214) data 0.000 (0.037) loss 0.2983 (0.4787) acc 93.6274 (89.5840) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.172 (0.210) data 0.000 (0.033) loss 0.5505 (0.4793) acc 87.5000 (89.5033) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.132  alpha2: 0.022 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [46/50] batch [5/51] time 0.177 (0.495) data 0.000 (0.316) loss 0.5197 (0.5263) acc 87.2340 (88.7576) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [10/51] time 0.164 (0.334) data 0.000 (0.158) loss 0.8520 (0.5492) acc 80.8511 (88.3366) lr 7.0224e-05 eta 0:01:21
epoch [46/50] batch [15/51] time 0.186 (0.282) data 0.000 (0.106) loss 0.4518 (0.5157) acc 90.6250 (89.1779) lr 7.0224e-05 eta 0:01:07
epoch [46/50] batch [20/51] time 0.193 (0.255) data 0.000 (0.079) loss 0.3876 (0.5232) acc 90.7407 (89.2723) lr 7.0224e-05 eta 0:00:59
epoch [46/50] batch [25/51] time 0.178 (0.239) data 0.000 (0.063) loss 0.3947 (0.5205) acc 92.4528 (89.1625) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.175 (0.229) data 0.000 (0.053) loss 0.3748 (0.5019) acc 94.1176 (89.3559) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.184 (0.223) data 0.000 (0.046) loss 0.6243 (0.5102) acc 87.5000 (88.9818) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.040) loss 0.4892 (0.5094) acc 89.5000 (88.9153) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.168 (0.212) data 0.000 (0.036) loss 0.6578 (0.5071) acc 88.0000 (89.0398) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.161 (0.207) data 0.000 (0.032) loss 0.3307 (0.5024) acc 93.6170 (89.1268) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.130  alpha2: 0.021 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [47/50] batch [5/51] time 0.172 (0.524) data 0.000 (0.344) loss 0.3611 (0.4521) acc 93.1373 (90.7754) lr 4.8943e-05 eta 0:01:44
epoch [47/50] batch [10/51] time 0.170 (0.348) data 0.000 (0.172) loss 0.2782 (0.4446) acc 95.0980 (90.7415) lr 4.8943e-05 eta 0:01:07
epoch [47/50] batch [15/51] time 0.174 (0.291) data 0.000 (0.115) loss 0.4063 (0.4610) acc 92.7885 (90.3055) lr 4.8943e-05 eta 0:00:54
epoch [47/50] batch [20/51] time 0.178 (0.261) data 0.000 (0.086) loss 0.3166 (0.4690) acc 94.9074 (89.8273) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [25/51] time 0.171 (0.244) data 0.000 (0.069) loss 0.5173 (0.4650) acc 91.1765 (90.2030) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [30/51] time 0.184 (0.233) data 0.000 (0.058) loss 0.4349 (0.4665) acc 93.3673 (90.0114) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.178 (0.225) data 0.000 (0.049) loss 0.3461 (0.4692) acc 93.0556 (90.1003) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.166 (0.219) data 0.000 (0.043) loss 0.3270 (0.4728) acc 93.3673 (89.9385) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.164 (0.213) data 0.000 (0.038) loss 0.5103 (0.4770) acc 92.7083 (89.9282) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.168 (0.209) data 0.000 (0.035) loss 0.5849 (0.4755) acc 85.0000 (89.8547) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.130  alpha2: 0.021 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [48/50] batch [5/51] time 0.179 (0.483) data 0.001 (0.307) loss 0.5324 (0.4353) acc 89.4231 (92.7310) lr 3.1417e-05 eta 0:01:11
epoch [48/50] batch [10/51] time 0.184 (0.331) data 0.000 (0.154) loss 0.4780 (0.4583) acc 87.2340 (90.3348) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.174 (0.277) data 0.000 (0.103) loss 0.6307 (0.4637) acc 87.5000 (90.4652) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [20/51] time 0.174 (0.253) data 0.001 (0.077) loss 0.4778 (0.4716) acc 87.5000 (89.4638) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.184 (0.239) data 0.000 (0.062) loss 0.3771 (0.4715) acc 94.6429 (89.9864) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.181 (0.229) data 0.000 (0.051) loss 0.5661 (0.4847) acc 87.7273 (89.9470) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.164 (0.222) data 0.000 (0.044) loss 0.6410 (0.4872) acc 85.6383 (89.7276) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.168 (0.216) data 0.000 (0.039) loss 0.6690 (0.4875) acc 88.0000 (89.7843) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.164 (0.211) data 0.000 (0.034) loss 0.4774 (0.4872) acc 92.1875 (89.7903) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.175 (0.207) data 0.000 (0.031) loss 0.4041 (0.4772) acc 91.0377 (90.0391) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.130  alpha2: 0.019 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [49/50] batch [5/51] time 0.174 (0.462) data 0.000 (0.278) loss 0.4901 (0.3975) acc 86.5385 (91.7415) lr 1.7713e-05 eta 0:00:44
epoch [49/50] batch [10/51] time 0.180 (0.320) data 0.000 (0.139) loss 0.2991 (0.3926) acc 91.6667 (91.2475) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.209 (0.273) data 0.000 (0.093) loss 0.2970 (0.4262) acc 95.6140 (90.6137) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [20/51] time 0.179 (0.248) data 0.000 (0.070) loss 0.6042 (0.4541) acc 86.1702 (90.2171) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.177 (0.234) data 0.000 (0.056) loss 0.4406 (0.4436) acc 89.7959 (90.4679) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.176 (0.224) data 0.000 (0.047) loss 0.5194 (0.4668) acc 89.3617 (89.8710) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.168 (0.217) data 0.000 (0.040) loss 0.5734 (0.4794) acc 86.7347 (89.3636) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.180 (0.212) data 0.000 (0.035) loss 0.3245 (0.4809) acc 96.3636 (89.5718) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.166 (0.207) data 0.000 (0.031) loss 0.6104 (0.4848) acc 89.7959 (89.4368) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.165 (0.203) data 0.000 (0.028) loss 0.6588 (0.4844) acc 86.7347 (89.4021) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.131  alpha2: 0.019 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.26 <<<
epoch [50/50] batch [5/51] time 0.179 (0.459) data 0.001 (0.270) loss 0.4411 (0.4530) acc 91.9811 (89.5127) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.196 (0.318) data 0.000 (0.135) loss 0.5371 (0.5018) acc 90.6250 (89.2634) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.165 (0.271) data 0.000 (0.090) loss 0.4822 (0.4957) acc 88.5417 (89.1941) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.169 (0.247) data 0.000 (0.068) loss 0.3832 (0.5000) acc 90.8163 (89.5704) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.163 (0.232) data 0.000 (0.054) loss 0.5822 (0.4995) acc 87.7660 (89.9967) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.172 (0.223) data 0.000 (0.045) loss 0.5481 (0.4925) acc 86.9565 (89.9992) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.171 (0.217) data 0.000 (0.039) loss 0.5702 (0.4877) acc 85.7843 (90.0329) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.176 (0.211) data 0.000 (0.034) loss 0.3998 (0.4877) acc 95.2830 (89.9588) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.161 (0.207) data 0.000 (0.030) loss 0.5222 (0.4884) acc 89.8936 (89.7955) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.166 (0.203) data 0.000 (0.027) loss 0.4633 (0.4902) acc 89.2857 (89.8552) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.29, 0.21, 0.19, 0.17, 0.17, 0.16, 0.17, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.15, 0.16, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16]
* matched noise rate: [0.11, 0.06, 0.05, 0.05, 0.06, 0.06, 0.06, 0.07, 0.08, 0.07, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07, 0.08, 0.08, 0.09, 0.08, 0.08, 0.08, 0.09, 0.08, 0.08, 0.08, 0.08, 0.09, 0.09, 0.09, 0.09, 0.08, 0.09, 0.09, 0.09, 0.09, 0.09]
* unmatched noise rate: [0.52, 0.33, 0.3, 0.28, 0.31, 0.28, 0.29, 0.27, 0.27, 0.27, 0.26, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.26, 0.26, 0.25, 0.25, 0.27, 0.26, 0.27, 0.27, 0.26, 0.26, 0.26, 0.26, 0.26, 0.27, 0.27, 0.26, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:03<01:13,  3.07s/it] 12%|█▏        | 3/25 [00:03<00:18,  1.16it/s] 20%|██        | 5/25 [00:03<00:09,  2.18it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.35it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.62it/s] 44%|████▍     | 11/25 [00:03<00:02,  5.91it/s] 52%|█████▏    | 13/25 [00:04<00:01,  7.16it/s] 60%|██████    | 15/25 [00:04<00:01,  8.28it/s] 68%|██████▊   | 17/25 [00:04<00:00,  9.14it/s] 76%|███████▌  | 19/25 [00:04<00:00,  9.96it/s] 84%|████████▍ | 21/25 [00:04<00:00, 10.51it/s] 92%|█████████▏| 23/25 [00:04<00:00, 11.04it/s]100%|██████████| 25/25 [00:05<00:00,  7.46it/s]100%|██████████| 25/25 [00:05<00:00,  4.50it/s]
=> result
* total: 2,463
* correct: 2,010
* accuracy: 81.6%
* error: 18.4%
* macro_f1: 79.7%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 16	acc: 88.9%
* class: 2 (canterbury bells)	total: 12	correct: 7	acc: 58.3%
* class: 3 (sweet pea)	total: 17	correct: 11	acc: 64.7%
* class: 4 (english marigold)	total: 20	correct: 12	acc: 60.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 23	acc: 88.5%
* class: 11 (colt's foot)	total: 26	correct: 18	acc: 69.2%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 21	acc: 84.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 11	acc: 84.6%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 11	acc: 91.7%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 20	acc: 76.9%
* class: 30 (carnation)	total: 16	correct: 11	acc: 68.8%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 20	acc: 90.9%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 16	acc: 94.1%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 17	acc: 85.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 2	acc: 16.7%
* class: 45 (wallflower)	total: 59	correct: 54	acc: 91.5%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 26	acc: 92.9%
* class: 50 (petunia)	total: 77	correct: 39	acc: 50.6%
* class: 51 (wild pansy)	total: 26	correct: 24	acc: 92.3%
* class: 52 (primula)	total: 28	correct: 26	acc: 92.9%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 20	acc: 95.2%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 17	acc: 85.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 18	acc: 94.7%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 14	acc: 87.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 21	acc: 91.3%
* class: 71 (azalea)	total: 29	correct: 23	acc: 79.3%
* class: 72 (water lily)	total: 58	correct: 56	acc: 96.6%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 30	acc: 93.8%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 40	acc: 95.2%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 32	acc: 100.0%
* class: 80 (frangipani)	total: 50	correct: 49	acc: 98.0%
* class: 81 (clematis)	total: 34	correct: 34	acc: 100.0%
* class: 82 (hibiscus)	total: 39	correct: 36	acc: 92.3%
* class: 83 (columbine)	total: 26	correct: 21	acc: 80.8%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 33	acc: 71.7%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 21	acc: 84.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 49	acc: 100.0%
* class: 94 (bougainvillea)	total: 38	correct: 35	acc: 92.1%
* class: 95 (camellia)	total: 27	correct: 21	acc: 77.8%
* class: 96 (mallow)	total: 20	correct: 9	acc: 45.0%
* class: 97 (mexican petunia)	total: 25	correct: 15	acc: 60.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 9	acc: 52.9%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 83.3%
Elapsed: 0:28:39
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_8-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.265 (1.132) data 0.000 (0.355) loss 4.5669 (4.7351) acc 6.2500 (3.1250) lr 1.0000e-05 eta 0:48:01
epoch [1/50] batch [10/51] time 0.274 (0.699) data 0.000 (0.177) loss 4.3845 (4.6373) acc 6.2500 (5.6250) lr 1.0000e-05 eta 0:29:36
epoch [1/50] batch [15/51] time 0.264 (0.555) data 0.000 (0.118) loss 4.2365 (4.6030) acc 9.3750 (6.0417) lr 1.0000e-05 eta 0:23:26
epoch [1/50] batch [20/51] time 0.264 (0.483) data 0.000 (0.089) loss 4.4155 (4.5831) acc 15.6250 (6.7188) lr 1.0000e-05 eta 0:20:21
epoch [1/50] batch [25/51] time 0.263 (0.440) data 0.000 (0.071) loss 4.3005 (4.5020) acc 15.6250 (9.1250) lr 1.0000e-05 eta 0:18:31
epoch [1/50] batch [30/51] time 0.264 (0.411) data 0.000 (0.059) loss 4.3613 (4.4848) acc 18.7500 (9.4792) lr 1.0000e-05 eta 0:17:15
epoch [1/50] batch [35/51] time 0.263 (0.390) data 0.000 (0.051) loss 4.3047 (4.4962) acc 9.3750 (9.8214) lr 1.0000e-05 eta 0:16:21
epoch [1/50] batch [40/51] time 0.261 (0.374) data 0.000 (0.045) loss 4.5983 (4.4726) acc 9.3750 (9.9219) lr 1.0000e-05 eta 0:15:38
epoch [1/50] batch [45/51] time 0.261 (0.362) data 0.000 (0.040) loss 4.6625 (4.4541) acc 9.3750 (10.7639) lr 1.0000e-05 eta 0:15:05
epoch [1/50] batch [50/51] time 0.261 (0.351) data 0.000 (0.036) loss 5.0208 (4.4622) acc 6.2500 (11.0625) lr 1.0000e-05 eta 0:14:38
epoch [2/50] batch [5/51] time 0.262 (0.530) data 0.000 (0.243) loss 4.5186 (4.3065) acc 28.1250 (18.1250) lr 2.0000e-03 eta 0:22:00
epoch [2/50] batch [10/51] time 0.265 (0.399) data 0.000 (0.121) loss 4.0188 (4.2373) acc 21.8750 (17.8125) lr 2.0000e-03 eta 0:16:32
epoch [2/50] batch [15/51] time 0.262 (0.354) data 0.000 (0.081) loss 4.2885 (4.1948) acc 21.8750 (18.5417) lr 2.0000e-03 eta 0:14:38
epoch [2/50] batch [20/51] time 0.285 (0.334) data 0.000 (0.061) loss 3.9025 (4.1838) acc 34.3750 (20.0000) lr 2.0000e-03 eta 0:13:48
epoch [2/50] batch [25/51] time 0.270 (0.321) data 0.000 (0.049) loss 3.6912 (4.1814) acc 34.3750 (19.8750) lr 2.0000e-03 eta 0:13:14
epoch [2/50] batch [30/51] time 0.263 (0.312) data 0.000 (0.041) loss 3.9387 (4.1502) acc 28.1250 (21.0417) lr 2.0000e-03 eta 0:12:51
epoch [2/50] batch [35/51] time 0.268 (0.306) data 0.000 (0.035) loss 3.6865 (4.0943) acc 31.2500 (22.6786) lr 2.0000e-03 eta 0:12:34
epoch [2/50] batch [40/51] time 0.261 (0.301) data 0.000 (0.031) loss 4.1452 (4.1205) acc 15.6250 (22.5000) lr 2.0000e-03 eta 0:12:19
epoch [2/50] batch [45/51] time 0.263 (0.296) data 0.000 (0.027) loss 4.3604 (4.1215) acc 18.7500 (22.4306) lr 2.0000e-03 eta 0:12:07
epoch [2/50] batch [50/51] time 0.262 (0.293) data 0.000 (0.024) loss 3.9783 (4.1100) acc 28.1250 (22.7500) lr 2.0000e-03 eta 0:11:57
epoch [3/50] batch [5/51] time 0.272 (0.595) data 0.000 (0.316) loss 3.3443 (3.8855) acc 46.8750 (33.7500) lr 1.9980e-03 eta 0:24:13
epoch [3/50] batch [10/51] time 0.262 (0.432) data 0.000 (0.158) loss 4.1739 (3.9929) acc 25.0000 (28.4375) lr 1.9980e-03 eta 0:17:33
epoch [3/50] batch [15/51] time 0.280 (0.378) data 0.000 (0.106) loss 3.9681 (3.9784) acc 28.1250 (28.9583) lr 1.9980e-03 eta 0:15:18
epoch [3/50] batch [20/51] time 0.270 (0.349) data 0.000 (0.079) loss 3.9133 (3.9805) acc 34.3750 (28.7500) lr 1.9980e-03 eta 0:14:07
epoch [3/50] batch [25/51] time 0.272 (0.333) data 0.000 (0.063) loss 4.4793 (4.0109) acc 15.6250 (27.7500) lr 1.9980e-03 eta 0:13:26
epoch [3/50] batch [30/51] time 0.265 (0.321) data 0.000 (0.053) loss 3.5252 (3.9998) acc 25.0000 (27.7083) lr 1.9980e-03 eta 0:12:56
epoch [3/50] batch [35/51] time 0.262 (0.314) data 0.000 (0.045) loss 3.9403 (3.9994) acc 37.5000 (28.4821) lr 1.9980e-03 eta 0:12:36
epoch [3/50] batch [40/51] time 0.260 (0.307) data 0.000 (0.040) loss 4.0670 (3.9940) acc 25.0000 (28.6719) lr 1.9980e-03 eta 0:12:19
epoch [3/50] batch [45/51] time 0.260 (0.302) data 0.000 (0.035) loss 3.9640 (3.9911) acc 34.3750 (28.4722) lr 1.9980e-03 eta 0:12:05
epoch [3/50] batch [50/51] time 0.260 (0.298) data 0.000 (0.032) loss 3.6702 (3.9858) acc 37.5000 (28.5625) lr 1.9980e-03 eta 0:11:53
epoch [4/50] batch [5/51] time 0.274 (0.629) data 0.000 (0.346) loss 4.0873 (3.9021) acc 25.0000 (33.7500) lr 1.9921e-03 eta 0:25:03
epoch [4/50] batch [10/51] time 0.262 (0.447) data 0.000 (0.173) loss 4.0881 (3.9055) acc 37.5000 (33.7500) lr 1.9921e-03 eta 0:17:46
epoch [4/50] batch [15/51] time 0.266 (0.386) data 0.000 (0.116) loss 3.9256 (3.9767) acc 28.1250 (30.8333) lr 1.9921e-03 eta 0:15:20
epoch [4/50] batch [20/51] time 0.268 (0.356) data 0.000 (0.087) loss 3.4752 (3.9416) acc 43.7500 (30.9375) lr 1.9921e-03 eta 0:14:06
epoch [4/50] batch [25/51] time 0.262 (0.339) data 0.000 (0.069) loss 3.3298 (3.8736) acc 37.5000 (32.1250) lr 1.9921e-03 eta 0:13:24
epoch [4/50] batch [30/51] time 0.263 (0.327) data 0.000 (0.058) loss 4.6664 (3.9076) acc 15.6250 (31.4583) lr 1.9921e-03 eta 0:12:53
epoch [4/50] batch [35/51] time 0.265 (0.318) data 0.000 (0.050) loss 4.1098 (3.9428) acc 28.1250 (31.1607) lr 1.9921e-03 eta 0:12:31
epoch [4/50] batch [40/51] time 0.261 (0.311) data 0.000 (0.043) loss 4.1750 (3.9824) acc 37.5000 (30.5469) lr 1.9921e-03 eta 0:12:13
epoch [4/50] batch [45/51] time 0.261 (0.306) data 0.000 (0.039) loss 3.6550 (3.9735) acc 34.3750 (30.8333) lr 1.9921e-03 eta 0:11:59
epoch [4/50] batch [50/51] time 0.260 (0.301) data 0.000 (0.035) loss 3.6151 (3.9590) acc 40.6250 (31.0000) lr 1.9921e-03 eta 0:11:46
epoch [5/50] batch [5/51] time 0.269 (0.533) data 0.000 (0.240) loss 4.2559 (3.8530) acc 31.2500 (33.7500) lr 1.9823e-03 eta 0:20:48
epoch [5/50] batch [10/51] time 0.266 (0.400) data 0.000 (0.120) loss 4.0327 (4.0441) acc 25.0000 (29.3750) lr 1.9823e-03 eta 0:15:33
epoch [5/50] batch [15/51] time 0.263 (0.354) data 0.000 (0.080) loss 3.9038 (3.9718) acc 37.5000 (30.2083) lr 1.9823e-03 eta 0:13:44
epoch [5/50] batch [20/51] time 0.262 (0.331) data 0.000 (0.060) loss 3.8610 (3.9562) acc 37.5000 (30.0000) lr 1.9823e-03 eta 0:12:50
epoch [5/50] batch [25/51] time 0.262 (0.318) data 0.000 (0.048) loss 3.8911 (3.9471) acc 34.3750 (30.6250) lr 1.9823e-03 eta 0:12:18
epoch [5/50] batch [30/51] time 0.263 (0.310) data 0.000 (0.040) loss 4.1692 (3.9427) acc 21.8750 (30.4167) lr 1.9823e-03 eta 0:11:56
epoch [5/50] batch [35/51] time 0.274 (0.304) data 0.000 (0.034) loss 4.0564 (3.9020) acc 25.0000 (31.8750) lr 1.9823e-03 eta 0:11:41
epoch [5/50] batch [40/51] time 0.262 (0.298) data 0.000 (0.030) loss 3.9053 (3.9255) acc 34.3750 (31.6406) lr 1.9823e-03 eta 0:11:28
epoch [5/50] batch [45/51] time 0.263 (0.294) data 0.000 (0.027) loss 4.0270 (3.9114) acc 21.8750 (31.7361) lr 1.9823e-03 eta 0:11:17
epoch [5/50] batch [50/51] time 0.262 (0.291) data 0.000 (0.024) loss 4.1605 (3.9276) acc 28.1250 (31.1875) lr 1.9823e-03 eta 0:11:08
epoch [6/50] batch [5/51] time 0.273 (0.620) data 0.000 (0.345) loss 3.8295 (3.9709) acc 31.2500 (26.2500) lr 1.9686e-03 eta 0:23:39
epoch [6/50] batch [10/51] time 0.280 (0.446) data 0.000 (0.173) loss 3.5562 (3.7718) acc 37.5000 (32.8125) lr 1.9686e-03 eta 0:16:59
epoch [6/50] batch [15/51] time 0.265 (0.388) data 0.000 (0.115) loss 3.5869 (3.7923) acc 37.5000 (32.7083) lr 1.9686e-03 eta 0:14:43
epoch [6/50] batch [20/51] time 0.267 (0.358) data 0.000 (0.087) loss 4.1923 (3.7995) acc 25.0000 (33.1250) lr 1.9686e-03 eta 0:13:33
epoch [6/50] batch [25/51] time 0.280 (0.341) data 0.000 (0.069) loss 3.6336 (3.8405) acc 31.2500 (32.7500) lr 1.9686e-03 eta 0:12:54
epoch [6/50] batch [30/51] time 0.275 (0.330) data 0.000 (0.058) loss 4.0353 (3.8840) acc 34.3750 (32.8125) lr 1.9686e-03 eta 0:12:27
epoch [6/50] batch [35/51] time 0.281 (0.322) data 0.000 (0.050) loss 3.6913 (3.8927) acc 31.2500 (32.4107) lr 1.9686e-03 eta 0:12:06
epoch [6/50] batch [40/51] time 0.261 (0.315) data 0.000 (0.043) loss 3.8636 (3.8947) acc 34.3750 (31.9531) lr 1.9686e-03 eta 0:11:49
epoch [6/50] batch [45/51] time 0.261 (0.309) data 0.000 (0.039) loss 3.7057 (3.9016) acc 46.8750 (31.8750) lr 1.9686e-03 eta 0:11:34
epoch [6/50] batch [50/51] time 0.261 (0.304) data 0.000 (0.035) loss 3.4960 (3.8810) acc 40.6250 (32.1250) lr 1.9686e-03 eta 0:11:22
epoch [7/50] batch [5/51] time 0.278 (0.641) data 0.000 (0.359) loss 3.8495 (3.9649) acc 34.3750 (31.2500) lr 1.9511e-03 eta 0:23:54
epoch [7/50] batch [10/51] time 0.274 (0.456) data 0.000 (0.179) loss 4.1453 (3.9768) acc 34.3750 (30.3125) lr 1.9511e-03 eta 0:16:57
epoch [7/50] batch [15/51] time 0.275 (0.395) data 0.000 (0.120) loss 4.4402 (3.9198) acc 12.5000 (30.6250) lr 1.9511e-03 eta 0:14:40
epoch [7/50] batch [20/51] time 0.269 (0.364) data 0.000 (0.090) loss 3.8911 (3.8804) acc 28.1250 (31.5625) lr 1.9511e-03 eta 0:13:28
epoch [7/50] batch [25/51] time 0.273 (0.344) data 0.000 (0.072) loss 4.3008 (3.8974) acc 15.6250 (30.5000) lr 1.9511e-03 eta 0:12:44
epoch [7/50] batch [30/51] time 0.277 (0.332) data 0.013 (0.061) loss 4.3900 (3.8845) acc 12.5000 (31.2500) lr 1.9511e-03 eta 0:12:14
epoch [7/50] batch [35/51] time 0.275 (0.323) data 0.000 (0.052) loss 3.8744 (3.8992) acc 34.3750 (30.6250) lr 1.9511e-03 eta 0:11:53
epoch [7/50] batch [40/51] time 0.261 (0.315) data 0.000 (0.046) loss 3.9293 (3.8847) acc 21.8750 (31.1719) lr 1.9511e-03 eta 0:11:34
epoch [7/50] batch [45/51] time 0.261 (0.309) data 0.000 (0.041) loss 3.6377 (3.8631) acc 40.6250 (31.6667) lr 1.9511e-03 eta 0:11:20
epoch [7/50] batch [50/51] time 0.262 (0.305) data 0.000 (0.037) loss 3.6626 (3.8554) acc 34.3750 (31.8750) lr 1.9511e-03 eta 0:11:08
epoch [8/50] batch [5/51] time 0.264 (0.604) data 0.000 (0.318) loss 3.9139 (3.7522) acc 37.5000 (35.0000) lr 1.9298e-03 eta 0:22:02
epoch [8/50] batch [10/51] time 0.289 (0.438) data 0.000 (0.159) loss 3.7718 (3.8534) acc 28.1250 (32.1875) lr 1.9298e-03 eta 0:15:55
epoch [8/50] batch [15/51] time 0.264 (0.381) data 0.000 (0.106) loss 4.0912 (3.8881) acc 37.5000 (31.6667) lr 1.9298e-03 eta 0:13:49
epoch [8/50] batch [20/51] time 0.263 (0.352) data 0.000 (0.080) loss 3.9874 (3.8500) acc 37.5000 (31.8750) lr 1.9298e-03 eta 0:12:45
epoch [8/50] batch [25/51] time 0.263 (0.336) data 0.000 (0.064) loss 4.1914 (3.8405) acc 18.7500 (32.1250) lr 1.9298e-03 eta 0:12:07
epoch [8/50] batch [30/51] time 0.263 (0.325) data 0.000 (0.053) loss 3.8994 (3.8300) acc 34.3750 (32.1875) lr 1.9298e-03 eta 0:11:42
epoch [8/50] batch [35/51] time 0.270 (0.317) data 0.000 (0.046) loss 3.7525 (3.7982) acc 34.3750 (33.2143) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [40/51] time 0.262 (0.310) data 0.000 (0.040) loss 3.6516 (3.8056) acc 37.5000 (33.4375) lr 1.9298e-03 eta 0:11:06
epoch [8/50] batch [45/51] time 0.260 (0.304) data 0.000 (0.036) loss 3.9814 (3.8127) acc 18.7500 (32.6389) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [50/51] time 0.260 (0.300) data 0.000 (0.032) loss 4.3248 (3.8204) acc 31.2500 (33.1250) lr 1.9298e-03 eta 0:10:42
epoch [9/50] batch [5/51] time 0.264 (0.570) data 0.000 (0.286) loss 3.9286 (3.6529) acc 31.2500 (34.3750) lr 1.9048e-03 eta 0:20:18
epoch [9/50] batch [10/51] time 0.264 (0.420) data 0.000 (0.143) loss 3.9355 (3.7783) acc 34.3750 (32.8125) lr 1.9048e-03 eta 0:14:55
epoch [9/50] batch [15/51] time 0.263 (0.371) data 0.000 (0.095) loss 3.4818 (3.7933) acc 46.8750 (33.5417) lr 1.9048e-03 eta 0:13:08
epoch [9/50] batch [20/51] time 0.269 (0.344) data 0.000 (0.072) loss 3.9043 (3.8450) acc 31.2500 (32.0312) lr 1.9048e-03 eta 0:12:10
epoch [9/50] batch [25/51] time 0.277 (0.330) data 0.000 (0.057) loss 3.2907 (3.7803) acc 46.8750 (33.2500) lr 1.9048e-03 eta 0:11:38
epoch [9/50] batch [30/51] time 0.267 (0.319) data 0.000 (0.048) loss 3.3137 (3.7942) acc 46.8750 (33.2292) lr 1.9048e-03 eta 0:11:14
epoch [9/50] batch [35/51] time 0.278 (0.312) data 0.000 (0.041) loss 2.8323 (3.7541) acc 62.5000 (34.0179) lr 1.9048e-03 eta 0:10:57
epoch [9/50] batch [40/51] time 0.262 (0.306) data 0.000 (0.036) loss 3.4143 (3.7594) acc 40.6250 (33.9062) lr 1.9048e-03 eta 0:10:42
epoch [9/50] batch [45/51] time 0.262 (0.301) data 0.000 (0.032) loss 3.8557 (3.7829) acc 34.3750 (33.8194) lr 1.9048e-03 eta 0:10:31
epoch [9/50] batch [50/51] time 0.261 (0.297) data 0.000 (0.029) loss 4.1332 (3.8026) acc 28.1250 (33.5625) lr 1.9048e-03 eta 0:10:21
epoch [10/50] batch [5/51] time 0.276 (0.542) data 0.000 (0.261) loss 3.9695 (3.9295) acc 34.3750 (31.2500) lr 1.8763e-03 eta 0:18:51
epoch [10/50] batch [10/51] time 0.276 (0.408) data 0.000 (0.131) loss 3.6498 (3.8109) acc 34.3750 (33.7500) lr 1.8763e-03 eta 0:14:09
epoch [10/50] batch [15/51] time 0.273 (0.361) data 0.000 (0.088) loss 3.7121 (3.7663) acc 37.5000 (32.9167) lr 1.8763e-03 eta 0:12:30
epoch [10/50] batch [20/51] time 0.266 (0.338) data 0.000 (0.066) loss 3.7523 (3.7318) acc 50.0000 (34.3750) lr 1.8763e-03 eta 0:11:39
epoch [10/50] batch [25/51] time 0.264 (0.324) data 0.000 (0.053) loss 4.0197 (3.7879) acc 28.1250 (34.0000) lr 1.8763e-03 eta 0:11:08
epoch [10/50] batch [30/51] time 0.262 (0.314) data 0.000 (0.044) loss 3.8506 (3.7977) acc 25.0000 (33.7500) lr 1.8763e-03 eta 0:10:46
epoch [10/50] batch [35/51] time 0.269 (0.307) data 0.000 (0.038) loss 3.7978 (3.7769) acc 37.5000 (34.1071) lr 1.8763e-03 eta 0:10:31
epoch [10/50] batch [40/51] time 0.260 (0.301) data 0.000 (0.033) loss 3.4024 (3.7892) acc 34.3750 (33.8281) lr 1.8763e-03 eta 0:10:18
epoch [10/50] batch [45/51] time 0.260 (0.297) data 0.000 (0.029) loss 3.2610 (3.7959) acc 43.7500 (33.8194) lr 1.8763e-03 eta 0:10:07
epoch [10/50] batch [50/51] time 0.260 (0.293) data 0.000 (0.027) loss 3.5811 (3.7937) acc 40.6250 (33.6250) lr 1.8763e-03 eta 0:09:58
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.699  alpha2: 0.314 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.38 <<<
epoch [11/50] batch [5/51] time 0.870 (1.017) data 0.000 (0.325) loss 2.2355 (2.3594) acc 72.0000 (69.1190) lr 1.8443e-03 eta 0:34:29
epoch [11/50] batch [10/51] time 0.163 (0.843) data 0.000 (0.163) loss 2.1027 (2.2391) acc 68.4783 (68.8591) lr 1.8443e-03 eta 0:28:30
epoch [11/50] batch [15/51] time 0.770 (0.657) data 0.000 (0.109) loss 1.7500 (2.1664) acc 63.5417 (67.1356) lr 1.8443e-03 eta 0:22:09
epoch [11/50] batch [20/51] time 0.165 (0.561) data 0.000 (0.082) loss 2.1438 (2.1372) acc 71.8750 (66.4879) lr 1.8443e-03 eta 0:18:52
epoch [11/50] batch [25/51] time 0.155 (0.506) data 0.000 (0.065) loss 2.1188 (2.1292) acc 65.6977 (65.7427) lr 1.8443e-03 eta 0:16:59
epoch [11/50] batch [30/51] time 0.171 (0.449) data 0.000 (0.054) loss 2.0319 (2.1301) acc 64.2857 (65.8893) lr 1.8443e-03 eta 0:15:02
epoch [11/50] batch [35/51] time 0.170 (0.409) data 0.000 (0.047) loss 1.7454 (2.1143) acc 85.2041 (65.7902) lr 1.8443e-03 eta 0:13:39
epoch [11/50] batch [40/51] time 0.166 (0.379) data 0.000 (0.041) loss 1.8859 (2.1115) acc 66.3265 (65.8692) lr 1.8443e-03 eta 0:12:37
epoch [11/50] batch [45/51] time 0.147 (0.354) data 0.000 (0.036) loss 2.4490 (2.0954) acc 59.1463 (66.0342) lr 1.8443e-03 eta 0:11:47
epoch [11/50] batch [50/51] time 0.170 (0.335) data 0.000 (0.033) loss 1.9347 (2.0992) acc 68.1373 (65.7199) lr 1.8443e-03 eta 0:11:06
>>> alpha1: 0.560  alpha2: 0.232 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [12/50] batch [5/51] time 0.166 (0.606) data 0.000 (0.307) loss 1.4344 (1.2196) acc 75.0000 (77.4095) lr 1.8090e-03 eta 0:20:02
epoch [12/50] batch [10/51] time 0.163 (0.385) data 0.000 (0.153) loss 1.2750 (1.3156) acc 75.0000 (73.9720) lr 1.8090e-03 eta 0:12:41
epoch [12/50] batch [15/51] time 0.168 (0.312) data 0.000 (0.102) loss 1.2580 (1.3063) acc 73.9796 (73.7840) lr 1.8090e-03 eta 0:10:15
epoch [12/50] batch [20/51] time 0.161 (0.277) data 0.000 (0.078) loss 1.4440 (1.3160) acc 73.3696 (73.8372) lr 1.8090e-03 eta 0:09:04
epoch [12/50] batch [25/51] time 0.164 (0.253) data 0.000 (0.062) loss 1.0764 (1.2976) acc 76.5957 (73.7893) lr 1.8090e-03 eta 0:08:17
epoch [12/50] batch [30/51] time 0.156 (0.238) data 0.000 (0.052) loss 1.3305 (1.2775) acc 72.7273 (74.3837) lr 1.8090e-03 eta 0:07:46
epoch [12/50] batch [35/51] time 0.176 (0.246) data 0.000 (0.044) loss 1.0468 (1.3261) acc 86.7347 (73.7709) lr 1.8090e-03 eta 0:08:01
epoch [12/50] batch [40/51] time 0.173 (0.236) data 0.000 (0.039) loss 1.0913 (1.3060) acc 75.9615 (73.8472) lr 1.8090e-03 eta 0:07:40
epoch [12/50] batch [45/51] time 0.166 (0.228) data 0.000 (0.035) loss 1.1716 (1.2946) acc 76.0204 (73.8703) lr 1.8090e-03 eta 0:07:22
epoch [12/50] batch [50/51] time 0.164 (0.221) data 0.000 (0.031) loss 1.0625 (1.2776) acc 81.2500 (74.2399) lr 1.8090e-03 eta 0:07:09
>>> alpha1: 0.446  alpha2: 0.161 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.34 <<<
epoch [13/50] batch [5/51] time 0.185 (0.530) data 0.000 (0.353) loss 1.1625 (1.0377) acc 71.3542 (77.8271) lr 1.7705e-03 eta 0:17:05
epoch [13/50] batch [10/51] time 0.175 (0.348) data 0.000 (0.177) loss 0.9847 (1.0362) acc 74.4445 (76.8471) lr 1.7705e-03 eta 0:11:11
epoch [13/50] batch [15/51] time 0.159 (0.288) data 0.000 (0.118) loss 1.2427 (1.0531) acc 75.5556 (76.4194) lr 1.7705e-03 eta 0:09:13
epoch [13/50] batch [20/51] time 0.159 (0.257) data 0.000 (0.088) loss 1.2864 (1.0212) acc 70.0000 (76.8521) lr 1.7705e-03 eta 0:08:13
epoch [13/50] batch [25/51] time 0.157 (0.240) data 0.000 (0.071) loss 1.1983 (1.0286) acc 72.7778 (76.2829) lr 1.7705e-03 eta 0:07:39
epoch [13/50] batch [30/51] time 0.804 (0.248) data 0.000 (0.059) loss 0.9265 (1.0063) acc 80.5556 (76.8520) lr 1.7705e-03 eta 0:07:54
epoch [13/50] batch [35/51] time 0.171 (0.236) data 0.000 (0.051) loss 0.7737 (1.0017) acc 82.8431 (77.2994) lr 1.7705e-03 eta 0:07:29
epoch [13/50] batch [40/51] time 0.156 (0.227) data 0.000 (0.044) loss 1.1872 (1.0026) acc 75.0000 (77.4332) lr 1.7705e-03 eta 0:07:10
epoch [13/50] batch [45/51] time 0.163 (0.220) data 0.000 (0.039) loss 0.9157 (0.9975) acc 75.0000 (77.6193) lr 1.7705e-03 eta 0:06:56
epoch [13/50] batch [50/51] time 0.165 (0.215) data 0.000 (0.036) loss 0.9990 (0.9959) acc 78.5714 (77.6047) lr 1.7705e-03 eta 0:06:45
>>> alpha1: 0.395  alpha2: 0.134 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.32 <<<
epoch [14/50] batch [5/51] time 0.169 (0.462) data 0.000 (0.291) loss 1.0581 (0.9659) acc 71.3542 (77.0844) lr 1.7290e-03 eta 0:14:29
epoch [14/50] batch [10/51] time 0.163 (0.313) data 0.000 (0.146) loss 0.9103 (0.9536) acc 82.4468 (77.7108) lr 1.7290e-03 eta 0:09:48
epoch [14/50] batch [15/51] time 0.182 (0.268) data 0.000 (0.097) loss 0.5702 (0.9276) acc 89.7959 (78.8370) lr 1.7290e-03 eta 0:08:20
epoch [14/50] batch [20/51] time 0.159 (0.243) data 0.000 (0.073) loss 1.1505 (0.9254) acc 65.5556 (78.1840) lr 1.7290e-03 eta 0:07:34
epoch [14/50] batch [25/51] time 0.164 (0.228) data 0.001 (0.059) loss 0.7522 (0.9048) acc 82.9787 (78.8172) lr 1.7290e-03 eta 0:07:05
epoch [14/50] batch [30/51] time 0.166 (0.218) data 0.000 (0.049) loss 0.9775 (0.9067) acc 80.1020 (78.9375) lr 1.7290e-03 eta 0:06:43
epoch [14/50] batch [35/51] time 0.166 (0.210) data 0.000 (0.042) loss 1.1206 (0.9070) acc 76.0417 (79.4282) lr 1.7290e-03 eta 0:06:28
epoch [14/50] batch [40/51] time 0.627 (0.216) data 0.000 (0.037) loss 1.1215 (0.9118) acc 78.1250 (79.4515) lr 1.7290e-03 eta 0:06:38
epoch [14/50] batch [45/51] time 0.154 (0.210) data 0.000 (0.033) loss 0.8330 (0.9011) acc 82.3864 (79.8238) lr 1.7290e-03 eta 0:06:26
epoch [14/50] batch [50/51] time 0.157 (0.205) data 0.000 (0.029) loss 0.7723 (0.8913) acc 82.2222 (79.9352) lr 1.7290e-03 eta 0:06:16
>>> alpha1: 0.358  alpha2: 0.115 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.20 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.32 <<<
epoch [15/50] batch [5/51] time 0.167 (0.496) data 0.001 (0.320) loss 1.0060 (0.8144) acc 71.3542 (81.7997) lr 1.6845e-03 eta 0:15:07
epoch [15/50] batch [10/51] time 0.173 (0.333) data 0.000 (0.160) loss 1.0637 (0.8973) acc 72.8723 (79.0051) lr 1.6845e-03 eta 0:10:08
epoch [15/50] batch [15/51] time 0.166 (0.280) data 0.000 (0.107) loss 0.9266 (0.9246) acc 77.1739 (78.7497) lr 1.6845e-03 eta 0:08:29
epoch [15/50] batch [20/51] time 0.172 (0.252) data 0.000 (0.080) loss 0.7331 (0.9198) acc 80.3922 (78.9522) lr 1.6845e-03 eta 0:07:38
epoch [15/50] batch [25/51] time 0.162 (0.235) data 0.000 (0.064) loss 0.9169 (0.9113) acc 78.7234 (79.1726) lr 1.6845e-03 eta 0:07:05
epoch [15/50] batch [30/51] time 0.157 (0.225) data 0.000 (0.054) loss 0.9011 (0.8990) acc 83.8889 (79.4879) lr 1.6845e-03 eta 0:06:46
epoch [15/50] batch [35/51] time 0.170 (0.216) data 0.000 (0.046) loss 1.1587 (0.9008) acc 72.8261 (79.4849) lr 1.6845e-03 eta 0:06:29
epoch [15/50] batch [40/51] time 0.170 (0.210) data 0.000 (0.040) loss 0.8693 (0.8812) acc 82.3529 (79.9768) lr 1.6845e-03 eta 0:06:16
epoch [15/50] batch [45/51] time 0.156 (0.205) data 0.000 (0.036) loss 0.7757 (0.8668) acc 82.2222 (80.3817) lr 1.6845e-03 eta 0:06:06
epoch [15/50] batch [50/51] time 0.163 (0.201) data 0.000 (0.032) loss 0.8065 (0.8588) acc 81.7708 (80.7196) lr 1.6845e-03 eta 0:05:58
>>> alpha1: 0.265  alpha2: 0.042 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [16/50] batch [5/51] time 0.184 (0.454) data 0.000 (0.272) loss 0.7259 (0.7258) acc 82.0755 (83.8389) lr 1.6374e-03 eta 0:13:27
epoch [16/50] batch [10/51] time 0.168 (0.315) data 0.001 (0.136) loss 0.9038 (0.7780) acc 80.7292 (82.5501) lr 1.6374e-03 eta 0:09:19
epoch [16/50] batch [15/51] time 0.176 (0.268) data 0.000 (0.091) loss 0.7582 (0.7708) acc 83.4906 (82.3022) lr 1.6374e-03 eta 0:07:54
epoch [16/50] batch [20/51] time 0.160 (0.243) data 0.000 (0.068) loss 0.7434 (0.7849) acc 78.4091 (81.9604) lr 1.6374e-03 eta 0:07:08
epoch [16/50] batch [25/51] time 0.173 (0.230) data 0.000 (0.055) loss 0.9361 (0.7741) acc 77.5000 (82.2876) lr 1.6374e-03 eta 0:06:44
epoch [16/50] batch [30/51] time 0.176 (0.244) data 0.000 (0.046) loss 0.7680 (0.7637) acc 79.4118 (82.5913) lr 1.6374e-03 eta 0:07:08
epoch [16/50] batch [35/51] time 0.829 (0.252) data 0.000 (0.039) loss 0.7513 (0.7832) acc 80.9091 (81.9763) lr 1.6374e-03 eta 0:07:20
epoch [16/50] batch [40/51] time 0.160 (0.241) data 0.000 (0.034) loss 0.8743 (0.7972) acc 81.3830 (81.9058) lr 1.6374e-03 eta 0:07:00
epoch [16/50] batch [45/51] time 0.172 (0.232) data 0.000 (0.030) loss 0.9393 (0.8011) acc 79.8077 (81.8707) lr 1.6374e-03 eta 0:06:43
epoch [16/50] batch [50/51] time 0.168 (0.225) data 0.000 (0.027) loss 0.8296 (0.8071) acc 80.3922 (81.6286) lr 1.6374e-03 eta 0:06:30
>>> alpha1: 0.221  alpha2: 0.019 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [17/50] batch [5/51] time 0.177 (0.456) data 0.000 (0.279) loss 0.5998 (0.6754) acc 89.0625 (85.3709) lr 1.5878e-03 eta 0:13:07
epoch [17/50] batch [10/51] time 0.172 (0.319) data 0.001 (0.140) loss 0.6435 (0.6923) acc 89.5000 (85.1509) lr 1.5878e-03 eta 0:09:09
epoch [17/50] batch [15/51] time 0.170 (0.268) data 0.000 (0.093) loss 0.7075 (0.7308) acc 90.0000 (84.3670) lr 1.5878e-03 eta 0:07:41
epoch [17/50] batch [20/51] time 0.161 (0.245) data 0.000 (0.070) loss 0.7569 (0.7147) acc 81.9149 (84.4343) lr 1.5878e-03 eta 0:06:59
epoch [17/50] batch [25/51] time 0.172 (0.229) data 0.000 (0.056) loss 0.6552 (0.7433) acc 86.7647 (83.4760) lr 1.5878e-03 eta 0:06:31
epoch [17/50] batch [30/51] time 0.161 (0.219) data 0.000 (0.047) loss 0.6977 (0.7536) acc 81.9149 (83.0785) lr 1.5878e-03 eta 0:06:13
epoch [17/50] batch [35/51] time 0.182 (0.213) data 0.000 (0.040) loss 0.7896 (0.7503) acc 81.6327 (83.4075) lr 1.5878e-03 eta 0:06:02
epoch [17/50] batch [40/51] time 0.166 (0.208) data 0.000 (0.035) loss 0.7946 (0.7507) acc 84.0000 (83.6764) lr 1.5878e-03 eta 0:05:52
epoch [17/50] batch [45/51] time 0.177 (0.204) data 0.000 (0.031) loss 0.7092 (0.7400) acc 81.9444 (83.8820) lr 1.5878e-03 eta 0:05:43
epoch [17/50] batch [50/51] time 0.158 (0.200) data 0.000 (0.028) loss 1.2522 (0.7500) acc 73.9130 (83.6761) lr 1.5878e-03 eta 0:05:36
>>> alpha1: 0.201  alpha2: 0.017 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [18/50] batch [5/51] time 0.171 (0.465) data 0.000 (0.285) loss 0.5552 (0.6756) acc 86.7647 (85.2437) lr 1.5358e-03 eta 0:13:00
epoch [18/50] batch [10/51] time 0.171 (0.321) data 0.000 (0.143) loss 0.7613 (0.6787) acc 83.8542 (85.2835) lr 1.5358e-03 eta 0:08:56
epoch [18/50] batch [15/51] time 0.169 (0.271) data 0.000 (0.095) loss 0.7369 (0.6604) acc 84.5000 (85.8210) lr 1.5358e-03 eta 0:07:32
epoch [18/50] batch [20/51] time 0.180 (0.246) data 0.002 (0.072) loss 0.4599 (0.6646) acc 91.5094 (85.7665) lr 1.5358e-03 eta 0:06:49
epoch [18/50] batch [25/51] time 0.169 (0.231) data 0.000 (0.057) loss 0.6599 (0.6951) acc 86.0000 (84.7524) lr 1.5358e-03 eta 0:06:22
epoch [18/50] batch [30/51] time 0.168 (0.245) data 0.000 (0.048) loss 0.8170 (0.7005) acc 80.6122 (84.4433) lr 1.5358e-03 eta 0:06:44
epoch [18/50] batch [35/51] time 0.177 (0.234) data 0.000 (0.041) loss 0.6561 (0.7073) acc 87.0000 (84.1983) lr 1.5358e-03 eta 0:06:26
epoch [18/50] batch [40/51] time 0.162 (0.226) data 0.000 (0.036) loss 0.8921 (0.7112) acc 82.9787 (84.3315) lr 1.5358e-03 eta 0:06:11
epoch [18/50] batch [45/51] time 0.162 (0.219) data 0.000 (0.032) loss 0.7545 (0.7248) acc 82.4468 (84.0260) lr 1.5358e-03 eta 0:05:59
epoch [18/50] batch [50/51] time 0.173 (0.214) data 0.000 (0.029) loss 0.6726 (0.7181) acc 85.5769 (84.2998) lr 1.5358e-03 eta 0:05:49
>>> alpha1: 0.188  alpha2: 0.018 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.32 <<<
epoch [19/50] batch [5/51] time 0.171 (0.478) data 0.000 (0.296) loss 0.7695 (0.6854) acc 81.3726 (83.7955) lr 1.4818e-03 eta 0:12:57
epoch [19/50] batch [10/51] time 0.183 (0.326) data 0.000 (0.148) loss 0.5834 (0.6598) acc 88.5000 (85.7007) lr 1.4818e-03 eta 0:08:49
epoch [19/50] batch [15/51] time 0.181 (0.277) data 0.001 (0.099) loss 0.6367 (0.6610) acc 82.8704 (85.5533) lr 1.4818e-03 eta 0:07:28
epoch [19/50] batch [20/51] time 0.162 (0.251) data 0.000 (0.074) loss 0.8387 (0.6810) acc 79.7872 (84.5304) lr 1.4818e-03 eta 0:06:44
epoch [19/50] batch [25/51] time 0.167 (0.235) data 0.001 (0.060) loss 0.7772 (0.6930) acc 82.8125 (84.4438) lr 1.4818e-03 eta 0:06:18
epoch [19/50] batch [30/51] time 0.184 (0.226) data 0.000 (0.050) loss 0.6675 (0.7031) acc 83.9623 (84.2984) lr 1.4818e-03 eta 0:06:01
epoch [19/50] batch [35/51] time 0.178 (0.218) data 0.000 (0.043) loss 0.6665 (0.6991) acc 87.0370 (84.1171) lr 1.4818e-03 eta 0:05:48
epoch [19/50] batch [40/51] time 0.165 (0.213) data 0.000 (0.037) loss 0.4362 (0.6953) acc 90.3061 (84.2148) lr 1.4818e-03 eta 0:05:38
epoch [19/50] batch [45/51] time 0.167 (0.208) data 0.000 (0.033) loss 0.6200 (0.6951) acc 86.0000 (84.3691) lr 1.4818e-03 eta 0:05:29
epoch [19/50] batch [50/51] time 0.159 (0.203) data 0.000 (0.030) loss 0.7252 (0.7016) acc 82.6087 (84.3092) lr 1.4818e-03 eta 0:05:21
>>> alpha1: 0.178  alpha2: 0.024 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.31 <<<
epoch [20/50] batch [5/51] time 0.179 (0.506) data 0.000 (0.327) loss 0.6353 (0.7103) acc 84.7222 (85.6882) lr 1.4258e-03 eta 0:13:17
epoch [20/50] batch [10/51] time 0.169 (0.339) data 0.000 (0.164) loss 0.5703 (0.7585) acc 82.5000 (82.6916) lr 1.4258e-03 eta 0:08:52
epoch [20/50] batch [15/51] time 0.174 (0.287) data 0.000 (0.109) loss 0.5294 (0.6922) acc 87.0192 (84.3597) lr 1.4258e-03 eta 0:07:29
epoch [20/50] batch [20/51] time 0.165 (0.259) data 0.000 (0.082) loss 0.6499 (0.7008) acc 86.1702 (84.2366) lr 1.4258e-03 eta 0:06:44
epoch [20/50] batch [25/51] time 0.182 (0.244) data 0.000 (0.066) loss 0.8685 (0.6919) acc 78.0000 (84.4879) lr 1.4258e-03 eta 0:06:19
epoch [20/50] batch [30/51] time 0.174 (0.233) data 0.000 (0.055) loss 0.8347 (0.7011) acc 78.5000 (83.7339) lr 1.4258e-03 eta 0:06:00
epoch [20/50] batch [35/51] time 0.179 (0.224) data 0.000 (0.047) loss 0.5344 (0.6928) acc 88.2653 (84.0983) lr 1.4258e-03 eta 0:05:46
epoch [20/50] batch [40/51] time 0.169 (0.217) data 0.000 (0.041) loss 0.7078 (0.6940) acc 81.3726 (84.0900) lr 1.4258e-03 eta 0:05:34
epoch [20/50] batch [45/51] time 0.158 (0.211) data 0.000 (0.037) loss 0.7725 (0.7022) acc 83.6956 (83.8177) lr 1.4258e-03 eta 0:05:24
epoch [20/50] batch [50/51] time 0.173 (0.208) data 0.000 (0.033) loss 0.7768 (0.6994) acc 84.6154 (83.9372) lr 1.4258e-03 eta 0:05:18
>>> alpha1: 0.173  alpha2: 0.027 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.31 <<<
epoch [21/50] batch [5/51] time 0.172 (0.484) data 0.001 (0.304) loss 0.4830 (0.6209) acc 85.0000 (86.5366) lr 1.3681e-03 eta 0:12:17
epoch [21/50] batch [10/51] time 0.171 (0.331) data 0.000 (0.152) loss 0.9831 (0.6540) acc 77.4510 (85.7775) lr 1.3681e-03 eta 0:08:22
epoch [21/50] batch [15/51] time 0.164 (0.278) data 0.000 (0.102) loss 0.6521 (0.6532) acc 82.8125 (85.7426) lr 1.3681e-03 eta 0:07:01
epoch [21/50] batch [20/51] time 0.179 (0.254) data 0.000 (0.076) loss 0.5107 (0.6477) acc 85.2041 (85.5167) lr 1.3681e-03 eta 0:06:23
epoch [21/50] batch [25/51] time 0.176 (0.239) data 0.000 (0.061) loss 0.7094 (0.6507) acc 84.4340 (85.3674) lr 1.3681e-03 eta 0:05:59
epoch [21/50] batch [30/51] time 0.181 (0.229) data 0.000 (0.052) loss 0.6438 (0.6469) acc 84.2593 (85.5888) lr 1.3681e-03 eta 0:05:43
epoch [21/50] batch [35/51] time 0.161 (0.220) data 0.000 (0.044) loss 0.8041 (0.6556) acc 78.7234 (85.4001) lr 1.3681e-03 eta 0:05:29
epoch [21/50] batch [40/51] time 0.169 (0.215) data 0.001 (0.039) loss 0.7738 (0.6653) acc 92.6471 (85.4920) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.035) loss 0.5618 (0.6518) acc 85.7843 (85.8300) lr 1.3681e-03 eta 0:05:12
epoch [21/50] batch [50/51] time 0.169 (0.206) data 0.000 (0.031) loss 1.1019 (0.6585) acc 72.5490 (85.4379) lr 1.3681e-03 eta 0:05:05
>>> alpha1: 0.168  alpha2: 0.035 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [22/50] batch [5/51] time 0.184 (0.515) data 0.000 (0.330) loss 0.8281 (0.7597) acc 83.3333 (82.3005) lr 1.3090e-03 eta 0:12:38
epoch [22/50] batch [10/51] time 0.174 (0.347) data 0.000 (0.165) loss 0.7867 (0.7218) acc 84.6154 (83.5270) lr 1.3090e-03 eta 0:08:30
epoch [22/50] batch [15/51] time 0.183 (0.290) data 0.000 (0.110) loss 0.5700 (0.6970) acc 85.8491 (84.0069) lr 1.3090e-03 eta 0:07:04
epoch [22/50] batch [20/51] time 0.164 (0.260) data 0.000 (0.083) loss 0.6770 (0.6799) acc 88.0682 (84.9859) lr 1.3090e-03 eta 0:06:19
epoch [22/50] batch [25/51] time 0.183 (0.245) data 0.000 (0.066) loss 0.6373 (0.6490) acc 83.3333 (85.6687) lr 1.3090e-03 eta 0:05:55
epoch [22/50] batch [30/51] time 0.165 (0.233) data 0.000 (0.055) loss 0.5978 (0.6428) acc 85.8696 (85.6220) lr 1.3090e-03 eta 0:05:38
epoch [22/50] batch [35/51] time 0.192 (0.226) data 0.000 (0.047) loss 0.6270 (0.6480) acc 84.5745 (85.3978) lr 1.3090e-03 eta 0:05:25
epoch [22/50] batch [40/51] time 0.159 (0.219) data 0.000 (0.041) loss 0.7521 (0.6447) acc 84.7826 (85.4745) lr 1.3090e-03 eta 0:05:15
epoch [22/50] batch [45/51] time 0.166 (0.214) data 0.000 (0.037) loss 0.7484 (0.6572) acc 81.1225 (85.1245) lr 1.3090e-03 eta 0:05:06
epoch [22/50] batch [50/51] time 0.173 (0.209) data 0.000 (0.033) loss 0.8685 (0.6548) acc 83.1731 (85.2480) lr 1.3090e-03 eta 0:04:58
>>> alpha1: 0.163  alpha2: 0.034 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.30 <<<
epoch [23/50] batch [5/51] time 0.184 (0.510) data 0.001 (0.325) loss 0.6652 (0.6176) acc 86.0000 (86.5174) lr 1.2487e-03 eta 0:12:06
epoch [23/50] batch [10/51] time 0.166 (0.341) data 0.000 (0.163) loss 0.5934 (0.6121) acc 86.4583 (86.1001) lr 1.2487e-03 eta 0:08:03
epoch [23/50] batch [15/51] time 0.173 (0.287) data 0.000 (0.109) loss 0.4369 (0.7123) acc 90.1961 (85.5117) lr 1.2487e-03 eta 0:06:45
epoch [23/50] batch [20/51] time 0.191 (0.259) data 0.000 (0.082) loss 0.4318 (0.6781) acc 90.7407 (86.1095) lr 1.2487e-03 eta 0:06:05
epoch [23/50] batch [25/51] time 0.173 (0.243) data 0.000 (0.065) loss 0.4315 (0.6553) acc 91.1765 (86.6625) lr 1.2487e-03 eta 0:05:41
epoch [23/50] batch [30/51] time 0.165 (0.232) data 0.000 (0.054) loss 0.8317 (0.6628) acc 75.5208 (85.9068) lr 1.2487e-03 eta 0:05:23
epoch [23/50] batch [35/51] time 0.175 (0.223) data 0.000 (0.047) loss 0.8287 (0.6760) acc 81.7708 (85.2495) lr 1.2487e-03 eta 0:05:11
epoch [23/50] batch [40/51] time 0.177 (0.217) data 0.000 (0.041) loss 0.6725 (0.6757) acc 81.1321 (84.9920) lr 1.2487e-03 eta 0:05:01
epoch [23/50] batch [45/51] time 0.173 (0.212) data 0.000 (0.036) loss 0.6962 (0.6796) acc 86.0577 (84.9127) lr 1.2487e-03 eta 0:04:53
epoch [23/50] batch [50/51] time 0.168 (0.208) data 0.000 (0.033) loss 0.6007 (0.6750) acc 86.0000 (85.0454) lr 1.2487e-03 eta 0:04:46
>>> alpha1: 0.159  alpha2: 0.038 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [24/50] batch [5/51] time 0.180 (0.715) data 0.000 (0.389) loss 0.4221 (0.5607) acc 89.1509 (88.0200) lr 1.1874e-03 eta 0:16:21
epoch [24/50] batch [10/51] time 0.174 (0.444) data 0.001 (0.195) loss 0.5140 (0.5905) acc 91.1765 (87.4419) lr 1.1874e-03 eta 0:10:07
epoch [24/50] batch [15/51] time 0.211 (0.357) data 0.001 (0.130) loss 0.5654 (0.5960) acc 86.2745 (86.2394) lr 1.1874e-03 eta 0:08:06
epoch [24/50] batch [20/51] time 0.174 (0.313) data 0.000 (0.098) loss 0.5761 (0.5956) acc 89.5833 (86.4483) lr 1.1874e-03 eta 0:07:04
epoch [24/50] batch [25/51] time 0.178 (0.286) data 0.001 (0.078) loss 0.6173 (0.6059) acc 88.4615 (86.0919) lr 1.1874e-03 eta 0:06:26
epoch [24/50] batch [30/51] time 0.169 (0.268) data 0.000 (0.065) loss 0.6785 (0.6096) acc 83.5000 (85.8744) lr 1.1874e-03 eta 0:06:00
epoch [24/50] batch [35/51] time 0.169 (0.254) data 0.000 (0.056) loss 0.4737 (0.6069) acc 92.8571 (86.2528) lr 1.1874e-03 eta 0:05:40
epoch [24/50] batch [40/51] time 0.184 (0.245) data 0.000 (0.049) loss 0.5677 (0.6086) acc 86.6071 (86.1732) lr 1.1874e-03 eta 0:05:27
epoch [24/50] batch [45/51] time 0.166 (0.236) data 0.001 (0.044) loss 0.7906 (0.6265) acc 82.4468 (85.7861) lr 1.1874e-03 eta 0:05:14
epoch [24/50] batch [50/51] time 0.175 (0.230) data 0.000 (0.039) loss 0.5287 (0.6291) acc 91.8269 (85.8591) lr 1.1874e-03 eta 0:05:04
>>> alpha1: 0.157  alpha2: 0.038 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [25/50] batch [5/51] time 0.172 (0.483) data 0.000 (0.301) loss 0.7088 (0.5802) acc 81.1225 (86.5413) lr 1.1253e-03 eta 0:10:37
epoch [25/50] batch [10/51] time 0.187 (0.333) data 0.000 (0.151) loss 0.5131 (0.5634) acc 89.2857 (87.6920) lr 1.1253e-03 eta 0:07:17
epoch [25/50] batch [15/51] time 0.170 (0.281) data 0.001 (0.101) loss 0.5758 (0.5907) acc 86.0000 (87.0223) lr 1.1253e-03 eta 0:06:08
epoch [25/50] batch [20/51] time 0.172 (0.254) data 0.000 (0.076) loss 0.4827 (0.5940) acc 89.2157 (86.7970) lr 1.1253e-03 eta 0:05:32
epoch [25/50] batch [25/51] time 0.175 (0.238) data 0.000 (0.061) loss 0.4823 (0.5970) acc 90.3846 (86.5563) lr 1.1253e-03 eta 0:05:09
epoch [25/50] batch [30/51] time 0.169 (0.228) data 0.000 (0.051) loss 0.5414 (0.6027) acc 88.5000 (85.9776) lr 1.1253e-03 eta 0:04:55
epoch [25/50] batch [35/51] time 0.176 (0.220) data 0.000 (0.043) loss 0.6912 (0.5976) acc 84.1346 (85.9843) lr 1.1253e-03 eta 0:04:44
epoch [25/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.038) loss 0.7145 (0.5993) acc 81.9444 (86.1615) lr 1.1253e-03 eta 0:04:36
epoch [25/50] batch [45/51] time 0.171 (0.210) data 0.001 (0.034) loss 0.7509 (0.6061) acc 82.3529 (85.9917) lr 1.1253e-03 eta 0:04:29
epoch [25/50] batch [50/51] time 0.160 (0.206) data 0.000 (0.030) loss 0.6200 (0.6039) acc 85.3261 (86.0424) lr 1.1253e-03 eta 0:04:22
>>> alpha1: 0.153  alpha2: 0.035 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [26/50] batch [5/51] time 0.194 (0.505) data 0.000 (0.318) loss 0.8180 (0.5855) acc 78.8462 (88.4641) lr 1.0628e-03 eta 0:10:40
epoch [26/50] batch [10/51] time 0.201 (0.345) data 0.000 (0.159) loss 0.4559 (0.5762) acc 89.9038 (87.6062) lr 1.0628e-03 eta 0:07:16
epoch [26/50] batch [15/51] time 0.179 (0.289) data 0.001 (0.106) loss 0.5922 (0.5934) acc 88.2075 (87.3989) lr 1.0628e-03 eta 0:06:04
epoch [26/50] batch [20/51] time 0.178 (0.261) data 0.000 (0.080) loss 0.5374 (0.5888) acc 88.2075 (87.5250) lr 1.0628e-03 eta 0:05:27
epoch [26/50] batch [25/51] time 0.175 (0.244) data 0.000 (0.064) loss 0.4604 (0.5868) acc 88.2353 (87.5879) lr 1.0628e-03 eta 0:05:05
epoch [26/50] batch [30/51] time 0.185 (0.234) data 0.000 (0.053) loss 0.5880 (0.5942) acc 81.3636 (87.0225) lr 1.0628e-03 eta 0:04:51
epoch [26/50] batch [35/51] time 0.187 (0.226) data 0.000 (0.046) loss 0.6471 (0.5949) acc 85.0000 (86.7874) lr 1.0628e-03 eta 0:04:40
epoch [26/50] batch [40/51] time 0.170 (0.220) data 0.000 (0.040) loss 0.4482 (0.5982) acc 92.1569 (86.7321) lr 1.0628e-03 eta 0:04:31
epoch [26/50] batch [45/51] time 0.180 (0.214) data 0.000 (0.036) loss 0.6459 (0.5998) acc 85.1852 (86.6350) lr 1.0628e-03 eta 0:04:23
epoch [26/50] batch [50/51] time 0.171 (0.210) data 0.000 (0.032) loss 0.7218 (0.5962) acc 80.3922 (86.7131) lr 1.0628e-03 eta 0:04:17
>>> alpha1: 0.149  alpha2: 0.033 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [27/50] batch [5/51] time 0.170 (0.486) data 0.000 (0.310) loss 0.6635 (1.0155) acc 84.1837 (79.6330) lr 1.0000e-03 eta 0:09:52
epoch [27/50] batch [10/51] time 0.190 (0.335) data 0.000 (0.155) loss 0.6325 (0.8272) acc 78.9216 (82.0447) lr 1.0000e-03 eta 0:06:46
epoch [27/50] batch [15/51] time 0.164 (0.282) data 0.000 (0.104) loss 0.6945 (0.7171) acc 87.5000 (84.8796) lr 1.0000e-03 eta 0:05:41
epoch [27/50] batch [20/51] time 0.165 (0.254) data 0.000 (0.078) loss 0.5404 (0.6768) acc 85.4167 (85.3183) lr 1.0000e-03 eta 0:05:05
epoch [27/50] batch [25/51] time 0.189 (0.240) data 0.000 (0.062) loss 0.5646 (0.6767) acc 90.3509 (85.5357) lr 1.0000e-03 eta 0:04:47
epoch [27/50] batch [30/51] time 0.174 (0.230) data 0.000 (0.052) loss 0.5071 (0.6692) acc 90.3846 (85.7921) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [35/51] time 0.189 (0.222) data 0.000 (0.045) loss 0.4481 (0.6616) acc 88.4259 (85.7055) lr 1.0000e-03 eta 0:04:24
epoch [27/50] batch [40/51] time 0.171 (0.217) data 0.000 (0.039) loss 0.5580 (0.6548) acc 91.1765 (85.6451) lr 1.0000e-03 eta 0:04:16
epoch [27/50] batch [45/51] time 0.167 (0.211) data 0.000 (0.035) loss 0.3996 (0.6491) acc 90.8163 (85.6164) lr 1.0000e-03 eta 0:04:08
epoch [27/50] batch [50/51] time 0.170 (0.207) data 0.000 (0.031) loss 0.7410 (0.6445) acc 82.8431 (85.7442) lr 1.0000e-03 eta 0:04:02
>>> alpha1: 0.148  alpha2: 0.037 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [28/50] batch [5/51] time 0.163 (0.525) data 0.000 (0.348) loss 0.5332 (0.5071) acc 88.2979 (89.5920) lr 9.3721e-04 eta 0:10:13
epoch [28/50] batch [10/51] time 0.173 (0.352) data 0.000 (0.174) loss 0.4953 (0.5383) acc 91.6667 (88.2876) lr 9.3721e-04 eta 0:06:49
epoch [28/50] batch [15/51] time 0.170 (0.297) data 0.001 (0.116) loss 0.3640 (0.5310) acc 95.9184 (88.6482) lr 9.3721e-04 eta 0:05:43
epoch [28/50] batch [20/51] time 0.178 (0.267) data 0.001 (0.087) loss 0.4574 (0.5545) acc 88.2353 (88.6253) lr 9.3721e-04 eta 0:05:07
epoch [28/50] batch [25/51] time 0.172 (0.249) data 0.000 (0.070) loss 0.6168 (0.5710) acc 85.7843 (88.0814) lr 9.3721e-04 eta 0:04:45
epoch [28/50] batch [30/51] time 0.180 (0.238) data 0.001 (0.058) loss 0.5159 (0.5643) acc 87.5000 (88.0963) lr 9.3721e-04 eta 0:04:31
epoch [28/50] batch [35/51] time 0.180 (0.229) data 0.000 (0.050) loss 0.4673 (0.5650) acc 89.4231 (87.8766) lr 9.3721e-04 eta 0:04:20
epoch [28/50] batch [40/51] time 0.183 (0.223) data 0.000 (0.044) loss 0.4833 (0.5643) acc 91.0714 (87.8813) lr 9.3721e-04 eta 0:04:12
epoch [28/50] batch [45/51] time 0.167 (0.217) data 0.000 (0.039) loss 0.5709 (0.5705) acc 85.2041 (87.6827) lr 9.3721e-04 eta 0:04:04
epoch [28/50] batch [50/51] time 0.169 (0.212) data 0.000 (0.035) loss 0.6625 (0.5787) acc 82.5000 (87.5129) lr 9.3721e-04 eta 0:03:58
>>> alpha1: 0.149  alpha2: 0.042 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.29 <<<
epoch [29/50] batch [5/51] time 0.177 (0.492) data 0.000 (0.305) loss 0.6172 (0.5549) acc 87.0192 (88.1827) lr 8.7467e-04 eta 0:09:09
epoch [29/50] batch [10/51] time 0.184 (0.335) data 0.000 (0.153) loss 0.5432 (0.5641) acc 89.7059 (88.2058) lr 8.7467e-04 eta 0:06:12
epoch [29/50] batch [15/51] time 0.173 (0.280) data 0.001 (0.102) loss 0.5908 (0.5736) acc 85.7843 (88.1056) lr 8.7467e-04 eta 0:05:09
epoch [29/50] batch [20/51] time 0.183 (0.254) data 0.001 (0.076) loss 0.4879 (0.5892) acc 89.5000 (87.7118) lr 8.7467e-04 eta 0:04:40
epoch [29/50] batch [25/51] time 0.182 (0.240) data 0.000 (0.061) loss 0.5071 (0.5942) acc 90.8654 (87.4144) lr 8.7467e-04 eta 0:04:23
epoch [29/50] batch [30/51] time 0.185 (0.231) data 0.000 (0.051) loss 0.5248 (0.5891) acc 89.4231 (87.6891) lr 8.7467e-04 eta 0:04:12
epoch [29/50] batch [35/51] time 0.173 (0.224) data 0.000 (0.044) loss 0.5925 (0.5874) acc 88.5000 (87.4277) lr 8.7467e-04 eta 0:04:03
epoch [29/50] batch [40/51] time 0.170 (0.219) data 0.000 (0.038) loss 0.6512 (0.5917) acc 88.0000 (87.1666) lr 8.7467e-04 eta 0:03:56
epoch [29/50] batch [45/51] time 0.161 (0.213) data 0.001 (0.034) loss 0.7072 (0.5898) acc 85.3261 (87.2142) lr 8.7467e-04 eta 0:03:49
epoch [29/50] batch [50/51] time 0.186 (0.210) data 0.000 (0.031) loss 0.4897 (0.5794) acc 90.3509 (87.4688) lr 8.7467e-04 eta 0:03:44
>>> alpha1: 0.148  alpha2: 0.047 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.28 <<<
epoch [30/50] batch [5/51] time 0.169 (0.504) data 0.001 (0.323) loss 0.5459 (0.5461) acc 85.7143 (87.6373) lr 8.1262e-04 eta 0:08:57
epoch [30/50] batch [10/51] time 0.184 (0.342) data 0.000 (0.162) loss 0.5182 (0.5582) acc 87.5000 (87.4399) lr 8.1262e-04 eta 0:06:03
epoch [30/50] batch [15/51] time 0.166 (0.286) data 0.000 (0.108) loss 0.3238 (0.5483) acc 94.8980 (88.2459) lr 8.1262e-04 eta 0:05:01
epoch [30/50] batch [20/51] time 0.184 (0.259) data 0.000 (0.081) loss 0.5274 (0.5470) acc 90.9574 (88.1867) lr 8.1262e-04 eta 0:04:32
epoch [30/50] batch [25/51] time 0.190 (0.244) data 0.000 (0.065) loss 0.6818 (0.5424) acc 80.5000 (88.0815) lr 8.1262e-04 eta 0:04:14
epoch [30/50] batch [30/51] time 0.191 (0.233) data 0.001 (0.054) loss 0.4544 (0.5403) acc 87.2549 (87.6040) lr 8.1262e-04 eta 0:04:02
epoch [30/50] batch [35/51] time 0.172 (0.245) data 0.000 (0.046) loss 0.5764 (0.5326) acc 86.7347 (87.8027) lr 8.1262e-04 eta 0:04:13
epoch [30/50] batch [40/51] time 0.165 (0.237) data 0.000 (0.041) loss 0.9869 (0.5497) acc 77.6042 (87.5174) lr 8.1262e-04 eta 0:04:04
epoch [30/50] batch [45/51] time 0.176 (0.229) data 0.001 (0.036) loss 0.4605 (0.5488) acc 91.3462 (87.6673) lr 8.1262e-04 eta 0:03:55
epoch [30/50] batch [50/51] time 0.177 (0.224) data 0.000 (0.033) loss 0.5435 (0.5504) acc 89.1509 (87.6841) lr 8.1262e-04 eta 0:03:48
>>> alpha1: 0.146  alpha2: 0.051 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.30 <<<
epoch [31/50] batch [5/51] time 0.173 (0.497) data 0.001 (0.312) loss 0.5174 (0.5568) acc 86.5000 (89.5459) lr 7.5131e-04 eta 0:08:24
epoch [31/50] batch [10/51] time 0.899 (0.412) data 0.000 (0.156) loss 0.3238 (0.5148) acc 92.9167 (89.4858) lr 7.5131e-04 eta 0:06:55
epoch [31/50] batch [15/51] time 0.185 (0.332) data 0.000 (0.104) loss 0.6173 (0.5434) acc 86.4583 (88.5155) lr 7.5131e-04 eta 0:05:33
epoch [31/50] batch [20/51] time 0.186 (0.295) data 0.000 (0.078) loss 0.6609 (0.5502) acc 88.4259 (88.3446) lr 7.5131e-04 eta 0:04:54
epoch [31/50] batch [25/51] time 0.174 (0.271) data 0.000 (0.063) loss 0.4977 (0.5419) acc 90.6863 (88.4071) lr 7.5131e-04 eta 0:04:29
epoch [31/50] batch [30/51] time 0.169 (0.255) data 0.000 (0.052) loss 0.6843 (0.5488) acc 88.5000 (88.2569) lr 7.5131e-04 eta 0:04:12
epoch [31/50] batch [35/51] time 0.175 (0.243) data 0.000 (0.045) loss 0.5963 (0.5533) acc 84.0000 (88.1532) lr 7.5131e-04 eta 0:03:59
epoch [31/50] batch [40/51] time 0.170 (0.234) data 0.000 (0.039) loss 0.7051 (0.5615) acc 87.7451 (87.9088) lr 7.5131e-04 eta 0:03:49
epoch [31/50] batch [45/51] time 0.177 (0.228) data 0.000 (0.035) loss 0.4331 (0.5633) acc 93.8679 (87.9049) lr 7.5131e-04 eta 0:03:42
epoch [31/50] batch [50/51] time 0.173 (0.223) data 0.000 (0.031) loss 0.5509 (0.5580) acc 83.6538 (87.9469) lr 7.5131e-04 eta 0:03:36
>>> alpha1: 0.147  alpha2: 0.054 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [32/50] batch [5/51] time 0.159 (0.502) data 0.001 (0.326) loss 0.4638 (0.4973) acc 90.0000 (89.4042) lr 6.9098e-04 eta 0:08:03
epoch [32/50] batch [10/51] time 0.198 (0.339) data 0.000 (0.163) loss 0.4558 (0.4945) acc 84.7222 (88.6844) lr 6.9098e-04 eta 0:05:25
epoch [32/50] batch [15/51] time 0.176 (0.286) data 0.000 (0.109) loss 0.6486 (0.4971) acc 85.5000 (88.6504) lr 6.9098e-04 eta 0:04:32
epoch [32/50] batch [20/51] time 0.180 (0.259) data 0.000 (0.082) loss 0.7751 (0.5187) acc 83.0000 (88.1718) lr 6.9098e-04 eta 0:04:05
epoch [32/50] batch [25/51] time 0.180 (0.242) data 0.000 (0.065) loss 0.5534 (0.5257) acc 91.3636 (88.2829) lr 6.9098e-04 eta 0:03:48
epoch [32/50] batch [30/51] time 0.155 (0.231) data 0.000 (0.055) loss 0.6617 (0.5316) acc 83.5227 (88.5212) lr 6.9098e-04 eta 0:03:36
epoch [32/50] batch [35/51] time 0.202 (0.224) data 0.000 (0.047) loss 0.4677 (0.5365) acc 89.2157 (88.4796) lr 6.9098e-04 eta 0:03:29
epoch [32/50] batch [40/51] time 0.180 (0.218) data 0.000 (0.041) loss 0.6389 (0.5331) acc 88.1818 (88.6176) lr 6.9098e-04 eta 0:03:22
epoch [32/50] batch [45/51] time 0.168 (0.213) data 0.000 (0.037) loss 0.4745 (0.5334) acc 92.0000 (88.6086) lr 6.9098e-04 eta 0:03:16
epoch [32/50] batch [50/51] time 0.171 (0.208) data 0.000 (0.033) loss 0.5670 (0.5378) acc 85.2941 (88.3796) lr 6.9098e-04 eta 0:03:11
>>> alpha1: 0.144  alpha2: 0.052 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [33/50] batch [5/51] time 0.170 (0.509) data 0.000 (0.318) loss 0.7091 (0.5178) acc 82.7778 (88.7241) lr 6.3188e-04 eta 0:07:44
epoch [33/50] batch [10/51] time 0.182 (0.344) data 0.000 (0.159) loss 0.5895 (0.5204) acc 87.2549 (89.1411) lr 6.3188e-04 eta 0:05:12
epoch [33/50] batch [15/51] time 0.168 (0.286) data 0.001 (0.106) loss 0.7232 (0.5385) acc 82.1429 (88.1805) lr 6.3188e-04 eta 0:04:18
epoch [33/50] batch [20/51] time 0.169 (0.258) data 0.000 (0.080) loss 0.5401 (0.5525) acc 89.0000 (88.0121) lr 6.3188e-04 eta 0:03:51
epoch [33/50] batch [25/51] time 0.180 (0.241) data 0.000 (0.064) loss 0.5129 (0.5427) acc 91.0000 (88.4407) lr 6.3188e-04 eta 0:03:35
epoch [33/50] batch [30/51] time 0.170 (0.232) data 0.001 (0.053) loss 0.5942 (0.5361) acc 89.0000 (88.5562) lr 6.3188e-04 eta 0:03:26
epoch [33/50] batch [35/51] time 0.171 (0.224) data 0.000 (0.046) loss 0.4424 (0.5412) acc 90.6250 (88.3535) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [40/51] time 0.167 (0.218) data 0.000 (0.040) loss 0.7325 (0.5375) acc 86.2245 (88.4939) lr 6.3188e-04 eta 0:03:11
epoch [33/50] batch [45/51] time 0.166 (0.212) data 0.000 (0.036) loss 0.6472 (0.5391) acc 85.9375 (88.5087) lr 6.3188e-04 eta 0:03:05
epoch [33/50] batch [50/51] time 0.179 (0.208) data 0.000 (0.032) loss 0.4864 (0.5376) acc 88.2075 (88.3461) lr 6.3188e-04 eta 0:03:00
>>> alpha1: 0.140  alpha2: 0.050 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.19 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [34/50] batch [5/51] time 0.167 (0.461) data 0.000 (0.284) loss 0.5715 (0.5352) acc 82.9787 (88.0100) lr 5.7422e-04 eta 0:06:37
epoch [34/50] batch [10/51] time 0.173 (0.321) data 0.000 (0.142) loss 0.6116 (0.5492) acc 84.8039 (88.4102) lr 5.7422e-04 eta 0:04:34
epoch [34/50] batch [15/51] time 0.171 (0.272) data 0.000 (0.095) loss 0.5150 (0.5281) acc 88.0000 (88.8090) lr 5.7422e-04 eta 0:03:51
epoch [34/50] batch [20/51] time 0.176 (0.249) data 0.000 (0.071) loss 0.4395 (0.5204) acc 86.9318 (88.5557) lr 5.7422e-04 eta 0:03:31
epoch [34/50] batch [25/51] time 0.182 (0.234) data 0.000 (0.057) loss 0.5700 (0.5264) acc 85.7143 (88.1746) lr 5.7422e-04 eta 0:03:17
epoch [34/50] batch [30/51] time 0.175 (0.224) data 0.001 (0.048) loss 0.4157 (0.5291) acc 91.3462 (87.9565) lr 5.7422e-04 eta 0:03:07
epoch [34/50] batch [35/51] time 0.173 (0.218) data 0.001 (0.041) loss 0.6221 (0.5228) acc 85.7843 (87.9539) lr 5.7422e-04 eta 0:03:01
epoch [34/50] batch [40/51] time 0.162 (0.212) data 0.000 (0.036) loss 0.4582 (0.5248) acc 92.0213 (87.9871) lr 5.7422e-04 eta 0:02:55
epoch [34/50] batch [45/51] time 0.176 (0.207) data 0.000 (0.032) loss 0.5857 (0.5349) acc 83.0189 (87.7493) lr 5.7422e-04 eta 0:02:50
epoch [34/50] batch [50/51] time 0.169 (0.203) data 0.000 (0.029) loss 0.3753 (0.5334) acc 93.5000 (87.8344) lr 5.7422e-04 eta 0:02:45
>>> alpha1: 0.138  alpha2: 0.049 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [35/50] batch [5/51] time 0.185 (0.519) data 0.000 (0.336) loss 0.5237 (0.4564) acc 88.2353 (89.9334) lr 5.1825e-04 eta 0:07:01
epoch [35/50] batch [10/51] time 0.174 (0.345) data 0.000 (0.168) loss 0.3993 (0.4748) acc 95.0000 (89.6319) lr 5.1825e-04 eta 0:04:38
epoch [35/50] batch [15/51] time 0.187 (0.291) data 0.000 (0.112) loss 0.3607 (0.4847) acc 90.9091 (89.7039) lr 5.1825e-04 eta 0:03:52
epoch [35/50] batch [20/51] time 0.167 (0.261) data 0.000 (0.084) loss 0.4355 (0.4825) acc 89.7959 (89.8588) lr 5.1825e-04 eta 0:03:28
epoch [35/50] batch [25/51] time 0.176 (0.244) data 0.000 (0.067) loss 0.5362 (0.5673) acc 88.5000 (88.7071) lr 5.1825e-04 eta 0:03:13
epoch [35/50] batch [30/51] time 0.172 (0.233) data 0.000 (0.056) loss 0.6104 (0.6231) acc 84.8039 (87.8889) lr 5.1825e-04 eta 0:03:03
epoch [35/50] batch [35/51] time 0.180 (0.225) data 0.001 (0.048) loss 0.6925 (0.6263) acc 81.2500 (87.5772) lr 5.1825e-04 eta 0:02:55
epoch [35/50] batch [40/51] time 0.171 (0.220) data 0.000 (0.042) loss 0.4446 (0.6077) acc 90.1961 (87.8571) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [45/51] time 0.168 (0.214) data 0.000 (0.038) loss 0.6276 (0.6076) acc 89.0000 (87.6810) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [50/51] time 0.162 (0.209) data 0.001 (0.034) loss 0.5351 (0.6021) acc 89.3617 (87.6228) lr 5.1825e-04 eta 0:02:40
>>> alpha1: 0.138  alpha2: 0.049 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [36/50] batch [5/51] time 0.188 (0.504) data 0.001 (0.316) loss 0.5856 (0.4968) acc 85.0877 (89.0449) lr 4.6417e-04 eta 0:06:22
epoch [36/50] batch [10/51] time 0.173 (0.343) data 0.000 (0.158) loss 0.5654 (0.4928) acc 89.5833 (89.6953) lr 4.6417e-04 eta 0:04:19
epoch [36/50] batch [15/51] time 0.181 (0.286) data 0.000 (0.106) loss 0.4631 (0.5073) acc 90.3061 (89.3634) lr 4.6417e-04 eta 0:03:34
epoch [36/50] batch [20/51] time 0.187 (0.259) data 0.000 (0.079) loss 0.4949 (0.4981) acc 89.1509 (89.1134) lr 4.6417e-04 eta 0:03:13
epoch [36/50] batch [25/51] time 0.168 (0.243) data 0.001 (0.064) loss 0.4406 (0.5075) acc 89.7959 (88.9147) lr 4.6417e-04 eta 0:02:59
epoch [36/50] batch [30/51] time 0.190 (0.233) data 0.000 (0.053) loss 0.5007 (0.5016) acc 88.2353 (89.2116) lr 4.6417e-04 eta 0:02:51
epoch [36/50] batch [35/51] time 0.183 (0.224) data 0.000 (0.046) loss 0.6333 (0.5097) acc 83.8542 (88.7955) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [40/51] time 0.177 (0.218) data 0.000 (0.040) loss 0.6545 (0.5109) acc 84.9057 (88.7565) lr 4.6417e-04 eta 0:02:38
epoch [36/50] batch [45/51] time 0.174 (0.213) data 0.001 (0.036) loss 0.8189 (0.5174) acc 80.3922 (88.4917) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [50/51] time 0.167 (0.209) data 0.001 (0.032) loss 0.7120 (0.5238) acc 83.3333 (88.2139) lr 4.6417e-04 eta 0:02:29
>>> alpha1: 0.137  alpha2: 0.049 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [37/50] batch [5/51] time 0.184 (0.461) data 0.000 (0.273) loss 0.3951 (0.5288) acc 91.3265 (90.0512) lr 4.1221e-04 eta 0:05:27
epoch [37/50] batch [10/51] time 0.218 (0.322) data 0.000 (0.137) loss 0.2509 (0.4736) acc 95.9091 (90.4638) lr 4.1221e-04 eta 0:03:46
epoch [37/50] batch [15/51] time 0.187 (0.276) data 0.001 (0.091) loss 0.6186 (0.4935) acc 85.1064 (89.8826) lr 4.1221e-04 eta 0:03:12
epoch [37/50] batch [20/51] time 0.186 (0.253) data 0.000 (0.069) loss 0.4630 (0.4781) acc 90.6863 (90.1877) lr 4.1221e-04 eta 0:02:55
epoch [37/50] batch [25/51] time 0.175 (0.237) data 0.000 (0.055) loss 0.6078 (0.4769) acc 86.5385 (90.2731) lr 4.1221e-04 eta 0:02:43
epoch [37/50] batch [30/51] time 0.171 (0.227) data 0.000 (0.046) loss 0.5960 (0.4964) acc 84.6939 (89.8346) lr 4.1221e-04 eta 0:02:35
epoch [37/50] batch [35/51] time 0.190 (0.220) data 0.000 (0.039) loss 0.3973 (0.4939) acc 91.8478 (89.6588) lr 4.1221e-04 eta 0:02:29
epoch [37/50] batch [40/51] time 0.162 (0.215) data 0.000 (0.034) loss 0.6393 (0.4964) acc 85.8696 (89.7061) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [45/51] time 0.178 (0.210) data 0.000 (0.031) loss 0.6130 (0.5031) acc 83.9623 (89.3532) lr 4.1221e-04 eta 0:02:20
epoch [37/50] batch [50/51] time 0.164 (0.206) data 0.000 (0.028) loss 0.6779 (0.5050) acc 86.1702 (89.2298) lr 4.1221e-04 eta 0:02:17
>>> alpha1: 0.138  alpha2: 0.052 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [38/50] batch [5/51] time 0.178 (0.532) data 0.000 (0.353) loss 0.4409 (0.5533) acc 87.7358 (86.1211) lr 3.6258e-04 eta 0:05:50
epoch [38/50] batch [10/51] time 0.166 (0.355) data 0.000 (0.177) loss 0.7367 (0.5722) acc 86.9792 (86.2467) lr 3.6258e-04 eta 0:03:51
epoch [38/50] batch [15/51] time 0.175 (0.294) data 0.000 (0.118) loss 0.4347 (0.5571) acc 87.9808 (86.8526) lr 3.6258e-04 eta 0:03:10
epoch [38/50] batch [20/51] time 0.169 (0.266) data 0.000 (0.089) loss 0.4006 (0.5379) acc 90.3061 (87.2714) lr 3.6258e-04 eta 0:02:50
epoch [38/50] batch [25/51] time 0.178 (0.248) data 0.000 (0.071) loss 0.3818 (0.5222) acc 91.9811 (87.8017) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [30/51] time 0.178 (0.237) data 0.000 (0.060) loss 0.4789 (0.5187) acc 90.5660 (87.8363) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [35/51] time 0.182 (0.227) data 0.000 (0.051) loss 0.4243 (0.5275) acc 89.1509 (87.6083) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [40/51] time 0.167 (0.221) data 0.000 (0.045) loss 0.2912 (0.5248) acc 96.3542 (87.7923) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [45/51] time 0.159 (0.216) data 0.001 (0.040) loss 0.7356 (0.5241) acc 85.0000 (87.9614) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [50/51] time 0.172 (0.211) data 0.000 (0.036) loss 0.4860 (0.5125) acc 87.2449 (88.3884) lr 3.6258e-04 eta 0:02:09
>>> alpha1: 0.140  alpha2: 0.057 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.29 <<<
epoch [39/50] batch [5/51] time 0.185 (0.499) data 0.000 (0.317) loss 0.5366 (0.5965) acc 92.1569 (87.8631) lr 3.1545e-04 eta 0:05:02
epoch [39/50] batch [10/51] time 0.201 (0.339) data 0.000 (0.159) loss 0.3765 (0.5738) acc 92.4107 (87.4395) lr 3.1545e-04 eta 0:03:23
epoch [39/50] batch [15/51] time 0.169 (0.285) data 0.000 (0.106) loss 0.4978 (0.5548) acc 90.0000 (88.1705) lr 3.1545e-04 eta 0:02:49
epoch [39/50] batch [20/51] time 0.196 (0.260) data 0.001 (0.080) loss 0.3856 (0.5441) acc 92.7885 (88.1302) lr 3.1545e-04 eta 0:02:34
epoch [39/50] batch [25/51] time 0.176 (0.244) data 0.000 (0.064) loss 0.4900 (0.5325) acc 85.8696 (88.4768) lr 3.1545e-04 eta 0:02:23
epoch [39/50] batch [30/51] time 0.179 (0.233) data 0.001 (0.053) loss 0.8159 (0.5339) acc 85.6383 (88.6932) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [35/51] time 0.162 (0.225) data 0.000 (0.046) loss 0.4915 (0.5220) acc 91.4894 (88.7768) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [40/51] time 0.183 (0.219) data 0.000 (0.040) loss 0.4175 (0.5178) acc 88.8393 (88.8658) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [45/51] time 0.171 (0.214) data 0.000 (0.036) loss 0.6782 (0.5204) acc 84.3137 (88.6084) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [50/51] time 0.166 (0.210) data 0.000 (0.032) loss 0.4764 (0.5165) acc 85.7143 (88.5414) lr 3.1545e-04 eta 0:01:57
>>> alpha1: 0.140  alpha2: 0.058 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.28 <<<
epoch [40/50] batch [5/51] time 0.181 (0.469) data 0.001 (0.291) loss 0.7569 (0.6016) acc 82.8125 (87.4362) lr 2.7103e-04 eta 0:04:20
epoch [40/50] batch [10/51] time 0.169 (0.326) data 0.001 (0.146) loss 0.4525 (0.5449) acc 89.5000 (88.4168) lr 2.7103e-04 eta 0:02:59
epoch [40/50] batch [15/51] time 0.188 (0.277) data 0.001 (0.097) loss 0.3304 (0.5277) acc 93.4211 (89.0143) lr 2.7103e-04 eta 0:02:31
epoch [40/50] batch [20/51] time 0.197 (0.253) data 0.001 (0.073) loss 0.5634 (0.5392) acc 91.0377 (88.7115) lr 2.7103e-04 eta 0:02:16
epoch [40/50] batch [25/51] time 0.176 (0.237) data 0.000 (0.058) loss 0.5084 (0.5320) acc 89.1304 (88.6359) lr 2.7103e-04 eta 0:02:06
epoch [40/50] batch [30/51] time 0.165 (0.226) data 0.001 (0.049) loss 0.4841 (0.5273) acc 90.5556 (88.7792) lr 2.7103e-04 eta 0:02:00
epoch [40/50] batch [35/51] time 0.177 (0.220) data 0.000 (0.042) loss 0.7446 (0.5263) acc 84.8039 (88.5834) lr 2.7103e-04 eta 0:01:55
epoch [40/50] batch [40/51] time 0.175 (0.215) data 0.000 (0.037) loss 0.2921 (0.5124) acc 93.2692 (88.8548) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [45/51] time 0.171 (0.210) data 0.000 (0.033) loss 0.5074 (0.5076) acc 89.7059 (88.9605) lr 2.7103e-04 eta 0:01:48
epoch [40/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.029) loss 0.4065 (0.5158) acc 91.0377 (88.8440) lr 2.7103e-04 eta 0:01:45
>>> alpha1: 0.138  alpha2: 0.058 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [41/50] batch [5/51] time 0.179 (0.566) data 0.000 (0.377) loss 0.5020 (0.4494) acc 92.9245 (91.5304) lr 2.2949e-04 eta 0:04:45
epoch [41/50] batch [10/51] time 0.168 (0.372) data 0.000 (0.189) loss 0.7363 (0.4580) acc 81.1111 (91.0167) lr 2.2949e-04 eta 0:03:06
epoch [41/50] batch [15/51] time 0.163 (0.309) data 0.000 (0.126) loss 0.6379 (0.4954) acc 87.7660 (90.1809) lr 2.2949e-04 eta 0:02:32
epoch [41/50] batch [20/51] time 0.170 (0.275) data 0.000 (0.095) loss 0.4158 (0.4807) acc 89.5833 (90.3246) lr 2.2949e-04 eta 0:02:14
epoch [41/50] batch [25/51] time 0.173 (0.255) data 0.000 (0.076) loss 0.4965 (0.4896) acc 93.1373 (89.7569) lr 2.2949e-04 eta 0:02:03
epoch [41/50] batch [30/51] time 0.172 (0.242) data 0.000 (0.063) loss 0.5210 (0.4830) acc 91.1765 (89.8625) lr 2.2949e-04 eta 0:01:56
epoch [41/50] batch [35/51] time 0.176 (0.233) data 0.000 (0.054) loss 0.5441 (0.4799) acc 88.2075 (89.7699) lr 2.2949e-04 eta 0:01:50
epoch [41/50] batch [40/51] time 0.165 (0.226) data 0.000 (0.047) loss 0.4672 (0.4802) acc 90.1042 (89.8728) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [45/51] time 0.173 (0.220) data 0.000 (0.042) loss 0.3748 (0.4777) acc 95.1923 (89.9637) lr 2.2949e-04 eta 0:01:42
epoch [41/50] batch [50/51] time 0.169 (0.215) data 0.000 (0.038) loss 0.4854 (0.4758) acc 89.7959 (89.9525) lr 2.2949e-04 eta 0:01:39
>>> alpha1: 0.136  alpha2: 0.056 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [42/50] batch [5/51] time 0.171 (0.518) data 0.000 (0.338) loss 0.5308 (0.4761) acc 89.5833 (90.8544) lr 1.9098e-04 eta 0:03:55
epoch [42/50] batch [10/51] time 0.170 (0.343) data 0.001 (0.169) loss 0.5638 (0.5179) acc 87.7660 (88.9033) lr 1.9098e-04 eta 0:02:34
epoch [42/50] batch [15/51] time 0.172 (0.287) data 0.000 (0.113) loss 0.4492 (0.4915) acc 85.1064 (89.0093) lr 1.9098e-04 eta 0:02:07
epoch [42/50] batch [20/51] time 0.188 (0.260) data 0.001 (0.085) loss 0.3666 (0.4718) acc 94.8113 (89.8418) lr 1.9098e-04 eta 0:01:53
epoch [42/50] batch [25/51] time 0.193 (0.245) data 0.001 (0.068) loss 0.5218 (0.4852) acc 88.9423 (89.5344) lr 1.9098e-04 eta 0:01:46
epoch [42/50] batch [30/51] time 0.174 (0.235) data 0.001 (0.057) loss 0.4420 (0.4801) acc 89.2157 (89.6671) lr 1.9098e-04 eta 0:01:40
epoch [42/50] batch [35/51] time 0.173 (0.227) data 0.000 (0.049) loss 0.5807 (0.4906) acc 85.7843 (89.3154) lr 1.9098e-04 eta 0:01:36
epoch [42/50] batch [40/51] time 0.181 (0.221) data 0.000 (0.043) loss 0.3783 (0.4830) acc 92.2727 (89.4794) lr 1.9098e-04 eta 0:01:32
epoch [42/50] batch [45/51] time 0.174 (0.216) data 0.000 (0.038) loss 0.6152 (0.4809) acc 87.5000 (89.5257) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [50/51] time 0.161 (0.211) data 0.000 (0.034) loss 0.5364 (0.4865) acc 87.2340 (89.4901) lr 1.9098e-04 eta 0:01:26
>>> alpha1: 0.135  alpha2: 0.058 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [43/50] batch [5/51] time 0.182 (0.484) data 0.000 (0.308) loss 0.5355 (0.4723) acc 86.5385 (89.7947) lr 1.5567e-04 eta 0:03:15
epoch [43/50] batch [10/51] time 0.171 (0.330) data 0.000 (0.154) loss 0.5585 (0.4631) acc 89.2157 (90.8866) lr 1.5567e-04 eta 0:02:11
epoch [43/50] batch [15/51] time 0.169 (0.280) data 0.000 (0.103) loss 0.4903 (0.4782) acc 89.5000 (90.6446) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [20/51] time 0.173 (0.255) data 0.000 (0.077) loss 0.6369 (0.4742) acc 82.1429 (90.3163) lr 1.5567e-04 eta 0:01:38
epoch [43/50] batch [25/51] time 0.159 (0.240) data 0.000 (0.062) loss 0.6585 (0.4806) acc 85.5556 (89.9647) lr 1.5567e-04 eta 0:01:31
epoch [43/50] batch [30/51] time 0.176 (0.228) data 0.001 (0.052) loss 0.4040 (0.4908) acc 90.3061 (89.7728) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [35/51] time 0.176 (0.222) data 0.001 (0.044) loss 0.6390 (0.4872) acc 85.0000 (89.6445) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.177 (0.217) data 0.000 (0.039) loss 0.5653 (0.4887) acc 85.8491 (89.3759) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.165 (0.212) data 0.000 (0.035) loss 0.3829 (0.4837) acc 92.1875 (89.5115) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.164 (0.207) data 0.000 (0.031) loss 0.4906 (0.4834) acc 89.0625 (89.4364) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.133  alpha2: 0.056 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.27 <<<
epoch [44/50] batch [5/51] time 0.191 (0.555) data 0.001 (0.372) loss 0.4195 (0.5128) acc 92.9245 (88.4794) lr 1.2369e-04 eta 0:03:15
epoch [44/50] batch [10/51] time 0.182 (0.367) data 0.000 (0.186) loss 0.4924 (0.5287) acc 88.4259 (88.1139) lr 1.2369e-04 eta 0:02:07
epoch [44/50] batch [15/51] time 0.172 (0.303) data 0.001 (0.124) loss 0.5436 (0.4983) acc 88.7255 (89.2750) lr 1.2369e-04 eta 0:01:43
epoch [44/50] batch [20/51] time 0.874 (0.308) data 0.001 (0.093) loss 0.4444 (0.4947) acc 92.3729 (89.5814) lr 1.2369e-04 eta 0:01:43
epoch [44/50] batch [25/51] time 0.188 (0.281) data 0.000 (0.075) loss 0.3632 (0.4940) acc 90.3846 (89.2855) lr 1.2369e-04 eta 0:01:33
epoch [44/50] batch [30/51] time 0.180 (0.264) data 0.000 (0.062) loss 0.2167 (0.4851) acc 96.3636 (89.5496) lr 1.2369e-04 eta 0:01:26
epoch [44/50] batch [35/51] time 0.180 (0.252) data 0.000 (0.053) loss 0.4985 (0.4934) acc 91.1458 (89.5507) lr 1.2369e-04 eta 0:01:21
epoch [44/50] batch [40/51] time 0.169 (0.242) data 0.000 (0.047) loss 0.5139 (0.4937) acc 91.1765 (89.6392) lr 1.2369e-04 eta 0:01:16
epoch [44/50] batch [45/51] time 0.175 (0.234) data 0.000 (0.042) loss 0.2718 (0.4943) acc 95.1923 (89.6420) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [50/51] time 0.178 (0.228) data 0.000 (0.037) loss 0.3925 (0.4876) acc 91.9811 (89.8188) lr 1.2369e-04 eta 0:01:10
>>> alpha1: 0.131  alpha2: 0.056 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.27 <<<
epoch [45/50] batch [5/51] time 0.187 (0.458) data 0.001 (0.269) loss 0.4105 (0.4919) acc 94.0909 (89.3221) lr 9.5173e-05 eta 0:02:17
epoch [45/50] batch [10/51] time 0.183 (0.318) data 0.000 (0.135) loss 0.6144 (0.5230) acc 87.2727 (88.7265) lr 9.5173e-05 eta 0:01:34
epoch [45/50] batch [15/51] time 0.171 (0.270) data 0.000 (0.090) loss 0.5091 (0.5093) acc 87.7451 (89.1712) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [20/51] time 0.181 (0.247) data 0.001 (0.068) loss 0.5329 (0.5047) acc 83.8235 (88.6955) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [25/51] time 0.180 (0.234) data 0.001 (0.054) loss 0.5522 (0.5098) acc 90.6863 (88.7937) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [30/51] time 0.170 (0.226) data 0.001 (0.045) loss 0.6244 (0.5172) acc 85.7143 (88.4871) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [35/51] time 0.175 (0.218) data 0.000 (0.039) loss 0.3505 (0.5037) acc 90.3846 (88.8013) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [40/51] time 0.185 (0.213) data 0.001 (0.034) loss 0.5871 (0.5008) acc 86.1607 (88.9971) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.030) loss 0.5244 (0.4947) acc 87.0000 (89.1569) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [50/51] time 0.170 (0.205) data 0.000 (0.027) loss 0.4956 (0.5303) acc 89.2157 (88.8533) lr 9.5173e-05 eta 0:00:52
>>> alpha1: 0.131  alpha2: 0.060 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [46/50] batch [5/51] time 0.187 (0.446) data 0.000 (0.260) loss 0.4717 (0.5077) acc 90.6250 (89.8309) lr 7.0224e-05 eta 0:01:51
epoch [46/50] batch [10/51] time 0.162 (0.311) data 0.000 (0.130) loss 0.5022 (0.5133) acc 93.0851 (89.5918) lr 7.0224e-05 eta 0:01:16
epoch [46/50] batch [15/51] time 0.178 (0.269) data 0.001 (0.087) loss 0.5657 (0.4836) acc 85.3774 (89.5535) lr 7.0224e-05 eta 0:01:04
epoch [46/50] batch [20/51] time 0.188 (0.247) data 0.001 (0.065) loss 0.4127 (0.4900) acc 91.8269 (89.2817) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [25/51] time 0.179 (0.233) data 0.000 (0.052) loss 0.5985 (0.5128) acc 87.5000 (88.8624) lr 7.0224e-05 eta 0:00:53
epoch [46/50] batch [30/51] time 0.160 (0.223) data 0.000 (0.044) loss 0.5786 (0.5129) acc 90.0000 (88.8627) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [35/51] time 0.186 (0.216) data 0.000 (0.037) loss 0.4992 (0.5084) acc 91.5094 (88.9769) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [40/51] time 0.166 (0.210) data 0.000 (0.033) loss 0.2340 (0.5008) acc 95.0000 (89.2053) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [45/51] time 0.166 (0.205) data 0.000 (0.029) loss 0.6183 (0.4995) acc 91.0000 (89.4698) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [50/51] time 0.169 (0.202) data 0.000 (0.026) loss 0.6028 (0.4965) acc 85.2941 (89.3433) lr 7.0224e-05 eta 0:00:41
>>> alpha1: 0.131  alpha2: 0.061 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.28 <<<
epoch [47/50] batch [5/51] time 0.192 (0.475) data 0.000 (0.289) loss 0.3968 (0.3741) acc 90.6863 (92.4891) lr 4.8943e-05 eta 0:01:34
epoch [47/50] batch [10/51] time 0.177 (0.325) data 0.000 (0.145) loss 0.4477 (0.4474) acc 88.6792 (90.3964) lr 4.8943e-05 eta 0:01:03
epoch [47/50] batch [15/51] time 0.165 (0.277) data 0.001 (0.097) loss 0.4478 (0.4448) acc 88.5417 (90.5992) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [20/51] time 0.166 (0.250) data 0.000 (0.072) loss 0.4098 (0.4496) acc 92.3469 (90.2593) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.183 (0.236) data 0.000 (0.058) loss 0.7149 (0.4665) acc 86.7924 (90.0283) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [30/51] time 0.174 (0.227) data 0.000 (0.048) loss 0.5336 (0.4690) acc 87.9808 (90.0858) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.169 (0.220) data 0.000 (0.042) loss 0.6128 (0.5190) acc 85.0000 (89.4243) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.037) loss 0.8324 (0.5240) acc 82.3529 (89.2050) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.033) loss 0.2528 (0.5229) acc 95.0000 (89.3908) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.166 (0.206) data 0.000 (0.029) loss 0.5165 (0.5122) acc 88.7755 (89.6628) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.131  alpha2: 0.060 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [48/50] batch [5/51] time 0.187 (0.469) data 0.001 (0.284) loss 0.3966 (0.4610) acc 95.5882 (90.8025) lr 3.1417e-05 eta 0:01:09
epoch [48/50] batch [10/51] time 0.169 (0.322) data 0.000 (0.142) loss 0.6411 (0.4853) acc 82.0000 (89.1372) lr 3.1417e-05 eta 0:00:46
epoch [48/50] batch [15/51] time 0.181 (0.274) data 0.000 (0.095) loss 0.4551 (0.4779) acc 89.7059 (89.1510) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [20/51] time 0.181 (0.247) data 0.000 (0.071) loss 0.4053 (0.4836) acc 95.9091 (89.1936) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [25/51] time 0.183 (0.235) data 0.000 (0.057) loss 0.7216 (0.4917) acc 81.8627 (88.9679) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.186 (0.225) data 0.000 (0.048) loss 0.3528 (0.4796) acc 93.6274 (89.2723) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [35/51] time 0.172 (0.218) data 0.000 (0.041) loss 0.4603 (0.4762) acc 88.7255 (89.1633) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.182 (0.213) data 0.000 (0.036) loss 0.2708 (0.4736) acc 94.1964 (89.3590) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.166 (0.208) data 0.000 (0.032) loss 0.3842 (0.4749) acc 92.3469 (89.3704) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.173 (0.204) data 0.000 (0.029) loss 0.3386 (0.4786) acc 92.7885 (89.3325) lr 3.1417e-05 eta 0:00:20
>>> alpha1: 0.131  alpha2: 0.060 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.29 <<<
epoch [49/50] batch [5/51] time 0.167 (0.474) data 0.000 (0.289) loss 0.5573 (0.4985) acc 86.2245 (89.0601) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [10/51] time 0.193 (0.326) data 0.001 (0.145) loss 0.3805 (0.4851) acc 95.2830 (89.7792) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [15/51] time 0.180 (0.277) data 0.001 (0.096) loss 0.4847 (0.4675) acc 92.1296 (90.4841) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [20/51] time 0.178 (0.254) data 0.000 (0.072) loss 0.4223 (0.4554) acc 91.1458 (90.6431) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.176 (0.239) data 0.000 (0.058) loss 0.4186 (0.4752) acc 90.1961 (90.3647) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.194 (0.229) data 0.000 (0.048) loss 0.5162 (0.4764) acc 87.0370 (90.1659) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.176 (0.222) data 0.000 (0.041) loss 0.5442 (0.4880) acc 91.4894 (89.9388) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.036) loss 0.4436 (0.4821) acc 92.1875 (90.0342) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.032) loss 0.5523 (0.4822) acc 87.7551 (90.0450) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.166 (0.207) data 0.000 (0.029) loss 0.6497 (0.4822) acc 87.7551 (90.0928) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.131  alpha2: 0.059 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.09 & unmatched refined noisy rate: 0.28 <<<
epoch [50/50] batch [5/51] time 0.177 (0.488) data 0.000 (0.317) loss 0.4657 (0.5034) acc 91.0377 (90.7547) lr 7.8853e-06 eta 0:00:22
epoch [50/50] batch [10/51] time 0.160 (0.333) data 0.000 (0.159) loss 0.4935 (0.5187) acc 88.0435 (89.0954) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.181 (0.282) data 0.001 (0.106) loss 0.3157 (0.5051) acc 93.0556 (89.2351) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.171 (0.256) data 0.000 (0.079) loss 0.6009 (0.4864) acc 85.7843 (89.2479) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.182 (0.241) data 0.000 (0.064) loss 0.4801 (0.4913) acc 86.2745 (88.9181) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.192 (0.231) data 0.000 (0.053) loss 0.4291 (0.4969) acc 92.2727 (89.0575) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.196 (0.224) data 0.000 (0.046) loss 0.4813 (0.4914) acc 90.7407 (89.5098) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.177 (0.218) data 0.000 (0.040) loss 0.3671 (0.4841) acc 95.3704 (89.6823) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.172 (0.213) data 0.000 (0.035) loss 0.3749 (0.4859) acc 91.3462 (89.5404) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.168 (0.207) data 0.000 (0.032) loss 0.6011 (0.4857) acc 84.8039 (89.5091) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.25, 0.21, 0.22, 0.21, 0.2, 0.19, 0.19, 0.19, 0.19, 0.19, 0.18, 0.18, 0.18, 0.18, 0.19, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.17, 0.18, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17]
* matched noise rate: [0.11, 0.09, 0.08, 0.08, 0.07, 0.09, 0.09, 0.09, 0.09, 0.1, 0.09, 0.09, 0.09, 0.1, 0.1, 0.11, 0.1, 0.1, 0.11, 0.11, 0.1, 0.1, 0.09, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.11, 0.1, 0.1, 0.1, 0.11, 0.1, 0.1, 0.1, 0.09, 0.09, 0.09]
* unmatched noise rate: [0.38, 0.32, 0.34, 0.32, 0.32, 0.31, 0.32, 0.31, 0.32, 0.31, 0.31, 0.3, 0.3, 0.3, 0.3, 0.29, 0.29, 0.29, 0.29, 0.28, 0.3, 0.29, 0.28, 0.29, 0.27, 0.28, 0.29, 0.28, 0.29, 0.28, 0.28, 0.28, 0.27, 0.27, 0.27, 0.28, 0.28, 0.28, 0.29, 0.28]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<00:59,  2.47s/it] 12%|█▏        | 3/25 [00:02<00:15,  1.43it/s] 20%|██        | 5/25 [00:02<00:07,  2.61it/s] 28%|██▊       | 7/25 [00:02<00:04,  3.92it/s] 36%|███▌      | 9/25 [00:03<00:03,  5.28it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.60it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.81it/s] 60%|██████    | 15/25 [00:03<00:01,  8.82it/s] 68%|██████▊   | 17/25 [00:03<00:00,  9.29it/s] 76%|███████▌  | 19/25 [00:04<00:00,  9.01it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.63it/s] 92%|█████████▏| 23/25 [00:04<00:00, 10.32it/s]100%|██████████| 25/25 [00:04<00:00,  7.86it/s]100%|██████████| 25/25 [00:04<00:00,  5.03it/s]
=> result
* total: 2,463
* correct: 1,973
* accuracy: 80.1%
* error: 19.9%
* macro_f1: 75.9%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 17	acc: 94.4%
* class: 2 (canterbury bells)	total: 12	correct: 0	acc: 0.0%
* class: 3 (sweet pea)	total: 17	correct: 7	acc: 41.2%
* class: 4 (english marigold)	total: 20	correct: 7	acc: 35.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 12	acc: 85.7%
* class: 10 (snapdragon)	total: 26	correct: 20	acc: 76.9%
* class: 11 (colt's foot)	total: 26	correct: 16	acc: 61.5%
* class: 12 (king protea)	total: 15	correct: 12	acc: 80.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 14	acc: 93.3%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 22	acc: 88.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 11	acc: 91.7%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 24	acc: 88.9%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 20	acc: 76.9%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 13	acc: 92.9%
* class: 32 (love in the mist)	total: 14	correct: 3	acc: 21.4%
* class: 33 (mexican aster)	total: 12	correct: 11	acc: 91.7%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 22	acc: 100.0%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 17	acc: 100.0%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 27	acc: 96.4%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 55	acc: 93.2%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 26	acc: 92.9%
* class: 50 (petunia)	total: 77	correct: 74	acc: 96.1%
* class: 51 (wild pansy)	total: 26	correct: 24	acc: 92.3%
* class: 52 (primula)	total: 28	correct: 15	acc: 53.6%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 21	acc: 100.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 31	acc: 93.9%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 15	acc: 93.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 12	acc: 92.3%
* class: 67 (bearded iris)	total: 16	correct: 12	acc: 75.0%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 26	acc: 89.7%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 49	acc: 96.1%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 28	acc: 87.5%
* class: 76 (passion flower)	total: 75	correct: 74	acc: 98.7%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 32	acc: 94.1%
* class: 82 (hibiscus)	total: 39	correct: 38	acc: 97.4%
* class: 83 (columbine)	total: 26	correct: 23	acc: 88.5%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 39	acc: 84.8%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 12	acc: 52.2%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 36	acc: 94.7%
* class: 95 (camellia)	total: 27	correct: 21	acc: 77.8%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 16	acc: 64.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 11	acc: 64.7%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 80.3%
Elapsed: 0:28:38
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '8', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_8-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 8
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.266 (1.138) data 0.000 (0.373) loss 4.5558 (4.7427) acc 6.2500 (5.6250) lr 1.0000e-05 eta 0:48:15
epoch [1/50] batch [10/51] time 0.263 (0.702) data 0.000 (0.187) loss 4.6492 (4.6615) acc 6.2500 (5.9375) lr 1.0000e-05 eta 0:29:42
epoch [1/50] batch [15/51] time 0.259 (0.555) data 0.000 (0.125) loss 4.4665 (4.5755) acc 3.1250 (5.4167) lr 1.0000e-05 eta 0:23:27
epoch [1/50] batch [20/51] time 0.261 (0.483) data 0.000 (0.094) loss 4.4650 (4.5380) acc 9.3750 (5.6250) lr 1.0000e-05 eta 0:20:21
epoch [1/50] batch [25/51] time 0.259 (0.439) data 0.000 (0.075) loss 4.3474 (4.5422) acc 3.1250 (5.3750) lr 1.0000e-05 eta 0:18:28
epoch [1/50] batch [30/51] time 0.263 (0.410) data 0.000 (0.062) loss 4.2977 (4.5317) acc 6.2500 (5.7292) lr 1.0000e-05 eta 0:17:12
epoch [1/50] batch [35/51] time 0.271 (0.389) data 0.000 (0.054) loss 4.1097 (4.5067) acc 15.6250 (6.5179) lr 1.0000e-05 eta 0:16:17
epoch [1/50] batch [40/51] time 0.256 (0.372) data 0.000 (0.047) loss 4.2044 (4.4845) acc 12.5000 (6.8750) lr 1.0000e-05 eta 0:15:34
epoch [1/50] batch [45/51] time 0.256 (0.359) data 0.000 (0.042) loss 4.5334 (4.4650) acc 0.0000 (7.1528) lr 1.0000e-05 eta 0:15:00
epoch [1/50] batch [50/51] time 0.256 (0.349) data 0.000 (0.038) loss 4.5496 (4.4566) acc 3.1250 (7.3125) lr 1.0000e-05 eta 0:14:32
epoch [2/50] batch [5/51] time 0.273 (0.595) data 0.000 (0.303) loss 4.3290 (4.2422) acc 15.6250 (16.2500) lr 2.0000e-03 eta 0:24:44
epoch [2/50] batch [10/51] time 0.282 (0.430) data 0.000 (0.152) loss 3.7343 (4.1057) acc 21.8750 (18.4375) lr 2.0000e-03 eta 0:17:49
epoch [2/50] batch [15/51] time 0.259 (0.374) data 0.000 (0.101) loss 4.2399 (4.0940) acc 21.8750 (19.5833) lr 2.0000e-03 eta 0:15:29
epoch [2/50] batch [20/51] time 0.266 (0.347) data 0.000 (0.076) loss 4.3684 (4.1594) acc 18.7500 (18.1250) lr 2.0000e-03 eta 0:14:19
epoch [2/50] batch [25/51] time 0.265 (0.331) data 0.000 (0.061) loss 4.0166 (4.1617) acc 34.3750 (18.7500) lr 2.0000e-03 eta 0:13:38
epoch [2/50] batch [30/51] time 0.260 (0.320) data 0.000 (0.051) loss 4.1628 (4.1365) acc 28.1250 (19.2708) lr 2.0000e-03 eta 0:13:09
epoch [2/50] batch [35/51] time 0.270 (0.313) data 0.000 (0.043) loss 4.0833 (4.1197) acc 18.7500 (19.5536) lr 2.0000e-03 eta 0:12:50
epoch [2/50] batch [40/51] time 0.257 (0.306) data 0.000 (0.038) loss 4.2451 (4.1018) acc 28.1250 (20.2344) lr 2.0000e-03 eta 0:12:33
epoch [2/50] batch [45/51] time 0.258 (0.301) data 0.000 (0.034) loss 4.0107 (4.0869) acc 28.1250 (20.7639) lr 2.0000e-03 eta 0:12:18
epoch [2/50] batch [50/51] time 0.258 (0.296) data 0.000 (0.030) loss 3.9052 (4.0622) acc 31.2500 (21.5625) lr 2.0000e-03 eta 0:12:06
epoch [3/50] batch [5/51] time 0.272 (0.648) data 0.000 (0.366) loss 3.8551 (3.9768) acc 28.1250 (27.5000) lr 1.9980e-03 eta 0:26:22
epoch [3/50] batch [10/51] time 0.271 (0.458) data 0.000 (0.183) loss 4.0192 (3.9809) acc 25.0000 (24.6875) lr 1.9980e-03 eta 0:18:36
epoch [3/50] batch [15/51] time 0.258 (0.392) data 0.000 (0.122) loss 4.1496 (3.9483) acc 28.1250 (25.2083) lr 1.9980e-03 eta 0:15:53
epoch [3/50] batch [20/51] time 0.260 (0.359) data 0.000 (0.092) loss 3.9584 (3.8791) acc 31.2500 (26.8750) lr 1.9980e-03 eta 0:14:32
epoch [3/50] batch [25/51] time 0.258 (0.340) data 0.000 (0.073) loss 3.7412 (3.9065) acc 34.3750 (27.2500) lr 1.9980e-03 eta 0:13:44
epoch [3/50] batch [30/51] time 0.268 (0.328) data 0.000 (0.061) loss 3.9911 (3.9324) acc 15.6250 (26.6667) lr 1.9980e-03 eta 0:13:12
epoch [3/50] batch [35/51] time 0.259 (0.318) data 0.000 (0.053) loss 3.3740 (3.9360) acc 40.6250 (26.4286) lr 1.9980e-03 eta 0:12:48
epoch [3/50] batch [40/51] time 0.256 (0.311) data 0.000 (0.046) loss 3.7566 (3.9155) acc 37.5000 (27.1875) lr 1.9980e-03 eta 0:12:29
epoch [3/50] batch [45/51] time 0.256 (0.305) data 0.000 (0.041) loss 4.6455 (3.9158) acc 15.6250 (27.5000) lr 1.9980e-03 eta 0:12:12
epoch [3/50] batch [50/51] time 0.256 (0.300) data 0.000 (0.037) loss 4.2717 (3.9292) acc 15.6250 (27.5625) lr 1.9980e-03 eta 0:11:59
epoch [4/50] batch [5/51] time 0.267 (0.570) data 0.000 (0.288) loss 3.7325 (3.8286) acc 37.5000 (33.1250) lr 1.9921e-03 eta 0:22:43
epoch [4/50] batch [10/51] time 0.267 (0.418) data 0.000 (0.144) loss 4.0182 (3.8354) acc 34.3750 (33.7500) lr 1.9921e-03 eta 0:16:38
epoch [4/50] batch [15/51] time 0.269 (0.368) data 0.000 (0.096) loss 3.4523 (3.8363) acc 40.6250 (33.1250) lr 1.9921e-03 eta 0:14:36
epoch [4/50] batch [20/51] time 0.270 (0.342) data 0.000 (0.072) loss 4.2829 (3.8454) acc 21.8750 (31.8750) lr 1.9921e-03 eta 0:13:32
epoch [4/50] batch [25/51] time 0.260 (0.326) data 0.000 (0.058) loss 4.1546 (3.8694) acc 25.0000 (30.8750) lr 1.9921e-03 eta 0:12:54
epoch [4/50] batch [30/51] time 0.265 (0.316) data 0.000 (0.048) loss 3.4181 (3.8468) acc 40.6250 (31.5625) lr 1.9921e-03 eta 0:12:27
epoch [4/50] batch [35/51] time 0.260 (0.309) data 0.000 (0.041) loss 3.8437 (3.8731) acc 28.1250 (31.5179) lr 1.9921e-03 eta 0:12:09
epoch [4/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.036) loss 3.7086 (3.8820) acc 34.3750 (30.7812) lr 1.9921e-03 eta 0:11:53
epoch [4/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.032) loss 3.1846 (3.8741) acc 43.7500 (30.5556) lr 1.9921e-03 eta 0:11:39
epoch [4/50] batch [50/51] time 0.258 (0.294) data 0.000 (0.029) loss 3.8509 (3.8678) acc 28.1250 (30.6250) lr 1.9921e-03 eta 0:11:28
epoch [5/50] batch [5/51] time 0.277 (0.597) data 0.000 (0.309) loss 4.1499 (3.6049) acc 34.3750 (34.3750) lr 1.9823e-03 eta 0:23:17
epoch [5/50] batch [10/51] time 0.266 (0.432) data 0.000 (0.155) loss 3.3905 (3.6210) acc 43.7500 (36.2500) lr 1.9823e-03 eta 0:16:49
epoch [5/50] batch [15/51] time 0.260 (0.378) data 0.000 (0.103) loss 4.5060 (3.7380) acc 21.8750 (33.9583) lr 1.9823e-03 eta 0:14:40
epoch [5/50] batch [20/51] time 0.273 (0.350) data 0.000 (0.077) loss 4.4070 (3.7894) acc 15.6250 (32.0312) lr 1.9823e-03 eta 0:13:34
epoch [5/50] batch [25/51] time 0.260 (0.333) data 0.000 (0.062) loss 3.4314 (3.7525) acc 40.6250 (31.7500) lr 1.9823e-03 eta 0:12:52
epoch [5/50] batch [30/51] time 0.259 (0.321) data 0.000 (0.052) loss 4.2680 (3.7615) acc 28.1250 (32.8125) lr 1.9823e-03 eta 0:12:23
epoch [5/50] batch [35/51] time 0.260 (0.313) data 0.000 (0.044) loss 3.7627 (3.7787) acc 31.2500 (32.7679) lr 1.9823e-03 eta 0:12:02
epoch [5/50] batch [40/51] time 0.257 (0.306) data 0.000 (0.039) loss 3.4939 (3.7883) acc 43.7500 (32.8125) lr 1.9823e-03 eta 0:11:46
epoch [5/50] batch [45/51] time 0.257 (0.301) data 0.000 (0.035) loss 4.3790 (3.7967) acc 21.8750 (32.8472) lr 1.9823e-03 eta 0:11:32
epoch [5/50] batch [50/51] time 0.258 (0.297) data 0.000 (0.031) loss 3.7684 (3.8070) acc 21.8750 (32.2500) lr 1.9823e-03 eta 0:11:20
epoch [6/50] batch [5/51] time 0.264 (0.573) data 0.000 (0.293) loss 3.0750 (3.5912) acc 46.8750 (35.6250) lr 1.9686e-03 eta 0:21:52
epoch [6/50] batch [10/51] time 0.270 (0.420) data 0.000 (0.147) loss 3.5618 (3.8193) acc 28.1250 (30.0000) lr 1.9686e-03 eta 0:15:59
epoch [6/50] batch [15/51] time 0.259 (0.367) data 0.000 (0.098) loss 3.7499 (3.8391) acc 37.5000 (31.2500) lr 1.9686e-03 eta 0:13:57
epoch [6/50] batch [20/51] time 0.268 (0.342) data 0.000 (0.073) loss 3.6299 (3.7859) acc 31.2500 (31.8750) lr 1.9686e-03 eta 0:12:56
epoch [6/50] batch [25/51] time 0.272 (0.326) data 0.000 (0.059) loss 3.6935 (3.8053) acc 28.1250 (31.6250) lr 1.9686e-03 eta 0:12:20
epoch [6/50] batch [30/51] time 0.259 (0.315) data 0.000 (0.049) loss 3.6536 (3.7887) acc 37.5000 (31.3542) lr 1.9686e-03 eta 0:11:54
epoch [6/50] batch [35/51] time 0.260 (0.308) data 0.000 (0.042) loss 3.6262 (3.7703) acc 40.6250 (32.3214) lr 1.9686e-03 eta 0:11:36
epoch [6/50] batch [40/51] time 0.257 (0.303) data 0.000 (0.037) loss 3.7018 (3.7630) acc 34.3750 (32.2656) lr 1.9686e-03 eta 0:11:22
epoch [6/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.033) loss 3.9873 (3.7693) acc 34.3750 (32.3611) lr 1.9686e-03 eta 0:11:09
epoch [6/50] batch [50/51] time 0.258 (0.294) data 0.000 (0.030) loss 4.0901 (3.7726) acc 31.2500 (32.2500) lr 1.9686e-03 eta 0:10:59
epoch [7/50] batch [5/51] time 0.260 (0.585) data 0.000 (0.308) loss 3.7789 (3.9015) acc 31.2500 (28.1250) lr 1.9511e-03 eta 0:21:50
epoch [7/50] batch [10/51] time 0.265 (0.425) data 0.000 (0.154) loss 3.0997 (3.7579) acc 53.1250 (32.5000) lr 1.9511e-03 eta 0:15:48
epoch [7/50] batch [15/51] time 0.270 (0.372) data 0.000 (0.103) loss 4.1245 (3.7660) acc 21.8750 (32.2917) lr 1.9511e-03 eta 0:13:50
epoch [7/50] batch [20/51] time 0.260 (0.346) data 0.000 (0.077) loss 3.6131 (3.7498) acc 34.3750 (33.1250) lr 1.9511e-03 eta 0:12:49
epoch [7/50] batch [25/51] time 0.260 (0.330) data 0.000 (0.062) loss 3.4126 (3.7540) acc 34.3750 (32.1250) lr 1.9511e-03 eta 0:12:12
epoch [7/50] batch [30/51] time 0.272 (0.319) data 0.000 (0.052) loss 4.1150 (3.7697) acc 21.8750 (31.7708) lr 1.9511e-03 eta 0:11:46
epoch [7/50] batch [35/51] time 0.261 (0.312) data 0.000 (0.044) loss 3.3523 (3.7524) acc 46.8750 (32.4107) lr 1.9511e-03 eta 0:11:29
epoch [7/50] batch [40/51] time 0.258 (0.305) data 0.000 (0.039) loss 3.6653 (3.7247) acc 28.1250 (32.4219) lr 1.9511e-03 eta 0:11:13
epoch [7/50] batch [45/51] time 0.258 (0.300) data 0.000 (0.034) loss 4.1788 (3.7111) acc 28.1250 (32.9167) lr 1.9511e-03 eta 0:10:59
epoch [7/50] batch [50/51] time 0.257 (0.296) data 0.000 (0.031) loss 3.1477 (3.6971) acc 46.8750 (33.8125) lr 1.9511e-03 eta 0:10:49
epoch [8/50] batch [5/51] time 0.259 (0.610) data 0.000 (0.338) loss 3.4659 (3.7315) acc 31.2500 (33.1250) lr 1.9298e-03 eta 0:22:15
epoch [8/50] batch [10/51] time 0.272 (0.441) data 0.000 (0.169) loss 3.1872 (3.6835) acc 37.5000 (33.1250) lr 1.9298e-03 eta 0:16:02
epoch [8/50] batch [15/51] time 0.276 (0.383) data 0.000 (0.113) loss 3.6129 (3.6601) acc 37.5000 (33.3333) lr 1.9298e-03 eta 0:13:53
epoch [8/50] batch [20/51] time 0.264 (0.354) data 0.000 (0.085) loss 3.4471 (3.7064) acc 40.6250 (32.3438) lr 1.9298e-03 eta 0:12:48
epoch [8/50] batch [25/51] time 0.268 (0.336) data 0.000 (0.068) loss 3.3601 (3.6850) acc 46.8750 (34.0000) lr 1.9298e-03 eta 0:12:08
epoch [8/50] batch [30/51] time 0.271 (0.324) data 0.000 (0.056) loss 3.9275 (3.7120) acc 28.1250 (33.2292) lr 1.9298e-03 eta 0:11:40
epoch [8/50] batch [35/51] time 0.259 (0.315) data 0.000 (0.048) loss 3.3983 (3.7116) acc 37.5000 (33.7500) lr 1.9298e-03 eta 0:11:19
epoch [8/50] batch [40/51] time 0.258 (0.308) data 0.000 (0.042) loss 3.5482 (3.6883) acc 28.1250 (33.7500) lr 1.9298e-03 eta 0:11:02
epoch [8/50] batch [45/51] time 0.257 (0.302) data 0.000 (0.038) loss 3.9967 (3.6924) acc 37.5000 (34.0972) lr 1.9298e-03 eta 0:10:49
epoch [8/50] batch [50/51] time 0.258 (0.298) data 0.000 (0.034) loss 3.0810 (3.6814) acc 46.8750 (34.2500) lr 1.9298e-03 eta 0:10:38
epoch [9/50] batch [5/51] time 0.263 (0.537) data 0.000 (0.261) loss 3.5301 (3.5920) acc 43.7500 (36.8750) lr 1.9048e-03 eta 0:19:07
epoch [9/50] batch [10/51] time 0.276 (0.402) data 0.000 (0.131) loss 3.2241 (3.5962) acc 37.5000 (37.1875) lr 1.9048e-03 eta 0:14:16
epoch [9/50] batch [15/51] time 0.280 (0.359) data 0.000 (0.087) loss 3.1623 (3.5805) acc 43.7500 (36.8750) lr 1.9048e-03 eta 0:12:43
epoch [9/50] batch [20/51] time 0.272 (0.336) data 0.000 (0.065) loss 3.3725 (3.6086) acc 46.8750 (36.7188) lr 1.9048e-03 eta 0:11:52
epoch [9/50] batch [25/51] time 0.264 (0.322) data 0.000 (0.052) loss 3.7081 (3.5831) acc 37.5000 (36.3750) lr 1.9048e-03 eta 0:11:22
epoch [9/50] batch [30/51] time 0.266 (0.313) data 0.000 (0.044) loss 3.4185 (3.6100) acc 37.5000 (35.9375) lr 1.9048e-03 eta 0:11:01
epoch [9/50] batch [35/51] time 0.278 (0.307) data 0.000 (0.038) loss 3.3617 (3.6265) acc 50.0000 (35.8929) lr 1.9048e-03 eta 0:10:46
epoch [9/50] batch [40/51] time 0.259 (0.302) data 0.000 (0.033) loss 3.4586 (3.6283) acc 31.2500 (34.9219) lr 1.9048e-03 eta 0:10:33
epoch [9/50] batch [45/51] time 0.257 (0.297) data 0.000 (0.029) loss 3.7959 (3.6395) acc 28.1250 (34.7222) lr 1.9048e-03 eta 0:10:22
epoch [9/50] batch [50/51] time 0.257 (0.293) data 0.000 (0.026) loss 3.2253 (3.6332) acc 53.1250 (35.1875) lr 1.9048e-03 eta 0:10:12
epoch [10/50] batch [5/51] time 0.269 (0.608) data 0.000 (0.330) loss 3.6325 (3.5282) acc 34.3750 (38.7500) lr 1.8763e-03 eta 0:21:07
epoch [10/50] batch [10/51] time 0.260 (0.435) data 0.000 (0.165) loss 3.4685 (3.4609) acc 40.6250 (41.8750) lr 1.8763e-03 eta 0:15:05
epoch [10/50] batch [15/51] time 0.273 (0.379) data 0.000 (0.110) loss 4.1534 (3.6132) acc 21.8750 (38.7500) lr 1.8763e-03 eta 0:13:06
epoch [10/50] batch [20/51] time 0.262 (0.351) data 0.000 (0.083) loss 3.5779 (3.5818) acc 31.2500 (38.9062) lr 1.8763e-03 eta 0:12:06
epoch [10/50] batch [25/51] time 0.261 (0.334) data 0.000 (0.066) loss 3.7525 (3.5373) acc 37.5000 (39.6250) lr 1.8763e-03 eta 0:11:29
epoch [10/50] batch [30/51] time 0.261 (0.323) data 0.000 (0.055) loss 3.5989 (3.5478) acc 37.5000 (38.9583) lr 1.8763e-03 eta 0:11:05
epoch [10/50] batch [35/51] time 0.267 (0.315) data 0.000 (0.047) loss 3.3416 (3.5812) acc 37.5000 (37.8571) lr 1.8763e-03 eta 0:10:48
epoch [10/50] batch [40/51] time 0.259 (0.309) data 0.000 (0.041) loss 4.3236 (3.5961) acc 15.6250 (36.8750) lr 1.8763e-03 eta 0:10:33
epoch [10/50] batch [45/51] time 0.258 (0.303) data 0.000 (0.037) loss 3.6227 (3.5991) acc 34.3750 (36.5278) lr 1.8763e-03 eta 0:10:20
epoch [10/50] batch [50/51] time 0.257 (0.298) data 0.000 (0.033) loss 4.1073 (3.6022) acc 28.1250 (36.5625) lr 1.8763e-03 eta 0:10:09
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.723  alpha2: 0.344 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.35 <<<
epoch [11/50] batch [5/51] time 0.781 (0.959) data 0.000 (0.277) loss 1.8387 (2.0339) acc 70.6522 (67.2806) lr 1.8443e-03 eta 0:32:32
epoch [11/50] batch [10/51] time 0.655 (0.730) data 0.000 (0.138) loss 1.9617 (1.9798) acc 66.0714 (66.6997) lr 1.8443e-03 eta 0:24:42
epoch [11/50] batch [15/51] time 0.191 (0.612) data 0.000 (0.092) loss 1.7871 (1.9976) acc 68.0000 (65.0112) lr 1.8443e-03 eta 0:20:39
epoch [11/50] batch [20/51] time 0.170 (0.554) data 0.000 (0.069) loss 1.8358 (1.9699) acc 70.7447 (65.5907) lr 1.8443e-03 eta 0:18:39
epoch [11/50] batch [25/51] time 0.152 (0.498) data 0.001 (0.056) loss 1.9560 (1.9303) acc 57.1429 (65.8568) lr 1.8443e-03 eta 0:16:43
epoch [11/50] batch [30/51] time 0.163 (0.442) data 0.000 (0.046) loss 1.9381 (1.9118) acc 75.0000 (66.0581) lr 1.8443e-03 eta 0:14:47
epoch [11/50] batch [35/51] time 0.172 (0.403) data 0.000 (0.040) loss 2.0008 (1.9088) acc 63.5417 (66.3049) lr 1.8443e-03 eta 0:13:28
epoch [11/50] batch [40/51] time 0.167 (0.390) data 0.000 (0.035) loss 1.7012 (1.8944) acc 75.0000 (66.8540) lr 1.8443e-03 eta 0:13:00
epoch [11/50] batch [45/51] time 0.159 (0.378) data 0.000 (0.031) loss 1.7032 (1.9033) acc 65.2174 (66.3575) lr 1.8443e-03 eta 0:12:34
epoch [11/50] batch [50/51] time 0.150 (0.356) data 0.000 (0.028) loss 1.9390 (1.8983) acc 61.9048 (66.3504) lr 1.8443e-03 eta 0:11:48
>>> alpha1: 0.566  alpha2: 0.259 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.18 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [12/50] batch [5/51] time 0.161 (0.468) data 0.000 (0.294) loss 1.2873 (1.3145) acc 72.2222 (71.1403) lr 1.8090e-03 eta 0:15:29
epoch [12/50] batch [10/51] time 0.174 (0.317) data 0.000 (0.147) loss 0.9457 (1.2773) acc 82.5000 (73.2816) lr 1.8090e-03 eta 0:10:27
epoch [12/50] batch [15/51] time 0.177 (0.267) data 0.000 (0.098) loss 1.2138 (1.2465) acc 67.8571 (73.9021) lr 1.8090e-03 eta 0:08:46
epoch [12/50] batch [20/51] time 0.162 (0.242) data 0.000 (0.074) loss 1.2824 (1.2467) acc 67.5532 (72.8075) lr 1.8090e-03 eta 0:07:56
epoch [12/50] batch [25/51] time 0.170 (0.245) data 0.000 (0.059) loss 1.0586 (1.2524) acc 77.9412 (72.5576) lr 1.8090e-03 eta 0:08:01
epoch [12/50] batch [30/51] time 0.160 (0.232) data 0.000 (0.049) loss 1.2502 (1.2628) acc 73.8889 (72.7928) lr 1.8090e-03 eta 0:07:34
epoch [12/50] batch [35/51] time 0.175 (0.224) data 0.001 (0.042) loss 1.0649 (1.2412) acc 77.2222 (73.1369) lr 1.8090e-03 eta 0:07:16
epoch [12/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.037) loss 1.2601 (1.2392) acc 70.3125 (73.0617) lr 1.8090e-03 eta 0:07:00
epoch [12/50] batch [45/51] time 0.170 (0.211) data 0.000 (0.033) loss 1.1937 (1.2311) acc 79.9020 (73.1671) lr 1.8090e-03 eta 0:06:49
epoch [12/50] batch [50/51] time 0.163 (0.206) data 0.000 (0.030) loss 1.1175 (1.2223) acc 73.4375 (73.3763) lr 1.8090e-03 eta 0:06:39
>>> alpha1: 0.448  alpha2: 0.181 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.17 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [13/50] batch [5/51] time 0.192 (0.514) data 0.001 (0.337) loss 0.5930 (0.9746) acc 86.3095 (77.2669) lr 1.7705e-03 eta 0:16:32
epoch [13/50] batch [10/51] time 0.177 (0.345) data 0.000 (0.169) loss 0.9690 (0.9549) acc 76.6667 (78.1372) lr 1.7705e-03 eta 0:11:04
epoch [13/50] batch [15/51] time 0.162 (0.285) data 0.000 (0.113) loss 0.9365 (0.9490) acc 79.2553 (78.5314) lr 1.7705e-03 eta 0:09:07
epoch [13/50] batch [20/51] time 0.160 (0.254) data 0.000 (0.085) loss 0.8848 (0.9595) acc 75.5814 (77.8540) lr 1.7705e-03 eta 0:08:06
epoch [13/50] batch [25/51] time 0.156 (0.236) data 0.000 (0.068) loss 1.2120 (0.9857) acc 75.0000 (77.2982) lr 1.7705e-03 eta 0:07:30
epoch [13/50] batch [30/51] time 0.167 (0.223) data 0.000 (0.056) loss 1.0634 (0.9752) acc 75.5102 (77.6199) lr 1.7705e-03 eta 0:07:06
epoch [13/50] batch [35/51] time 0.187 (0.235) data 0.000 (0.048) loss 0.9152 (0.9580) acc 81.3726 (78.2015) lr 1.7705e-03 eta 0:07:27
epoch [13/50] batch [40/51] time 0.159 (0.227) data 0.000 (0.042) loss 1.1960 (0.9775) acc 76.6304 (77.5785) lr 1.7705e-03 eta 0:07:11
epoch [13/50] batch [45/51] time 0.159 (0.220) data 0.000 (0.038) loss 1.1133 (0.9756) acc 72.2222 (77.4664) lr 1.7705e-03 eta 0:06:56
epoch [13/50] batch [50/51] time 0.165 (0.214) data 0.001 (0.034) loss 0.9221 (0.9769) acc 76.0417 (77.3790) lr 1.7705e-03 eta 0:06:44
>>> alpha1: 0.385  alpha2: 0.143 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.26 <<<
epoch [14/50] batch [5/51] time 0.174 (0.418) data 0.000 (0.252) loss 1.0204 (0.9901) acc 75.0000 (76.1469) lr 1.7290e-03 eta 0:13:06
epoch [14/50] batch [10/51] time 0.155 (0.292) data 0.000 (0.126) loss 1.1712 (0.9633) acc 72.7273 (77.2213) lr 1.7290e-03 eta 0:09:09
epoch [14/50] batch [15/51] time 0.177 (0.251) data 0.000 (0.084) loss 1.1320 (0.9425) acc 71.2264 (78.2396) lr 1.7290e-03 eta 0:07:50
epoch [14/50] batch [20/51] time 0.164 (0.231) data 0.000 (0.063) loss 0.9495 (0.9287) acc 75.5208 (78.3358) lr 1.7290e-03 eta 0:07:11
epoch [14/50] batch [25/51] time 0.159 (0.217) data 0.000 (0.051) loss 0.7680 (1.0177) acc 80.5556 (76.6750) lr 1.7290e-03 eta 0:06:44
epoch [14/50] batch [30/51] time 0.167 (0.209) data 0.000 (0.042) loss 0.7257 (0.9838) acc 86.7347 (77.6806) lr 1.7290e-03 eta 0:06:27
epoch [14/50] batch [35/51] time 0.165 (0.222) data 0.000 (0.036) loss 0.7084 (0.9618) acc 85.1064 (78.3190) lr 1.7290e-03 eta 0:06:51
epoch [14/50] batch [40/51] time 0.152 (0.215) data 0.000 (0.032) loss 0.8807 (0.9400) acc 77.9070 (78.8570) lr 1.7290e-03 eta 0:06:37
epoch [14/50] batch [45/51] time 0.150 (0.210) data 0.000 (0.028) loss 1.1131 (0.9374) acc 70.8333 (78.7650) lr 1.7290e-03 eta 0:06:25
epoch [14/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.025) loss 0.9118 (0.9438) acc 82.0000 (78.3325) lr 1.7290e-03 eta 0:06:15
>>> alpha1: 0.346  alpha2: 0.119 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.28 <<<
epoch [15/50] batch [5/51] time 0.186 (0.459) data 0.000 (0.282) loss 0.8185 (0.8931) acc 81.0000 (78.5396) lr 1.6845e-03 eta 0:14:00
epoch [15/50] batch [10/51] time 0.167 (0.317) data 0.000 (0.141) loss 0.8366 (0.8346) acc 80.6122 (80.6643) lr 1.6845e-03 eta 0:09:39
epoch [15/50] batch [15/51] time 0.173 (0.270) data 0.000 (0.094) loss 0.9782 (0.8619) acc 78.9216 (80.4866) lr 1.6845e-03 eta 0:08:12
epoch [15/50] batch [20/51] time 0.180 (0.248) data 0.000 (0.071) loss 0.9518 (0.8790) acc 75.0000 (79.5194) lr 1.6845e-03 eta 0:07:30
epoch [15/50] batch [25/51] time 0.179 (0.237) data 0.000 (0.057) loss 0.8984 (0.8647) acc 77.4510 (79.8855) lr 1.6845e-03 eta 0:07:09
epoch [15/50] batch [30/51] time 0.191 (0.228) data 0.000 (0.047) loss 0.9998 (0.8602) acc 77.4510 (80.0722) lr 1.6845e-03 eta 0:06:51
epoch [15/50] batch [35/51] time 0.172 (0.221) data 0.000 (0.041) loss 0.8178 (0.8571) acc 79.4118 (79.9716) lr 1.6845e-03 eta 0:06:37
epoch [15/50] batch [40/51] time 0.155 (0.214) data 0.000 (0.036) loss 1.0379 (0.8618) acc 72.7273 (79.8018) lr 1.6845e-03 eta 0:06:25
epoch [15/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.032) loss 0.9150 (0.8510) acc 78.0000 (80.1021) lr 1.6845e-03 eta 0:06:14
epoch [15/50] batch [50/51] time 0.161 (0.205) data 0.000 (0.028) loss 1.0981 (0.8560) acc 70.2128 (79.9709) lr 1.6845e-03 eta 0:06:05
>>> alpha1: 0.255  alpha2: 0.036 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [16/50] batch [5/51] time 0.182 (0.463) data 0.000 (0.282) loss 0.7631 (0.7611) acc 81.1321 (82.3487) lr 1.6374e-03 eta 0:13:43
epoch [16/50] batch [10/51] time 0.165 (0.319) data 0.000 (0.141) loss 0.9628 (0.8241) acc 76.0417 (79.9191) lr 1.6374e-03 eta 0:09:26
epoch [16/50] batch [15/51] time 0.177 (0.271) data 0.000 (0.094) loss 0.8764 (0.7906) acc 76.5306 (80.0011) lr 1.6374e-03 eta 0:08:00
epoch [16/50] batch [20/51] time 0.182 (0.249) data 0.000 (0.071) loss 0.8345 (0.7957) acc 77.5510 (80.0972) lr 1.6374e-03 eta 0:07:19
epoch [16/50] batch [25/51] time 0.163 (0.232) data 0.000 (0.057) loss 0.9582 (0.8245) acc 78.7234 (79.9191) lr 1.6374e-03 eta 0:06:49
epoch [16/50] batch [30/51] time 0.181 (0.243) data 0.000 (0.047) loss 0.7015 (0.8151) acc 86.7347 (80.4755) lr 1.6374e-03 eta 0:07:05
epoch [16/50] batch [35/51] time 0.174 (0.233) data 0.000 (0.040) loss 0.8056 (0.8106) acc 79.3269 (80.5747) lr 1.6374e-03 eta 0:06:47
epoch [16/50] batch [40/51] time 0.171 (0.225) data 0.000 (0.035) loss 0.8583 (0.8168) acc 77.4510 (80.7595) lr 1.6374e-03 eta 0:06:32
epoch [16/50] batch [45/51] time 0.172 (0.218) data 0.000 (0.032) loss 0.5020 (0.7984) acc 84.6154 (81.1749) lr 1.6374e-03 eta 0:06:20
epoch [16/50] batch [50/51] time 0.160 (0.213) data 0.000 (0.028) loss 0.6275 (0.7874) acc 86.7021 (81.6118) lr 1.6374e-03 eta 0:06:09
>>> alpha1: 0.216  alpha2: 0.006 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.16 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [17/50] batch [5/51] time 0.176 (0.498) data 0.001 (0.321) loss 0.7307 (0.7639) acc 83.6538 (82.3888) lr 1.5878e-03 eta 0:14:20
epoch [17/50] batch [10/51] time 0.176 (0.334) data 0.000 (0.160) loss 0.6204 (0.7580) acc 86.5000 (82.5276) lr 1.5878e-03 eta 0:09:35
epoch [17/50] batch [15/51] time 0.194 (0.280) data 0.001 (0.107) loss 0.5491 (0.7654) acc 88.0000 (82.7301) lr 1.5878e-03 eta 0:08:01
epoch [17/50] batch [20/51] time 0.178 (0.255) data 0.000 (0.080) loss 0.6271 (0.7750) acc 86.7924 (82.5997) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [25/51] time 0.159 (0.240) data 0.000 (0.064) loss 1.1109 (0.7726) acc 72.2222 (82.9118) lr 1.5878e-03 eta 0:06:49
epoch [17/50] batch [30/51] time 0.174 (0.229) data 0.000 (0.054) loss 1.0155 (0.7732) acc 72.8261 (82.6386) lr 1.5878e-03 eta 0:06:30
epoch [17/50] batch [35/51] time 0.178 (0.222) data 0.000 (0.046) loss 0.8914 (0.7676) acc 81.6038 (82.8697) lr 1.5878e-03 eta 0:06:17
epoch [17/50] batch [40/51] time 0.160 (0.216) data 0.000 (0.040) loss 0.9530 (0.7594) acc 80.3191 (83.1021) lr 1.5878e-03 eta 0:06:05
epoch [17/50] batch [45/51] time 0.183 (0.211) data 0.001 (0.036) loss 0.7777 (0.7617) acc 75.4717 (82.6630) lr 1.5878e-03 eta 0:05:55
epoch [17/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.032) loss 0.6616 (0.7642) acc 89.1509 (82.5049) lr 1.5878e-03 eta 0:05:48
>>> alpha1: 0.201  alpha2: 0.004 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.28 <<<
epoch [18/50] batch [5/51] time 0.176 (0.464) data 0.000 (0.287) loss 0.8142 (0.7325) acc 80.3922 (83.8870) lr 1.5358e-03 eta 0:12:57
epoch [18/50] batch [10/51] time 0.175 (0.317) data 0.000 (0.144) loss 0.9620 (0.7814) acc 75.0000 (82.5282) lr 1.5358e-03 eta 0:08:49
epoch [18/50] batch [15/51] time 0.187 (0.273) data 0.000 (0.096) loss 0.6741 (0.7254) acc 83.8235 (84.4827) lr 1.5358e-03 eta 0:07:35
epoch [18/50] batch [20/51] time 0.156 (0.246) data 0.000 (0.072) loss 0.9782 (0.7362) acc 76.1364 (83.7846) lr 1.5358e-03 eta 0:06:49
epoch [18/50] batch [25/51] time 0.175 (0.233) data 0.000 (0.058) loss 0.7880 (0.7260) acc 82.6531 (84.0207) lr 1.5358e-03 eta 0:06:26
epoch [18/50] batch [30/51] time 0.169 (0.223) data 0.000 (0.048) loss 0.7591 (0.7251) acc 80.1020 (83.7184) lr 1.5358e-03 eta 0:06:08
epoch [18/50] batch [35/51] time 0.203 (0.218) data 0.000 (0.041) loss 0.6805 (0.7195) acc 85.1852 (83.9147) lr 1.5358e-03 eta 0:06:00
epoch [18/50] batch [40/51] time 0.173 (0.213) data 0.000 (0.036) loss 0.6269 (0.7280) acc 87.5000 (83.8316) lr 1.5358e-03 eta 0:05:49
epoch [18/50] batch [45/51] time 0.162 (0.208) data 0.000 (0.032) loss 0.4079 (0.7118) acc 89.8936 (84.1785) lr 1.5358e-03 eta 0:05:40
epoch [18/50] batch [50/51] time 0.172 (0.204) data 0.000 (0.029) loss 0.7939 (0.7105) acc 81.2500 (84.1223) lr 1.5358e-03 eta 0:05:33
>>> alpha1: 0.190  alpha2: 0.001 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.27 <<<
epoch [19/50] batch [5/51] time 0.181 (0.449) data 0.000 (0.266) loss 0.7974 (0.6883) acc 81.9444 (85.2272) lr 1.4818e-03 eta 0:12:11
epoch [19/50] batch [10/51] time 0.185 (0.316) data 0.000 (0.133) loss 0.8891 (0.7506) acc 79.0000 (84.8804) lr 1.4818e-03 eta 0:08:32
epoch [19/50] batch [15/51] time 0.170 (0.268) data 0.000 (0.089) loss 2.6730 (0.8496) acc 59.5000 (83.6047) lr 1.4818e-03 eta 0:07:13
epoch [19/50] batch [20/51] time 0.167 (0.247) data 0.000 (0.068) loss 0.7578 (0.8640) acc 84.1837 (84.1602) lr 1.4818e-03 eta 0:06:38
epoch [19/50] batch [25/51] time 0.164 (0.233) data 0.000 (0.054) loss 0.5888 (0.8325) acc 89.4445 (84.0836) lr 1.4818e-03 eta 0:06:14
epoch [19/50] batch [30/51] time 0.176 (0.222) data 0.000 (0.045) loss 0.6000 (0.8298) acc 84.5000 (83.5456) lr 1.4818e-03 eta 0:05:56
epoch [19/50] batch [35/51] time 0.197 (0.217) data 0.000 (0.039) loss 0.7176 (0.8101) acc 79.0909 (83.3372) lr 1.4818e-03 eta 0:05:46
epoch [19/50] batch [40/51] time 0.173 (0.212) data 0.000 (0.034) loss 0.6896 (0.7896) acc 88.9423 (83.8756) lr 1.4818e-03 eta 0:05:36
epoch [19/50] batch [45/51] time 0.165 (0.207) data 0.000 (0.030) loss 0.6414 (0.7856) acc 89.2857 (83.7235) lr 1.4818e-03 eta 0:05:27
epoch [19/50] batch [50/51] time 0.160 (0.203) data 0.000 (0.027) loss 0.8904 (0.7764) acc 74.4565 (83.6759) lr 1.4818e-03 eta 0:05:20
>>> alpha1: 0.185  alpha2: 0.004 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [20/50] batch [5/51] time 0.195 (0.487) data 0.000 (0.303) loss 0.5343 (0.6020) acc 89.2157 (86.1005) lr 1.4258e-03 eta 0:12:47
epoch [20/50] batch [10/51] time 0.186 (0.333) data 0.000 (0.152) loss 0.6269 (0.6298) acc 88.2075 (86.0126) lr 1.4258e-03 eta 0:08:42
epoch [20/50] batch [15/51] time 0.183 (0.281) data 0.000 (0.101) loss 0.3876 (0.6131) acc 93.6274 (86.2724) lr 1.4258e-03 eta 0:07:19
epoch [20/50] batch [20/51] time 0.177 (0.254) data 0.000 (0.076) loss 0.4702 (0.6352) acc 92.4528 (85.7801) lr 1.4258e-03 eta 0:06:36
epoch [20/50] batch [25/51] time 0.176 (0.239) data 0.000 (0.061) loss 0.5373 (0.6436) acc 89.7059 (85.6883) lr 1.4258e-03 eta 0:06:11
epoch [20/50] batch [30/51] time 0.155 (0.228) data 0.000 (0.051) loss 0.6373 (0.6583) acc 79.2683 (84.9618) lr 1.4258e-03 eta 0:05:53
epoch [20/50] batch [35/51] time 0.179 (0.220) data 0.000 (0.044) loss 0.5589 (0.6606) acc 89.3519 (84.9374) lr 1.4258e-03 eta 0:05:40
epoch [20/50] batch [40/51] time 0.167 (0.215) data 0.000 (0.038) loss 0.8357 (0.6534) acc 86.0000 (85.1344) lr 1.4258e-03 eta 0:05:30
epoch [20/50] batch [45/51] time 0.166 (0.209) data 0.000 (0.034) loss 0.5087 (0.6525) acc 86.2245 (84.9362) lr 1.4258e-03 eta 0:05:21
epoch [20/50] batch [50/51] time 0.176 (0.206) data 0.000 (0.031) loss 0.6950 (0.6602) acc 82.5472 (84.7046) lr 1.4258e-03 eta 0:05:15
>>> alpha1: 0.180  alpha2: 0.005 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.27 <<<
epoch [21/50] batch [5/51] time 0.172 (0.680) data 0.000 (0.354) loss 0.6043 (0.5887) acc 86.2245 (87.2959) lr 1.3681e-03 eta 0:17:17
epoch [21/50] batch [10/51] time 0.180 (0.427) data 0.000 (0.177) loss 0.7983 (0.6206) acc 83.0189 (86.7447) lr 1.3681e-03 eta 0:10:48
epoch [21/50] batch [15/51] time 0.167 (0.343) data 0.001 (0.118) loss 0.5500 (0.6292) acc 86.7347 (86.4019) lr 1.3681e-03 eta 0:08:39
epoch [21/50] batch [20/51] time 0.189 (0.301) data 0.001 (0.089) loss 0.6849 (0.6345) acc 82.4561 (86.1794) lr 1.3681e-03 eta 0:07:34
epoch [21/50] batch [25/51] time 0.190 (0.276) data 0.000 (0.071) loss 0.6966 (0.6286) acc 86.3636 (86.3552) lr 1.3681e-03 eta 0:06:55
epoch [21/50] batch [30/51] time 0.171 (0.259) data 0.000 (0.059) loss 0.6616 (0.6507) acc 83.1633 (85.6451) lr 1.3681e-03 eta 0:06:27
epoch [21/50] batch [35/51] time 0.171 (0.246) data 0.000 (0.051) loss 0.4966 (0.6410) acc 90.1961 (85.9594) lr 1.3681e-03 eta 0:06:07
epoch [21/50] batch [40/51] time 0.181 (0.238) data 0.000 (0.045) loss 0.5269 (0.6375) acc 90.9091 (86.0836) lr 1.3681e-03 eta 0:05:54
epoch [21/50] batch [45/51] time 0.169 (0.231) data 0.000 (0.040) loss 0.7633 (0.6453) acc 84.5000 (85.8641) lr 1.3681e-03 eta 0:05:42
epoch [21/50] batch [50/51] time 0.173 (0.225) data 0.000 (0.036) loss 0.6184 (0.6498) acc 85.0962 (85.6357) lr 1.3681e-03 eta 0:05:32
>>> alpha1: 0.173  alpha2: 0.000 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [22/50] batch [5/51] time 0.181 (0.624) data 0.000 (0.301) loss 0.5011 (0.6512) acc 88.4259 (86.3518) lr 1.3090e-03 eta 0:15:20
epoch [22/50] batch [10/51] time 0.193 (0.403) data 0.000 (0.150) loss 0.5140 (0.6289) acc 91.0377 (86.5050) lr 1.3090e-03 eta 0:09:52
epoch [22/50] batch [15/51] time 0.189 (0.327) data 0.001 (0.100) loss 0.5370 (0.6287) acc 89.1509 (86.7860) lr 1.3090e-03 eta 0:07:58
epoch [22/50] batch [20/51] time 0.192 (0.290) data 0.000 (0.075) loss 0.5949 (0.6279) acc 85.5263 (86.3625) lr 1.3090e-03 eta 0:07:02
epoch [22/50] batch [25/51] time 0.183 (0.266) data 0.000 (0.060) loss 0.5451 (0.6211) acc 88.4615 (86.7019) lr 1.3090e-03 eta 0:06:26
epoch [22/50] batch [30/51] time 0.171 (0.251) data 0.000 (0.050) loss 0.7806 (0.6253) acc 79.0816 (86.5991) lr 1.3090e-03 eta 0:06:03
epoch [22/50] batch [35/51] time 0.171 (0.240) data 0.000 (0.043) loss 0.6137 (0.6221) acc 85.7955 (86.6643) lr 1.3090e-03 eta 0:05:46
epoch [22/50] batch [40/51] time 0.169 (0.232) data 0.000 (0.038) loss 0.5069 (0.6289) acc 88.7255 (86.4431) lr 1.3090e-03 eta 0:05:33
epoch [22/50] batch [45/51] time 0.175 (0.225) data 0.000 (0.034) loss 0.4671 (0.6336) acc 89.1509 (86.0844) lr 1.3090e-03 eta 0:05:22
epoch [22/50] batch [50/51] time 0.177 (0.220) data 0.000 (0.030) loss 0.6011 (0.6373) acc 85.1852 (85.9925) lr 1.3090e-03 eta 0:05:14
>>> alpha1: 0.169  alpha2: -0.000 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [23/50] batch [5/51] time 0.167 (0.534) data 0.000 (0.351) loss 0.7002 (0.6215) acc 83.6735 (86.8662) lr 1.2487e-03 eta 0:12:39
epoch [23/50] batch [10/51] time 0.170 (0.356) data 0.000 (0.176) loss 0.5743 (0.6119) acc 88.0435 (87.0070) lr 1.2487e-03 eta 0:08:25
epoch [23/50] batch [15/51] time 0.202 (0.296) data 0.000 (0.117) loss 0.6686 (0.7677) acc 86.2245 (84.8652) lr 1.2487e-03 eta 0:06:58
epoch [23/50] batch [20/51] time 0.171 (0.265) data 0.000 (0.088) loss 0.7570 (0.7483) acc 81.8627 (84.6130) lr 1.2487e-03 eta 0:06:12
epoch [23/50] batch [25/51] time 0.182 (0.246) data 0.000 (0.070) loss 0.4330 (0.7104) acc 89.0909 (85.0983) lr 1.2487e-03 eta 0:05:45
epoch [23/50] batch [30/51] time 0.183 (0.235) data 0.000 (0.059) loss 0.4506 (0.6897) acc 89.8148 (85.3833) lr 1.2487e-03 eta 0:05:29
epoch [23/50] batch [35/51] time 0.199 (0.228) data 0.000 (0.050) loss 0.5765 (0.6764) acc 87.2642 (85.5408) lr 1.2487e-03 eta 0:05:17
epoch [23/50] batch [40/51] time 0.183 (0.222) data 0.000 (0.044) loss 0.4689 (0.6684) acc 91.0714 (85.8400) lr 1.2487e-03 eta 0:05:07
epoch [23/50] batch [45/51] time 0.164 (0.215) data 0.000 (0.039) loss 0.9501 (0.6740) acc 76.0417 (85.5996) lr 1.2487e-03 eta 0:04:57
epoch [23/50] batch [50/51] time 0.159 (0.210) data 0.000 (0.035) loss 0.7431 (0.6665) acc 80.9783 (85.7126) lr 1.2487e-03 eta 0:04:49
>>> alpha1: 0.165  alpha2: -0.002 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [24/50] batch [5/51] time 0.171 (0.475) data 0.000 (0.303) loss 0.5776 (0.5630) acc 84.8039 (86.3983) lr 1.1874e-03 eta 0:10:51
epoch [24/50] batch [10/51] time 0.190 (0.323) data 0.000 (0.152) loss 0.4665 (0.5744) acc 91.6667 (87.1853) lr 1.1874e-03 eta 0:07:21
epoch [24/50] batch [15/51] time 0.177 (0.272) data 0.000 (0.101) loss 0.6428 (0.5935) acc 84.3750 (86.4461) lr 1.1874e-03 eta 0:06:10
epoch [24/50] batch [20/51] time 0.180 (0.248) data 0.001 (0.076) loss 0.6958 (0.6057) acc 86.5000 (86.3794) lr 1.1874e-03 eta 0:05:36
epoch [24/50] batch [25/51] time 0.173 (0.233) data 0.000 (0.061) loss 0.6153 (0.6051) acc 90.1961 (86.5509) lr 1.1874e-03 eta 0:05:14
epoch [24/50] batch [30/51] time 0.173 (0.224) data 0.001 (0.051) loss 0.6882 (0.6012) acc 83.8235 (86.8486) lr 1.1874e-03 eta 0:05:01
epoch [24/50] batch [35/51] time 0.179 (0.217) data 0.000 (0.043) loss 0.4380 (0.5963) acc 90.6863 (86.9810) lr 1.1874e-03 eta 0:04:50
epoch [24/50] batch [40/51] time 0.171 (0.212) data 0.000 (0.038) loss 0.3942 (0.5905) acc 92.1569 (87.1494) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [45/51] time 0.171 (0.207) data 0.000 (0.034) loss 0.6941 (0.5910) acc 83.8235 (86.9069) lr 1.1874e-03 eta 0:04:35
epoch [24/50] batch [50/51] time 0.170 (0.203) data 0.000 (0.031) loss 0.7733 (0.5985) acc 82.3529 (86.5963) lr 1.1874e-03 eta 0:04:29
>>> alpha1: 0.160  alpha2: -0.005 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [25/50] batch [5/51] time 0.164 (0.484) data 0.000 (0.310) loss 0.8373 (0.5678) acc 80.3191 (87.0472) lr 1.1253e-03 eta 0:10:39
epoch [25/50] batch [10/51] time 0.185 (0.331) data 0.000 (0.155) loss 0.5080 (0.5863) acc 88.8889 (86.9297) lr 1.1253e-03 eta 0:07:15
epoch [25/50] batch [15/51] time 0.165 (0.277) data 0.000 (0.104) loss 0.5909 (0.6267) acc 87.5000 (86.0691) lr 1.1253e-03 eta 0:06:02
epoch [25/50] batch [20/51] time 0.191 (0.253) data 0.000 (0.078) loss 0.4588 (0.6075) acc 93.2692 (86.7494) lr 1.1253e-03 eta 0:05:29
epoch [25/50] batch [25/51] time 0.177 (0.237) data 0.000 (0.062) loss 0.7170 (0.6117) acc 81.7308 (86.5071) lr 1.1253e-03 eta 0:05:08
epoch [25/50] batch [30/51] time 0.192 (0.228) data 0.000 (0.052) loss 0.6310 (0.6141) acc 88.2075 (86.4560) lr 1.1253e-03 eta 0:04:55
epoch [25/50] batch [35/51] time 0.209 (0.222) data 0.000 (0.045) loss 0.5972 (0.6107) acc 88.2653 (86.2808) lr 1.1253e-03 eta 0:04:46
epoch [25/50] batch [40/51] time 0.160 (0.215) data 0.000 (0.039) loss 0.6046 (0.6408) acc 83.6956 (86.1076) lr 1.1253e-03 eta 0:04:36
epoch [25/50] batch [45/51] time 0.164 (0.210) data 0.001 (0.035) loss 0.5326 (0.6254) acc 86.7021 (86.3507) lr 1.1253e-03 eta 0:04:29
epoch [25/50] batch [50/51] time 0.161 (0.206) data 0.000 (0.031) loss 0.5894 (0.6221) acc 85.1064 (86.3787) lr 1.1253e-03 eta 0:04:22
>>> alpha1: 0.154  alpha2: -0.005 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.25 <<<
epoch [26/50] batch [5/51] time 0.175 (0.456) data 0.001 (0.276) loss 0.7572 (0.5601) acc 86.1702 (89.9698) lr 1.0628e-03 eta 0:09:38
epoch [26/50] batch [10/51] time 0.166 (0.314) data 0.000 (0.138) loss 0.6571 (0.5555) acc 86.9318 (89.3815) lr 1.0628e-03 eta 0:06:37
epoch [26/50] batch [15/51] time 0.178 (0.268) data 0.000 (0.092) loss 0.7582 (0.5795) acc 85.4167 (89.0939) lr 1.0628e-03 eta 0:05:37
epoch [26/50] batch [20/51] time 0.154 (0.243) data 0.000 (0.069) loss 0.5202 (0.5788) acc 91.2791 (88.9735) lr 1.0628e-03 eta 0:05:04
epoch [26/50] batch [25/51] time 0.161 (0.228) data 0.000 (0.055) loss 0.5153 (0.5771) acc 89.6739 (88.7054) lr 1.0628e-03 eta 0:04:44
epoch [26/50] batch [30/51] time 0.173 (0.218) data 0.000 (0.046) loss 0.7204 (0.5731) acc 86.0000 (88.4658) lr 1.0628e-03 eta 0:04:31
epoch [26/50] batch [35/51] time 0.165 (0.212) data 0.000 (0.040) loss 0.6018 (0.5629) acc 88.5417 (88.6193) lr 1.0628e-03 eta 0:04:23
epoch [26/50] batch [40/51] time 0.172 (0.207) data 0.000 (0.035) loss 0.5296 (0.5705) acc 88.4615 (88.2733) lr 1.0628e-03 eta 0:04:15
epoch [26/50] batch [45/51] time 0.159 (0.202) data 0.000 (0.031) loss 0.6150 (0.5657) acc 83.6956 (88.2703) lr 1.0628e-03 eta 0:04:08
epoch [26/50] batch [50/51] time 0.170 (0.199) data 0.000 (0.028) loss 0.5782 (0.5681) acc 84.3137 (88.0815) lr 1.0628e-03 eta 0:04:04
>>> alpha1: 0.151  alpha2: -0.003 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [27/50] batch [5/51] time 0.171 (0.448) data 0.002 (0.262) loss 0.5878 (0.6330) acc 85.2041 (87.8508) lr 1.0000e-03 eta 0:09:05
epoch [27/50] batch [10/51] time 0.168 (0.311) data 0.000 (0.131) loss 0.4523 (0.6000) acc 92.1875 (87.5616) lr 1.0000e-03 eta 0:06:17
epoch [27/50] batch [15/51] time 0.186 (0.268) data 0.000 (0.088) loss 0.6545 (0.5826) acc 86.4583 (88.0735) lr 1.0000e-03 eta 0:05:24
epoch [27/50] batch [20/51] time 0.192 (0.247) data 0.000 (0.066) loss 0.5513 (0.5753) acc 88.2075 (87.7277) lr 1.0000e-03 eta 0:04:56
epoch [27/50] batch [25/51] time 0.181 (0.233) data 0.000 (0.053) loss 0.3836 (0.5713) acc 91.9811 (87.6187) lr 1.0000e-03 eta 0:04:38
epoch [27/50] batch [30/51] time 0.158 (0.223) data 0.000 (0.044) loss 0.7613 (0.5725) acc 83.5227 (87.5971) lr 1.0000e-03 eta 0:04:25
epoch [27/50] batch [35/51] time 0.195 (0.217) data 0.000 (0.038) loss 0.6443 (0.5684) acc 86.7647 (87.9121) lr 1.0000e-03 eta 0:04:17
epoch [27/50] batch [40/51] time 0.176 (0.211) data 0.000 (0.033) loss 0.3878 (0.5758) acc 92.4528 (87.7314) lr 1.0000e-03 eta 0:04:10
epoch [27/50] batch [45/51] time 0.179 (0.206) data 0.000 (0.029) loss 0.5189 (0.5805) acc 90.7407 (87.5065) lr 1.0000e-03 eta 0:04:03
epoch [27/50] batch [50/51] time 0.170 (0.202) data 0.000 (0.027) loss 0.4345 (0.5814) acc 91.1765 (87.4625) lr 1.0000e-03 eta 0:03:57
>>> alpha1: 0.147  alpha2: -0.002 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.24 <<<
epoch [28/50] batch [5/51] time 0.196 (0.429) data 0.000 (0.235) loss 0.5479 (0.4428) acc 86.5741 (90.6026) lr 9.3721e-04 eta 0:08:20
epoch [28/50] batch [10/51] time 0.173 (0.304) data 0.000 (0.118) loss 0.5434 (0.4942) acc 86.0577 (88.7052) lr 9.3721e-04 eta 0:05:53
epoch [28/50] batch [15/51] time 0.181 (0.262) data 0.000 (0.078) loss 0.5037 (0.4904) acc 87.9630 (88.9449) lr 9.3721e-04 eta 0:05:03
epoch [28/50] batch [20/51] time 0.168 (0.241) data 0.000 (0.059) loss 0.5152 (0.4925) acc 90.3061 (89.2132) lr 9.3721e-04 eta 0:04:38
epoch [28/50] batch [25/51] time 0.174 (0.228) data 0.000 (0.047) loss 0.7127 (0.5042) acc 85.2041 (88.8582) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [30/51] time 0.185 (0.220) data 0.000 (0.039) loss 0.7517 (0.5243) acc 84.1346 (88.7131) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [35/51] time 0.178 (0.215) data 0.000 (0.034) loss 0.7104 (0.5409) acc 81.3830 (88.2984) lr 9.3721e-04 eta 0:04:04
epoch [28/50] batch [40/51] time 0.168 (0.209) data 0.000 (0.030) loss 0.3410 (0.5314) acc 95.9184 (88.5699) lr 9.3721e-04 eta 0:03:56
epoch [28/50] batch [45/51] time 0.165 (0.205) data 0.000 (0.026) loss 0.5096 (0.5368) acc 88.5417 (88.3929) lr 9.3721e-04 eta 0:03:51
epoch [28/50] batch [50/51] time 0.163 (0.202) data 0.000 (0.024) loss 0.5977 (0.5339) acc 88.5417 (88.5242) lr 9.3721e-04 eta 0:03:46
>>> alpha1: 0.147  alpha2: 0.002 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [29/50] batch [5/51] time 0.185 (0.546) data 0.000 (0.359) loss 0.4925 (0.4768) acc 92.1296 (90.6744) lr 8.7467e-04 eta 0:10:09
epoch [29/50] batch [10/51] time 0.167 (0.361) data 0.000 (0.180) loss 0.5259 (0.4669) acc 90.3061 (90.7335) lr 8.7467e-04 eta 0:06:41
epoch [29/50] batch [15/51] time 0.193 (0.299) data 0.000 (0.120) loss 0.4701 (0.5137) acc 87.5000 (89.3524) lr 8.7467e-04 eta 0:05:31
epoch [29/50] batch [20/51] time 0.162 (0.268) data 0.000 (0.090) loss 0.6535 (0.5024) acc 81.9149 (89.2352) lr 8.7467e-04 eta 0:04:54
epoch [29/50] batch [25/51] time 0.167 (0.250) data 0.000 (0.072) loss 0.7920 (0.5254) acc 83.6735 (88.9541) lr 8.7467e-04 eta 0:04:34
epoch [29/50] batch [30/51] time 0.174 (0.238) data 0.000 (0.060) loss 0.5426 (0.5311) acc 87.9808 (88.4967) lr 8.7467e-04 eta 0:04:20
epoch [29/50] batch [35/51] time 0.196 (0.230) data 0.000 (0.052) loss 0.6284 (0.5475) acc 82.5000 (88.0568) lr 8.7467e-04 eta 0:04:09
epoch [29/50] batch [40/51] time 0.168 (0.223) data 0.000 (0.045) loss 0.5594 (0.5512) acc 89.5000 (88.1079) lr 8.7467e-04 eta 0:04:01
epoch [29/50] batch [45/51] time 0.173 (0.218) data 0.000 (0.040) loss 0.4348 (0.5456) acc 88.9423 (88.1541) lr 8.7467e-04 eta 0:03:54
epoch [29/50] batch [50/51] time 0.169 (0.213) data 0.000 (0.036) loss 0.6295 (0.5559) acc 86.7647 (87.9336) lr 8.7467e-04 eta 0:03:48
>>> alpha1: 0.144  alpha2: 0.005 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.26 <<<
epoch [30/50] batch [5/51] time 0.175 (0.443) data 0.000 (0.251) loss 0.4950 (0.4970) acc 89.7059 (90.0820) lr 8.1262e-04 eta 0:07:52
epoch [30/50] batch [10/51] time 0.178 (0.312) data 0.000 (0.126) loss 0.5477 (0.5522) acc 84.1837 (87.7358) lr 8.1262e-04 eta 0:05:31
epoch [30/50] batch [15/51] time 0.186 (0.269) data 0.000 (0.084) loss 0.5539 (0.5631) acc 87.2727 (87.4432) lr 8.1262e-04 eta 0:04:44
epoch [30/50] batch [20/51] time 0.178 (0.247) data 0.000 (0.063) loss 0.4718 (0.5492) acc 89.6226 (87.6601) lr 8.1262e-04 eta 0:04:19
epoch [30/50] batch [25/51] time 0.180 (0.232) data 0.000 (0.051) loss 0.6561 (0.5439) acc 89.8936 (87.7114) lr 8.1262e-04 eta 0:04:03
epoch [30/50] batch [30/51] time 0.183 (0.223) data 0.000 (0.042) loss 0.5024 (0.5308) acc 91.0714 (88.1058) lr 8.1262e-04 eta 0:03:52
epoch [30/50] batch [35/51] time 0.170 (0.216) data 0.000 (0.036) loss 0.5313 (0.5351) acc 87.2340 (87.8047) lr 8.1262e-04 eta 0:03:43
epoch [30/50] batch [40/51] time 0.170 (0.210) data 0.000 (0.032) loss 0.6904 (0.5438) acc 85.2941 (87.7867) lr 8.1262e-04 eta 0:03:36
epoch [30/50] batch [45/51] time 0.191 (0.206) data 0.000 (0.028) loss 0.5206 (0.5449) acc 88.8393 (87.8008) lr 8.1262e-04 eta 0:03:31
epoch [30/50] batch [50/51] time 0.177 (0.202) data 0.000 (0.025) loss 0.4757 (0.5470) acc 88.6792 (87.5980) lr 8.1262e-04 eta 0:03:26
>>> alpha1: 0.142  alpha2: 0.007 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [31/50] batch [5/51] time 0.175 (0.528) data 0.000 (0.350) loss 0.4989 (0.5024) acc 88.9423 (88.9867) lr 7.5131e-04 eta 0:08:56
epoch [31/50] batch [10/51] time 0.174 (0.351) data 0.000 (0.175) loss 0.5630 (0.5124) acc 87.5000 (88.4651) lr 7.5131e-04 eta 0:05:54
epoch [31/50] batch [15/51] time 0.167 (0.292) data 0.000 (0.117) loss 0.6566 (0.5240) acc 84.1837 (88.0585) lr 7.5131e-04 eta 0:04:52
epoch [31/50] batch [20/51] time 0.159 (0.262) data 0.001 (0.088) loss 0.5272 (0.5231) acc 91.1111 (88.6197) lr 7.5131e-04 eta 0:04:22
epoch [31/50] batch [25/51] time 0.171 (0.244) data 0.000 (0.070) loss 0.7507 (0.5396) acc 84.8958 (88.4304) lr 7.5131e-04 eta 0:04:03
epoch [31/50] batch [30/51] time 0.180 (0.233) data 0.001 (0.059) loss 0.5442 (0.5421) acc 86.7924 (88.1443) lr 7.5131e-04 eta 0:03:50
epoch [31/50] batch [35/51] time 0.203 (0.227) data 0.000 (0.050) loss 0.5873 (0.5548) acc 87.5000 (87.8077) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [40/51] time 0.170 (0.220) data 0.000 (0.044) loss 0.7194 (0.5608) acc 82.8431 (87.7348) lr 7.5131e-04 eta 0:03:35
epoch [31/50] batch [45/51] time 0.168 (0.214) data 0.000 (0.039) loss 0.4782 (0.5423) acc 91.0000 (88.1467) lr 7.5131e-04 eta 0:03:28
epoch [31/50] batch [50/51] time 0.168 (0.210) data 0.000 (0.035) loss 0.5040 (0.5449) acc 92.0000 (88.2651) lr 7.5131e-04 eta 0:03:23
>>> alpha1: 0.140  alpha2: 0.007 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [32/50] batch [5/51] time 0.177 (0.474) data 0.000 (0.300) loss 0.5310 (0.4537) acc 89.1509 (90.1968) lr 6.9098e-04 eta 0:07:37
epoch [32/50] batch [10/51] time 0.167 (0.325) data 0.000 (0.150) loss 0.4300 (0.4596) acc 89.7959 (89.5451) lr 6.9098e-04 eta 0:05:11
epoch [32/50] batch [15/51] time 0.160 (0.275) data 0.000 (0.100) loss 0.4048 (0.4668) acc 91.8478 (89.9290) lr 6.9098e-04 eta 0:04:22
epoch [32/50] batch [20/51] time 0.160 (0.250) data 0.000 (0.075) loss 0.4593 (0.4714) acc 89.6739 (89.9635) lr 6.9098e-04 eta 0:03:56
epoch [32/50] batch [25/51] time 0.190 (0.235) data 0.000 (0.060) loss 0.5867 (0.4979) acc 87.7551 (88.9261) lr 6.9098e-04 eta 0:03:41
epoch [32/50] batch [30/51] time 0.183 (0.225) data 0.000 (0.050) loss 0.5701 (0.5152) acc 88.0000 (88.7392) lr 6.9098e-04 eta 0:03:31
epoch [32/50] batch [35/51] time 0.173 (0.218) data 0.000 (0.043) loss 0.3084 (0.5123) acc 96.6346 (88.8881) lr 6.9098e-04 eta 0:03:23
epoch [32/50] batch [40/51] time 0.171 (0.213) data 0.000 (0.038) loss 0.3252 (0.5060) acc 94.6078 (89.0148) lr 6.9098e-04 eta 0:03:17
epoch [32/50] batch [45/51] time 0.162 (0.208) data 0.000 (0.034) loss 0.5837 (0.5029) acc 86.1702 (89.0703) lr 6.9098e-04 eta 0:03:12
epoch [32/50] batch [50/51] time 0.168 (0.204) data 0.000 (0.030) loss 0.5930 (0.5018) acc 89.7959 (89.2034) lr 6.9098e-04 eta 0:03:07
>>> alpha1: 0.137  alpha2: 0.007 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [33/50] batch [5/51] time 0.176 (0.478) data 0.000 (0.292) loss 0.4902 (0.4959) acc 88.5870 (89.8837) lr 6.3188e-04 eta 0:07:16
epoch [33/50] batch [10/51] time 0.164 (0.327) data 0.000 (0.146) loss 0.3874 (0.5455) acc 90.4255 (88.7396) lr 6.3188e-04 eta 0:04:56
epoch [33/50] batch [15/51] time 0.171 (0.276) data 0.000 (0.098) loss 0.5990 (0.5376) acc 86.4583 (88.7105) lr 6.3188e-04 eta 0:04:09
epoch [33/50] batch [20/51] time 0.173 (0.251) data 0.000 (0.073) loss 0.5518 (0.5279) acc 86.7347 (88.7092) lr 6.3188e-04 eta 0:03:45
epoch [33/50] batch [25/51] time 0.168 (0.236) data 0.000 (0.059) loss 0.3538 (0.5185) acc 93.3673 (89.0665) lr 6.3188e-04 eta 0:03:31
epoch [33/50] batch [30/51] time 0.165 (0.227) data 0.000 (0.049) loss 0.7710 (0.5270) acc 88.3333 (88.8046) lr 6.3188e-04 eta 0:03:21
epoch [33/50] batch [35/51] time 0.185 (0.219) data 0.000 (0.042) loss 0.4586 (0.5304) acc 89.0000 (88.7697) lr 6.3188e-04 eta 0:03:13
epoch [33/50] batch [40/51] time 0.172 (0.214) data 0.000 (0.037) loss 0.6546 (0.5402) acc 87.0192 (88.5773) lr 6.3188e-04 eta 0:03:07
epoch [33/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.033) loss 0.5606 (0.5339) acc 89.7959 (88.5276) lr 6.3188e-04 eta 0:03:02
epoch [33/50] batch [50/51] time 0.174 (0.206) data 0.000 (0.030) loss 0.5078 (0.5311) acc 87.5000 (88.5200) lr 6.3188e-04 eta 0:02:58
>>> alpha1: 0.134  alpha2: 0.007 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.24 <<<
epoch [34/50] batch [5/51] time 0.173 (0.449) data 0.000 (0.275) loss 0.3059 (0.4519) acc 96.5686 (90.5778) lr 5.7422e-04 eta 0:06:27
epoch [34/50] batch [10/51] time 0.171 (0.309) data 0.000 (0.137) loss 0.5002 (0.4909) acc 88.0208 (89.4671) lr 5.7422e-04 eta 0:04:24
epoch [34/50] batch [15/51] time 0.192 (0.266) data 0.000 (0.092) loss 0.4375 (0.5089) acc 87.9630 (88.5742) lr 5.7422e-04 eta 0:03:46
epoch [34/50] batch [20/51] time 0.173 (0.244) data 0.000 (0.069) loss 0.4842 (0.5054) acc 90.3846 (88.7543) lr 5.7422e-04 eta 0:03:26
epoch [34/50] batch [25/51] time 0.178 (0.230) data 0.000 (0.055) loss 0.7044 (0.5143) acc 86.7347 (88.8705) lr 5.7422e-04 eta 0:03:13
epoch [34/50] batch [30/51] time 0.168 (0.221) data 0.000 (0.046) loss 0.4433 (0.5046) acc 90.4255 (89.0030) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [35/51] time 0.177 (0.215) data 0.000 (0.039) loss 0.6051 (0.5072) acc 87.7451 (88.9580) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [40/51] time 0.183 (0.211) data 0.000 (0.035) loss 0.3439 (0.5012) acc 94.1964 (89.2102) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [45/51] time 0.166 (0.205) data 0.000 (0.031) loss 0.4790 (0.5126) acc 85.7143 (88.9580) lr 5.7422e-04 eta 0:02:48
epoch [34/50] batch [50/51] time 0.159 (0.201) data 0.000 (0.028) loss 0.6737 (0.5134) acc 81.5217 (88.8375) lr 5.7422e-04 eta 0:02:44
>>> alpha1: 0.135  alpha2: 0.008 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.24 <<<
epoch [35/50] batch [5/51] time 0.179 (0.481) data 0.000 (0.299) loss 0.3286 (0.4417) acc 97.1698 (91.5850) lr 5.1825e-04 eta 0:06:29
epoch [35/50] batch [10/51] time 0.170 (0.327) data 0.000 (0.149) loss 0.2955 (0.4405) acc 94.6078 (91.3170) lr 5.1825e-04 eta 0:04:23
epoch [35/50] batch [15/51] time 0.179 (0.276) data 0.000 (0.100) loss 0.3803 (0.4642) acc 92.4528 (90.7700) lr 5.1825e-04 eta 0:03:40
epoch [35/50] batch [20/51] time 0.163 (0.251) data 0.001 (0.075) loss 0.4996 (0.4528) acc 85.1064 (90.5939) lr 5.1825e-04 eta 0:03:19
epoch [35/50] batch [25/51] time 0.177 (0.236) data 0.000 (0.060) loss 0.4628 (0.4608) acc 89.0625 (90.5513) lr 5.1825e-04 eta 0:03:07
epoch [35/50] batch [30/51] time 0.164 (0.225) data 0.000 (0.050) loss 0.6977 (0.4772) acc 82.2917 (90.0228) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.177 (0.218) data 0.000 (0.043) loss 0.6030 (0.4847) acc 87.7660 (89.5201) lr 5.1825e-04 eta 0:02:50
epoch [35/50] batch [40/51] time 0.166 (0.212) data 0.000 (0.038) loss 0.4598 (0.4775) acc 94.8980 (89.7699) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [45/51] time 0.168 (0.207) data 0.000 (0.033) loss 0.4006 (0.4752) acc 92.5000 (89.9912) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [50/51] time 0.173 (0.204) data 0.000 (0.030) loss 0.7005 (0.4861) acc 85.0962 (89.7520) lr 5.1825e-04 eta 0:02:36
>>> alpha1: 0.134  alpha2: 0.010 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.24 <<<
epoch [36/50] batch [5/51] time 0.175 (0.429) data 0.000 (0.253) loss 0.4269 (0.4420) acc 88.7255 (88.7503) lr 4.6417e-04 eta 0:05:26
epoch [36/50] batch [10/51] time 0.182 (0.304) data 0.000 (0.127) loss 0.4794 (0.4829) acc 88.2653 (88.8629) lr 4.6417e-04 eta 0:03:49
epoch [36/50] batch [15/51] time 0.175 (0.260) data 0.000 (0.085) loss 0.5739 (0.4861) acc 86.2245 (89.1398) lr 4.6417e-04 eta 0:03:15
epoch [36/50] batch [20/51] time 0.175 (0.241) data 0.000 (0.063) loss 0.4801 (0.4719) acc 90.9574 (89.5046) lr 4.6417e-04 eta 0:02:59
epoch [36/50] batch [25/51] time 0.178 (0.229) data 0.000 (0.051) loss 0.5372 (0.4858) acc 87.5000 (89.1866) lr 4.6417e-04 eta 0:02:49
epoch [36/50] batch [30/51] time 0.175 (0.219) data 0.000 (0.042) loss 0.4478 (0.4881) acc 87.5000 (88.9903) lr 4.6417e-04 eta 0:02:41
epoch [36/50] batch [35/51] time 0.167 (0.214) data 0.000 (0.036) loss 0.4727 (0.4852) acc 87.2449 (89.1822) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [40/51] time 0.171 (0.208) data 0.000 (0.032) loss 0.3985 (0.4825) acc 93.1373 (89.4375) lr 4.6417e-04 eta 0:02:31
epoch [36/50] batch [45/51] time 0.173 (0.205) data 0.000 (0.028) loss 0.5325 (0.4880) acc 91.3462 (89.3485) lr 4.6417e-04 eta 0:02:27
epoch [36/50] batch [50/51] time 0.168 (0.201) data 0.000 (0.026) loss 0.4865 (0.4904) acc 88.0000 (89.2190) lr 4.6417e-04 eta 0:02:23
>>> alpha1: 0.133  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.25 <<<
epoch [37/50] batch [5/51] time 0.199 (0.489) data 0.000 (0.292) loss 0.4266 (0.4174) acc 91.2037 (90.8408) lr 4.1221e-04 eta 0:05:46
epoch [37/50] batch [10/51] time 0.177 (0.334) data 0.000 (0.146) loss 0.5190 (0.4654) acc 86.0577 (89.5500) lr 4.1221e-04 eta 0:03:54
epoch [37/50] batch [15/51] time 0.174 (0.280) data 0.000 (0.098) loss 0.4339 (0.4619) acc 91.3462 (89.5615) lr 4.1221e-04 eta 0:03:15
epoch [37/50] batch [20/51] time 0.204 (0.255) data 0.000 (0.073) loss 0.3287 (0.4623) acc 93.7500 (89.9425) lr 4.1221e-04 eta 0:02:56
epoch [37/50] batch [25/51] time 0.178 (0.241) data 0.000 (0.059) loss 0.7121 (0.4738) acc 83.0000 (89.8661) lr 4.1221e-04 eta 0:02:46
epoch [37/50] batch [30/51] time 0.162 (0.230) data 0.000 (0.049) loss 0.5872 (0.4654) acc 89.8936 (90.1654) lr 4.1221e-04 eta 0:02:37
epoch [37/50] batch [35/51] time 0.188 (0.222) data 0.000 (0.042) loss 0.4759 (0.4782) acc 89.9038 (89.6566) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [40/51] time 0.155 (0.216) data 0.000 (0.037) loss 0.6011 (0.4932) acc 88.6364 (89.4457) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [45/51] time 0.183 (0.211) data 0.000 (0.033) loss 0.3871 (0.4878) acc 93.3036 (89.7148) lr 4.1221e-04 eta 0:02:21
epoch [37/50] batch [50/51] time 0.158 (0.208) data 0.000 (0.029) loss 0.8041 (0.4887) acc 86.1111 (89.6712) lr 4.1221e-04 eta 0:02:17
>>> alpha1: 0.132  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [38/50] batch [5/51] time 0.174 (0.487) data 0.000 (0.297) loss 0.5221 (0.5162) acc 90.8654 (89.8731) lr 3.6258e-04 eta 0:05:20
epoch [38/50] batch [10/51] time 0.180 (0.333) data 0.000 (0.149) loss 0.5287 (0.4877) acc 85.5769 (90.1730) lr 3.6258e-04 eta 0:03:37
epoch [38/50] batch [15/51] time 0.168 (0.279) data 0.000 (0.099) loss 0.5104 (0.5043) acc 86.7347 (89.1598) lr 3.6258e-04 eta 0:03:00
epoch [38/50] batch [20/51] time 0.181 (0.254) data 0.000 (0.075) loss 0.4330 (0.5364) acc 91.8182 (88.5138) lr 3.6258e-04 eta 0:02:43
epoch [38/50] batch [25/51] time 0.171 (0.239) data 0.000 (0.060) loss 0.6909 (0.5274) acc 88.2353 (88.8971) lr 3.6258e-04 eta 0:02:32
epoch [38/50] batch [30/51] time 0.171 (0.229) data 0.000 (0.050) loss 0.5574 (0.5264) acc 84.3137 (88.9441) lr 3.6258e-04 eta 0:02:24
epoch [38/50] batch [35/51] time 0.171 (0.221) data 0.000 (0.043) loss 0.4800 (0.5198) acc 87.7451 (89.0145) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [40/51] time 0.177 (0.215) data 0.000 (0.037) loss 0.4347 (0.5107) acc 92.4528 (89.3978) lr 3.6258e-04 eta 0:02:14
epoch [38/50] batch [45/51] time 0.177 (0.210) data 0.000 (0.033) loss 0.3134 (0.5039) acc 96.2264 (89.6000) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [50/51] time 0.170 (0.206) data 0.000 (0.030) loss 0.4549 (0.4997) acc 90.1961 (89.6660) lr 3.6258e-04 eta 0:02:06
>>> alpha1: 0.132  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.24 <<<
epoch [39/50] batch [5/51] time 0.176 (0.469) data 0.001 (0.289) loss 0.4447 (0.4506) acc 87.5000 (89.6932) lr 3.1545e-04 eta 0:04:44
epoch [39/50] batch [10/51] time 0.172 (0.323) data 0.000 (0.145) loss 0.5622 (0.4878) acc 87.2549 (88.6450) lr 3.1545e-04 eta 0:03:14
epoch [39/50] batch [15/51] time 0.168 (0.277) data 0.000 (0.097) loss 0.5377 (0.4787) acc 86.7347 (89.0114) lr 3.1545e-04 eta 0:02:45
epoch [39/50] batch [20/51] time 0.163 (0.251) data 0.000 (0.072) loss 0.7671 (0.4915) acc 82.9787 (88.5646) lr 3.1545e-04 eta 0:02:28
epoch [39/50] batch [25/51] time 0.183 (0.236) data 0.000 (0.058) loss 0.4482 (0.4817) acc 90.8163 (88.9229) lr 3.1545e-04 eta 0:02:18
epoch [39/50] batch [30/51] time 0.172 (0.226) data 0.000 (0.048) loss 0.5992 (0.4989) acc 87.2549 (89.2124) lr 3.1545e-04 eta 0:02:11
epoch [39/50] batch [35/51] time 0.172 (0.218) data 0.001 (0.042) loss 0.4938 (0.4962) acc 85.2941 (89.2295) lr 3.1545e-04 eta 0:02:06
epoch [39/50] batch [40/51] time 0.168 (0.212) data 0.000 (0.036) loss 0.4574 (0.4945) acc 88.5000 (89.3127) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [45/51] time 0.182 (0.208) data 0.000 (0.032) loss 0.3276 (0.4915) acc 90.9091 (89.2703) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [50/51] time 0.162 (0.204) data 0.000 (0.029) loss 0.5896 (0.4966) acc 85.6383 (89.2496) lr 3.1545e-04 eta 0:01:54
>>> alpha1: 0.130  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.15 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [40/50] batch [5/51] time 0.179 (0.453) data 0.000 (0.268) loss 0.4000 (0.4339) acc 92.7083 (91.5383) lr 2.7103e-04 eta 0:04:12
epoch [40/50] batch [10/51] time 0.191 (0.317) data 0.000 (0.134) loss 0.3141 (0.4212) acc 95.9184 (91.3539) lr 2.7103e-04 eta 0:02:54
epoch [40/50] batch [15/51] time 0.180 (0.269) data 0.000 (0.089) loss 0.4726 (0.4284) acc 92.1569 (91.2588) lr 2.7103e-04 eta 0:02:27
epoch [40/50] batch [20/51] time 0.905 (0.281) data 0.000 (0.067) loss 0.4649 (0.4499) acc 90.2542 (90.7241) lr 2.7103e-04 eta 0:02:32
epoch [40/50] batch [25/51] time 0.192 (0.261) data 0.000 (0.054) loss 0.5718 (0.4502) acc 88.4615 (90.6494) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [30/51] time 0.171 (0.247) data 0.000 (0.045) loss 0.3306 (0.4436) acc 90.6863 (90.8417) lr 2.7103e-04 eta 0:02:11
epoch [40/50] batch [35/51] time 0.173 (0.237) data 0.000 (0.039) loss 0.5570 (0.4532) acc 86.7347 (90.4646) lr 2.7103e-04 eta 0:02:04
epoch [40/50] batch [40/51] time 0.166 (0.228) data 0.000 (0.034) loss 0.5367 (0.4609) acc 88.2653 (90.1654) lr 2.7103e-04 eta 0:01:58
epoch [40/50] batch [45/51] time 0.169 (0.221) data 0.000 (0.030) loss 0.5240 (0.4673) acc 90.5000 (90.0760) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [50/51] time 0.167 (0.216) data 0.000 (0.027) loss 0.4103 (0.4678) acc 93.3673 (89.8941) lr 2.7103e-04 eta 0:01:50
>>> alpha1: 0.129  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [41/50] batch [5/51] time 0.186 (0.478) data 0.000 (0.299) loss 0.3039 (0.4863) acc 93.8679 (89.4839) lr 2.2949e-04 eta 0:04:01
epoch [41/50] batch [10/51] time 0.183 (0.328) data 0.000 (0.150) loss 0.4341 (0.5277) acc 91.0714 (89.5350) lr 2.2949e-04 eta 0:02:44
epoch [41/50] batch [15/51] time 0.186 (0.279) data 0.000 (0.100) loss 0.5530 (0.4909) acc 90.5000 (90.1153) lr 2.2949e-04 eta 0:02:17
epoch [41/50] batch [20/51] time 0.169 (0.253) data 0.000 (0.075) loss 0.5304 (0.4852) acc 89.5000 (90.1880) lr 2.2949e-04 eta 0:02:03
epoch [41/50] batch [25/51] time 0.202 (0.237) data 0.000 (0.060) loss 0.3151 (0.4823) acc 93.9655 (90.1259) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [30/51] time 0.169 (0.227) data 0.000 (0.050) loss 0.4745 (0.4799) acc 92.5000 (90.2845) lr 2.2949e-04 eta 0:01:48
epoch [41/50] batch [35/51] time 0.186 (0.220) data 0.000 (0.043) loss 0.6346 (0.4920) acc 86.7347 (90.1890) lr 2.2949e-04 eta 0:01:44
epoch [41/50] batch [40/51] time 0.174 (0.214) data 0.000 (0.038) loss 0.4648 (0.4838) acc 87.9808 (90.1971) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [45/51] time 0.174 (0.209) data 0.000 (0.034) loss 0.5273 (0.4882) acc 88.9423 (90.0878) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [50/51] time 0.180 (0.205) data 0.000 (0.030) loss 0.4039 (0.4841) acc 91.8269 (90.1336) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.130  alpha2: 0.009 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [42/50] batch [5/51] time 0.168 (0.445) data 0.000 (0.268) loss 0.4010 (0.4269) acc 90.3061 (90.5054) lr 1.9098e-04 eta 0:03:21
epoch [42/50] batch [10/51] time 0.188 (0.314) data 0.000 (0.134) loss 0.5253 (0.4440) acc 90.0000 (89.8770) lr 1.9098e-04 eta 0:02:20
epoch [42/50] batch [15/51] time 0.163 (0.269) data 0.000 (0.090) loss 0.6645 (0.4850) acc 82.9787 (88.7408) lr 1.9098e-04 eta 0:01:59
epoch [42/50] batch [20/51] time 0.183 (0.246) data 0.000 (0.067) loss 0.2918 (0.4850) acc 94.6429 (88.8133) lr 1.9098e-04 eta 0:01:48
epoch [42/50] batch [25/51] time 0.174 (0.232) data 0.000 (0.054) loss 0.2637 (0.4795) acc 95.6731 (89.2041) lr 1.9098e-04 eta 0:01:40
epoch [42/50] batch [30/51] time 0.176 (0.223) data 0.000 (0.045) loss 0.5944 (0.4666) acc 85.5000 (89.6736) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [35/51] time 0.170 (0.216) data 0.000 (0.039) loss 0.4063 (0.4615) acc 93.5000 (90.0991) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.167 (0.211) data 0.000 (0.034) loss 0.4214 (0.4687) acc 95.9184 (89.9338) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.179 (0.207) data 0.000 (0.030) loss 0.4302 (0.4692) acc 90.5660 (89.9118) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.171 (0.204) data 0.000 (0.027) loss 0.3735 (0.4698) acc 95.0980 (89.9744) lr 1.9098e-04 eta 0:01:23
>>> alpha1: 0.130  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [43/50] batch [5/51] time 0.166 (0.492) data 0.000 (0.308) loss 0.6103 (0.4756) acc 87.2449 (91.0245) lr 1.5567e-04 eta 0:03:18
epoch [43/50] batch [10/51] time 0.181 (0.334) data 0.000 (0.154) loss 0.4306 (0.4624) acc 93.7500 (90.8495) lr 1.5567e-04 eta 0:02:12
epoch [43/50] batch [15/51] time 0.178 (0.281) data 0.000 (0.103) loss 0.4502 (0.4809) acc 91.2037 (90.1656) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [20/51] time 0.167 (0.254) data 0.000 (0.077) loss 0.4574 (0.4718) acc 92.5000 (90.3832) lr 1.5567e-04 eta 0:01:38
epoch [43/50] batch [25/51] time 0.170 (0.237) data 0.001 (0.062) loss 0.4479 (0.4807) acc 90.7609 (90.0373) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.180 (0.227) data 0.000 (0.051) loss 0.5176 (0.4853) acc 88.6792 (89.6377) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.188 (0.221) data 0.000 (0.044) loss 0.3916 (0.4821) acc 90.1961 (89.7020) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.167 (0.215) data 0.000 (0.039) loss 0.4706 (0.4850) acc 88.7755 (89.5172) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [45/51] time 0.165 (0.210) data 0.000 (0.034) loss 0.5099 (0.4766) acc 91.1458 (89.7948) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.174 (0.206) data 0.000 (0.031) loss 0.4216 (0.4734) acc 91.8269 (89.9687) lr 1.5567e-04 eta 0:01:13
>>> alpha1: 0.129  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [44/50] batch [5/51] time 0.159 (0.506) data 0.000 (0.327) loss 0.5047 (0.4453) acc 89.4445 (91.4552) lr 1.2369e-04 eta 0:02:57
epoch [44/50] batch [10/51] time 0.179 (0.342) data 0.000 (0.164) loss 0.4449 (0.4555) acc 90.5000 (90.8511) lr 1.2369e-04 eta 0:01:58
epoch [44/50] batch [15/51] time 0.185 (0.288) data 0.000 (0.109) loss 0.6011 (0.4485) acc 90.1961 (91.4502) lr 1.2369e-04 eta 0:01:38
epoch [44/50] batch [20/51] time 0.181 (0.260) data 0.000 (0.082) loss 0.4159 (0.4500) acc 95.4082 (91.4050) lr 1.2369e-04 eta 0:01:27
epoch [44/50] batch [25/51] time 0.171 (0.243) data 0.000 (0.066) loss 0.3597 (0.4730) acc 96.5686 (90.5997) lr 1.2369e-04 eta 0:01:20
epoch [44/50] batch [30/51] time 0.169 (0.231) data 0.000 (0.055) loss 0.5419 (0.4689) acc 90.5000 (90.6652) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [35/51] time 0.192 (0.223) data 0.000 (0.047) loss 0.4670 (0.4738) acc 90.0943 (90.5339) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.166 (0.218) data 0.000 (0.041) loss 0.6173 (0.4703) acc 86.7347 (90.6598) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.171 (0.212) data 0.000 (0.037) loss 0.4511 (0.4765) acc 90.6863 (90.3605) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [50/51] time 0.177 (0.208) data 0.000 (0.033) loss 0.3234 (0.4727) acc 91.5094 (90.4353) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.130  alpha2: 0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.08 & unmatched refined noisy rate: 0.24 <<<
epoch [45/50] batch [5/51] time 0.178 (0.491) data 0.000 (0.304) loss 0.5207 (0.4475) acc 86.5385 (89.7862) lr 9.5173e-05 eta 0:02:27
epoch [45/50] batch [10/51] time 0.178 (0.333) data 0.000 (0.152) loss 0.4722 (0.4619) acc 91.8269 (90.2956) lr 9.5173e-05 eta 0:01:38
epoch [45/50] batch [15/51] time 0.204 (0.284) data 0.000 (0.101) loss 0.4006 (0.4546) acc 93.0556 (90.6694) lr 9.5173e-05 eta 0:01:22
epoch [45/50] batch [20/51] time 0.202 (0.258) data 0.000 (0.076) loss 0.2962 (0.4461) acc 94.4915 (90.7662) lr 9.5173e-05 eta 0:01:13
epoch [45/50] batch [25/51] time 0.179 (0.241) data 0.000 (0.061) loss 0.4186 (0.4362) acc 87.9808 (90.9971) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [30/51] time 0.181 (0.230) data 0.000 (0.051) loss 0.4491 (0.4296) acc 91.5094 (91.0389) lr 9.5173e-05 eta 0:01:03
epoch [45/50] batch [35/51] time 0.192 (0.223) data 0.000 (0.044) loss 0.4750 (0.4404) acc 89.0625 (90.7137) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.172 (0.217) data 0.000 (0.038) loss 0.4519 (0.4476) acc 90.8654 (90.4879) lr 9.5173e-05 eta 0:00:57
epoch [45/50] batch [45/51] time 0.161 (0.211) data 0.000 (0.034) loss 0.3775 (0.4487) acc 93.6170 (90.6260) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.176 (0.207) data 0.000 (0.031) loss 0.3148 (0.4480) acc 92.4528 (90.6113) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.131  alpha2: 0.014 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [46/50] batch [5/51] time 0.183 (0.493) data 0.001 (0.311) loss 0.3935 (0.4333) acc 90.2778 (92.0284) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [10/51] time 0.181 (0.340) data 0.000 (0.156) loss 0.4430 (0.4233) acc 93.5185 (92.1503) lr 7.0224e-05 eta 0:01:23
epoch [46/50] batch [15/51] time 0.159 (0.287) data 0.000 (0.104) loss 0.4982 (0.4387) acc 90.0000 (91.5932) lr 7.0224e-05 eta 0:01:08
epoch [46/50] batch [20/51] time 0.167 (0.259) data 0.000 (0.078) loss 0.4701 (0.4548) acc 88.5417 (90.8875) lr 7.0224e-05 eta 0:01:00
epoch [46/50] batch [25/51] time 0.177 (0.243) data 0.000 (0.062) loss 0.4000 (0.4675) acc 92.4528 (90.6283) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.169 (0.233) data 0.000 (0.052) loss 0.3505 (0.4623) acc 94.2708 (90.7570) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [35/51] time 0.181 (0.225) data 0.000 (0.045) loss 0.5289 (0.4674) acc 85.2041 (90.5382) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [40/51] time 0.161 (0.218) data 0.000 (0.039) loss 0.5005 (0.4646) acc 88.2979 (90.5250) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.185 (0.214) data 0.000 (0.035) loss 0.3272 (0.4551) acc 93.8596 (90.7050) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.165 (0.209) data 0.000 (0.031) loss 0.3633 (0.4579) acc 95.4082 (90.7554) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.131  alpha2: 0.013 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [47/50] batch [5/51] time 0.171 (0.491) data 0.000 (0.313) loss 0.3380 (0.4201) acc 90.0000 (91.6080) lr 4.8943e-05 eta 0:01:37
epoch [47/50] batch [10/51] time 0.173 (0.333) data 0.000 (0.157) loss 0.3747 (0.3977) acc 92.3077 (91.7481) lr 4.8943e-05 eta 0:01:04
epoch [47/50] batch [15/51] time 0.173 (0.281) data 0.000 (0.105) loss 0.3535 (0.4056) acc 87.9808 (91.3977) lr 4.8943e-05 eta 0:00:53
epoch [47/50] batch [20/51] time 0.178 (0.257) data 0.000 (0.079) loss 0.3939 (0.4215) acc 94.4444 (91.4835) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [25/51] time 0.175 (0.241) data 0.000 (0.063) loss 0.2036 (0.4142) acc 97.0588 (91.6296) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [30/51] time 0.192 (0.230) data 0.000 (0.053) loss 0.3908 (0.4145) acc 93.2292 (91.4990) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.162 (0.222) data 0.001 (0.045) loss 0.5638 (0.4258) acc 92.9348 (91.4528) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.169 (0.217) data 0.000 (0.040) loss 0.5062 (0.4277) acc 85.7843 (91.3554) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.035) loss 0.4992 (0.4331) acc 91.6667 (91.2683) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.173 (0.208) data 0.000 (0.032) loss 0.4591 (0.4340) acc 89.9038 (91.2049) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.130  alpha2: 0.012 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.06 & unmatched refined noisy rate: 0.25 <<<
epoch [48/50] batch [5/51] time 0.163 (0.488) data 0.000 (0.312) loss 0.4443 (0.4703) acc 90.9574 (89.1184) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [10/51] time 0.186 (0.335) data 0.000 (0.156) loss 0.7126 (0.5054) acc 85.0962 (89.3925) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.167 (0.281) data 0.000 (0.104) loss 0.5430 (0.4733) acc 85.2041 (89.8570) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [20/51] time 0.167 (0.254) data 0.000 (0.078) loss 0.4126 (0.4705) acc 94.3878 (90.0044) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [25/51] time 0.174 (0.238) data 0.000 (0.063) loss 0.3996 (0.4709) acc 93.2692 (90.1232) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.168 (0.228) data 0.000 (0.052) loss 0.4324 (0.4703) acc 91.0000 (90.0400) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.165 (0.220) data 0.000 (0.045) loss 0.4460 (0.4660) acc 90.6250 (90.3375) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [40/51] time 0.180 (0.214) data 0.000 (0.039) loss 0.4460 (0.4638) acc 90.4546 (90.4660) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.159 (0.209) data 0.000 (0.035) loss 0.4374 (0.4566) acc 93.4783 (90.6689) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [50/51] time 0.182 (0.205) data 0.000 (0.031) loss 0.3300 (0.4529) acc 95.9821 (90.6884) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.130  alpha2: 0.013 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [49/50] batch [5/51] time 0.181 (0.521) data 0.000 (0.337) loss 0.3420 (0.4706) acc 94.4444 (89.9710) lr 1.7713e-05 eta 0:00:50
epoch [49/50] batch [10/51] time 0.183 (0.349) data 0.000 (0.168) loss 0.5201 (0.4552) acc 89.7059 (90.8547) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [15/51] time 0.174 (0.291) data 0.000 (0.112) loss 0.5351 (0.4648) acc 90.3061 (90.4612) lr 1.7713e-05 eta 0:00:25
epoch [49/50] batch [20/51] time 0.188 (0.263) data 0.000 (0.084) loss 0.3962 (0.4514) acc 93.6274 (91.0144) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [25/51] time 0.181 (0.246) data 0.000 (0.068) loss 0.3989 (0.4408) acc 93.2692 (91.1575) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.172 (0.235) data 0.000 (0.056) loss 0.5525 (0.4469) acc 88.7255 (91.2665) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.169 (0.227) data 0.000 (0.048) loss 0.4331 (0.4446) acc 93.5000 (91.0973) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [40/51] time 0.169 (0.220) data 0.000 (0.042) loss 0.5454 (0.4500) acc 87.7451 (90.8827) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.176 (0.215) data 0.000 (0.038) loss 0.3454 (0.4488) acc 92.4528 (90.8556) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.176 (0.210) data 0.000 (0.034) loss 0.2751 (0.4441) acc 95.7547 (90.9541) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.132  alpha2: 0.015 <<<
>>> noisy rate: 0.50 --> refined noisy rate: 0.14 --> matched refined noisy rate: 0.07 & unmatched refined noisy rate: 0.25 <<<
epoch [50/50] batch [5/51] time 0.185 (0.515) data 0.000 (0.326) loss 0.3299 (0.4674) acc 93.1818 (91.6756) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.176 (0.346) data 0.000 (0.163) loss 0.3550 (0.4198) acc 91.5000 (91.9548) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.174 (0.290) data 0.000 (0.109) loss 0.3728 (0.4577) acc 92.7885 (90.6662) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.177 (0.261) data 0.000 (0.082) loss 0.3973 (0.4645) acc 95.4082 (90.9018) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.182 (0.244) data 0.000 (0.065) loss 0.5180 (0.4636) acc 89.5000 (90.5658) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.178 (0.233) data 0.000 (0.055) loss 0.4320 (0.4562) acc 90.0943 (90.5903) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.175 (0.225) data 0.000 (0.047) loss 0.5446 (0.4538) acc 87.7451 (90.7318) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.176 (0.218) data 0.000 (0.041) loss 0.4442 (0.4615) acc 91.6667 (90.5584) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.169 (0.213) data 0.000 (0.036) loss 0.6011 (0.4610) acc 86.2745 (90.4912) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.166 (0.208) data 0.000 (0.033) loss 0.4383 (0.4603) acc 91.0000 (90.5958) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_8FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.23, 0.18, 0.17, 0.16, 0.16, 0.16, 0.16, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.14, 0.15, 0.15, 0.14, 0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14]
* matched noise rate: [0.07, 0.07, 0.06, 0.06, 0.07, 0.07, 0.07, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.06, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.08, 0.07, 0.07, 0.07, 0.07, 0.08, 0.08, 0.08, 0.08, 0.07, 0.07, 0.06, 0.07, 0.07]
* unmatched noise rate: [0.35, 0.28, 0.26, 0.26, 0.28, 0.27, 0.27, 0.28, 0.27, 0.27, 0.27, 0.26, 0.26, 0.25, 0.25, 0.25, 0.25, 0.24, 0.25, 0.26, 0.25, 0.25, 0.25, 0.24, 0.24, 0.24, 0.25, 0.25, 0.24, 0.25, 0.25, 0.24, 0.24, 0.24, 0.24, 0.25, 0.25, 0.25, 0.25, 0.25]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<00:59,  2.48s/it]  8%|▊         | 2/25 [00:02<00:24,  1.08s/it] 16%|█▌        | 4/25 [00:02<00:09,  2.18it/s] 24%|██▍       | 6/25 [00:02<00:05,  3.52it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.94it/s] 40%|████      | 10/25 [00:03<00:02,  6.33it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.61it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.71it/s] 64%|██████▍   | 16/25 [00:03<00:00,  9.62it/s] 72%|███████▏  | 18/25 [00:04<00:01,  6.91it/s] 80%|████████  | 20/25 [00:04<00:00,  8.01it/s] 88%|████████▊ | 22/25 [00:04<00:00,  8.98it/s] 96%|█████████▌| 24/25 [00:04<00:00,  9.80it/s]100%|██████████| 25/25 [00:05<00:00,  4.78it/s]
=> result
* total: 2,463
* correct: 2,054
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 82.6%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 16	acc: 88.9%
* class: 2 (canterbury bells)	total: 12	correct: 1	acc: 8.3%
* class: 3 (sweet pea)	total: 17	correct: 12	acc: 70.6%
* class: 4 (english marigold)	total: 20	correct: 13	acc: 65.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 20	acc: 76.9%
* class: 11 (colt's foot)	total: 26	correct: 20	acc: 76.9%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 11	acc: 84.6%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 14	acc: 56.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 24	acc: 88.9%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 12	acc: 100.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 22	acc: 84.6%
* class: 30 (carnation)	total: 16	correct: 14	acc: 87.5%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 14	acc: 100.0%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 10	acc: 83.3%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 12	acc: 54.5%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 9	acc: 69.2%
* class: 39 (lenten rose)	total: 20	correct: 19	acc: 95.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 55	acc: 93.2%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 25	acc: 89.3%
* class: 50 (petunia)	total: 77	correct: 38	acc: 49.4%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 25	acc: 89.3%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 20	acc: 95.2%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 18	acc: 90.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 15	acc: 93.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 12	acc: 92.3%
* class: 67 (bearded iris)	total: 16	correct: 13	acc: 81.2%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 25	acc: 86.2%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 34	acc: 94.4%
* class: 75 (morning glory)	total: 32	correct: 28	acc: 87.5%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 31	acc: 96.9%
* class: 80 (frangipani)	total: 50	correct: 49	acc: 98.0%
* class: 81 (clematis)	total: 34	correct: 34	acc: 100.0%
* class: 82 (hibiscus)	total: 39	correct: 33	acc: 84.6%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 12	acc: 70.6%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 36	acc: 78.3%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 20	acc: 87.0%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 11	acc: 78.6%
* class: 93 (foxglove)	total: 49	correct: 48	acc: 98.0%
* class: 94 (bougainvillea)	total: 38	correct: 33	acc: 86.8%
* class: 95 (camellia)	total: 27	correct: 24	acc: 88.9%
* class: 96 (mallow)	total: 20	correct: 18	acc: 90.0%
* class: 97 (mexican petunia)	total: 25	correct: 0	acc: 0.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 9	acc: 52.9%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 85.5%
Elapsed: 0:28:26
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_10-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.260 (1.147) data 0.000 (0.314) loss 4.9468 (4.8733) acc 0.0000 (5.0000) lr 1.0000e-05 eta 0:48:39
epoch [1/50] batch [10/51] time 0.260 (0.707) data 0.000 (0.157) loss 4.5584 (4.7410) acc 6.2500 (5.0000) lr 1.0000e-05 eta 0:29:55
epoch [1/50] batch [15/51] time 0.273 (0.560) data 0.000 (0.105) loss 4.8205 (4.7334) acc 3.1250 (5.0000) lr 1.0000e-05 eta 0:23:39
epoch [1/50] batch [20/51] time 0.276 (0.487) data 0.000 (0.079) loss 4.5640 (4.6544) acc 12.5000 (6.4062) lr 1.0000e-05 eta 0:20:32
epoch [1/50] batch [25/51] time 0.260 (0.442) data 0.000 (0.063) loss 4.6146 (4.6563) acc 3.1250 (6.6250) lr 1.0000e-05 eta 0:18:35
epoch [1/50] batch [30/51] time 0.259 (0.412) data 0.000 (0.052) loss 4.8025 (4.6488) acc 6.2500 (6.7708) lr 1.0000e-05 eta 0:17:17
epoch [1/50] batch [35/51] time 0.264 (0.390) data 0.000 (0.045) loss 4.3061 (4.6183) acc 12.5000 (7.3214) lr 1.0000e-05 eta 0:16:21
epoch [1/50] batch [40/51] time 0.259 (0.374) data 0.000 (0.039) loss 4.6021 (4.5988) acc 6.2500 (7.4219) lr 1.0000e-05 eta 0:15:39
epoch [1/50] batch [45/51] time 0.258 (0.361) data 0.000 (0.035) loss 4.8055 (4.5938) acc 9.3750 (7.6389) lr 1.0000e-05 eta 0:15:04
epoch [1/50] batch [50/51] time 0.257 (0.351) data 0.000 (0.032) loss 4.5000 (4.5857) acc 9.3750 (7.5625) lr 1.0000e-05 eta 0:14:37
epoch [2/50] batch [5/51] time 0.263 (0.568) data 0.000 (0.277) loss 4.3689 (4.8785) acc 12.5000 (7.5000) lr 2.0000e-03 eta 0:23:36
epoch [2/50] batch [10/51] time 0.268 (0.417) data 0.000 (0.139) loss 4.6151 (4.6970) acc 9.3750 (7.5000) lr 2.0000e-03 eta 0:17:18
epoch [2/50] batch [15/51] time 0.263 (0.366) data 0.000 (0.093) loss 4.4000 (4.6449) acc 15.6250 (6.6667) lr 2.0000e-03 eta 0:15:09
epoch [2/50] batch [20/51] time 0.262 (0.340) data 0.000 (0.070) loss 4.3595 (4.5945) acc 6.2500 (6.7188) lr 2.0000e-03 eta 0:14:04
epoch [2/50] batch [25/51] time 0.269 (0.325) data 0.000 (0.056) loss 4.3899 (4.5865) acc 12.5000 (6.6250) lr 2.0000e-03 eta 0:13:25
epoch [2/50] batch [30/51] time 0.259 (0.315) data 0.000 (0.046) loss 4.3327 (4.5495) acc 12.5000 (7.9167) lr 2.0000e-03 eta 0:12:57
epoch [2/50] batch [35/51] time 0.260 (0.307) data 0.000 (0.040) loss 4.2946 (4.5354) acc 15.6250 (8.3036) lr 2.0000e-03 eta 0:12:36
epoch [2/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.035) loss 4.4059 (4.5144) acc 18.7500 (8.9844) lr 2.0000e-03 eta 0:12:21
epoch [2/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.031) loss 4.6466 (4.5094) acc 0.0000 (9.3056) lr 2.0000e-03 eta 0:12:08
epoch [2/50] batch [50/51] time 0.257 (0.293) data 0.000 (0.028) loss 4.6387 (4.5082) acc 12.5000 (9.6875) lr 2.0000e-03 eta 0:11:57
epoch [3/50] batch [5/51] time 0.270 (0.628) data 0.000 (0.347) loss 4.2861 (4.2703) acc 15.6250 (17.5000) lr 1.9980e-03 eta 0:25:35
epoch [3/50] batch [10/51] time 0.282 (0.451) data 0.000 (0.174) loss 4.2326 (4.3177) acc 9.3750 (16.2500) lr 1.9980e-03 eta 0:18:19
epoch [3/50] batch [15/51] time 0.260 (0.389) data 0.000 (0.116) loss 4.0509 (4.3023) acc 12.5000 (14.3750) lr 1.9980e-03 eta 0:15:45
epoch [3/50] batch [20/51] time 0.273 (0.359) data 0.000 (0.087) loss 3.7426 (4.2888) acc 21.8750 (14.8438) lr 1.9980e-03 eta 0:14:31
epoch [3/50] batch [25/51] time 0.260 (0.341) data 0.000 (0.070) loss 4.6147 (4.3162) acc 6.2500 (14.0000) lr 1.9980e-03 eta 0:13:45
epoch [3/50] batch [30/51] time 0.260 (0.328) data 0.000 (0.058) loss 4.3530 (4.3183) acc 12.5000 (14.1667) lr 1.9980e-03 eta 0:13:13
epoch [3/50] batch [35/51] time 0.277 (0.320) data 0.000 (0.050) loss 4.4968 (4.3348) acc 9.3750 (13.4821) lr 1.9980e-03 eta 0:12:51
epoch [3/50] batch [40/51] time 0.261 (0.313) data 0.000 (0.044) loss 4.3665 (4.3391) acc 9.3750 (12.8906) lr 1.9980e-03 eta 0:12:32
epoch [3/50] batch [45/51] time 0.259 (0.307) data 0.000 (0.039) loss 4.4539 (4.3476) acc 3.1250 (12.5694) lr 1.9980e-03 eta 0:12:17
epoch [3/50] batch [50/51] time 0.259 (0.302) data 0.000 (0.035) loss 4.4099 (4.3553) acc 9.3750 (12.6875) lr 1.9980e-03 eta 0:12:03
epoch [4/50] batch [5/51] time 0.313 (0.601) data 0.000 (0.312) loss 4.0856 (4.3360) acc 21.8750 (15.6250) lr 1.9921e-03 eta 0:23:57
epoch [4/50] batch [10/51] time 0.272 (0.435) data 0.000 (0.156) loss 4.1392 (4.3064) acc 18.7500 (14.3750) lr 1.9921e-03 eta 0:17:19
epoch [4/50] batch [15/51] time 0.269 (0.378) data 0.000 (0.104) loss 4.2841 (4.3117) acc 15.6250 (15.2083) lr 1.9921e-03 eta 0:15:00
epoch [4/50] batch [20/51] time 0.264 (0.352) data 0.000 (0.078) loss 4.2356 (4.2963) acc 12.5000 (15.3125) lr 1.9921e-03 eta 0:13:56
epoch [4/50] batch [25/51] time 0.261 (0.335) data 0.000 (0.063) loss 4.4018 (4.2883) acc 18.7500 (16.0000) lr 1.9921e-03 eta 0:13:15
epoch [4/50] batch [30/51] time 0.273 (0.324) data 0.000 (0.052) loss 4.6409 (4.2991) acc 6.2500 (15.6250) lr 1.9921e-03 eta 0:12:47
epoch [4/50] batch [35/51] time 0.277 (0.316) data 0.000 (0.045) loss 4.1073 (4.2764) acc 31.2500 (16.7857) lr 1.9921e-03 eta 0:12:27
epoch [4/50] batch [40/51] time 0.259 (0.310) data 0.000 (0.039) loss 4.4981 (4.2767) acc 12.5000 (16.5625) lr 1.9921e-03 eta 0:12:10
epoch [4/50] batch [45/51] time 0.258 (0.304) data 0.000 (0.035) loss 4.3709 (4.2745) acc 15.6250 (16.5972) lr 1.9921e-03 eta 0:11:54
epoch [4/50] batch [50/51] time 0.257 (0.299) data 0.000 (0.031) loss 4.6595 (4.2893) acc 18.7500 (16.3750) lr 1.9921e-03 eta 0:11:42
epoch [5/50] batch [5/51] time 0.295 (0.628) data 0.000 (0.336) loss 4.1436 (4.2056) acc 15.6250 (20.0000) lr 1.9823e-03 eta 0:24:29
epoch [5/50] batch [10/51] time 0.261 (0.449) data 0.000 (0.168) loss 4.2072 (4.2664) acc 15.6250 (16.2500) lr 1.9823e-03 eta 0:17:28
epoch [5/50] batch [15/51] time 0.265 (0.390) data 0.000 (0.112) loss 4.0658 (4.2405) acc 31.2500 (18.1250) lr 1.9823e-03 eta 0:15:09
epoch [5/50] batch [20/51] time 0.272 (0.360) data 0.000 (0.084) loss 4.3139 (4.2845) acc 12.5000 (17.5000) lr 1.9823e-03 eta 0:13:57
epoch [5/50] batch [25/51] time 0.268 (0.342) data 0.000 (0.067) loss 4.3910 (4.2682) acc 9.3750 (18.0000) lr 1.9823e-03 eta 0:13:12
epoch [5/50] batch [30/51] time 0.268 (0.329) data 0.000 (0.056) loss 4.2130 (4.2422) acc 18.7500 (18.0208) lr 1.9823e-03 eta 0:12:42
epoch [5/50] batch [35/51] time 0.268 (0.320) data 0.000 (0.048) loss 4.1528 (4.2402) acc 15.6250 (18.0357) lr 1.9823e-03 eta 0:12:19
epoch [5/50] batch [40/51] time 0.260 (0.313) data 0.000 (0.042) loss 4.7950 (4.2544) acc 6.2500 (17.5781) lr 1.9823e-03 eta 0:12:01
epoch [5/50] batch [45/51] time 0.259 (0.307) data 0.000 (0.038) loss 4.2321 (4.2365) acc 9.3750 (18.0556) lr 1.9823e-03 eta 0:11:46
epoch [5/50] batch [50/51] time 0.259 (0.302) data 0.000 (0.034) loss 4.0490 (4.2453) acc 18.7500 (17.7500) lr 1.9823e-03 eta 0:11:33
epoch [6/50] batch [5/51] time 0.276 (0.587) data 0.000 (0.306) loss 4.2557 (4.1410) acc 6.2500 (18.7500) lr 1.9686e-03 eta 0:22:24
epoch [6/50] batch [10/51] time 0.275 (0.428) data 0.000 (0.153) loss 5.0338 (4.2099) acc 9.3750 (16.2500) lr 1.9686e-03 eta 0:16:17
epoch [6/50] batch [15/51] time 0.262 (0.375) data 0.000 (0.102) loss 4.3114 (4.1839) acc 15.6250 (17.2917) lr 1.9686e-03 eta 0:14:13
epoch [6/50] batch [20/51] time 0.271 (0.348) data 0.000 (0.077) loss 4.2709 (4.1747) acc 18.7500 (17.9688) lr 1.9686e-03 eta 0:13:11
epoch [6/50] batch [25/51] time 0.262 (0.332) data 0.000 (0.061) loss 4.4562 (4.1441) acc 12.5000 (19.5000) lr 1.9686e-03 eta 0:12:34
epoch [6/50] batch [30/51] time 0.274 (0.322) data 0.000 (0.051) loss 4.2379 (4.1757) acc 25.0000 (18.8542) lr 1.9686e-03 eta 0:12:09
epoch [6/50] batch [35/51] time 0.276 (0.315) data 0.000 (0.044) loss 4.0645 (4.1890) acc 28.1250 (18.8393) lr 1.9686e-03 eta 0:11:52
epoch [6/50] batch [40/51] time 0.259 (0.309) data 0.000 (0.039) loss 4.2653 (4.1843) acc 15.6250 (18.6719) lr 1.9686e-03 eta 0:11:36
epoch [6/50] batch [45/51] time 0.259 (0.303) data 0.000 (0.034) loss 3.9363 (4.1831) acc 21.8750 (18.5417) lr 1.9686e-03 eta 0:11:22
epoch [6/50] batch [50/51] time 0.259 (0.299) data 0.000 (0.031) loss 4.6396 (4.2078) acc 18.7500 (18.2500) lr 1.9686e-03 eta 0:11:10
epoch [7/50] batch [5/51] time 0.283 (0.664) data 0.000 (0.379) loss 3.8877 (3.9590) acc 21.8750 (23.1250) lr 1.9511e-03 eta 0:24:46
epoch [7/50] batch [10/51] time 0.266 (0.464) data 0.000 (0.190) loss 4.1185 (4.1285) acc 15.6250 (19.3750) lr 1.9511e-03 eta 0:17:17
epoch [7/50] batch [15/51] time 0.263 (0.398) data 0.000 (0.127) loss 4.1768 (4.1109) acc 18.7500 (20.4167) lr 1.9511e-03 eta 0:14:47
epoch [7/50] batch [20/51] time 0.268 (0.365) data 0.000 (0.095) loss 3.8600 (4.1039) acc 31.2500 (20.0000) lr 1.9511e-03 eta 0:13:31
epoch [7/50] batch [25/51] time 0.261 (0.346) data 0.000 (0.077) loss 3.9875 (4.1171) acc 25.0000 (19.8750) lr 1.9511e-03 eta 0:12:46
epoch [7/50] batch [30/51] time 0.267 (0.332) data 0.000 (0.064) loss 4.3138 (4.1162) acc 18.7500 (20.2083) lr 1.9511e-03 eta 0:12:15
epoch [7/50] batch [35/51] time 0.260 (0.323) data 0.000 (0.055) loss 4.1413 (4.1315) acc 15.6250 (20.0000) lr 1.9511e-03 eta 0:11:53
epoch [7/50] batch [40/51] time 0.256 (0.315) data 0.000 (0.048) loss 4.3754 (4.1340) acc 15.6250 (20.0781) lr 1.9511e-03 eta 0:11:34
epoch [7/50] batch [45/51] time 0.256 (0.309) data 0.000 (0.043) loss 4.1353 (4.1577) acc 25.0000 (19.5833) lr 1.9511e-03 eta 0:11:18
epoch [7/50] batch [50/51] time 0.258 (0.303) data 0.000 (0.038) loss 4.5613 (4.1661) acc 9.3750 (19.1875) lr 1.9511e-03 eta 0:11:05
epoch [8/50] batch [5/51] time 0.281 (0.565) data 0.000 (0.274) loss 3.7160 (4.0879) acc 31.2500 (21.2500) lr 1.9298e-03 eta 0:20:36
epoch [8/50] batch [10/51] time 0.272 (0.416) data 0.000 (0.137) loss 4.0328 (4.0942) acc 18.7500 (20.9375) lr 1.9298e-03 eta 0:15:08
epoch [8/50] batch [15/51] time 0.261 (0.367) data 0.000 (0.091) loss 4.2126 (4.1250) acc 21.8750 (21.2500) lr 1.9298e-03 eta 0:13:18
epoch [8/50] batch [20/51] time 0.264 (0.343) data 0.000 (0.069) loss 4.4211 (4.1247) acc 9.3750 (20.4688) lr 1.9298e-03 eta 0:12:24
epoch [8/50] batch [25/51] time 0.271 (0.327) data 0.000 (0.055) loss 3.8350 (4.0982) acc 25.0000 (21.5000) lr 1.9298e-03 eta 0:11:49
epoch [8/50] batch [30/51] time 0.259 (0.317) data 0.000 (0.046) loss 4.2612 (4.1173) acc 21.8750 (21.1458) lr 1.9298e-03 eta 0:11:26
epoch [8/50] batch [35/51] time 0.265 (0.309) data 0.000 (0.039) loss 3.9436 (4.1388) acc 25.0000 (20.8929) lr 1.9298e-03 eta 0:11:07
epoch [8/50] batch [40/51] time 0.259 (0.303) data 0.000 (0.034) loss 4.3376 (4.1431) acc 12.5000 (20.5469) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [45/51] time 0.258 (0.298) data 0.000 (0.031) loss 4.1158 (4.1523) acc 28.1250 (20.4167) lr 1.9298e-03 eta 0:10:41
epoch [8/50] batch [50/51] time 0.257 (0.294) data 0.000 (0.028) loss 3.8978 (4.1519) acc 28.1250 (20.4375) lr 1.9298e-03 eta 0:10:30
epoch [9/50] batch [5/51] time 0.285 (0.587) data 0.000 (0.308) loss 4.6528 (4.1886) acc 18.7500 (23.1250) lr 1.9048e-03 eta 0:20:54
epoch [9/50] batch [10/51] time 0.276 (0.428) data 0.000 (0.154) loss 4.3230 (4.1297) acc 12.5000 (23.1250) lr 1.9048e-03 eta 0:15:12
epoch [9/50] batch [15/51] time 0.263 (0.373) data 0.000 (0.103) loss 4.1617 (4.1114) acc 15.6250 (21.6667) lr 1.9048e-03 eta 0:13:13
epoch [9/50] batch [20/51] time 0.271 (0.348) data 0.000 (0.077) loss 4.2627 (4.1211) acc 15.6250 (20.4688) lr 1.9048e-03 eta 0:12:17
epoch [9/50] batch [25/51] time 0.270 (0.332) data 0.000 (0.062) loss 4.5127 (4.1352) acc 6.2500 (19.6250) lr 1.9048e-03 eta 0:11:43
epoch [9/50] batch [30/51] time 0.283 (0.322) data 0.000 (0.052) loss 3.6680 (4.0903) acc 28.1250 (21.0417) lr 1.9048e-03 eta 0:11:20
epoch [9/50] batch [35/51] time 0.260 (0.314) data 0.000 (0.044) loss 3.9332 (4.0959) acc 31.2500 (20.9821) lr 1.9048e-03 eta 0:11:02
epoch [9/50] batch [40/51] time 0.258 (0.308) data 0.000 (0.039) loss 4.3951 (4.1026) acc 15.6250 (20.9375) lr 1.9048e-03 eta 0:10:46
epoch [9/50] batch [45/51] time 0.259 (0.302) data 0.000 (0.034) loss 3.9430 (4.1061) acc 21.8750 (20.9722) lr 1.9048e-03 eta 0:10:33
epoch [9/50] batch [50/51] time 0.258 (0.298) data 0.000 (0.031) loss 4.2085 (4.1117) acc 18.7500 (20.8750) lr 1.9048e-03 eta 0:10:22
epoch [10/50] batch [5/51] time 0.299 (0.710) data 0.000 (0.429) loss 4.0994 (4.0641) acc 21.8750 (21.8750) lr 1.8763e-03 eta 0:24:41
epoch [10/50] batch [10/51] time 0.262 (0.489) data 0.000 (0.215) loss 4.3147 (4.0641) acc 6.2500 (20.6250) lr 1.8763e-03 eta 0:16:57
epoch [10/50] batch [15/51] time 0.270 (0.416) data 0.000 (0.143) loss 3.9074 (4.0646) acc 31.2500 (22.0833) lr 1.8763e-03 eta 0:14:23
epoch [10/50] batch [20/51] time 0.260 (0.378) data 0.000 (0.107) loss 3.7786 (4.0804) acc 21.8750 (21.7188) lr 1.8763e-03 eta 0:13:03
epoch [10/50] batch [25/51] time 0.275 (0.357) data 0.000 (0.086) loss 4.1690 (4.0981) acc 25.0000 (22.0000) lr 1.8763e-03 eta 0:12:16
epoch [10/50] batch [30/51] time 0.273 (0.342) data 0.000 (0.072) loss 3.8933 (4.1136) acc 25.0000 (21.7708) lr 1.8763e-03 eta 0:11:44
epoch [10/50] batch [35/51] time 0.266 (0.331) data 0.000 (0.062) loss 3.8030 (4.0941) acc 28.1250 (21.6964) lr 1.8763e-03 eta 0:11:20
epoch [10/50] batch [40/51] time 0.258 (0.322) data 0.000 (0.054) loss 3.7414 (4.0835) acc 34.3750 (22.3438) lr 1.8763e-03 eta 0:11:00
epoch [10/50] batch [45/51] time 0.259 (0.315) data 0.000 (0.048) loss 4.1610 (4.0782) acc 18.7500 (22.5694) lr 1.8763e-03 eta 0:10:44
epoch [10/50] batch [50/51] time 0.259 (0.310) data 0.000 (0.043) loss 4.1199 (4.0826) acc 25.0000 (22.3750) lr 1.8763e-03 eta 0:10:32
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.806  alpha2: 0.381 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.75 <<<
epoch [11/50] batch [5/51] time 0.170 (1.003) data 0.000 (0.321) loss 2.8974 (3.0955) acc 40.9574 (47.8015) lr 1.8443e-03 eta 0:34:00
epoch [11/50] batch [10/51] time 0.166 (0.794) data 0.000 (0.161) loss 2.6982 (2.9586) acc 50.0000 (49.5196) lr 1.8443e-03 eta 0:26:51
epoch [11/50] batch [15/51] time 0.168 (0.720) data 0.000 (0.107) loss 3.2075 (2.9895) acc 43.6170 (46.4067) lr 1.8443e-03 eta 0:24:17
epoch [11/50] batch [20/51] time 0.172 (0.619) data 0.000 (0.081) loss 2.7920 (2.9468) acc 56.3726 (46.4672) lr 1.8443e-03 eta 0:20:50
epoch [11/50] batch [25/51] time 0.170 (0.530) data 0.000 (0.065) loss 2.6336 (2.9279) acc 49.4898 (46.3012) lr 1.8443e-03 eta 0:17:48
epoch [11/50] batch [30/51] time 0.924 (0.518) data 0.000 (0.054) loss 2.4142 (2.8847) acc 58.1818 (47.8039) lr 1.8443e-03 eta 0:17:21
epoch [11/50] batch [35/51] time 0.188 (0.468) data 0.000 (0.046) loss 2.6635 (2.8637) acc 49.0000 (47.8186) lr 1.8443e-03 eta 0:15:38
epoch [11/50] batch [40/51] time 0.167 (0.431) data 0.000 (0.041) loss 2.9876 (2.8625) acc 40.8163 (47.5675) lr 1.8443e-03 eta 0:14:21
epoch [11/50] batch [45/51] time 0.160 (0.401) data 0.000 (0.036) loss 2.7073 (2.8619) acc 53.2609 (47.5180) lr 1.8443e-03 eta 0:13:20
epoch [11/50] batch [50/51] time 0.160 (0.378) data 0.000 (0.032) loss 3.2397 (2.8652) acc 31.5217 (47.4488) lr 1.8443e-03 eta 0:12:31
>>> alpha1: 0.724  alpha2: 0.330 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.50 <<<
epoch [12/50] batch [5/51] time 0.795 (0.742) data 0.000 (0.332) loss 1.8617 (2.0886) acc 73.8095 (61.7024) lr 1.8090e-03 eta 0:24:31
epoch [12/50] batch [10/51] time 0.148 (0.449) data 0.000 (0.166) loss 2.0106 (1.9985) acc 55.4878 (61.4892) lr 1.8090e-03 eta 0:14:49
epoch [12/50] batch [15/51] time 0.164 (0.352) data 0.001 (0.111) loss 1.5404 (1.9917) acc 68.6170 (60.8299) lr 1.8090e-03 eta 0:11:34
epoch [12/50] batch [20/51] time 0.152 (0.331) data 0.000 (0.083) loss 1.9675 (2.0161) acc 63.9535 (60.7850) lr 1.8090e-03 eta 0:10:51
epoch [12/50] batch [25/51] time 0.154 (0.296) data 0.000 (0.067) loss 1.9026 (2.0209) acc 61.6279 (60.4738) lr 1.8090e-03 eta 0:09:40
epoch [12/50] batch [30/51] time 0.153 (0.290) data 0.000 (0.056) loss 1.9021 (2.0122) acc 56.9767 (60.2283) lr 1.8090e-03 eta 0:09:28
epoch [12/50] batch [35/51] time 0.155 (0.271) data 0.000 (0.048) loss 1.8402 (1.9759) acc 68.7500 (60.5102) lr 1.8090e-03 eta 0:08:48
epoch [12/50] batch [40/51] time 0.153 (0.256) data 0.000 (0.042) loss 1.9061 (1.9629) acc 67.6136 (60.9417) lr 1.8090e-03 eta 0:08:19
epoch [12/50] batch [45/51] time 0.151 (0.245) data 0.000 (0.037) loss 2.0227 (1.9452) acc 63.3721 (61.1862) lr 1.8090e-03 eta 0:07:56
epoch [12/50] batch [50/51] time 0.158 (0.236) data 0.000 (0.033) loss 1.9580 (1.9512) acc 55.9783 (60.9355) lr 1.8090e-03 eta 0:07:37
>>> alpha1: 0.634  alpha2: 0.279 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.53 <<<
epoch [13/50] batch [5/51] time 0.942 (0.672) data 0.000 (0.329) loss 1.7048 (1.7086) acc 59.2105 (63.5593) lr 1.7705e-03 eta 0:21:38
epoch [13/50] batch [10/51] time 0.193 (0.424) data 0.000 (0.165) loss 1.5365 (1.6638) acc 61.3208 (63.1483) lr 1.7705e-03 eta 0:13:36
epoch [13/50] batch [15/51] time 0.175 (0.340) data 0.000 (0.110) loss 1.5683 (1.6022) acc 71.6981 (64.6094) lr 1.7705e-03 eta 0:10:52
epoch [13/50] batch [20/51] time 0.175 (0.336) data 0.000 (0.082) loss 1.4039 (1.5676) acc 68.7500 (65.3860) lr 1.7705e-03 eta 0:10:44
epoch [13/50] batch [25/51] time 0.172 (0.304) data 0.001 (0.066) loss 1.6905 (1.5591) acc 61.2245 (66.4921) lr 1.7705e-03 eta 0:09:42
epoch [13/50] batch [30/51] time 0.195 (0.284) data 0.000 (0.055) loss 1.5944 (1.5480) acc 63.2075 (66.5535) lr 1.7705e-03 eta 0:09:02
epoch [13/50] batch [35/51] time 0.197 (0.270) data 0.000 (0.047) loss 1.4347 (1.5359) acc 67.4528 (66.7871) lr 1.7705e-03 eta 0:08:32
epoch [13/50] batch [40/51] time 0.168 (0.257) data 0.000 (0.041) loss 1.7539 (1.5483) acc 63.7755 (66.6968) lr 1.7705e-03 eta 0:08:08
epoch [13/50] batch [45/51] time 0.168 (0.248) data 0.000 (0.037) loss 1.5660 (1.5427) acc 62.5000 (66.5853) lr 1.7705e-03 eta 0:07:49
epoch [13/50] batch [50/51] time 0.171 (0.240) data 0.000 (0.033) loss 1.2659 (1.5448) acc 67.1569 (66.0574) lr 1.7705e-03 eta 0:07:33
>>> alpha1: 0.574  alpha2: 0.247 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.51 <<<
epoch [14/50] batch [5/51] time 0.181 (0.508) data 0.001 (0.318) loss 1.2235 (1.2661) acc 73.5577 (72.7631) lr 1.7290e-03 eta 0:15:56
epoch [14/50] batch [10/51] time 0.178 (0.342) data 0.000 (0.159) loss 1.0822 (1.3351) acc 79.2453 (71.2455) lr 1.7290e-03 eta 0:10:42
epoch [14/50] batch [15/51] time 0.166 (0.289) data 0.000 (0.106) loss 1.5864 (1.3612) acc 68.2292 (71.1326) lr 1.7290e-03 eta 0:09:00
epoch [14/50] batch [20/51] time 0.176 (0.301) data 0.000 (0.080) loss 1.4441 (1.3556) acc 63.2653 (70.8226) lr 1.7290e-03 eta 0:09:22
epoch [14/50] batch [25/51] time 0.200 (0.277) data 0.000 (0.064) loss 1.1741 (1.3146) acc 77.4038 (71.7431) lr 1.7290e-03 eta 0:08:36
epoch [14/50] batch [30/51] time 0.185 (0.261) data 0.001 (0.053) loss 1.2690 (1.3081) acc 68.3962 (71.6230) lr 1.7290e-03 eta 0:08:04
epoch [14/50] batch [35/51] time 0.206 (0.249) data 0.017 (0.046) loss 1.6903 (1.3246) acc 63.4615 (71.2305) lr 1.7290e-03 eta 0:07:41
epoch [14/50] batch [40/51] time 0.175 (0.240) data 0.000 (0.040) loss 1.1964 (1.3351) acc 74.0566 (70.8296) lr 1.7290e-03 eta 0:07:22
epoch [14/50] batch [45/51] time 0.177 (0.232) data 0.000 (0.036) loss 1.2986 (1.3401) acc 75.9434 (71.0911) lr 1.7290e-03 eta 0:07:07
epoch [14/50] batch [50/51] time 0.186 (0.226) data 0.000 (0.032) loss 1.3070 (1.3470) acc 70.4545 (70.8840) lr 1.7290e-03 eta 0:06:56
>>> alpha1: 0.516  alpha2: 0.212 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.27 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.47 <<<
epoch [15/50] batch [5/51] time 0.185 (0.504) data 0.000 (0.311) loss 1.3223 (1.1059) acc 69.8980 (76.0170) lr 1.6845e-03 eta 0:15:23
epoch [15/50] batch [10/51] time 0.187 (0.424) data 0.001 (0.156) loss 1.2750 (1.1099) acc 68.8596 (75.8243) lr 1.6845e-03 eta 0:12:54
epoch [15/50] batch [15/51] time 0.176 (0.344) data 0.001 (0.104) loss 1.1579 (1.1774) acc 77.9412 (75.2565) lr 1.6845e-03 eta 0:10:27
epoch [15/50] batch [20/51] time 0.187 (0.305) data 0.001 (0.078) loss 1.2090 (1.2120) acc 75.8929 (74.3161) lr 1.6845e-03 eta 0:09:13
epoch [15/50] batch [25/51] time 0.177 (0.280) data 0.000 (0.063) loss 1.0914 (1.2008) acc 76.4151 (74.3364) lr 1.6845e-03 eta 0:08:26
epoch [15/50] batch [30/51] time 0.172 (0.264) data 0.000 (0.052) loss 1.4293 (1.1949) acc 71.5000 (74.3393) lr 1.6845e-03 eta 0:07:56
epoch [15/50] batch [35/51] time 0.186 (0.253) data 0.001 (0.045) loss 1.3157 (1.2176) acc 71.8182 (73.8689) lr 1.6845e-03 eta 0:07:35
epoch [15/50] batch [40/51] time 0.178 (0.245) data 0.000 (0.039) loss 1.1724 (1.2197) acc 69.2308 (73.3974) lr 1.6845e-03 eta 0:07:19
epoch [15/50] batch [45/51] time 0.173 (0.236) data 0.001 (0.035) loss 1.1905 (1.2273) acc 72.5490 (72.9099) lr 1.6845e-03 eta 0:07:03
epoch [15/50] batch [50/51] time 0.169 (0.230) data 0.000 (0.031) loss 1.4848 (1.2297) acc 64.2857 (72.8879) lr 1.6845e-03 eta 0:06:50
>>> alpha1: 0.376  alpha2: 0.114 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.39 <<<
epoch [16/50] batch [5/51] time 0.169 (0.511) data 0.001 (0.337) loss 1.0040 (1.1005) acc 75.5556 (75.8511) lr 1.6374e-03 eta 0:15:09
epoch [16/50] batch [10/51] time 0.173 (0.344) data 0.001 (0.169) loss 0.9556 (1.1006) acc 74.4898 (74.5267) lr 1.6374e-03 eta 0:10:11
epoch [16/50] batch [15/51] time 0.160 (0.283) data 0.000 (0.113) loss 1.1056 (1.0909) acc 74.4565 (74.4054) lr 1.6374e-03 eta 0:08:21
epoch [16/50] batch [20/51] time 0.164 (0.253) data 0.001 (0.085) loss 0.8095 (1.0378) acc 79.7872 (75.3716) lr 1.6374e-03 eta 0:07:27
epoch [16/50] batch [25/51] time 0.161 (0.237) data 0.001 (0.068) loss 0.9776 (1.0363) acc 76.0870 (75.3656) lr 1.6374e-03 eta 0:06:57
epoch [16/50] batch [30/51] time 0.160 (0.225) data 0.000 (0.057) loss 1.4168 (1.0637) acc 59.4444 (74.2753) lr 1.6374e-03 eta 0:06:34
epoch [16/50] batch [35/51] time 0.178 (0.218) data 0.000 (0.049) loss 0.7708 (1.0618) acc 85.5000 (74.3379) lr 1.6374e-03 eta 0:06:21
epoch [16/50] batch [40/51] time 0.158 (0.211) data 0.000 (0.043) loss 1.0959 (1.0669) acc 70.5556 (74.3247) lr 1.6374e-03 eta 0:06:08
epoch [16/50] batch [45/51] time 0.174 (0.206) data 0.000 (0.038) loss 0.7932 (1.0609) acc 81.2500 (74.3826) lr 1.6374e-03 eta 0:05:58
epoch [16/50] batch [50/51] time 0.161 (0.202) data 0.000 (0.034) loss 0.6306 (1.0536) acc 89.1304 (74.8178) lr 1.6374e-03 eta 0:05:50
>>> alpha1: 0.293  alpha2: 0.058 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [17/50] batch [5/51] time 0.162 (0.500) data 0.001 (0.318) loss 1.1678 (1.0050) acc 71.1111 (76.2704) lr 1.5878e-03 eta 0:14:24
epoch [17/50] batch [10/51] time 0.179 (0.339) data 0.001 (0.159) loss 0.6874 (0.9663) acc 84.1837 (77.3702) lr 1.5878e-03 eta 0:09:44
epoch [17/50] batch [15/51] time 0.175 (0.282) data 0.000 (0.106) loss 0.7876 (0.9546) acc 80.7292 (77.3947) lr 1.5878e-03 eta 0:08:04
epoch [17/50] batch [20/51] time 0.173 (0.255) data 0.000 (0.080) loss 1.2741 (0.9764) acc 64.6739 (76.7810) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [25/51] time 0.162 (0.237) data 0.000 (0.064) loss 0.8296 (0.9672) acc 78.2609 (76.9102) lr 1.5878e-03 eta 0:06:45
epoch [17/50] batch [30/51] time 0.176 (0.228) data 0.000 (0.053) loss 0.9711 (0.9457) acc 72.0000 (77.3941) lr 1.5878e-03 eta 0:06:28
epoch [17/50] batch [35/51] time 0.173 (0.219) data 0.000 (0.046) loss 0.8304 (0.9513) acc 86.7647 (77.4773) lr 1.5878e-03 eta 0:06:12
epoch [17/50] batch [40/51] time 0.166 (0.213) data 0.000 (0.040) loss 1.0154 (0.9454) acc 73.9796 (77.6979) lr 1.5878e-03 eta 0:06:01
epoch [17/50] batch [45/51] time 0.168 (0.208) data 0.000 (0.036) loss 1.2643 (0.9561) acc 77.5000 (77.6235) lr 1.5878e-03 eta 0:05:50
epoch [17/50] batch [50/51] time 0.164 (0.203) data 0.000 (0.032) loss 1.1913 (0.9782) acc 72.3958 (77.1104) lr 1.5878e-03 eta 0:05:42
>>> alpha1: 0.253  alpha2: 0.038 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.39 <<<
epoch [18/50] batch [5/51] time 0.189 (0.476) data 0.000 (0.290) loss 0.9062 (0.7941) acc 79.1667 (83.4587) lr 1.5358e-03 eta 0:13:17
epoch [18/50] batch [10/51] time 0.166 (0.325) data 0.000 (0.145) loss 0.9466 (0.8650) acc 77.6042 (81.2354) lr 1.5358e-03 eta 0:09:03
epoch [18/50] batch [15/51] time 0.172 (0.272) data 0.000 (0.097) loss 1.0379 (0.8816) acc 79.0000 (79.9706) lr 1.5358e-03 eta 0:07:33
epoch [18/50] batch [20/51] time 0.185 (0.246) data 0.000 (0.073) loss 0.8915 (0.8576) acc 81.0000 (80.9359) lr 1.5358e-03 eta 0:06:49
epoch [18/50] batch [25/51] time 0.161 (0.232) data 0.000 (0.058) loss 0.9531 (0.8601) acc 75.5814 (81.1113) lr 1.5358e-03 eta 0:06:24
epoch [18/50] batch [30/51] time 0.170 (0.220) data 0.000 (0.049) loss 1.1035 (0.8849) acc 75.0000 (80.1156) lr 1.5358e-03 eta 0:06:04
epoch [18/50] batch [35/51] time 0.170 (0.213) data 0.000 (0.042) loss 0.8513 (0.8811) acc 82.7778 (80.3941) lr 1.5358e-03 eta 0:05:51
epoch [18/50] batch [40/51] time 0.171 (0.208) data 0.000 (0.037) loss 0.6439 (0.8758) acc 86.0577 (80.2816) lr 1.5358e-03 eta 0:05:40
epoch [18/50] batch [45/51] time 0.160 (0.203) data 0.000 (0.032) loss 1.1619 (0.8826) acc 72.8723 (80.1610) lr 1.5358e-03 eta 0:05:31
epoch [18/50] batch [50/51] time 0.175 (0.199) data 0.000 (0.029) loss 0.5523 (0.8854) acc 88.2075 (79.9230) lr 1.5358e-03 eta 0:05:24
>>> alpha1: 0.234  alpha2: 0.035 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [19/50] batch [5/51] time 0.183 (0.513) data 0.000 (0.333) loss 0.4739 (0.8010) acc 89.2157 (81.7805) lr 1.4818e-03 eta 0:13:54
epoch [19/50] batch [10/51] time 0.189 (0.346) data 0.000 (0.167) loss 1.0682 (0.8574) acc 72.5000 (79.8493) lr 1.4818e-03 eta 0:09:21
epoch [19/50] batch [15/51] time 0.170 (0.290) data 0.000 (0.111) loss 0.8705 (0.8356) acc 80.0000 (81.4322) lr 1.4818e-03 eta 0:07:48
epoch [19/50] batch [20/51] time 0.187 (0.262) data 0.000 (0.084) loss 0.8435 (0.8149) acc 82.0000 (81.6559) lr 1.4818e-03 eta 0:07:02
epoch [19/50] batch [25/51] time 0.180 (0.246) data 0.000 (0.067) loss 0.7359 (0.8190) acc 81.8627 (81.1699) lr 1.4818e-03 eta 0:06:34
epoch [19/50] batch [30/51] time 0.178 (0.234) data 0.000 (0.056) loss 0.8059 (0.8304) acc 82.1429 (80.6262) lr 1.4818e-03 eta 0:06:14
epoch [19/50] batch [35/51] time 0.193 (0.226) data 0.000 (0.048) loss 0.7734 (0.8359) acc 84.4340 (80.5159) lr 1.4818e-03 eta 0:06:00
epoch [19/50] batch [40/51] time 0.162 (0.219) data 0.000 (0.042) loss 0.8632 (0.8461) acc 80.8511 (80.3350) lr 1.4818e-03 eta 0:05:48
epoch [19/50] batch [45/51] time 0.155 (0.213) data 0.000 (0.037) loss 0.8960 (0.8507) acc 80.6818 (80.2713) lr 1.4818e-03 eta 0:05:37
epoch [19/50] batch [50/51] time 0.165 (0.208) data 0.000 (0.034) loss 0.6823 (0.8512) acc 89.3617 (80.4552) lr 1.4818e-03 eta 0:05:29
>>> alpha1: 0.221  alpha2: 0.033 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.43 <<<
epoch [20/50] batch [5/51] time 0.178 (0.489) data 0.000 (0.307) loss 0.8105 (0.8178) acc 79.7872 (81.8211) lr 1.4258e-03 eta 0:12:51
epoch [20/50] batch [10/51] time 0.175 (0.337) data 0.000 (0.154) loss 0.7294 (0.7932) acc 81.2500 (81.7169) lr 1.4258e-03 eta 0:08:49
epoch [20/50] batch [15/51] time 0.182 (0.284) data 0.000 (0.103) loss 0.8226 (0.8181) acc 84.6939 (81.8983) lr 1.4258e-03 eta 0:07:24
epoch [20/50] batch [20/51] time 0.195 (0.258) data 0.000 (0.077) loss 0.5871 (0.8171) acc 85.6481 (81.2993) lr 1.4258e-03 eta 0:06:43
epoch [20/50] batch [25/51] time 0.177 (0.243) data 0.001 (0.062) loss 0.9624 (0.8185) acc 72.3958 (80.8798) lr 1.4258e-03 eta 0:06:17
epoch [20/50] batch [30/51] time 0.172 (0.233) data 0.001 (0.052) loss 0.9853 (0.8021) acc 75.0000 (81.3717) lr 1.4258e-03 eta 0:06:01
epoch [20/50] batch [35/51] time 0.184 (0.226) data 0.000 (0.044) loss 0.7521 (0.8128) acc 83.4906 (80.9848) lr 1.4258e-03 eta 0:05:49
epoch [20/50] batch [40/51] time 0.168 (0.220) data 0.000 (0.039) loss 0.8858 (0.8252) acc 77.0408 (80.7726) lr 1.4258e-03 eta 0:05:38
epoch [20/50] batch [45/51] time 0.174 (0.214) data 0.000 (0.034) loss 0.7384 (0.8203) acc 84.1346 (80.8806) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [50/51] time 0.159 (0.209) data 0.000 (0.031) loss 0.7292 (0.8173) acc 88.0435 (81.1262) lr 1.4258e-03 eta 0:05:20
>>> alpha1: 0.209  alpha2: 0.035 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.41 <<<
epoch [21/50] batch [5/51] time 0.173 (0.465) data 0.000 (0.281) loss 0.6875 (0.6926) acc 88.7255 (84.8973) lr 1.3681e-03 eta 0:11:48
epoch [21/50] batch [10/51] time 0.183 (0.325) data 0.000 (0.140) loss 0.5349 (0.6898) acc 88.2353 (85.3404) lr 1.3681e-03 eta 0:08:13
epoch [21/50] batch [15/51] time 0.169 (0.276) data 0.000 (0.094) loss 1.1244 (0.7552) acc 72.9592 (83.6428) lr 1.3681e-03 eta 0:06:57
epoch [21/50] batch [20/51] time 0.183 (0.253) data 0.001 (0.070) loss 0.7778 (0.7507) acc 81.2500 (83.5689) lr 1.3681e-03 eta 0:06:22
epoch [21/50] batch [25/51] time 0.169 (0.238) data 0.000 (0.056) loss 0.9223 (0.7661) acc 77.7778 (83.1407) lr 1.3681e-03 eta 0:05:57
epoch [21/50] batch [30/51] time 0.178 (0.226) data 0.000 (0.047) loss 0.7061 (0.7810) acc 83.6538 (82.5223) lr 1.3681e-03 eta 0:05:39
epoch [21/50] batch [35/51] time 0.180 (0.220) data 0.000 (0.040) loss 0.9576 (0.7962) acc 81.2500 (82.4736) lr 1.3681e-03 eta 0:05:28
epoch [21/50] batch [40/51] time 0.175 (0.215) data 0.000 (0.035) loss 0.8848 (0.7947) acc 74.4898 (82.2668) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [45/51] time 0.168 (0.210) data 0.000 (0.031) loss 1.0508 (0.7976) acc 69.5000 (82.0517) lr 1.3681e-03 eta 0:05:11
epoch [21/50] batch [50/51] time 0.159 (0.205) data 0.000 (0.028) loss 1.0203 (0.7956) acc 71.7391 (82.0348) lr 1.3681e-03 eta 0:05:04
>>> alpha1: 0.199  alpha2: 0.038 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.44 <<<
epoch [22/50] batch [5/51] time 0.180 (0.484) data 0.000 (0.299) loss 0.5105 (0.7590) acc 88.4259 (83.9242) lr 1.3090e-03 eta 0:11:52
epoch [22/50] batch [10/51] time 0.178 (0.331) data 0.000 (0.150) loss 0.7963 (0.7970) acc 82.6531 (82.1800) lr 1.3090e-03 eta 0:08:06
epoch [22/50] batch [15/51] time 0.168 (0.280) data 0.000 (0.100) loss 0.5460 (0.7714) acc 88.7755 (82.8324) lr 1.3090e-03 eta 0:06:50
epoch [22/50] batch [20/51] time 0.166 (0.256) data 0.000 (0.075) loss 1.0764 (0.7670) acc 70.3125 (82.9741) lr 1.3090e-03 eta 0:06:13
epoch [22/50] batch [25/51] time 0.193 (0.241) data 0.000 (0.060) loss 1.0255 (0.7794) acc 73.9583 (82.0080) lr 1.3090e-03 eta 0:05:50
epoch [22/50] batch [30/51] time 0.179 (0.231) data 0.000 (0.050) loss 0.6346 (0.7580) acc 86.4583 (82.7160) lr 1.3090e-03 eta 0:05:34
epoch [22/50] batch [35/51] time 0.172 (0.223) data 0.000 (0.043) loss 0.6506 (0.7586) acc 89.7059 (82.8464) lr 1.3090e-03 eta 0:05:21
epoch [22/50] batch [40/51] time 0.166 (0.217) data 0.000 (0.038) loss 0.7578 (0.7694) acc 80.7292 (82.5792) lr 1.3090e-03 eta 0:05:12
epoch [22/50] batch [45/51] time 0.171 (0.213) data 0.001 (0.033) loss 0.8989 (0.7690) acc 80.5000 (82.7169) lr 1.3090e-03 eta 0:05:04
epoch [22/50] batch [50/51] time 0.174 (0.209) data 0.000 (0.030) loss 0.8278 (0.7676) acc 80.7692 (82.6761) lr 1.3090e-03 eta 0:04:58
>>> alpha1: 0.193  alpha2: 0.043 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [23/50] batch [5/51] time 0.192 (0.482) data 0.000 (0.302) loss 0.5407 (1.0027) acc 90.2778 (81.3647) lr 1.2487e-03 eta 0:11:26
epoch [23/50] batch [10/51] time 0.169 (0.332) data 0.000 (0.151) loss 0.5456 (0.8277) acc 87.5000 (83.2070) lr 1.2487e-03 eta 0:07:51
epoch [23/50] batch [15/51] time 0.194 (0.282) data 0.000 (0.101) loss 0.7320 (0.8172) acc 84.3750 (82.8432) lr 1.2487e-03 eta 0:06:38
epoch [23/50] batch [20/51] time 0.162 (0.256) data 0.000 (0.076) loss 0.6283 (0.7848) acc 88.3333 (82.9230) lr 1.2487e-03 eta 0:06:00
epoch [23/50] batch [25/51] time 0.203 (0.241) data 0.000 (0.061) loss 0.6954 (0.7565) acc 82.0000 (83.2001) lr 1.2487e-03 eta 0:05:38
epoch [23/50] batch [30/51] time 0.169 (0.231) data 0.000 (0.051) loss 0.6720 (0.7656) acc 83.0000 (83.0330) lr 1.2487e-03 eta 0:05:22
epoch [23/50] batch [35/51] time 0.177 (0.223) data 0.000 (0.043) loss 0.6151 (0.7564) acc 84.9057 (83.4525) lr 1.2487e-03 eta 0:05:10
epoch [23/50] batch [40/51] time 0.175 (0.217) data 0.000 (0.038) loss 0.9453 (0.7653) acc 76.4706 (82.8608) lr 1.2487e-03 eta 0:05:00
epoch [23/50] batch [45/51] time 0.167 (0.211) data 0.000 (0.034) loss 1.1054 (0.7764) acc 70.9184 (82.1646) lr 1.2487e-03 eta 0:04:52
epoch [23/50] batch [50/51] time 0.169 (0.207) data 0.000 (0.030) loss 0.9689 (0.7846) acc 74.0000 (81.8300) lr 1.2487e-03 eta 0:04:44
>>> alpha1: 0.184  alpha2: 0.040 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.43 <<<
epoch [24/50] batch [5/51] time 0.168 (0.513) data 0.000 (0.333) loss 0.5781 (0.8415) acc 91.8367 (84.9702) lr 1.1874e-03 eta 0:11:43
epoch [24/50] batch [10/51] time 0.159 (0.342) data 0.000 (0.167) loss 0.8049 (0.7886) acc 82.2222 (84.2274) lr 1.1874e-03 eta 0:07:47
epoch [24/50] batch [15/51] time 0.178 (0.288) data 0.000 (0.111) loss 0.7258 (0.7583) acc 84.3750 (84.5547) lr 1.1874e-03 eta 0:06:32
epoch [24/50] batch [20/51] time 0.174 (0.259) data 0.000 (0.084) loss 0.8363 (0.7429) acc 79.3269 (84.3162) lr 1.1874e-03 eta 0:05:51
epoch [24/50] batch [25/51] time 0.183 (0.242) data 0.000 (0.067) loss 0.8575 (0.7430) acc 75.4717 (83.5155) lr 1.1874e-03 eta 0:05:27
epoch [24/50] batch [30/51] time 0.175 (0.231) data 0.000 (0.056) loss 0.6874 (0.7439) acc 83.6538 (83.3454) lr 1.1874e-03 eta 0:05:11
epoch [24/50] batch [35/51] time 0.177 (0.223) data 0.000 (0.048) loss 0.7060 (0.7440) acc 86.2245 (83.1717) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [40/51] time 0.177 (0.218) data 0.000 (0.042) loss 0.8250 (0.7687) acc 82.5472 (82.5190) lr 1.1874e-03 eta 0:04:51
epoch [24/50] batch [45/51] time 0.165 (0.212) data 0.000 (0.038) loss 0.6984 (0.7632) acc 81.7708 (82.6361) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [50/51] time 0.172 (0.208) data 0.000 (0.034) loss 0.6995 (0.7466) acc 79.3269 (83.0639) lr 1.1874e-03 eta 0:04:35
>>> alpha1: 0.178  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [25/50] batch [5/51] time 0.174 (0.450) data 0.000 (0.261) loss 0.6960 (0.7413) acc 87.5000 (85.0138) lr 1.1253e-03 eta 0:09:54
epoch [25/50] batch [10/51] time 0.171 (0.309) data 0.000 (0.130) loss 0.7519 (0.6972) acc 84.8039 (84.7043) lr 1.1253e-03 eta 0:06:46
epoch [25/50] batch [15/51] time 0.205 (0.266) data 0.000 (0.087) loss 0.9246 (0.7163) acc 78.2407 (84.0206) lr 1.1253e-03 eta 0:05:49
epoch [25/50] batch [20/51] time 0.173 (0.244) data 0.000 (0.065) loss 0.6683 (0.7190) acc 83.3333 (83.9888) lr 1.1253e-03 eta 0:05:19
epoch [25/50] batch [25/51] time 0.187 (0.232) data 0.000 (0.052) loss 0.7740 (0.6953) acc 77.8846 (84.2314) lr 1.1253e-03 eta 0:05:01
epoch [25/50] batch [30/51] time 0.173 (0.223) data 0.000 (0.044) loss 0.7876 (0.6868) acc 77.8846 (84.3128) lr 1.1253e-03 eta 0:04:49
epoch [25/50] batch [35/51] time 0.174 (0.216) data 0.000 (0.038) loss 0.7783 (0.6829) acc 84.0425 (84.5900) lr 1.1253e-03 eta 0:04:39
epoch [25/50] batch [40/51] time 0.161 (0.210) data 0.000 (0.033) loss 0.6434 (0.6784) acc 85.1064 (84.8307) lr 1.1253e-03 eta 0:04:30
epoch [25/50] batch [45/51] time 0.160 (0.205) data 0.000 (0.029) loss 0.6851 (0.6809) acc 85.6383 (84.8222) lr 1.1253e-03 eta 0:04:23
epoch [25/50] batch [50/51] time 0.170 (0.202) data 0.000 (0.026) loss 0.7066 (0.6894) acc 80.0000 (84.4969) lr 1.1253e-03 eta 0:04:17
>>> alpha1: 0.174  alpha2: 0.040 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [26/50] batch [5/51] time 0.166 (0.486) data 0.000 (0.310) loss 0.6838 (0.6244) acc 84.3750 (86.6435) lr 1.0628e-03 eta 0:10:17
epoch [26/50] batch [10/51] time 0.170 (0.332) data 0.000 (0.155) loss 0.5273 (0.6571) acc 92.6471 (86.2021) lr 1.0628e-03 eta 0:06:59
epoch [26/50] batch [15/51] time 0.155 (0.277) data 0.000 (0.104) loss 1.0939 (0.6706) acc 77.3256 (85.5465) lr 1.0628e-03 eta 0:05:49
epoch [26/50] batch [20/51] time 0.187 (0.251) data 0.000 (0.078) loss 0.5631 (0.6796) acc 85.6481 (85.1927) lr 1.0628e-03 eta 0:05:15
epoch [26/50] batch [25/51] time 0.171 (0.235) data 0.000 (0.062) loss 0.7624 (0.7781) acc 82.8431 (84.0021) lr 1.0628e-03 eta 0:04:54
epoch [26/50] batch [30/51] time 0.168 (0.224) data 0.001 (0.052) loss 0.7185 (0.7650) acc 83.5106 (84.0670) lr 1.0628e-03 eta 0:04:38
epoch [26/50] batch [35/51] time 0.188 (0.218) data 0.000 (0.045) loss 0.6253 (0.7484) acc 88.2653 (84.1239) lr 1.0628e-03 eta 0:04:30
epoch [26/50] batch [40/51] time 0.165 (0.213) data 0.000 (0.039) loss 0.7573 (0.7492) acc 81.2500 (83.9447) lr 1.0628e-03 eta 0:04:22
epoch [26/50] batch [45/51] time 0.164 (0.208) data 0.000 (0.035) loss 0.6588 (0.7508) acc 83.8542 (83.9078) lr 1.0628e-03 eta 0:04:15
epoch [26/50] batch [50/51] time 0.179 (0.204) data 0.000 (0.031) loss 0.5743 (0.7578) acc 86.5741 (83.9874) lr 1.0628e-03 eta 0:04:09
>>> alpha1: 0.169  alpha2: 0.041 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.43 <<<
epoch [27/50] batch [5/51] time 0.177 (0.436) data 0.002 (0.253) loss 0.6153 (0.6452) acc 88.2353 (85.2869) lr 1.0000e-03 eta 0:08:51
epoch [27/50] batch [10/51] time 0.182 (0.302) data 0.000 (0.126) loss 0.5350 (0.7370) acc 87.2642 (83.7396) lr 1.0000e-03 eta 0:06:06
epoch [27/50] batch [15/51] time 0.186 (0.263) data 0.001 (0.086) loss 0.7139 (0.7266) acc 86.4583 (84.1437) lr 1.0000e-03 eta 0:05:17
epoch [27/50] batch [20/51] time 0.181 (0.240) data 0.001 (0.065) loss 0.6253 (0.7275) acc 83.6538 (83.9511) lr 1.0000e-03 eta 0:04:48
epoch [27/50] batch [25/51] time 0.188 (0.229) data 0.001 (0.052) loss 0.4973 (0.7010) acc 87.0536 (84.6173) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [30/51] time 0.195 (0.222) data 0.000 (0.043) loss 0.5323 (0.6842) acc 83.1731 (84.9632) lr 1.0000e-03 eta 0:04:24
epoch [27/50] batch [35/51] time 0.168 (0.215) data 0.000 (0.037) loss 0.6724 (0.6782) acc 84.0425 (85.1938) lr 1.0000e-03 eta 0:04:15
epoch [27/50] batch [40/51] time 0.162 (0.210) data 0.000 (0.032) loss 0.7301 (0.6742) acc 84.5745 (85.2065) lr 1.0000e-03 eta 0:04:08
epoch [27/50] batch [45/51] time 0.162 (0.205) data 0.000 (0.029) loss 0.8541 (0.6758) acc 78.1915 (85.1917) lr 1.0000e-03 eta 0:04:01
epoch [27/50] batch [50/51] time 0.170 (0.201) data 0.000 (0.026) loss 0.6749 (0.6716) acc 85.2941 (85.3122) lr 1.0000e-03 eta 0:03:56
>>> alpha1: 0.168  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [28/50] batch [5/51] time 0.202 (0.466) data 0.000 (0.270) loss 0.7968 (0.6886) acc 79.4118 (85.4199) lr 9.3721e-04 eta 0:09:04
epoch [28/50] batch [10/51] time 0.165 (0.318) data 0.000 (0.135) loss 0.7118 (0.6661) acc 81.7708 (84.2834) lr 9.3721e-04 eta 0:06:09
epoch [28/50] batch [15/51] time 0.174 (0.269) data 0.000 (0.090) loss 0.8788 (0.6626) acc 80.3922 (84.4089) lr 9.3721e-04 eta 0:05:11
epoch [28/50] batch [20/51] time 0.169 (0.242) data 0.000 (0.068) loss 0.5854 (0.6829) acc 85.5000 (83.9339) lr 9.3721e-04 eta 0:04:39
epoch [28/50] batch [25/51] time 0.176 (0.229) data 0.000 (0.054) loss 0.8007 (0.6659) acc 85.2041 (84.5875) lr 9.3721e-04 eta 0:04:22
epoch [28/50] batch [30/51] time 0.177 (0.220) data 0.000 (0.045) loss 0.5580 (0.6697) acc 89.6226 (84.6714) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [35/51] time 0.181 (0.213) data 0.000 (0.039) loss 0.7071 (0.6640) acc 83.8542 (84.8599) lr 9.3721e-04 eta 0:04:02
epoch [28/50] batch [40/51] time 0.171 (0.208) data 0.000 (0.034) loss 0.3821 (0.6501) acc 91.6667 (85.0987) lr 9.3721e-04 eta 0:03:55
epoch [28/50] batch [45/51] time 0.169 (0.204) data 0.000 (0.030) loss 0.5736 (0.6535) acc 88.2353 (85.2996) lr 9.3721e-04 eta 0:03:49
epoch [28/50] batch [50/51] time 0.169 (0.200) data 0.000 (0.027) loss 0.5445 (0.6512) acc 93.8775 (85.2095) lr 9.3721e-04 eta 0:03:44
>>> alpha1: 0.167  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [29/50] batch [5/51] time 0.201 (0.460) data 0.016 (0.267) loss 0.7741 (0.6442) acc 79.2453 (86.1523) lr 8.7467e-04 eta 0:08:33
epoch [29/50] batch [10/51] time 0.180 (0.320) data 0.001 (0.134) loss 0.9201 (0.6648) acc 76.0870 (84.7516) lr 8.7467e-04 eta 0:05:55
epoch [29/50] batch [15/51] time 0.172 (0.269) data 0.000 (0.089) loss 0.6381 (0.6525) acc 85.2941 (85.2097) lr 8.7467e-04 eta 0:04:58
epoch [29/50] batch [20/51] time 0.184 (0.247) data 0.000 (0.067) loss 0.8442 (0.6705) acc 85.0000 (84.7338) lr 8.7467e-04 eta 0:04:32
epoch [29/50] batch [25/51] time 0.161 (0.232) data 0.000 (0.054) loss 0.6712 (0.6693) acc 84.5745 (84.8512) lr 8.7467e-04 eta 0:04:15
epoch [29/50] batch [30/51] time 0.157 (0.222) data 0.000 (0.045) loss 0.6541 (0.6630) acc 82.2222 (84.8274) lr 8.7467e-04 eta 0:04:02
epoch [29/50] batch [35/51] time 0.175 (0.214) data 0.000 (0.038) loss 0.7487 (0.6532) acc 84.8958 (85.0424) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [40/51] time 0.174 (0.209) data 0.000 (0.034) loss 0.5928 (0.6507) acc 86.0577 (84.9619) lr 8.7467e-04 eta 0:03:45
epoch [29/50] batch [45/51] time 0.173 (0.204) data 0.000 (0.030) loss 0.6342 (0.6479) acc 84.6154 (85.0331) lr 8.7467e-04 eta 0:03:40
epoch [29/50] batch [50/51] time 0.163 (0.200) data 0.000 (0.027) loss 0.7510 (0.6483) acc 77.6596 (85.0885) lr 8.7467e-04 eta 0:03:34
>>> alpha1: 0.161  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [30/50] batch [5/51] time 0.169 (0.461) data 0.000 (0.281) loss 0.8311 (0.7569) acc 78.9773 (83.1183) lr 8.1262e-04 eta 0:08:11
epoch [30/50] batch [10/51] time 0.181 (0.315) data 0.000 (0.141) loss 0.6736 (0.7046) acc 85.2041 (84.5935) lr 8.1262e-04 eta 0:05:34
epoch [30/50] batch [15/51] time 0.195 (0.273) data 0.000 (0.094) loss 0.5400 (0.6745) acc 89.2241 (85.4751) lr 8.1262e-04 eta 0:04:48
epoch [30/50] batch [20/51] time 0.195 (0.249) data 0.001 (0.070) loss 0.6059 (0.6592) acc 90.4255 (85.4376) lr 8.1262e-04 eta 0:04:21
epoch [30/50] batch [25/51] time 0.177 (0.235) data 0.000 (0.056) loss 0.5480 (0.6677) acc 87.7358 (85.2637) lr 8.1262e-04 eta 0:04:05
epoch [30/50] batch [30/51] time 0.186 (0.226) data 0.000 (0.047) loss 0.5131 (0.6452) acc 87.2807 (85.7968) lr 8.1262e-04 eta 0:03:55
epoch [30/50] batch [35/51] time 0.184 (0.219) data 0.000 (0.040) loss 0.7229 (0.6522) acc 83.3333 (85.3731) lr 8.1262e-04 eta 0:03:46
epoch [30/50] batch [40/51] time 0.165 (0.212) data 0.000 (0.035) loss 0.6644 (0.6557) acc 85.7143 (85.2074) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [45/51] time 0.184 (0.207) data 0.000 (0.031) loss 0.5745 (0.6516) acc 84.8214 (85.1922) lr 8.1262e-04 eta 0:03:32
epoch [30/50] batch [50/51] time 0.173 (0.203) data 0.000 (0.028) loss 0.7743 (0.6646) acc 85.0962 (84.9164) lr 8.1262e-04 eta 0:03:27
>>> alpha1: 0.160  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [31/50] batch [5/51] time 0.175 (0.563) data 0.000 (0.386) loss 0.5725 (0.5470) acc 86.7647 (88.2932) lr 7.5131e-04 eta 0:09:31
epoch [31/50] batch [10/51] time 0.176 (0.370) data 0.001 (0.193) loss 0.5639 (0.5459) acc 86.7347 (88.5550) lr 7.5131e-04 eta 0:06:13
epoch [31/50] batch [15/51] time 0.171 (0.304) data 0.000 (0.129) loss 0.7046 (0.5792) acc 88.7255 (88.4569) lr 7.5131e-04 eta 0:05:05
epoch [31/50] batch [20/51] time 0.173 (0.273) data 0.001 (0.097) loss 0.6544 (0.6011) acc 89.1304 (88.0436) lr 7.5131e-04 eta 0:04:33
epoch [31/50] batch [25/51] time 0.171 (0.253) data 0.000 (0.077) loss 0.5543 (0.6232) acc 86.4130 (87.0869) lr 7.5131e-04 eta 0:04:12
epoch [31/50] batch [30/51] time 0.186 (0.240) data 0.001 (0.065) loss 0.6558 (0.6264) acc 88.1818 (86.7921) lr 7.5131e-04 eta 0:03:57
epoch [31/50] batch [35/51] time 0.165 (0.230) data 0.000 (0.055) loss 0.7080 (0.6363) acc 84.8958 (86.4563) lr 7.5131e-04 eta 0:03:46
epoch [31/50] batch [40/51] time 0.168 (0.222) data 0.000 (0.049) loss 0.5664 (0.6467) acc 86.0000 (86.1554) lr 7.5131e-04 eta 0:03:37
epoch [31/50] batch [45/51] time 0.175 (0.216) data 0.000 (0.043) loss 0.5695 (0.6367) acc 88.4615 (86.3911) lr 7.5131e-04 eta 0:03:31
epoch [31/50] batch [50/51] time 0.168 (0.212) data 0.000 (0.039) loss 0.7750 (0.6373) acc 85.5000 (86.4734) lr 7.5131e-04 eta 0:03:25
>>> alpha1: 0.160  alpha2: 0.045 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [32/50] batch [5/51] time 0.174 (0.450) data 0.000 (0.269) loss 0.5308 (0.6932) acc 92.3469 (84.2189) lr 6.9098e-04 eta 0:07:13
epoch [32/50] batch [10/51] time 0.175 (0.315) data 0.001 (0.135) loss 0.6343 (0.6037) acc 86.0000 (86.9942) lr 6.9098e-04 eta 0:05:01
epoch [32/50] batch [15/51] time 0.172 (0.268) data 0.000 (0.090) loss 0.8597 (0.6370) acc 83.5000 (86.4594) lr 6.9098e-04 eta 0:04:15
epoch [32/50] batch [20/51] time 0.164 (0.245) data 0.000 (0.068) loss 0.5452 (0.6121) acc 84.0425 (86.5379) lr 6.9098e-04 eta 0:03:52
epoch [32/50] batch [25/51] time 0.172 (0.231) data 0.000 (0.054) loss 0.6257 (0.6165) acc 86.7021 (87.0214) lr 6.9098e-04 eta 0:03:38
epoch [32/50] batch [30/51] time 0.198 (0.223) data 0.000 (0.045) loss 0.7713 (0.6217) acc 81.2500 (86.5192) lr 6.9098e-04 eta 0:03:29
epoch [32/50] batch [35/51] time 0.170 (0.217) data 0.000 (0.039) loss 0.4151 (0.6139) acc 93.0000 (86.7376) lr 6.9098e-04 eta 0:03:22
epoch [32/50] batch [40/51] time 0.161 (0.210) data 0.000 (0.034) loss 0.5080 (0.6120) acc 91.4894 (86.9192) lr 6.9098e-04 eta 0:03:15
epoch [32/50] batch [45/51] time 0.174 (0.206) data 0.000 (0.030) loss 0.6545 (0.6149) acc 83.1731 (86.6982) lr 6.9098e-04 eta 0:03:09
epoch [32/50] batch [50/51] time 0.163 (0.202) data 0.000 (0.027) loss 0.6789 (0.6179) acc 89.5833 (86.7891) lr 6.9098e-04 eta 0:03:05
>>> alpha1: 0.156  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [33/50] batch [5/51] time 0.188 (0.461) data 0.000 (0.278) loss 0.6285 (0.6003) acc 87.5000 (85.8432) lr 6.3188e-04 eta 0:07:00
epoch [33/50] batch [10/51] time 0.175 (0.318) data 0.000 (0.139) loss 0.4710 (0.5686) acc 87.0192 (86.9269) lr 6.3188e-04 eta 0:04:49
epoch [33/50] batch [15/51] time 0.177 (0.270) data 0.000 (0.093) loss 0.6443 (0.5877) acc 88.2979 (87.0535) lr 6.3188e-04 eta 0:04:03
epoch [33/50] batch [20/51] time 0.172 (0.247) data 0.001 (0.070) loss 0.6805 (0.6122) acc 84.2391 (87.0284) lr 6.3188e-04 eta 0:03:41
epoch [33/50] batch [25/51] time 0.203 (0.234) data 0.000 (0.056) loss 0.6311 (0.6112) acc 82.1429 (86.8644) lr 6.3188e-04 eta 0:03:28
epoch [33/50] batch [30/51] time 0.180 (0.225) data 0.001 (0.047) loss 0.7016 (0.6024) acc 87.2222 (86.9560) lr 6.3188e-04 eta 0:03:19
epoch [33/50] batch [35/51] time 0.184 (0.219) data 0.000 (0.040) loss 0.8631 (0.6172) acc 76.9608 (86.5392) lr 6.3188e-04 eta 0:03:12
epoch [33/50] batch [40/51] time 0.167 (0.213) data 0.000 (0.035) loss 0.4780 (0.6201) acc 90.8163 (86.3156) lr 6.3188e-04 eta 0:03:06
epoch [33/50] batch [45/51] time 0.173 (0.208) data 0.000 (0.031) loss 0.7268 (0.6191) acc 82.2115 (86.3432) lr 6.3188e-04 eta 0:03:01
epoch [33/50] batch [50/51] time 0.161 (0.204) data 0.000 (0.028) loss 0.9238 (0.6257) acc 75.5319 (86.1579) lr 6.3188e-04 eta 0:02:57
>>> alpha1: 0.155  alpha2: 0.043 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [34/50] batch [5/51] time 0.193 (0.494) data 0.001 (0.312) loss 0.5611 (0.6461) acc 85.2041 (85.9329) lr 5.7422e-04 eta 0:07:05
epoch [34/50] batch [10/51] time 0.176 (0.335) data 0.001 (0.156) loss 0.5926 (0.6516) acc 84.5000 (85.5766) lr 5.7422e-04 eta 0:04:47
epoch [34/50] batch [15/51] time 0.176 (0.283) data 0.000 (0.104) loss 0.6249 (0.6354) acc 88.7755 (86.1833) lr 5.7422e-04 eta 0:04:01
epoch [34/50] batch [20/51] time 0.199 (0.257) data 0.000 (0.078) loss 0.7112 (0.6336) acc 83.6735 (86.4033) lr 5.7422e-04 eta 0:03:38
epoch [34/50] batch [25/51] time 0.175 (0.241) data 0.000 (0.063) loss 0.4896 (0.6240) acc 89.5833 (86.4454) lr 5.7422e-04 eta 0:03:23
epoch [34/50] batch [30/51] time 0.177 (0.229) data 0.000 (0.052) loss 0.8435 (0.6215) acc 80.6604 (86.2830) lr 5.7422e-04 eta 0:03:12
epoch [34/50] batch [35/51] time 0.164 (0.222) data 0.000 (0.045) loss 0.7435 (0.6261) acc 82.4468 (86.2869) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.039) loss 0.7813 (0.6230) acc 84.6939 (86.3860) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.035) loss 0.4541 (0.6215) acc 91.0000 (86.2725) lr 5.7422e-04 eta 0:02:52
epoch [34/50] batch [50/51] time 0.168 (0.206) data 0.000 (0.032) loss 0.5249 (0.6182) acc 89.0000 (86.5163) lr 5.7422e-04 eta 0:02:47
>>> alpha1: 0.156  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.40 <<<
epoch [35/50] batch [5/51] time 0.167 (0.452) data 0.000 (0.268) loss 0.6936 (0.5870) acc 85.6383 (88.1206) lr 5.1825e-04 eta 0:06:06
epoch [35/50] batch [10/51] time 0.171 (0.309) data 0.000 (0.134) loss 0.4516 (0.5694) acc 88.7255 (89.2200) lr 5.1825e-04 eta 0:04:09
epoch [35/50] batch [15/51] time 0.158 (0.264) data 0.000 (0.089) loss 0.6616 (0.5713) acc 83.8889 (88.5819) lr 5.1825e-04 eta 0:03:31
epoch [35/50] batch [20/51] time 0.164 (0.241) data 0.000 (0.067) loss 0.7325 (0.5798) acc 84.3750 (88.1944) lr 5.1825e-04 eta 0:03:12
epoch [35/50] batch [25/51] time 0.170 (0.226) data 0.000 (0.054) loss 0.6087 (0.6156) acc 85.5000 (87.8927) lr 5.1825e-04 eta 0:02:59
epoch [35/50] batch [30/51] time 0.180 (0.219) data 0.000 (0.045) loss 0.5077 (0.6114) acc 88.7755 (87.8886) lr 5.1825e-04 eta 0:02:52
epoch [35/50] batch [35/51] time 0.166 (0.213) data 0.001 (0.038) loss 0.6828 (0.6156) acc 82.6087 (87.3592) lr 5.1825e-04 eta 0:02:46
epoch [35/50] batch [40/51] time 0.159 (0.207) data 0.000 (0.034) loss 0.6495 (0.6182) acc 81.5217 (87.1675) lr 5.1825e-04 eta 0:02:40
epoch [35/50] batch [45/51] time 0.167 (0.203) data 0.000 (0.030) loss 0.4835 (0.6166) acc 88.0000 (87.0633) lr 5.1825e-04 eta 0:02:36
epoch [35/50] batch [50/51] time 0.177 (0.199) data 0.000 (0.027) loss 0.6097 (0.6158) acc 83.0189 (86.8577) lr 5.1825e-04 eta 0:02:32
>>> alpha1: 0.154  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [36/50] batch [5/51] time 0.164 (0.563) data 0.000 (0.388) loss 0.6277 (0.6336) acc 82.9787 (84.6787) lr 4.6417e-04 eta 0:07:08
epoch [36/50] batch [10/51] time 0.178 (0.366) data 0.001 (0.194) loss 0.6143 (0.5922) acc 85.8696 (86.5761) lr 4.6417e-04 eta 0:04:36
epoch [36/50] batch [15/51] time 0.182 (0.300) data 0.000 (0.130) loss 0.4064 (0.5632) acc 94.6078 (87.2142) lr 4.6417e-04 eta 0:03:45
epoch [36/50] batch [20/51] time 0.169 (0.269) data 0.000 (0.097) loss 0.5825 (0.5747) acc 83.5000 (86.6715) lr 4.6417e-04 eta 0:03:20
epoch [36/50] batch [25/51] time 0.172 (0.249) data 0.000 (0.078) loss 0.6442 (0.5785) acc 86.2745 (86.7480) lr 4.6417e-04 eta 0:03:04
epoch [36/50] batch [30/51] time 0.165 (0.237) data 0.000 (0.065) loss 0.3901 (0.5787) acc 93.7500 (87.0648) lr 4.6417e-04 eta 0:02:54
epoch [36/50] batch [35/51] time 0.170 (0.229) data 0.000 (0.056) loss 0.7549 (0.5861) acc 81.0000 (86.9310) lr 4.6417e-04 eta 0:02:46
epoch [36/50] batch [40/51] time 0.175 (0.223) data 0.000 (0.049) loss 0.7513 (0.5917) acc 85.0962 (87.0295) lr 4.6417e-04 eta 0:02:41
epoch [36/50] batch [45/51] time 0.178 (0.217) data 0.000 (0.043) loss 0.4762 (0.5897) acc 86.3208 (87.0663) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [50/51] time 0.169 (0.213) data 0.000 (0.039) loss 0.5895 (0.5847) acc 87.2449 (87.0599) lr 4.6417e-04 eta 0:02:32
>>> alpha1: 0.152  alpha2: 0.046 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [37/50] batch [5/51] time 0.178 (0.518) data 0.000 (0.337) loss 0.5641 (0.7200) acc 90.8654 (84.9344) lr 4.1221e-04 eta 0:06:07
epoch [37/50] batch [10/51] time 0.178 (0.346) data 0.000 (0.169) loss 0.5893 (0.6728) acc 90.1961 (85.9271) lr 4.1221e-04 eta 0:04:03
epoch [37/50] batch [15/51] time 0.187 (0.288) data 0.000 (0.112) loss 0.5762 (0.6934) acc 84.6154 (86.1451) lr 4.1221e-04 eta 0:03:21
epoch [37/50] batch [20/51] time 0.166 (0.259) data 0.000 (0.084) loss 0.7146 (0.6727) acc 85.7143 (86.1046) lr 4.1221e-04 eta 0:02:59
epoch [37/50] batch [25/51] time 0.178 (0.244) data 0.000 (0.068) loss 0.7014 (0.6706) acc 78.3654 (85.5482) lr 4.1221e-04 eta 0:02:48
epoch [37/50] batch [30/51] time 0.170 (0.231) data 0.000 (0.056) loss 0.4991 (0.6519) acc 89.0000 (86.1669) lr 4.1221e-04 eta 0:02:38
epoch [37/50] batch [35/51] time 0.179 (0.224) data 0.000 (0.048) loss 0.5430 (0.6482) acc 89.8148 (86.1477) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [40/51] time 0.168 (0.218) data 0.000 (0.042) loss 0.6355 (0.6375) acc 86.5000 (86.2507) lr 4.1221e-04 eta 0:02:26
epoch [37/50] batch [45/51] time 0.168 (0.212) data 0.000 (0.038) loss 0.7115 (0.6392) acc 81.5000 (86.0342) lr 4.1221e-04 eta 0:02:21
epoch [37/50] batch [50/51] time 0.157 (0.207) data 0.000 (0.034) loss 0.6175 (0.6346) acc 87.2222 (86.1711) lr 4.1221e-04 eta 0:02:17
>>> alpha1: 0.152  alpha2: 0.047 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [38/50] batch [5/51] time 0.183 (0.450) data 0.000 (0.272) loss 0.5687 (0.5832) acc 88.2075 (88.5820) lr 3.6258e-04 eta 0:04:55
epoch [38/50] batch [10/51] time 0.177 (0.309) data 0.001 (0.136) loss 0.5943 (0.5857) acc 85.5769 (88.2731) lr 3.6258e-04 eta 0:03:22
epoch [38/50] batch [15/51] time 0.181 (0.264) data 0.000 (0.091) loss 0.5848 (0.6071) acc 85.5769 (88.0829) lr 3.6258e-04 eta 0:02:50
epoch [38/50] batch [20/51] time 0.183 (0.243) data 0.001 (0.068) loss 0.5151 (0.5868) acc 89.1509 (87.9548) lr 3.6258e-04 eta 0:02:36
epoch [38/50] batch [25/51] time 0.195 (0.229) data 0.000 (0.055) loss 0.8603 (0.6157) acc 82.3529 (87.0135) lr 3.6258e-04 eta 0:02:26
epoch [38/50] batch [30/51] time 0.162 (0.219) data 0.000 (0.046) loss 0.4888 (0.6109) acc 88.2979 (87.2477) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [35/51] time 0.189 (0.213) data 0.000 (0.039) loss 0.4584 (0.6080) acc 91.8182 (87.2367) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [40/51] time 0.169 (0.208) data 0.000 (0.034) loss 0.7674 (0.6160) acc 85.2041 (87.0089) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [45/51] time 0.171 (0.204) data 0.000 (0.030) loss 0.8976 (0.6173) acc 77.4510 (86.9571) lr 3.6258e-04 eta 0:02:06
epoch [38/50] batch [50/51] time 0.164 (0.201) data 0.000 (0.027) loss 0.5217 (0.6103) acc 89.0625 (86.9377) lr 3.6258e-04 eta 0:02:03
>>> alpha1: 0.152  alpha2: 0.048 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [39/50] batch [5/51] time 0.186 (0.456) data 0.001 (0.275) loss 0.7957 (0.6238) acc 79.0816 (87.6030) lr 3.1545e-04 eta 0:04:36
epoch [39/50] batch [10/51] time 0.176 (0.318) data 0.000 (0.138) loss 0.5198 (0.6185) acc 86.5385 (87.1392) lr 3.1545e-04 eta 0:03:11
epoch [39/50] batch [15/51] time 0.175 (0.271) data 0.000 (0.092) loss 0.4098 (0.5979) acc 88.4615 (87.4564) lr 3.1545e-04 eta 0:02:41
epoch [39/50] batch [20/51] time 0.172 (0.248) data 0.000 (0.069) loss 0.7320 (0.6008) acc 78.9216 (86.7596) lr 3.1545e-04 eta 0:02:26
epoch [39/50] batch [25/51] time 0.181 (0.235) data 0.000 (0.056) loss 0.4940 (0.6020) acc 89.8148 (86.8677) lr 3.1545e-04 eta 0:02:17
epoch [39/50] batch [30/51] time 0.169 (0.224) data 0.001 (0.047) loss 0.8204 (0.6030) acc 79.0000 (86.9007) lr 3.1545e-04 eta 0:02:10
epoch [39/50] batch [35/51] time 0.185 (0.218) data 0.000 (0.040) loss 0.7700 (0.6106) acc 81.6038 (86.6905) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [40/51] time 0.167 (0.212) data 0.000 (0.035) loss 0.6623 (0.6046) acc 85.2041 (86.7402) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [45/51] time 0.170 (0.207) data 0.000 (0.031) loss 0.6681 (0.6095) acc 84.5000 (86.6166) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [50/51] time 0.174 (0.203) data 0.000 (0.028) loss 0.6747 (0.6078) acc 85.5769 (86.6127) lr 3.1545e-04 eta 0:01:54
>>> alpha1: 0.151  alpha2: 0.048 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [40/50] batch [5/51] time 0.162 (0.518) data 0.000 (0.348) loss 0.3549 (0.5692) acc 93.0851 (88.0169) lr 2.7103e-04 eta 0:04:47
epoch [40/50] batch [10/51] time 0.181 (0.344) data 0.000 (0.174) loss 0.4297 (0.5563) acc 90.3846 (88.8585) lr 2.7103e-04 eta 0:03:09
epoch [40/50] batch [15/51] time 0.178 (0.286) data 0.000 (0.116) loss 0.5182 (0.5759) acc 87.5000 (87.9002) lr 2.7103e-04 eta 0:02:36
epoch [40/50] batch [20/51] time 0.190 (0.258) data 0.000 (0.087) loss 0.4344 (0.5714) acc 90.1042 (87.2845) lr 2.7103e-04 eta 0:02:19
epoch [40/50] batch [25/51] time 0.154 (0.240) data 0.000 (0.070) loss 0.9348 (0.5924) acc 72.0930 (86.6364) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [30/51] time 0.183 (0.231) data 0.013 (0.059) loss 0.4910 (0.5985) acc 92.8571 (86.7493) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [35/51] time 0.185 (0.223) data 0.000 (0.050) loss 0.4333 (0.5945) acc 89.0909 (86.8530) lr 2.7103e-04 eta 0:01:57
epoch [40/50] batch [40/51] time 0.168 (0.217) data 0.000 (0.044) loss 0.4729 (0.5948) acc 91.0000 (87.0486) lr 2.7103e-04 eta 0:01:53
epoch [40/50] batch [45/51] time 0.171 (0.212) data 0.000 (0.039) loss 0.6761 (0.5979) acc 88.0000 (87.0900) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.178 (0.208) data 0.000 (0.035) loss 0.5731 (0.5944) acc 92.1296 (87.4512) lr 2.7103e-04 eta 0:01:46
>>> alpha1: 0.150  alpha2: 0.046 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [41/50] batch [5/51] time 0.183 (0.524) data 0.000 (0.338) loss 0.8883 (0.6245) acc 81.9149 (86.5286) lr 2.2949e-04 eta 0:04:24
epoch [41/50] batch [10/51] time 0.171 (0.349) data 0.000 (0.169) loss 0.5969 (0.5883) acc 87.7451 (87.8020) lr 2.2949e-04 eta 0:02:54
epoch [41/50] batch [15/51] time 0.167 (0.291) data 0.000 (0.113) loss 0.6079 (0.5831) acc 92.3469 (87.8379) lr 2.2949e-04 eta 0:02:24
epoch [41/50] batch [20/51] time 0.162 (0.260) data 0.000 (0.085) loss 0.6658 (0.5894) acc 84.0425 (87.7601) lr 2.2949e-04 eta 0:02:07
epoch [41/50] batch [25/51] time 0.167 (0.243) data 0.001 (0.068) loss 0.6188 (0.5893) acc 83.8542 (87.4753) lr 2.2949e-04 eta 0:01:58
epoch [41/50] batch [30/51] time 0.173 (0.232) data 0.000 (0.056) loss 0.4616 (0.5927) acc 93.6170 (87.5757) lr 2.2949e-04 eta 0:01:51
epoch [41/50] batch [35/51] time 0.178 (0.224) data 0.000 (0.048) loss 0.5912 (0.5948) acc 86.7647 (87.4948) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [40/51] time 0.174 (0.217) data 0.000 (0.042) loss 0.4596 (0.5950) acc 89.4231 (87.4592) lr 2.2949e-04 eta 0:01:42
epoch [41/50] batch [45/51] time 0.162 (0.212) data 0.000 (0.038) loss 0.6769 (0.5972) acc 83.5106 (87.3959) lr 2.2949e-04 eta 0:01:38
epoch [41/50] batch [50/51] time 0.156 (0.207) data 0.000 (0.034) loss 0.7450 (0.5970) acc 82.9545 (87.3760) lr 2.2949e-04 eta 0:01:35
>>> alpha1: 0.149  alpha2: 0.048 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [42/50] batch [5/51] time 0.187 (0.487) data 0.000 (0.308) loss 0.4573 (0.5704) acc 91.0377 (87.4526) lr 1.9098e-04 eta 0:03:40
epoch [42/50] batch [10/51] time 0.171 (0.328) data 0.000 (0.154) loss 0.5849 (0.5836) acc 89.0000 (86.5476) lr 1.9098e-04 eta 0:02:27
epoch [42/50] batch [15/51] time 0.180 (0.275) data 0.000 (0.103) loss 0.4466 (0.6518) acc 88.7255 (86.6436) lr 1.9098e-04 eta 0:02:02
epoch [42/50] batch [20/51] time 0.168 (0.249) data 0.000 (0.077) loss 0.6134 (0.6365) acc 87.7551 (86.5414) lr 1.9098e-04 eta 0:01:49
epoch [42/50] batch [25/51] time 0.173 (0.233) data 0.000 (0.062) loss 0.6316 (0.6167) acc 84.6154 (86.6690) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [30/51] time 0.174 (0.224) data 0.000 (0.052) loss 0.6242 (0.6206) acc 90.1961 (86.9038) lr 1.9098e-04 eta 0:01:36
epoch [42/50] batch [35/51] time 0.171 (0.216) data 0.000 (0.044) loss 0.4700 (0.6130) acc 90.5000 (86.9711) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [40/51] time 0.173 (0.211) data 0.000 (0.039) loss 0.5783 (0.6040) acc 91.8269 (87.3246) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [45/51] time 0.167 (0.206) data 0.000 (0.034) loss 0.5418 (0.6016) acc 86.2245 (87.1860) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [50/51] time 0.155 (0.202) data 0.000 (0.031) loss 0.5792 (0.5973) acc 86.1111 (87.1369) lr 1.9098e-04 eta 0:01:22
>>> alpha1: 0.149  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [43/50] batch [5/51] time 0.183 (0.416) data 0.000 (0.237) loss 0.6187 (0.4938) acc 92.1569 (89.8795) lr 1.5567e-04 eta 0:02:47
epoch [43/50] batch [10/51] time 0.184 (0.296) data 0.000 (0.119) loss 0.7197 (0.5721) acc 81.3830 (87.8451) lr 1.5567e-04 eta 0:01:57
epoch [43/50] batch [15/51] time 0.176 (0.255) data 0.000 (0.079) loss 0.5310 (0.5601) acc 85.7843 (88.0857) lr 1.5567e-04 eta 0:01:40
epoch [43/50] batch [20/51] time 0.177 (0.234) data 0.001 (0.059) loss 0.7560 (0.5887) acc 86.4583 (87.6903) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [25/51] time 0.181 (0.223) data 0.000 (0.048) loss 0.7354 (0.6017) acc 85.0962 (87.2723) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [30/51] time 0.185 (0.216) data 0.000 (0.040) loss 0.3690 (0.5888) acc 92.9825 (87.4322) lr 1.5567e-04 eta 0:01:21
epoch [43/50] batch [35/51] time 0.168 (0.209) data 0.000 (0.034) loss 0.5469 (0.5907) acc 82.3864 (86.9488) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [40/51] time 0.163 (0.205) data 0.000 (0.030) loss 0.4636 (0.5894) acc 91.4894 (86.8437) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [45/51] time 0.166 (0.201) data 0.000 (0.027) loss 0.5477 (0.5923) acc 86.9792 (86.6814) lr 1.5567e-04 eta 0:01:12
epoch [43/50] batch [50/51] time 0.165 (0.197) data 0.000 (0.024) loss 0.6506 (0.5892) acc 84.3750 (86.6818) lr 1.5567e-04 eta 0:01:10
>>> alpha1: 0.148  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [44/50] batch [5/51] time 0.165 (0.444) data 0.000 (0.264) loss 0.7458 (0.5549) acc 84.0425 (90.8340) lr 1.2369e-04 eta 0:02:36
epoch [44/50] batch [10/51] time 0.161 (0.308) data 0.000 (0.132) loss 0.5494 (0.5715) acc 86.9565 (89.4113) lr 1.2369e-04 eta 0:01:46
epoch [44/50] batch [15/51] time 0.155 (0.261) data 0.000 (0.088) loss 0.8458 (0.6024) acc 81.5476 (88.2846) lr 1.2369e-04 eta 0:01:29
epoch [44/50] batch [20/51] time 0.169 (0.239) data 0.000 (0.066) loss 0.6751 (0.6014) acc 86.5000 (88.0926) lr 1.2369e-04 eta 0:01:20
epoch [44/50] batch [25/51] time 0.184 (0.227) data 0.000 (0.053) loss 0.5117 (0.5938) acc 85.1064 (88.1008) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [30/51] time 0.188 (0.219) data 0.000 (0.044) loss 0.5536 (0.5750) acc 87.9808 (88.4257) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [35/51] time 0.189 (0.213) data 0.000 (0.038) loss 0.6347 (0.5698) acc 87.5000 (88.4952) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [40/51] time 0.173 (0.208) data 0.000 (0.033) loss 0.4161 (0.5665) acc 91.3462 (88.6358) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [45/51] time 0.157 (0.203) data 0.000 (0.030) loss 0.5444 (0.5689) acc 89.6739 (88.4764) lr 1.2369e-04 eta 0:01:03
epoch [44/50] batch [50/51] time 0.167 (0.200) data 0.000 (0.027) loss 0.6767 (0.5677) acc 80.1020 (88.3779) lr 1.2369e-04 eta 0:01:01
>>> alpha1: 0.146  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.40 <<<
epoch [45/50] batch [5/51] time 0.154 (0.464) data 0.000 (0.291) loss 0.6133 (0.5738) acc 86.0465 (88.6979) lr 9.5173e-05 eta 0:02:19
epoch [45/50] batch [10/51] time 0.175 (0.321) data 0.000 (0.146) loss 0.6217 (0.5702) acc 82.2115 (88.1162) lr 9.5173e-05 eta 0:01:35
epoch [45/50] batch [15/51] time 0.161 (0.269) data 0.000 (0.097) loss 0.8480 (0.5881) acc 82.6087 (87.3180) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [20/51] time 0.164 (0.244) data 0.000 (0.073) loss 0.5298 (0.5811) acc 87.7660 (87.3700) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [25/51] time 0.169 (0.230) data 0.000 (0.059) loss 0.7382 (0.6032) acc 88.2979 (86.7891) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [30/51] time 0.179 (0.221) data 0.003 (0.049) loss 0.6243 (0.6030) acc 86.5385 (86.6555) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [35/51] time 0.180 (0.216) data 0.000 (0.042) loss 0.5114 (0.5966) acc 87.5000 (86.7150) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [40/51] time 0.177 (0.210) data 0.000 (0.037) loss 0.5235 (0.5972) acc 90.5660 (87.0778) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [45/51] time 0.165 (0.206) data 0.000 (0.033) loss 0.6036 (0.5927) acc 91.6667 (87.4228) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [50/51] time 0.174 (0.202) data 0.000 (0.029) loss 0.4792 (0.5853) acc 91.8269 (87.7331) lr 9.5173e-05 eta 0:00:51
>>> alpha1: 0.147  alpha2: 0.052 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [46/50] batch [5/51] time 0.183 (0.439) data 0.000 (0.256) loss 0.5604 (0.6081) acc 86.7647 (85.9074) lr 7.0224e-05 eta 0:01:49
epoch [46/50] batch [10/51] time 0.174 (0.309) data 0.001 (0.128) loss 0.5294 (0.5793) acc 91.6667 (87.1032) lr 7.0224e-05 eta 0:01:15
epoch [46/50] batch [15/51] time 0.169 (0.266) data 0.000 (0.085) loss 0.4930 (0.5577) acc 90.5000 (87.3281) lr 7.0224e-05 eta 0:01:03
epoch [46/50] batch [20/51] time 0.180 (0.241) data 0.000 (0.064) loss 0.4805 (0.5590) acc 88.6792 (87.3517) lr 7.0224e-05 eta 0:00:56
epoch [46/50] batch [25/51] time 0.198 (0.227) data 0.019 (0.052) loss 0.7273 (0.5826) acc 85.0962 (86.5851) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [30/51] time 0.165 (0.218) data 0.000 (0.043) loss 0.3935 (0.5825) acc 90.1042 (86.8086) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [35/51] time 0.183 (0.212) data 0.000 (0.037) loss 0.6838 (0.5850) acc 85.8491 (86.9545) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [40/51] time 0.168 (0.207) data 0.000 (0.033) loss 0.5905 (0.5901) acc 90.5000 (87.0950) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [45/51] time 0.164 (0.202) data 0.000 (0.029) loss 0.6233 (0.5871) acc 85.4167 (87.1488) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [50/51] time 0.180 (0.199) data 0.000 (0.026) loss 0.4374 (0.5837) acc 95.0000 (87.3027) lr 7.0224e-05 eta 0:00:40
>>> alpha1: 0.148  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [47/50] batch [5/51] time 0.175 (0.465) data 0.000 (0.285) loss 0.4665 (0.5489) acc 93.2692 (88.9589) lr 4.8943e-05 eta 0:01:32
epoch [47/50] batch [10/51] time 0.165 (0.323) data 0.001 (0.143) loss 0.4741 (0.5388) acc 91.4894 (89.7197) lr 4.8943e-05 eta 0:01:02
epoch [47/50] batch [15/51] time 0.171 (0.275) data 0.000 (0.095) loss 0.5530 (0.5465) acc 88.2353 (89.3777) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [20/51] time 0.184 (0.250) data 0.000 (0.072) loss 0.5484 (0.5759) acc 89.6226 (88.0768) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.177 (0.235) data 0.000 (0.057) loss 0.7328 (0.6000) acc 83.3333 (87.2978) lr 4.8943e-05 eta 0:00:41
epoch [47/50] batch [30/51] time 0.174 (0.224) data 0.000 (0.048) loss 0.3957 (0.6038) acc 92.1875 (87.4381) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [35/51] time 0.177 (0.217) data 0.000 (0.041) loss 0.6003 (0.6040) acc 88.2075 (87.2922) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [40/51] time 0.169 (0.212) data 0.000 (0.036) loss 0.4819 (0.5970) acc 87.2549 (87.3464) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [45/51] time 0.159 (0.206) data 0.000 (0.032) loss 0.5316 (0.5966) acc 86.9565 (87.1928) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [50/51] time 0.159 (0.203) data 0.000 (0.029) loss 0.5026 (0.5871) acc 88.0435 (87.4707) lr 4.8943e-05 eta 0:00:31
>>> alpha1: 0.148  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [48/50] batch [5/51] time 0.175 (0.517) data 0.000 (0.335) loss 0.4425 (0.6176) acc 93.5000 (86.1790) lr 3.1417e-05 eta 0:01:16
epoch [48/50] batch [10/51] time 0.192 (0.351) data 0.000 (0.168) loss 0.7401 (0.5947) acc 89.0625 (86.8745) lr 3.1417e-05 eta 0:00:50
epoch [48/50] batch [15/51] time 0.165 (0.292) data 0.000 (0.112) loss 0.6705 (0.5738) acc 85.4651 (87.5775) lr 3.1417e-05 eta 0:00:40
epoch [48/50] batch [20/51] time 0.176 (0.263) data 0.000 (0.084) loss 0.6493 (0.5842) acc 88.0208 (87.5868) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.177 (0.245) data 0.000 (0.067) loss 0.5795 (0.5890) acc 85.8491 (87.1759) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.179 (0.233) data 0.000 (0.056) loss 0.5243 (0.5966) acc 91.5094 (87.3155) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.166 (0.224) data 0.000 (0.048) loss 0.7223 (0.5909) acc 81.7708 (87.2369) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.167 (0.219) data 0.000 (0.042) loss 0.6914 (0.5931) acc 89.2857 (87.2207) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.171 (0.213) data 0.000 (0.037) loss 0.4030 (0.5778) acc 90.6863 (87.6686) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.181 (0.209) data 0.000 (0.034) loss 0.6099 (0.5801) acc 87.2727 (87.6082) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.148  alpha2: 0.052 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [49/50] batch [5/51] time 0.173 (0.476) data 0.001 (0.298) loss 0.5534 (0.5159) acc 87.2549 (88.2616) lr 1.7713e-05 eta 0:00:46
epoch [49/50] batch [10/51] time 0.170 (0.326) data 0.000 (0.149) loss 0.9413 (0.5408) acc 79.5918 (88.2702) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.163 (0.278) data 0.000 (0.100) loss 0.5669 (0.5694) acc 90.4255 (87.7050) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [20/51] time 0.188 (0.254) data 0.000 (0.075) loss 0.8272 (0.5779) acc 81.8627 (87.7940) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [25/51] time 0.184 (0.239) data 0.000 (0.060) loss 0.3261 (0.5691) acc 95.3704 (88.1161) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.188 (0.228) data 0.000 (0.050) loss 0.6191 (0.5831) acc 87.0192 (87.7433) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.183 (0.221) data 0.001 (0.043) loss 0.6341 (0.5894) acc 82.8704 (87.2569) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.160 (0.215) data 0.000 (0.038) loss 0.7295 (0.5954) acc 85.8696 (87.0225) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.169 (0.210) data 0.000 (0.033) loss 0.6167 (0.5906) acc 89.2857 (87.3423) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [50/51] time 0.167 (0.206) data 0.000 (0.030) loss 0.7003 (0.5933) acc 85.7143 (87.3244) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.148  alpha2: 0.052 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [50/50] batch [5/51] time 0.180 (0.473) data 0.000 (0.285) loss 0.3687 (0.4942) acc 91.1765 (90.1648) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.174 (0.323) data 0.000 (0.143) loss 0.4699 (0.5062) acc 88.2353 (89.1836) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.183 (0.275) data 0.000 (0.095) loss 0.6580 (0.5258) acc 89.4231 (88.7318) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [20/51] time 0.176 (0.250) data 0.000 (0.071) loss 0.4366 (0.5633) acc 93.1373 (87.8975) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [25/51] time 0.177 (0.235) data 0.001 (0.057) loss 0.6236 (0.5581) acc 87.2449 (88.3507) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.180 (0.225) data 0.014 (0.048) loss 0.6255 (0.5526) acc 86.6667 (88.5866) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.180 (0.219) data 0.000 (0.041) loss 0.6955 (0.5503) acc 88.7255 (88.6739) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.176 (0.214) data 0.000 (0.036) loss 0.7633 (0.5649) acc 80.1020 (88.1691) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.169 (0.209) data 0.000 (0.032) loss 0.7874 (0.5673) acc 83.6735 (88.2616) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.164 (0.205) data 0.000 (0.029) loss 0.4536 (0.5659) acc 94.7917 (88.3587) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.45, 0.35, 0.3, 0.29, 0.27, 0.26, 0.26, 0.25, 0.26, 0.26, 0.25, 0.25, 0.25, 0.25, 0.26, 0.26, 0.26, 0.25, 0.25, 0.24, 0.25, 0.25, 0.24, 0.25, 0.25, 0.25, 0.24, 0.24, 0.25, 0.24, 0.24, 0.25, 0.25, 0.25, 0.24, 0.24, 0.25, 0.25, 0.25, 0.24]
* matched noise rate: [0.18, 0.1, 0.15, 0.15, 0.16, 0.12, 0.12, 0.12, 0.12, 0.13, 0.13, 0.12, 0.11, 0.11, 0.12, 0.12, 0.11, 0.11, 0.11, 0.11, 0.12, 0.11, 0.12, 0.11, 0.11, 0.12, 0.12, 0.12, 0.12, 0.12, 0.11, 0.12, 0.11, 0.11, 0.11, 0.11, 0.12, 0.12, 0.11, 0.11]
* unmatched noise rate: [0.75, 0.5, 0.53, 0.51, 0.47, 0.39, 0.4, 0.39, 0.42, 0.43, 0.41, 0.44, 0.42, 0.43, 0.43, 0.43, 0.43, 0.42, 0.41, 0.41, 0.42, 0.42, 0.41, 0.41, 0.4, 0.41, 0.4, 0.4, 0.42, 0.4, 0.41, 0.41, 0.41, 0.42, 0.4, 0.41, 0.41, 0.41, 0.41, 0.41]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:01,  2.57s/it] 12%|█▏        | 3/25 [00:02<00:15,  1.38it/s] 20%|██        | 5/25 [00:02<00:07,  2.53it/s] 24%|██▍       | 6/25 [00:02<00:06,  3.16it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.67it/s] 40%|████      | 10/25 [00:03<00:02,  6.11it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.43it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.58it/s] 64%|██████▍   | 16/25 [00:03<00:00,  9.52it/s] 72%|███████▏  | 18/25 [00:04<00:01,  6.92it/s] 80%|████████  | 20/25 [00:04<00:00,  8.01it/s] 88%|████████▊ | 22/25 [00:04<00:00,  8.99it/s] 96%|█████████▌| 24/25 [00:04<00:00,  9.81it/s]100%|██████████| 25/25 [00:05<00:00,  4.72it/s]
=> result
* total: 2,463
* correct: 1,784
* accuracy: 72.4%
* error: 27.6%
* macro_f1: 69.4%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 15	acc: 83.3%
* class: 2 (canterbury bells)	total: 12	correct: 6	acc: 50.0%
* class: 3 (sweet pea)	total: 17	correct: 3	acc: 17.6%
* class: 4 (english marigold)	total: 20	correct: 9	acc: 45.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 11	acc: 78.6%
* class: 9 (globe thistle)	total: 14	correct: 13	acc: 92.9%
* class: 10 (snapdragon)	total: 26	correct: 23	acc: 88.5%
* class: 11 (colt's foot)	total: 26	correct: 6	acc: 23.1%
* class: 12 (king protea)	total: 15	correct: 14	acc: 93.3%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 1	acc: 7.7%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 20	acc: 80.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 0	acc: 0.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 4	acc: 33.3%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 20	acc: 76.9%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 0	acc: 0.0%
* class: 33 (mexican aster)	total: 12	correct: 11	acc: 91.7%
* class: 34 (alpine sea holly)	total: 12	correct: 10	acc: 83.3%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 11	acc: 50.0%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 16	acc: 94.1%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 11	acc: 55.0%
* class: 40 (barbeton daisy)	total: 38	correct: 16	acc: 42.1%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 31	acc: 52.5%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 0	acc: 0.0%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 17	acc: 60.7%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 16	acc: 76.2%
* class: 55 (bishop of llandaff)	total: 33	correct: 32	acc: 97.0%
* class: 56 (gaura)	total: 20	correct: 18	acc: 90.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 19	acc: 95.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 14	acc: 93.3%
* class: 61 (japanese anemone)	total: 16	correct: 12	acc: 75.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 17	acc: 89.5%
* class: 66 (spring crocus)	total: 13	correct: 10	acc: 76.9%
* class: 67 (bearded iris)	total: 16	correct: 11	acc: 68.8%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 21	acc: 91.3%
* class: 71 (azalea)	total: 29	correct: 24	acc: 82.8%
* class: 72 (water lily)	total: 58	correct: 50	acc: 86.2%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 0	acc: 0.0%
* class: 76 (passion flower)	total: 75	correct: 72	acc: 96.0%
* class: 77 (lotus)	total: 42	correct: 40	acc: 95.2%
* class: 78 (toad lily)	total: 13	correct: 11	acc: 84.6%
* class: 79 (anthurium)	total: 32	correct: 32	acc: 100.0%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 30	acc: 88.2%
* class: 82 (hibiscus)	total: 39	correct: 35	acc: 89.7%
* class: 83 (columbine)	total: 26	correct: 22	acc: 84.6%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 15	acc: 88.2%
* class: 86 (magnolia)	total: 18	correct: 15	acc: 83.3%
* class: 87 (cyclamen)	total: 46	correct: 36	acc: 78.3%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 9	acc: 36.0%
* class: 90 (hippeastrum)	total: 23	correct: 21	acc: 91.3%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 1	acc: 7.1%
* class: 93 (foxglove)	total: 49	correct: 48	acc: 98.0%
* class: 94 (bougainvillea)	total: 38	correct: 23	acc: 60.5%
* class: 95 (camellia)	total: 27	correct: 24	acc: 88.9%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 24	acc: 96.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 12	acc: 70.6%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 74.5%
Elapsed: 0:28:33
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_10-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.272 (1.098) data 0.001 (0.416) loss 4.6425 (4.8461) acc 6.2500 (3.1250) lr 1.0000e-05 eta 0:46:34
epoch [1/50] batch [10/51] time 0.274 (0.685) data 0.000 (0.208) loss 4.5920 (4.8163) acc 6.2500 (5.0000) lr 1.0000e-05 eta 0:28:59
epoch [1/50] batch [15/51] time 0.267 (0.547) data 0.000 (0.139) loss 4.6103 (4.7912) acc 6.2500 (4.5833) lr 1.0000e-05 eta 0:23:05
epoch [1/50] batch [20/51] time 0.268 (0.477) data 0.000 (0.104) loss 4.5341 (4.7564) acc 15.6250 (5.1562) lr 1.0000e-05 eta 0:20:06
epoch [1/50] batch [25/51] time 0.264 (0.434) data 0.000 (0.083) loss 4.7430 (4.6985) acc 3.1250 (6.7500) lr 1.0000e-05 eta 0:18:15
epoch [1/50] batch [30/51] time 0.264 (0.406) data 0.000 (0.069) loss 4.6524 (4.6756) acc 12.5000 (6.8750) lr 1.0000e-05 eta 0:17:03
epoch [1/50] batch [35/51] time 0.268 (0.386) data 0.000 (0.060) loss 4.4972 (4.6787) acc 9.3750 (7.1429) lr 1.0000e-05 eta 0:16:10
epoch [1/50] batch [40/51] time 0.260 (0.371) data 0.000 (0.052) loss 4.5474 (4.6619) acc 6.2500 (6.9531) lr 1.0000e-05 eta 0:15:30
epoch [1/50] batch [45/51] time 0.264 (0.359) data 0.000 (0.046) loss 4.6404 (4.6471) acc 3.1250 (6.8056) lr 1.0000e-05 eta 0:14:58
epoch [1/50] batch [50/51] time 0.263 (0.349) data 0.000 (0.042) loss 4.7784 (4.6495) acc 3.1250 (6.8750) lr 1.0000e-05 eta 0:14:32
epoch [2/50] batch [5/51] time 0.289 (0.579) data 0.000 (0.289) loss 4.5867 (4.3889) acc 12.5000 (13.7500) lr 2.0000e-03 eta 0:24:03
epoch [2/50] batch [10/51] time 0.269 (0.424) data 0.000 (0.145) loss 4.5692 (4.3894) acc 12.5000 (12.8125) lr 2.0000e-03 eta 0:17:35
epoch [2/50] batch [15/51] time 0.277 (0.373) data 0.000 (0.096) loss 4.3338 (4.3914) acc 12.5000 (12.5000) lr 2.0000e-03 eta 0:15:26
epoch [2/50] batch [20/51] time 0.268 (0.347) data 0.000 (0.072) loss 3.8631 (4.3857) acc 31.2500 (13.1250) lr 2.0000e-03 eta 0:14:19
epoch [2/50] batch [25/51] time 0.270 (0.331) data 0.000 (0.058) loss 3.8353 (4.3714) acc 34.3750 (14.5000) lr 2.0000e-03 eta 0:13:37
epoch [2/50] batch [30/51] time 0.264 (0.321) data 0.000 (0.048) loss 4.3822 (4.3567) acc 12.5000 (14.3750) lr 2.0000e-03 eta 0:13:11
epoch [2/50] batch [35/51] time 0.271 (0.314) data 0.000 (0.042) loss 4.1144 (4.3294) acc 18.7500 (15.7143) lr 2.0000e-03 eta 0:12:53
epoch [2/50] batch [40/51] time 0.262 (0.308) data 0.000 (0.036) loss 4.1476 (4.3226) acc 9.3750 (15.8594) lr 2.0000e-03 eta 0:12:37
epoch [2/50] batch [45/51] time 0.261 (0.303) data 0.000 (0.032) loss 4.4455 (4.3203) acc 21.8750 (16.2500) lr 2.0000e-03 eta 0:12:23
epoch [2/50] batch [50/51] time 0.262 (0.299) data 0.000 (0.029) loss 4.5952 (4.3285) acc 15.6250 (16.5000) lr 2.0000e-03 eta 0:12:11
epoch [3/50] batch [5/51] time 0.276 (0.582) data 0.000 (0.287) loss 3.9337 (4.2271) acc 28.1250 (19.3750) lr 1.9980e-03 eta 0:23:41
epoch [3/50] batch [10/51] time 0.274 (0.429) data 0.000 (0.144) loss 4.3634 (4.2515) acc 21.8750 (19.0625) lr 1.9980e-03 eta 0:17:25
epoch [3/50] batch [15/51] time 0.263 (0.376) data 0.000 (0.096) loss 4.1263 (4.2356) acc 21.8750 (20.2083) lr 1.9980e-03 eta 0:15:15
epoch [3/50] batch [20/51] time 0.272 (0.350) data 0.000 (0.072) loss 3.8487 (4.2429) acc 28.1250 (19.3750) lr 1.9980e-03 eta 0:14:08
epoch [3/50] batch [25/51] time 0.267 (0.333) data 0.000 (0.058) loss 4.7791 (4.2737) acc 12.5000 (19.2500) lr 1.9980e-03 eta 0:13:27
epoch [3/50] batch [30/51] time 0.264 (0.323) data 0.000 (0.048) loss 4.0803 (4.2706) acc 25.0000 (19.0625) lr 1.9980e-03 eta 0:13:00
epoch [3/50] batch [35/51] time 0.262 (0.315) data 0.000 (0.041) loss 4.1581 (4.2740) acc 15.6250 (19.1964) lr 1.9980e-03 eta 0:12:40
epoch [3/50] batch [40/51] time 0.261 (0.308) data 0.000 (0.036) loss 4.3441 (4.2637) acc 9.3750 (19.0625) lr 1.9980e-03 eta 0:12:22
epoch [3/50] batch [45/51] time 0.260 (0.303) data 0.000 (0.032) loss 4.0256 (4.2576) acc 18.7500 (19.1667) lr 1.9980e-03 eta 0:12:08
epoch [3/50] batch [50/51] time 0.261 (0.299) data 0.000 (0.029) loss 3.7626 (4.2457) acc 28.1250 (19.5000) lr 1.9980e-03 eta 0:11:56
epoch [4/50] batch [5/51] time 0.263 (0.629) data 0.000 (0.346) loss 4.3960 (4.1911) acc 15.6250 (22.5000) lr 1.9921e-03 eta 0:25:04
epoch [4/50] batch [10/51] time 0.263 (0.447) data 0.000 (0.173) loss 4.2518 (4.1910) acc 18.7500 (22.8125) lr 1.9921e-03 eta 0:17:46
epoch [4/50] batch [15/51] time 0.262 (0.387) data 0.000 (0.116) loss 4.3424 (4.2514) acc 15.6250 (19.5833) lr 1.9921e-03 eta 0:15:22
epoch [4/50] batch [20/51] time 0.276 (0.358) data 0.000 (0.087) loss 4.1092 (4.2574) acc 28.1250 (19.6875) lr 1.9921e-03 eta 0:14:11
epoch [4/50] batch [25/51] time 0.264 (0.340) data 0.000 (0.069) loss 3.9227 (4.2184) acc 18.7500 (20.8750) lr 1.9921e-03 eta 0:13:27
epoch [4/50] batch [30/51] time 0.278 (0.329) data 0.003 (0.058) loss 4.6130 (4.2413) acc 15.6250 (20.8333) lr 1.9921e-03 eta 0:12:58
epoch [4/50] batch [35/51] time 0.270 (0.320) data 0.000 (0.050) loss 4.4578 (4.2412) acc 12.5000 (20.8036) lr 1.9921e-03 eta 0:12:36
epoch [4/50] batch [40/51] time 0.263 (0.314) data 0.000 (0.044) loss 4.2467 (4.2487) acc 18.7500 (20.4688) lr 1.9921e-03 eta 0:12:18
epoch [4/50] batch [45/51] time 0.263 (0.308) data 0.000 (0.039) loss 3.8818 (4.2377) acc 28.1250 (20.2778) lr 1.9921e-03 eta 0:12:04
epoch [4/50] batch [50/51] time 0.262 (0.303) data 0.000 (0.035) loss 3.7750 (4.2142) acc 31.2500 (20.6250) lr 1.9921e-03 eta 0:11:51
epoch [5/50] batch [5/51] time 0.281 (0.585) data 0.000 (0.311) loss 4.1537 (4.1917) acc 25.0000 (21.2500) lr 1.9823e-03 eta 0:22:49
epoch [5/50] batch [10/51] time 0.262 (0.428) data 0.000 (0.156) loss 4.1913 (4.2463) acc 18.7500 (19.3750) lr 1.9823e-03 eta 0:16:39
epoch [5/50] batch [15/51] time 0.274 (0.376) data 0.000 (0.104) loss 3.9318 (4.1829) acc 21.8750 (20.4167) lr 1.9823e-03 eta 0:14:36
epoch [5/50] batch [20/51] time 0.267 (0.350) data 0.000 (0.078) loss 4.1157 (4.2028) acc 25.0000 (20.0000) lr 1.9823e-03 eta 0:13:35
epoch [5/50] batch [25/51] time 0.274 (0.335) data 0.000 (0.062) loss 4.1753 (4.1905) acc 21.8750 (20.2500) lr 1.9823e-03 eta 0:12:57
epoch [5/50] batch [30/51] time 0.265 (0.324) data 0.000 (0.052) loss 4.5077 (4.2089) acc 15.6250 (19.6875) lr 1.9823e-03 eta 0:12:29
epoch [5/50] batch [35/51] time 0.275 (0.316) data 0.000 (0.045) loss 4.4629 (4.2109) acc 25.0000 (19.9107) lr 1.9823e-03 eta 0:12:09
epoch [5/50] batch [40/51] time 0.263 (0.309) data 0.000 (0.039) loss 4.1072 (4.2037) acc 18.7500 (20.0781) lr 1.9823e-03 eta 0:11:53
epoch [5/50] batch [45/51] time 0.265 (0.304) data 0.000 (0.035) loss 4.3035 (4.1964) acc 18.7500 (20.5556) lr 1.9823e-03 eta 0:11:39
epoch [5/50] batch [50/51] time 0.264 (0.300) data 0.000 (0.031) loss 4.3670 (4.2032) acc 21.8750 (20.5000) lr 1.9823e-03 eta 0:11:28
epoch [6/50] batch [5/51] time 0.296 (0.583) data 0.000 (0.282) loss 4.4066 (4.2038) acc 18.7500 (19.3750) lr 1.9686e-03 eta 0:22:14
epoch [6/50] batch [10/51] time 0.279 (0.427) data 0.000 (0.141) loss 3.9404 (4.0446) acc 28.1250 (25.0000) lr 1.9686e-03 eta 0:16:16
epoch [6/50] batch [15/51] time 0.274 (0.375) data 0.000 (0.094) loss 3.7997 (4.0790) acc 31.2500 (24.7917) lr 1.9686e-03 eta 0:14:15
epoch [6/50] batch [20/51] time 0.268 (0.349) data 0.000 (0.071) loss 4.5508 (4.1055) acc 18.7500 (24.3750) lr 1.9686e-03 eta 0:13:12
epoch [6/50] batch [25/51] time 0.265 (0.333) data 0.000 (0.057) loss 4.2362 (4.1184) acc 18.7500 (23.0000) lr 1.9686e-03 eta 0:12:36
epoch [6/50] batch [30/51] time 0.262 (0.322) data 0.000 (0.048) loss 4.0281 (4.1435) acc 21.8750 (22.8125) lr 1.9686e-03 eta 0:12:09
epoch [6/50] batch [35/51] time 0.268 (0.315) data 0.000 (0.041) loss 4.0428 (4.1570) acc 28.1250 (23.1250) lr 1.9686e-03 eta 0:11:51
epoch [6/50] batch [40/51] time 0.263 (0.308) data 0.000 (0.036) loss 4.2607 (4.1600) acc 12.5000 (22.4219) lr 1.9686e-03 eta 0:11:34
epoch [6/50] batch [45/51] time 0.266 (0.303) data 0.000 (0.032) loss 3.9538 (4.1628) acc 21.8750 (22.2222) lr 1.9686e-03 eta 0:11:22
epoch [6/50] batch [50/51] time 0.263 (0.299) data 0.000 (0.029) loss 3.9071 (4.1652) acc 34.3750 (22.4375) lr 1.9686e-03 eta 0:11:11
epoch [7/50] batch [5/51] time 0.293 (0.539) data 0.000 (0.238) loss 3.8412 (4.1021) acc 25.0000 (23.1250) lr 1.9511e-03 eta 0:20:06
epoch [7/50] batch [10/51] time 0.286 (0.405) data 0.000 (0.119) loss 4.1978 (4.1518) acc 18.7500 (21.8750) lr 1.9511e-03 eta 0:15:05
epoch [7/50] batch [15/51] time 0.264 (0.359) data 0.000 (0.079) loss 4.2021 (4.1206) acc 15.6250 (22.7083) lr 1.9511e-03 eta 0:13:20
epoch [7/50] batch [20/51] time 0.263 (0.337) data 0.000 (0.060) loss 4.0821 (4.1139) acc 15.6250 (22.1875) lr 1.9511e-03 eta 0:12:29
epoch [7/50] batch [25/51] time 0.272 (0.324) data 0.000 (0.048) loss 4.5857 (4.1446) acc 9.3750 (21.1250) lr 1.9511e-03 eta 0:11:58
epoch [7/50] batch [30/51] time 0.271 (0.314) data 0.000 (0.040) loss 4.4862 (4.1555) acc 9.3750 (20.7292) lr 1.9511e-03 eta 0:11:36
epoch [7/50] batch [35/51] time 0.276 (0.308) data 0.000 (0.034) loss 4.3625 (4.1682) acc 21.8750 (20.4464) lr 1.9511e-03 eta 0:11:20
epoch [7/50] batch [40/51] time 0.263 (0.302) data 0.000 (0.030) loss 4.1746 (4.1596) acc 18.7500 (21.2500) lr 1.9511e-03 eta 0:11:05
epoch [7/50] batch [45/51] time 0.262 (0.298) data 0.000 (0.027) loss 4.1704 (4.1412) acc 25.0000 (21.6667) lr 1.9511e-03 eta 0:10:54
epoch [7/50] batch [50/51] time 0.261 (0.294) data 0.000 (0.024) loss 3.8602 (4.1404) acc 28.1250 (21.7500) lr 1.9511e-03 eta 0:10:45
epoch [8/50] batch [5/51] time 0.285 (0.584) data 0.000 (0.285) loss 4.2686 (3.9615) acc 21.8750 (26.2500) lr 1.9298e-03 eta 0:21:18
epoch [8/50] batch [10/51] time 0.270 (0.429) data 0.000 (0.143) loss 4.1650 (4.0540) acc 18.7500 (25.0000) lr 1.9298e-03 eta 0:15:36
epoch [8/50] batch [15/51] time 0.288 (0.376) data 0.000 (0.095) loss 4.1898 (4.1060) acc 25.0000 (23.1250) lr 1.9298e-03 eta 0:13:38
epoch [8/50] batch [20/51] time 0.278 (0.349) data 0.000 (0.071) loss 4.1983 (4.0666) acc 18.7500 (23.9062) lr 1.9298e-03 eta 0:12:37
epoch [8/50] batch [25/51] time 0.280 (0.333) data 0.000 (0.057) loss 4.5381 (4.0959) acc 15.6250 (23.3750) lr 1.9298e-03 eta 0:12:02
epoch [8/50] batch [30/51] time 0.263 (0.323) data 0.000 (0.048) loss 4.1932 (4.0932) acc 18.7500 (23.1250) lr 1.9298e-03 eta 0:11:38
epoch [8/50] batch [35/51] time 0.261 (0.315) data 0.000 (0.041) loss 4.5968 (4.0857) acc 6.2500 (23.5714) lr 1.9298e-03 eta 0:11:20
epoch [8/50] batch [40/51] time 0.262 (0.309) data 0.000 (0.036) loss 4.1850 (4.1031) acc 25.0000 (23.4375) lr 1.9298e-03 eta 0:11:05
epoch [8/50] batch [45/51] time 0.264 (0.304) data 0.000 (0.032) loss 4.4210 (4.1213) acc 9.3750 (22.6389) lr 1.9298e-03 eta 0:10:52
epoch [8/50] batch [50/51] time 0.262 (0.300) data 0.000 (0.029) loss 4.2891 (4.1158) acc 21.8750 (22.9375) lr 1.9298e-03 eta 0:10:41
epoch [9/50] batch [5/51] time 0.266 (0.567) data 0.000 (0.283) loss 4.4854 (4.0688) acc 18.7500 (26.2500) lr 1.9048e-03 eta 0:20:11
epoch [9/50] batch [10/51] time 0.268 (0.418) data 0.000 (0.142) loss 3.9920 (4.0271) acc 21.8750 (22.8125) lr 1.9048e-03 eta 0:14:50
epoch [9/50] batch [15/51] time 0.262 (0.368) data 0.000 (0.094) loss 3.6391 (4.0318) acc 40.6250 (24.3750) lr 1.9048e-03 eta 0:13:01
epoch [9/50] batch [20/51] time 0.263 (0.342) data 0.000 (0.071) loss 4.0748 (4.0989) acc 15.6250 (21.8750) lr 1.9048e-03 eta 0:12:05
epoch [9/50] batch [25/51] time 0.274 (0.327) data 0.000 (0.057) loss 3.8816 (4.0788) acc 31.2500 (23.2500) lr 1.9048e-03 eta 0:11:32
epoch [9/50] batch [30/51] time 0.267 (0.317) data 0.000 (0.047) loss 3.6742 (4.0954) acc 31.2500 (23.0208) lr 1.9048e-03 eta 0:11:10
epoch [9/50] batch [35/51] time 0.272 (0.311) data 0.000 (0.041) loss 3.7136 (4.0887) acc 31.2500 (23.4821) lr 1.9048e-03 eta 0:10:55
epoch [9/50] batch [40/51] time 0.263 (0.306) data 0.000 (0.036) loss 4.0289 (4.0817) acc 28.1250 (23.6719) lr 1.9048e-03 eta 0:10:42
epoch [9/50] batch [45/51] time 0.262 (0.301) data 0.000 (0.032) loss 3.7660 (4.0865) acc 37.5000 (23.6111) lr 1.9048e-03 eta 0:10:31
epoch [9/50] batch [50/51] time 0.261 (0.297) data 0.000 (0.029) loss 4.1716 (4.0903) acc 25.0000 (23.7500) lr 1.9048e-03 eta 0:10:21
epoch [10/50] batch [5/51] time 0.289 (0.579) data 0.000 (0.290) loss 4.2658 (3.9863) acc 18.7500 (25.6250) lr 1.8763e-03 eta 0:20:06
epoch [10/50] batch [10/51] time 0.263 (0.425) data 0.000 (0.145) loss 3.8527 (3.9920) acc 25.0000 (25.6250) lr 1.8763e-03 eta 0:14:44
epoch [10/50] batch [15/51] time 0.269 (0.374) data 0.000 (0.097) loss 3.5368 (4.0091) acc 34.3750 (25.4167) lr 1.8763e-03 eta 0:12:55
epoch [10/50] batch [20/51] time 0.265 (0.347) data 0.000 (0.073) loss 3.7373 (4.0138) acc 37.5000 (25.9375) lr 1.8763e-03 eta 0:11:58
epoch [10/50] batch [25/51] time 0.262 (0.331) data 0.000 (0.058) loss 3.9620 (4.0335) acc 25.0000 (25.1250) lr 1.8763e-03 eta 0:11:23
epoch [10/50] batch [30/51] time 0.276 (0.320) data 0.000 (0.049) loss 4.4673 (4.0426) acc 15.6250 (24.4792) lr 1.8763e-03 eta 0:11:00
epoch [10/50] batch [35/51] time 0.283 (0.314) data 0.000 (0.042) loss 4.0417 (4.0549) acc 28.1250 (24.1071) lr 1.8763e-03 eta 0:10:44
epoch [10/50] batch [40/51] time 0.262 (0.308) data 0.000 (0.036) loss 3.5785 (4.0612) acc 28.1250 (23.5156) lr 1.8763e-03 eta 0:10:30
epoch [10/50] batch [45/51] time 0.263 (0.303) data 0.000 (0.032) loss 3.8754 (4.0612) acc 31.2500 (23.6806) lr 1.8763e-03 eta 0:10:19
epoch [10/50] batch [50/51] time 0.262 (0.299) data 0.000 (0.029) loss 3.8208 (4.0655) acc 31.2500 (23.3750) lr 1.8763e-03 eta 0:10:09
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.739  alpha2: 0.349 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.67 <<<
epoch [11/50] batch [5/51] time 0.187 (1.090) data 0.000 (0.315) loss 2.5756 (2.7356) acc 60.3774 (60.9514) lr 1.8443e-03 eta 0:36:58
epoch [11/50] batch [10/51] time 0.891 (0.849) data 0.000 (0.158) loss 2.7963 (2.6220) acc 61.1111 (59.3230) lr 1.8443e-03 eta 0:28:43
epoch [11/50] batch [15/51] time 0.178 (0.667) data 0.000 (0.105) loss 2.4369 (2.6334) acc 59.7222 (58.2418) lr 1.8443e-03 eta 0:22:30
epoch [11/50] batch [20/51] time 0.165 (0.589) data 0.000 (0.079) loss 2.9370 (2.6529) acc 42.7083 (57.1929) lr 1.8443e-03 eta 0:19:49
epoch [11/50] batch [25/51] time 0.162 (0.529) data 0.000 (0.063) loss 2.8620 (2.6308) acc 44.6809 (56.8159) lr 1.8443e-03 eta 0:17:46
epoch [11/50] batch [30/51] time 0.180 (0.497) data 0.000 (0.053) loss 2.2170 (2.6055) acc 69.2308 (58.1977) lr 1.8443e-03 eta 0:16:38
epoch [11/50] batch [35/51] time 0.222 (0.452) data 0.000 (0.045) loss 2.4275 (2.6193) acc 66.5179 (57.7919) lr 1.8443e-03 eta 0:15:06
epoch [11/50] batch [40/51] time 0.174 (0.417) data 0.000 (0.040) loss 2.5930 (2.6385) acc 61.5385 (57.9048) lr 1.8443e-03 eta 0:13:54
epoch [11/50] batch [45/51] time 0.177 (0.390) data 0.000 (0.035) loss 2.5736 (2.6064) acc 52.8302 (58.3876) lr 1.8443e-03 eta 0:12:58
epoch [11/50] batch [50/51] time 0.172 (0.381) data 0.000 (0.032) loss 2.5477 (2.6087) acc 52.4510 (58.3906) lr 1.8443e-03 eta 0:12:37
>>> alpha1: 0.647  alpha2: 0.303 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.41 <<<
epoch [12/50] batch [5/51] time 0.163 (0.738) data 0.000 (0.337) loss 1.5331 (1.4970) acc 68.0851 (72.9795) lr 1.8090e-03 eta 0:24:24
epoch [12/50] batch [10/51] time 0.164 (0.450) data 0.000 (0.169) loss 1.1982 (1.4660) acc 77.6596 (72.5258) lr 1.8090e-03 eta 0:14:51
epoch [12/50] batch [15/51] time 0.156 (0.429) data 0.000 (0.113) loss 1.7315 (1.4882) acc 68.1818 (72.2684) lr 1.8090e-03 eta 0:14:06
epoch [12/50] batch [20/51] time 0.165 (0.420) data 0.000 (0.085) loss 1.7624 (1.5147) acc 69.7674 (72.0274) lr 1.8090e-03 eta 0:13:46
epoch [12/50] batch [25/51] time 0.183 (0.368) data 0.000 (0.068) loss 0.9880 (1.4924) acc 81.5000 (72.2197) lr 1.8090e-03 eta 0:12:03
epoch [12/50] batch [30/51] time 0.155 (0.334) data 0.000 (0.056) loss 1.8844 (1.5144) acc 62.2093 (71.1693) lr 1.8090e-03 eta 0:10:54
epoch [12/50] batch [35/51] time 0.156 (0.310) data 0.000 (0.048) loss 1.3050 (1.5461) acc 78.9773 (70.3784) lr 1.8090e-03 eta 0:10:06
epoch [12/50] batch [40/51] time 0.158 (0.293) data 0.000 (0.042) loss 1.4824 (1.5401) acc 69.7674 (70.2556) lr 1.8090e-03 eta 0:09:30
epoch [12/50] batch [45/51] time 0.159 (0.278) data 0.000 (0.038) loss 1.5534 (1.5358) acc 67.3913 (70.4025) lr 1.8090e-03 eta 0:08:59
epoch [12/50] batch [50/51] time 0.159 (0.277) data 0.000 (0.034) loss 1.4164 (1.5167) acc 75.0000 (70.9395) lr 1.8090e-03 eta 0:08:56
>>> alpha1: 0.536  alpha2: 0.237 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.27 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.49 <<<
epoch [13/50] batch [5/51] time 0.215 (0.544) data 0.000 (0.351) loss 0.9575 (1.1209) acc 78.9474 (75.7182) lr 1.7705e-03 eta 0:17:31
epoch [13/50] batch [10/51] time 0.181 (0.367) data 0.000 (0.176) loss 1.1207 (1.1625) acc 72.5490 (74.1437) lr 1.7705e-03 eta 0:11:48
epoch [13/50] batch [15/51] time 0.193 (0.306) data 0.000 (0.117) loss 0.9557 (1.1549) acc 77.6786 (74.0657) lr 1.7705e-03 eta 0:09:49
epoch [13/50] batch [20/51] time 0.182 (0.276) data 0.000 (0.089) loss 1.4158 (1.1751) acc 65.9091 (73.6282) lr 1.7705e-03 eta 0:08:49
epoch [13/50] batch [25/51] time 0.175 (0.256) data 0.000 (0.071) loss 1.2894 (1.1674) acc 71.4286 (74.0737) lr 1.7705e-03 eta 0:08:10
epoch [13/50] batch [30/51] time 0.180 (0.269) data 0.000 (0.059) loss 1.3599 (1.1717) acc 70.3704 (74.4009) lr 1.7705e-03 eta 0:08:32
epoch [13/50] batch [35/51] time 0.180 (0.256) data 0.000 (0.051) loss 0.9973 (1.1592) acc 78.7736 (75.1139) lr 1.7705e-03 eta 0:08:07
epoch [13/50] batch [40/51] time 0.170 (0.246) data 0.000 (0.045) loss 1.5271 (1.1662) acc 60.2941 (74.5348) lr 1.7705e-03 eta 0:07:47
epoch [13/50] batch [45/51] time 0.181 (0.238) data 0.000 (0.040) loss 1.1537 (1.1611) acc 75.0000 (74.7175) lr 1.7705e-03 eta 0:07:30
epoch [13/50] batch [50/51] time 0.180 (0.232) data 0.000 (0.036) loss 1.0074 (1.1650) acc 79.6296 (74.6856) lr 1.7705e-03 eta 0:07:18
>>> alpha1: 0.444  alpha2: 0.181 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.44 <<<
epoch [14/50] batch [5/51] time 0.183 (0.474) data 0.000 (0.294) loss 1.2208 (1.0046) acc 70.7547 (76.7421) lr 1.7290e-03 eta 0:14:51
epoch [14/50] batch [10/51] time 0.176 (0.328) data 0.000 (0.147) loss 1.1358 (1.0465) acc 68.3673 (76.4559) lr 1.7290e-03 eta 0:10:15
epoch [14/50] batch [15/51] time 0.172 (0.279) data 0.000 (0.098) loss 1.0127 (1.0475) acc 77.4510 (76.9305) lr 1.7290e-03 eta 0:08:42
epoch [14/50] batch [20/51] time 0.168 (0.252) data 0.000 (0.074) loss 1.0756 (1.0345) acc 74.4898 (76.4913) lr 1.7290e-03 eta 0:07:50
epoch [14/50] batch [25/51] time 0.170 (0.237) data 0.000 (0.059) loss 1.2490 (1.0242) acc 68.0000 (76.6731) lr 1.7290e-03 eta 0:07:21
epoch [14/50] batch [30/51] time 0.168 (0.228) data 0.000 (0.049) loss 1.0912 (1.0175) acc 80.7292 (77.1177) lr 1.7290e-03 eta 0:07:03
epoch [14/50] batch [35/51] time 0.189 (0.221) data 0.000 (0.042) loss 1.0457 (1.0121) acc 77.2727 (77.3423) lr 1.7290e-03 eta 0:06:48
epoch [14/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.037) loss 0.8641 (0.9916) acc 75.4902 (77.5236) lr 1.7290e-03 eta 0:06:37
epoch [14/50] batch [45/51] time 0.163 (0.210) data 0.000 (0.033) loss 0.7931 (0.9762) acc 87.5000 (77.9595) lr 1.7290e-03 eta 0:06:26
epoch [14/50] batch [50/51] time 0.170 (0.206) data 0.000 (0.030) loss 1.1964 (0.9812) acc 74.5098 (77.7615) lr 1.7290e-03 eta 0:06:19
>>> alpha1: 0.388  alpha2: 0.150 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.44 <<<
epoch [15/50] batch [5/51] time 0.178 (0.521) data 0.000 (0.333) loss 0.7471 (0.8768) acc 86.3208 (80.7287) lr 1.6845e-03 eta 0:15:54
epoch [15/50] batch [10/51] time 0.179 (0.351) data 0.000 (0.167) loss 1.0234 (0.9712) acc 71.2963 (78.0864) lr 1.6845e-03 eta 0:10:40
epoch [15/50] batch [15/51] time 0.180 (0.293) data 0.000 (0.111) loss 0.8072 (0.9332) acc 79.0000 (79.4660) lr 1.6845e-03 eta 0:08:53
epoch [15/50] batch [20/51] time 0.193 (0.264) data 0.000 (0.083) loss 1.0002 (0.9145) acc 77.9412 (79.8428) lr 1.6845e-03 eta 0:07:59
epoch [15/50] batch [25/51] time 0.178 (0.247) data 0.000 (0.067) loss 0.9443 (0.9017) acc 77.5000 (79.7835) lr 1.6845e-03 eta 0:07:26
epoch [15/50] batch [30/51] time 0.172 (0.236) data 0.000 (0.056) loss 0.8329 (0.8855) acc 79.5000 (80.0911) lr 1.6845e-03 eta 0:07:06
epoch [15/50] batch [35/51] time 0.175 (0.228) data 0.001 (0.048) loss 0.8458 (0.8914) acc 82.0000 (79.9179) lr 1.6845e-03 eta 0:06:50
epoch [15/50] batch [40/51] time 0.173 (0.222) data 0.000 (0.042) loss 0.9682 (0.8959) acc 72.1154 (79.6166) lr 1.6845e-03 eta 0:06:38
epoch [15/50] batch [45/51] time 0.179 (0.217) data 0.000 (0.037) loss 0.7528 (0.8951) acc 82.8704 (79.6058) lr 1.6845e-03 eta 0:06:28
epoch [15/50] batch [50/51] time 0.178 (0.213) data 0.000 (0.034) loss 0.8380 (0.8955) acc 76.8519 (79.4712) lr 1.6845e-03 eta 0:06:20
>>> alpha1: 0.290  alpha2: 0.076 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.38 <<<
epoch [16/50] batch [5/51] time 0.171 (0.534) data 0.000 (0.350) loss 0.7373 (0.7017) acc 81.6327 (85.5784) lr 1.6374e-03 eta 0:15:50
epoch [16/50] batch [10/51] time 0.170 (0.355) data 0.001 (0.175) loss 0.7912 (0.7837) acc 84.0000 (83.2709) lr 1.6374e-03 eta 0:10:29
epoch [16/50] batch [15/51] time 0.171 (0.295) data 0.000 (0.117) loss 0.6854 (0.7771) acc 85.7843 (83.1349) lr 1.6374e-03 eta 0:08:42
epoch [16/50] batch [20/51] time 0.163 (0.265) data 0.000 (0.088) loss 0.8293 (0.7699) acc 79.8913 (82.7720) lr 1.6374e-03 eta 0:07:47
epoch [16/50] batch [25/51] time 0.171 (0.246) data 0.000 (0.070) loss 1.0312 (0.7993) acc 75.5000 (81.6152) lr 1.6374e-03 eta 0:07:13
epoch [16/50] batch [30/51] time 0.155 (0.234) data 0.000 (0.059) loss 1.0109 (0.7961) acc 73.8636 (81.6827) lr 1.6374e-03 eta 0:06:51
epoch [16/50] batch [35/51] time 0.164 (0.225) data 0.000 (0.050) loss 0.8156 (0.8199) acc 85.1064 (81.0159) lr 1.6374e-03 eta 0:06:34
epoch [16/50] batch [40/51] time 0.155 (0.218) data 0.000 (0.044) loss 1.1313 (0.8246) acc 73.2955 (81.1734) lr 1.6374e-03 eta 0:06:19
epoch [16/50] batch [45/51] time 0.163 (0.212) data 0.000 (0.039) loss 0.8602 (0.8269) acc 82.4468 (81.0688) lr 1.6374e-03 eta 0:06:08
epoch [16/50] batch [50/51] time 0.164 (0.207) data 0.000 (0.036) loss 0.5193 (0.8205) acc 85.4167 (81.1571) lr 1.6374e-03 eta 0:05:59
>>> alpha1: 0.224  alpha2: 0.031 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.38 <<<
epoch [17/50] batch [5/51] time 0.161 (0.551) data 0.001 (0.377) loss 0.8976 (0.8680) acc 83.6956 (80.6772) lr 1.5878e-03 eta 0:15:52
epoch [17/50] batch [10/51] time 0.164 (0.363) data 0.000 (0.189) loss 0.7835 (0.8467) acc 81.2500 (80.2679) lr 1.5878e-03 eta 0:10:25
epoch [17/50] batch [15/51] time 0.164 (0.298) data 0.000 (0.126) loss 0.8371 (0.8139) acc 81.2500 (80.9057) lr 1.5878e-03 eta 0:08:33
epoch [17/50] batch [20/51] time 0.159 (0.266) data 0.000 (0.095) loss 0.6230 (0.7894) acc 87.5000 (81.8583) lr 1.5878e-03 eta 0:07:36
epoch [17/50] batch [25/51] time 0.175 (0.249) data 0.000 (0.076) loss 0.8001 (0.7808) acc 77.6042 (81.7156) lr 1.5878e-03 eta 0:07:04
epoch [17/50] batch [30/51] time 0.161 (0.236) data 0.000 (0.064) loss 0.7639 (0.7926) acc 83.8889 (81.2539) lr 1.5878e-03 eta 0:06:41
epoch [17/50] batch [35/51] time 0.170 (0.227) data 0.000 (0.055) loss 0.8180 (0.7947) acc 80.3922 (81.3584) lr 1.5878e-03 eta 0:06:25
epoch [17/50] batch [40/51] time 0.156 (0.220) data 0.000 (0.048) loss 0.8614 (0.8011) acc 83.8889 (80.9906) lr 1.5878e-03 eta 0:06:12
epoch [17/50] batch [45/51] time 0.165 (0.213) data 0.000 (0.042) loss 1.0286 (0.8030) acc 76.0204 (81.1266) lr 1.5878e-03 eta 0:06:00
epoch [17/50] batch [50/51] time 0.165 (0.209) data 0.000 (0.038) loss 0.9041 (0.7983) acc 77.5510 (81.1982) lr 1.5878e-03 eta 0:05:51
>>> alpha1: 0.207  alpha2: 0.030 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.38 <<<
epoch [18/50] batch [5/51] time 0.187 (0.428) data 0.000 (0.244) loss 0.6251 (0.8542) acc 81.8627 (80.2118) lr 1.5358e-03 eta 0:11:58
epoch [18/50] batch [10/51] time 0.174 (0.300) data 0.000 (0.122) loss 0.8813 (0.7906) acc 77.4510 (81.0139) lr 1.5358e-03 eta 0:08:22
epoch [18/50] batch [15/51] time 0.164 (0.258) data 0.000 (0.082) loss 0.7398 (0.7588) acc 79.2553 (82.0013) lr 1.5358e-03 eta 0:07:09
epoch [18/50] batch [20/51] time 0.181 (0.235) data 0.002 (0.061) loss 0.6508 (0.7474) acc 85.6481 (81.9640) lr 1.5358e-03 eta 0:06:31
epoch [18/50] batch [25/51] time 0.192 (0.224) data 0.000 (0.049) loss 0.6563 (0.7497) acc 81.7308 (82.4876) lr 1.5358e-03 eta 0:06:10
epoch [18/50] batch [30/51] time 0.174 (0.215) data 0.000 (0.041) loss 0.6200 (0.7633) acc 84.3137 (82.0153) lr 1.5358e-03 eta 0:05:55
epoch [18/50] batch [35/51] time 0.157 (0.209) data 0.000 (0.035) loss 0.7316 (0.7642) acc 77.7778 (81.9339) lr 1.5358e-03 eta 0:05:44
epoch [18/50] batch [40/51] time 0.170 (0.205) data 0.000 (0.031) loss 0.9774 (0.7746) acc 76.9608 (81.8299) lr 1.5358e-03 eta 0:05:36
epoch [18/50] batch [45/51] time 0.170 (0.201) data 0.000 (0.027) loss 0.6245 (0.7634) acc 83.8235 (81.9701) lr 1.5358e-03 eta 0:05:28
epoch [18/50] batch [50/51] time 0.172 (0.197) data 0.000 (0.025) loss 0.8833 (0.7680) acc 78.3654 (81.8003) lr 1.5358e-03 eta 0:05:22
>>> alpha1: 0.196  alpha2: 0.039 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.39 <<<
epoch [19/50] batch [5/51] time 0.179 (0.495) data 0.000 (0.309) loss 0.9236 (0.6777) acc 75.4717 (83.3305) lr 1.4818e-03 eta 0:13:25
epoch [19/50] batch [10/51] time 0.157 (0.333) data 0.000 (0.155) loss 0.9153 (0.7957) acc 82.2222 (81.4941) lr 1.4818e-03 eta 0:08:59
epoch [19/50] batch [15/51] time 0.189 (0.281) data 0.000 (0.103) loss 0.6837 (0.7692) acc 82.4074 (81.5869) lr 1.4818e-03 eta 0:07:35
epoch [19/50] batch [20/51] time 0.151 (0.253) data 0.000 (0.078) loss 0.9119 (0.7934) acc 74.4048 (80.8661) lr 1.4818e-03 eta 0:06:48
epoch [19/50] batch [25/51] time 0.174 (0.237) data 0.001 (0.062) loss 0.6972 (0.7861) acc 81.8627 (81.0226) lr 1.4818e-03 eta 0:06:21
epoch [19/50] batch [30/51] time 0.197 (0.228) data 0.000 (0.052) loss 0.7436 (0.7785) acc 77.9412 (81.1423) lr 1.4818e-03 eta 0:06:04
epoch [19/50] batch [35/51] time 0.183 (0.221) data 0.000 (0.044) loss 0.5546 (0.7518) acc 83.1731 (81.8282) lr 1.4818e-03 eta 0:05:52
epoch [19/50] batch [40/51] time 0.166 (0.214) data 0.000 (0.039) loss 0.8168 (0.7536) acc 78.0000 (81.9626) lr 1.4818e-03 eta 0:05:41
epoch [19/50] batch [45/51] time 0.164 (0.209) data 0.000 (0.035) loss 0.5226 (0.7516) acc 85.2041 (82.2004) lr 1.4818e-03 eta 0:05:32
epoch [19/50] batch [50/51] time 0.157 (0.205) data 0.000 (0.031) loss 0.6110 (0.7468) acc 84.7826 (82.3300) lr 1.4818e-03 eta 0:05:24
>>> alpha1: 0.182  alpha2: 0.040 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.34 <<<
epoch [20/50] batch [5/51] time 0.181 (0.546) data 0.000 (0.362) loss 0.7550 (0.6869) acc 82.3529 (82.1474) lr 1.4258e-03 eta 0:14:20
epoch [20/50] batch [10/51] time 0.207 (0.361) data 0.000 (0.181) loss 0.6338 (0.7070) acc 86.1111 (83.2367) lr 1.4258e-03 eta 0:09:27
epoch [20/50] batch [15/51] time 0.185 (0.301) data 0.000 (0.121) loss 0.6456 (0.7298) acc 83.9623 (83.0602) lr 1.4258e-03 eta 0:07:50
epoch [20/50] batch [20/51] time 0.174 (0.269) data 0.000 (0.091) loss 0.7667 (0.7126) acc 82.6923 (83.3331) lr 1.4258e-03 eta 0:06:59
epoch [20/50] batch [25/51] time 0.175 (0.251) data 0.000 (0.073) loss 0.9702 (0.7066) acc 76.9608 (83.8274) lr 1.4258e-03 eta 0:06:30
epoch [20/50] batch [30/51] time 0.189 (0.238) data 0.000 (0.061) loss 0.6080 (0.7085) acc 81.2500 (83.6602) lr 1.4258e-03 eta 0:06:09
epoch [20/50] batch [35/51] time 0.191 (0.231) data 0.000 (0.052) loss 0.6023 (0.7245) acc 90.6250 (83.4444) lr 1.4258e-03 eta 0:05:56
epoch [20/50] batch [40/51] time 0.155 (0.223) data 0.000 (0.046) loss 0.7326 (0.7189) acc 77.8409 (83.5656) lr 1.4258e-03 eta 0:05:43
epoch [20/50] batch [45/51] time 0.159 (0.216) data 0.000 (0.041) loss 1.0794 (0.7261) acc 71.1956 (83.3313) lr 1.4258e-03 eta 0:05:32
epoch [20/50] batch [50/51] time 0.170 (0.212) data 0.000 (0.037) loss 0.6095 (0.7149) acc 86.2745 (83.6223) lr 1.4258e-03 eta 0:05:24
>>> alpha1: 0.169  alpha2: 0.037 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [21/50] batch [5/51] time 0.225 (0.501) data 0.000 (0.314) loss 0.7069 (0.7519) acc 83.0189 (83.0319) lr 1.3681e-03 eta 0:12:44
epoch [21/50] batch [10/51] time 0.174 (0.341) data 0.001 (0.157) loss 0.6496 (0.7268) acc 87.0192 (83.4482) lr 1.3681e-03 eta 0:08:38
epoch [21/50] batch [15/51] time 0.183 (0.284) data 0.000 (0.105) loss 0.5441 (0.6873) acc 90.0943 (84.5713) lr 1.3681e-03 eta 0:07:10
epoch [21/50] batch [20/51] time 0.156 (0.255) data 0.000 (0.079) loss 0.7174 (0.7111) acc 84.6591 (83.6383) lr 1.3681e-03 eta 0:06:25
epoch [21/50] batch [25/51] time 0.170 (0.240) data 0.000 (0.063) loss 0.6455 (0.7037) acc 85.7143 (83.8960) lr 1.3681e-03 eta 0:06:00
epoch [21/50] batch [30/51] time 0.191 (0.229) data 0.000 (0.053) loss 0.8528 (0.6973) acc 79.0816 (83.7572) lr 1.3681e-03 eta 0:05:43
epoch [21/50] batch [35/51] time 0.182 (0.221) data 0.000 (0.045) loss 0.5547 (0.6890) acc 87.7551 (84.1231) lr 1.3681e-03 eta 0:05:30
epoch [21/50] batch [40/51] time 0.181 (0.215) data 0.000 (0.039) loss 0.5755 (0.6959) acc 87.0192 (83.9698) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [45/51] time 0.173 (0.210) data 0.000 (0.035) loss 0.6858 (0.6947) acc 86.0000 (84.0872) lr 1.3681e-03 eta 0:05:12
epoch [21/50] batch [50/51] time 0.182 (0.207) data 0.000 (0.032) loss 0.8527 (0.6846) acc 67.8571 (84.1294) lr 1.3681e-03 eta 0:05:05
>>> alpha1: 0.162  alpha2: 0.040 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.36 <<<
epoch [22/50] batch [5/51] time 0.165 (0.514) data 0.000 (0.338) loss 0.7226 (0.6880) acc 81.7708 (81.7891) lr 1.3090e-03 eta 0:12:37
epoch [22/50] batch [10/51] time 0.177 (0.348) data 0.000 (0.169) loss 0.7384 (0.6405) acc 78.8462 (83.7992) lr 1.3090e-03 eta 0:08:30
epoch [22/50] batch [15/51] time 0.182 (0.291) data 0.000 (0.113) loss 0.6841 (0.6600) acc 81.3726 (84.1483) lr 1.3090e-03 eta 0:07:05
epoch [22/50] batch [20/51] time 0.164 (0.263) data 0.000 (0.085) loss 0.4994 (0.6540) acc 86.9792 (84.8262) lr 1.3090e-03 eta 0:06:23
epoch [22/50] batch [25/51] time 0.181 (0.245) data 0.000 (0.068) loss 0.9767 (0.6541) acc 73.8889 (84.7777) lr 1.3090e-03 eta 0:05:56
epoch [22/50] batch [30/51] time 0.159 (0.233) data 0.000 (0.057) loss 0.8403 (0.6581) acc 73.9130 (84.5995) lr 1.3090e-03 eta 0:05:37
epoch [22/50] batch [35/51] time 0.170 (0.225) data 0.000 (0.049) loss 0.8142 (0.6680) acc 82.8125 (84.4788) lr 1.3090e-03 eta 0:05:24
epoch [22/50] batch [40/51] time 0.168 (0.219) data 0.000 (0.042) loss 0.7033 (0.6683) acc 89.0000 (84.4611) lr 1.3090e-03 eta 0:05:14
epoch [22/50] batch [45/51] time 0.170 (0.213) data 0.000 (0.038) loss 0.5557 (0.6662) acc 85.2941 (84.3774) lr 1.3090e-03 eta 0:05:05
epoch [22/50] batch [50/51] time 0.162 (0.208) data 0.000 (0.034) loss 1.0883 (0.6790) acc 68.6170 (83.9496) lr 1.3090e-03 eta 0:04:57
>>> alpha1: 0.159  alpha2: 0.039 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [23/50] batch [5/51] time 0.190 (0.535) data 0.000 (0.347) loss 0.6124 (0.6206) acc 85.7843 (87.1592) lr 1.2487e-03 eta 0:12:40
epoch [23/50] batch [10/51] time 0.168 (0.353) data 0.000 (0.174) loss 0.5830 (0.6563) acc 87.0000 (85.7468) lr 1.2487e-03 eta 0:08:20
epoch [23/50] batch [15/51] time 0.173 (0.296) data 0.000 (0.116) loss 0.7126 (0.7675) acc 86.0577 (84.5084) lr 1.2487e-03 eta 0:06:57
epoch [23/50] batch [20/51] time 0.188 (0.265) data 0.000 (0.087) loss 0.4370 (0.7462) acc 90.7407 (83.9762) lr 1.2487e-03 eta 0:06:12
epoch [23/50] batch [25/51] time 0.174 (0.248) data 0.000 (0.070) loss 0.5256 (0.7127) acc 88.9423 (84.3521) lr 1.2487e-03 eta 0:05:47
epoch [23/50] batch [30/51] time 0.181 (0.236) data 0.000 (0.058) loss 0.5727 (0.6902) acc 86.0000 (84.5929) lr 1.2487e-03 eta 0:05:29
epoch [23/50] batch [35/51] time 0.192 (0.228) data 0.000 (0.050) loss 0.5597 (0.6877) acc 87.0370 (84.6378) lr 1.2487e-03 eta 0:05:18
epoch [23/50] batch [40/51] time 0.170 (0.221) data 0.000 (0.044) loss 0.6207 (0.6773) acc 79.4118 (84.6848) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [45/51] time 0.163 (0.214) data 0.000 (0.039) loss 0.6631 (0.6857) acc 81.7708 (84.3577) lr 1.2487e-03 eta 0:04:56
epoch [23/50] batch [50/51] time 0.163 (0.210) data 0.000 (0.035) loss 0.4913 (0.6775) acc 89.0625 (84.4180) lr 1.2487e-03 eta 0:04:48
>>> alpha1: 0.154  alpha2: 0.035 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [24/50] batch [5/51] time 0.183 (0.507) data 0.001 (0.328) loss 0.6399 (0.6946) acc 85.0000 (83.0249) lr 1.1874e-03 eta 0:11:35
epoch [24/50] batch [10/51] time 0.182 (0.342) data 0.001 (0.164) loss 0.5727 (0.6507) acc 87.7358 (84.6744) lr 1.1874e-03 eta 0:07:47
epoch [24/50] batch [15/51] time 0.174 (0.287) data 0.000 (0.111) loss 0.6575 (0.6201) acc 87.2549 (85.6757) lr 1.1874e-03 eta 0:06:31
epoch [24/50] batch [20/51] time 0.169 (0.259) data 0.000 (0.083) loss 0.4468 (0.6188) acc 91.0000 (86.1752) lr 1.1874e-03 eta 0:05:50
epoch [24/50] batch [25/51] time 0.181 (0.242) data 0.001 (0.066) loss 0.5228 (0.6103) acc 91.2037 (86.2058) lr 1.1874e-03 eta 0:05:27
epoch [24/50] batch [30/51] time 0.181 (0.232) data 0.000 (0.055) loss 0.8245 (0.6128) acc 84.0000 (86.2188) lr 1.1874e-03 eta 0:05:11
epoch [24/50] batch [35/51] time 0.187 (0.223) data 0.000 (0.048) loss 0.7052 (0.6198) acc 75.9259 (85.9200) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [40/51] time 0.165 (0.217) data 0.000 (0.042) loss 0.6902 (0.6194) acc 85.2041 (86.0096) lr 1.1874e-03 eta 0:04:50
epoch [24/50] batch [45/51] time 0.164 (0.212) data 0.000 (0.037) loss 0.9731 (0.6338) acc 81.7708 (85.7260) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [50/51] time 0.159 (0.207) data 0.000 (0.033) loss 0.7796 (0.6327) acc 86.4130 (85.8474) lr 1.1874e-03 eta 0:04:35
>>> alpha1: 0.151  alpha2: 0.036 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [25/50] batch [5/51] time 0.173 (0.530) data 0.000 (0.357) loss 0.7314 (0.6442) acc 82.3529 (85.4097) lr 1.1253e-03 eta 0:11:39
epoch [25/50] batch [10/51] time 0.182 (0.355) data 0.001 (0.179) loss 0.6336 (0.6354) acc 82.4074 (84.8650) lr 1.1253e-03 eta 0:07:47
epoch [25/50] batch [15/51] time 0.198 (0.297) data 0.000 (0.119) loss 0.4464 (0.6367) acc 92.7273 (85.7391) lr 1.1253e-03 eta 0:06:28
epoch [25/50] batch [20/51] time 0.185 (0.267) data 0.000 (0.090) loss 0.5852 (0.6284) acc 87.5000 (85.8322) lr 1.1253e-03 eta 0:05:49
epoch [25/50] batch [25/51] time 0.186 (0.251) data 0.000 (0.072) loss 0.4858 (0.6172) acc 87.2642 (86.0716) lr 1.1253e-03 eta 0:05:26
epoch [25/50] batch [30/51] time 0.181 (0.239) data 0.000 (0.060) loss 0.5109 (0.6283) acc 90.5660 (85.7532) lr 1.1253e-03 eta 0:05:10
epoch [25/50] batch [35/51] time 0.170 (0.230) data 0.000 (0.051) loss 0.8143 (0.6428) acc 82.0000 (85.7167) lr 1.1253e-03 eta 0:04:56
epoch [25/50] batch [40/51] time 0.165 (0.224) data 0.000 (0.045) loss 0.5793 (0.6410) acc 87.2449 (85.6720) lr 1.1253e-03 eta 0:04:47
epoch [25/50] batch [45/51] time 0.169 (0.218) data 0.000 (0.040) loss 0.5695 (0.6440) acc 91.6667 (85.7853) lr 1.1253e-03 eta 0:04:38
epoch [25/50] batch [50/51] time 0.166 (0.213) data 0.000 (0.036) loss 0.7652 (0.6324) acc 83.6735 (85.9211) lr 1.1253e-03 eta 0:04:31
>>> alpha1: 0.149  alpha2: 0.038 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [26/50] batch [5/51] time 0.170 (0.516) data 0.000 (0.333) loss 0.6987 (0.5276) acc 86.5000 (87.8469) lr 1.0628e-03 eta 0:10:55
epoch [26/50] batch [10/51] time 0.178 (0.346) data 0.000 (0.167) loss 0.5029 (0.5593) acc 89.7059 (88.0311) lr 1.0628e-03 eta 0:07:17
epoch [26/50] batch [15/51] time 0.182 (0.288) data 0.000 (0.111) loss 0.5439 (0.5666) acc 86.3208 (87.3453) lr 1.0628e-03 eta 0:06:03
epoch [26/50] batch [20/51] time 0.167 (0.260) data 0.000 (0.083) loss 0.8398 (0.5839) acc 74.4898 (86.4494) lr 1.0628e-03 eta 0:05:26
epoch [26/50] batch [25/51] time 0.213 (0.244) data 0.000 (0.067) loss 0.5383 (0.5813) acc 89.3519 (86.3605) lr 1.0628e-03 eta 0:05:04
epoch [26/50] batch [30/51] time 0.188 (0.233) data 0.001 (0.056) loss 0.8210 (0.6016) acc 83.4906 (85.8054) lr 1.0628e-03 eta 0:04:49
epoch [26/50] batch [35/51] time 0.178 (0.225) data 0.000 (0.048) loss 0.7231 (0.6127) acc 84.5000 (85.8711) lr 1.0628e-03 eta 0:04:38
epoch [26/50] batch [40/51] time 0.175 (0.218) data 0.000 (0.042) loss 0.8340 (0.6153) acc 83.0189 (86.1303) lr 1.0628e-03 eta 0:04:29
epoch [26/50] batch [45/51] time 0.166 (0.229) data 0.000 (0.037) loss 0.8168 (0.6154) acc 82.1429 (86.0418) lr 1.0628e-03 eta 0:04:42
epoch [26/50] batch [50/51] time 0.175 (0.224) data 0.000 (0.033) loss 0.4842 (0.6179) acc 91.5094 (86.0378) lr 1.0628e-03 eta 0:04:34
>>> alpha1: 0.150  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [27/50] batch [5/51] time 0.173 (0.493) data 0.000 (0.314) loss 0.3737 (1.0076) acc 91.8269 (80.7358) lr 1.0000e-03 eta 0:10:01
epoch [27/50] batch [10/51] time 0.177 (0.336) data 0.000 (0.157) loss 0.6312 (0.7994) acc 85.8491 (82.9937) lr 1.0000e-03 eta 0:06:48
epoch [27/50] batch [15/51] time 0.169 (0.282) data 0.000 (0.105) loss 0.5242 (0.7426) acc 90.0000 (84.3529) lr 1.0000e-03 eta 0:05:40
epoch [27/50] batch [20/51] time 0.171 (0.256) data 0.001 (0.079) loss 0.5293 (0.6941) acc 88.2653 (85.6010) lr 1.0000e-03 eta 0:05:08
epoch [27/50] batch [25/51] time 0.177 (0.240) data 0.000 (0.063) loss 0.6528 (0.6820) acc 86.7924 (85.8002) lr 1.0000e-03 eta 0:04:47
epoch [27/50] batch [30/51] time 0.182 (0.230) data 0.001 (0.053) loss 0.6354 (0.6669) acc 85.0962 (86.0362) lr 1.0000e-03 eta 0:04:34
epoch [27/50] batch [35/51] time 0.194 (0.223) data 0.000 (0.045) loss 0.4745 (0.6643) acc 92.2727 (86.0116) lr 1.0000e-03 eta 0:04:24
epoch [27/50] batch [40/51] time 0.182 (0.217) data 0.000 (0.040) loss 0.6773 (0.6514) acc 81.6964 (86.0517) lr 1.0000e-03 eta 0:04:16
epoch [27/50] batch [45/51] time 0.170 (0.211) data 0.000 (0.035) loss 0.5483 (0.6490) acc 86.2745 (85.9545) lr 1.0000e-03 eta 0:04:09
epoch [27/50] batch [50/51] time 0.167 (0.207) data 0.000 (0.032) loss 0.6029 (0.6477) acc 86.5000 (85.9031) lr 1.0000e-03 eta 0:04:03
>>> alpha1: 0.147  alpha2: 0.045 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.36 <<<
epoch [28/50] batch [5/51] time 0.168 (0.544) data 0.000 (0.362) loss 0.9603 (0.6025) acc 73.4694 (86.3137) lr 9.3721e-04 eta 0:10:35
epoch [28/50] batch [10/51] time 0.198 (0.365) data 0.000 (0.181) loss 0.6710 (0.5826) acc 83.1818 (87.2867) lr 9.3721e-04 eta 0:07:04
epoch [28/50] batch [15/51] time 0.182 (0.302) data 0.000 (0.121) loss 0.4710 (0.5674) acc 94.3396 (87.9153) lr 9.3721e-04 eta 0:05:49
epoch [28/50] batch [20/51] time 0.177 (0.270) data 0.000 (0.091) loss 0.6951 (0.5629) acc 83.0189 (88.0851) lr 9.3721e-04 eta 0:05:11
epoch [28/50] batch [25/51] time 0.165 (0.252) data 0.000 (0.073) loss 0.8261 (0.5755) acc 80.7292 (87.8840) lr 9.3721e-04 eta 0:04:49
epoch [28/50] batch [30/51] time 0.175 (0.239) data 0.000 (0.061) loss 0.6457 (0.5746) acc 85.5000 (87.6226) lr 9.3721e-04 eta 0:04:33
epoch [28/50] batch [35/51] time 0.169 (0.231) data 0.000 (0.052) loss 0.7204 (0.5654) acc 82.0000 (87.7364) lr 9.3721e-04 eta 0:04:22
epoch [28/50] batch [40/51] time 0.184 (0.224) data 0.000 (0.045) loss 0.6520 (0.5655) acc 77.6316 (87.4229) lr 9.3721e-04 eta 0:04:13
epoch [28/50] batch [45/51] time 0.163 (0.218) data 0.000 (0.040) loss 0.5888 (0.5763) acc 84.8958 (87.1364) lr 9.3721e-04 eta 0:04:05
epoch [28/50] batch [50/51] time 0.178 (0.213) data 0.000 (0.036) loss 0.6389 (0.5818) acc 84.2593 (87.1155) lr 9.3721e-04 eta 0:03:59
>>> alpha1: 0.144  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [29/50] batch [5/51] time 0.195 (0.480) data 0.000 (0.297) loss 0.4115 (0.6111) acc 92.9245 (87.6010) lr 8.7467e-04 eta 0:08:55
epoch [29/50] batch [10/51] time 0.170 (0.331) data 0.000 (0.149) loss 0.8174 (0.6007) acc 80.2083 (87.3342) lr 8.7467e-04 eta 0:06:08
epoch [29/50] batch [15/51] time 0.181 (0.279) data 0.000 (0.099) loss 0.7478 (0.6123) acc 85.4546 (87.5591) lr 8.7467e-04 eta 0:05:08
epoch [29/50] batch [20/51] time 0.185 (0.253) data 0.000 (0.075) loss 0.4492 (0.6012) acc 91.6667 (87.8688) lr 8.7467e-04 eta 0:04:39
epoch [29/50] batch [25/51] time 0.186 (0.239) data 0.000 (0.060) loss 0.5893 (0.6045) acc 83.9286 (87.5131) lr 8.7467e-04 eta 0:04:22
epoch [29/50] batch [30/51] time 0.178 (0.230) data 0.000 (0.050) loss 0.7200 (0.6181) acc 84.6939 (87.2117) lr 8.7467e-04 eta 0:04:10
epoch [29/50] batch [35/51] time 0.173 (0.221) data 0.000 (0.043) loss 0.5454 (0.6001) acc 88.4615 (87.5334) lr 8.7467e-04 eta 0:04:00
epoch [29/50] batch [40/51] time 0.171 (0.215) data 0.000 (0.037) loss 0.6815 (0.5987) acc 86.5385 (87.4242) lr 8.7467e-04 eta 0:03:53
epoch [29/50] batch [45/51] time 0.165 (0.211) data 0.000 (0.033) loss 0.6227 (0.5905) acc 87.2449 (87.4188) lr 8.7467e-04 eta 0:03:46
epoch [29/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.030) loss 0.4087 (0.5855) acc 92.5926 (87.7039) lr 8.7467e-04 eta 0:03:41
>>> alpha1: 0.144  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [30/50] batch [5/51] time 0.169 (0.578) data 0.000 (0.388) loss 0.3758 (0.5086) acc 94.8980 (90.8205) lr 8.1262e-04 eta 0:10:15
epoch [30/50] batch [10/51] time 0.169 (0.378) data 0.000 (0.194) loss 0.6037 (0.5508) acc 89.0000 (89.5208) lr 8.1262e-04 eta 0:06:40
epoch [30/50] batch [15/51] time 0.174 (0.310) data 0.000 (0.130) loss 0.6412 (0.5558) acc 84.6154 (88.6712) lr 8.1262e-04 eta 0:05:26
epoch [30/50] batch [20/51] time 0.181 (0.277) data 0.000 (0.097) loss 0.3951 (0.5467) acc 89.5455 (88.6958) lr 8.1262e-04 eta 0:04:51
epoch [30/50] batch [25/51] time 0.181 (0.258) data 0.000 (0.078) loss 0.6703 (0.5543) acc 85.4546 (87.9309) lr 8.1262e-04 eta 0:04:30
epoch [30/50] batch [30/51] time 0.177 (0.245) data 0.000 (0.065) loss 0.4645 (0.5582) acc 90.5660 (87.8602) lr 8.1262e-04 eta 0:04:15
epoch [30/50] batch [35/51] time 0.174 (0.237) data 0.000 (0.056) loss 0.5795 (0.5597) acc 90.8654 (87.8479) lr 8.1262e-04 eta 0:04:05
epoch [30/50] batch [40/51] time 0.166 (0.229) data 0.000 (0.049) loss 0.5995 (0.5643) acc 85.2041 (87.7212) lr 8.1262e-04 eta 0:03:55
epoch [30/50] batch [45/51] time 0.170 (0.222) data 0.000 (0.043) loss 0.4972 (0.5634) acc 90.1961 (87.6922) lr 8.1262e-04 eta 0:03:47
epoch [30/50] batch [50/51] time 0.165 (0.217) data 0.000 (0.039) loss 0.7286 (0.5690) acc 78.0612 (87.3849) lr 8.1262e-04 eta 0:03:41
>>> alpha1: 0.144  alpha2: 0.055 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.34 <<<
epoch [31/50] batch [5/51] time 0.175 (0.483) data 0.000 (0.306) loss 0.4430 (0.6296) acc 89.2157 (85.6536) lr 7.5131e-04 eta 0:08:10
epoch [31/50] batch [10/51] time 0.181 (0.330) data 0.000 (0.153) loss 0.5171 (0.5672) acc 89.5455 (87.3988) lr 7.5131e-04 eta 0:05:33
epoch [31/50] batch [15/51] time 0.171 (0.277) data 0.000 (0.102) loss 0.4279 (0.5504) acc 89.2157 (87.7337) lr 7.5131e-04 eta 0:04:38
epoch [31/50] batch [20/51] time 0.183 (0.253) data 0.001 (0.077) loss 0.5446 (0.5422) acc 91.3636 (88.0127) lr 7.5131e-04 eta 0:04:13
epoch [31/50] batch [25/51] time 0.173 (0.238) data 0.000 (0.062) loss 0.5440 (0.5367) acc 85.2941 (87.8797) lr 7.5131e-04 eta 0:03:57
epoch [31/50] batch [30/51] time 0.182 (0.229) data 0.001 (0.051) loss 0.5776 (0.5534) acc 89.1509 (87.4035) lr 7.5131e-04 eta 0:03:46
epoch [31/50] batch [35/51] time 0.167 (0.222) data 0.000 (0.044) loss 0.8304 (0.5523) acc 79.0816 (87.3380) lr 7.5131e-04 eta 0:03:39
epoch [31/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.039) loss 0.7260 (0.5589) acc 82.8125 (87.1297) lr 7.5131e-04 eta 0:03:31
epoch [31/50] batch [45/51] time 0.180 (0.211) data 0.000 (0.034) loss 0.4878 (0.5567) acc 90.0000 (87.3205) lr 7.5131e-04 eta 0:03:25
epoch [31/50] batch [50/51] time 0.172 (0.207) data 0.000 (0.031) loss 0.5787 (0.5553) acc 89.4231 (87.5285) lr 7.5131e-04 eta 0:03:20
>>> alpha1: 0.138  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [32/50] batch [5/51] time 0.172 (0.514) data 0.001 (0.335) loss 0.4818 (0.5211) acc 88.2653 (87.1231) lr 6.9098e-04 eta 0:08:15
epoch [32/50] batch [10/51] time 0.168 (0.344) data 0.000 (0.168) loss 0.5943 (0.5217) acc 86.7347 (87.3174) lr 6.9098e-04 eta 0:05:30
epoch [32/50] batch [15/51] time 0.164 (0.287) data 0.000 (0.112) loss 0.5056 (0.5317) acc 90.1042 (87.3628) lr 6.9098e-04 eta 0:04:34
epoch [32/50] batch [20/51] time 0.177 (0.260) data 0.001 (0.084) loss 0.5127 (0.5375) acc 88.6792 (87.2949) lr 6.9098e-04 eta 0:04:06
epoch [32/50] batch [25/51] time 0.177 (0.244) data 0.001 (0.067) loss 0.5530 (0.5441) acc 89.7959 (87.5320) lr 6.9098e-04 eta 0:03:50
epoch [32/50] batch [30/51] time 0.184 (0.233) data 0.000 (0.056) loss 0.5218 (0.5430) acc 87.5000 (87.9331) lr 6.9098e-04 eta 0:03:38
epoch [32/50] batch [35/51] time 0.171 (0.225) data 0.001 (0.048) loss 0.4928 (0.5454) acc 87.5000 (87.8657) lr 6.9098e-04 eta 0:03:30
epoch [32/50] batch [40/51] time 0.166 (0.219) data 0.000 (0.042) loss 0.7815 (0.5461) acc 78.0612 (87.9772) lr 6.9098e-04 eta 0:03:23
epoch [32/50] batch [45/51] time 0.166 (0.213) data 0.000 (0.038) loss 0.6136 (0.5467) acc 81.1225 (87.8382) lr 6.9098e-04 eta 0:03:17
epoch [32/50] batch [50/51] time 0.169 (0.208) data 0.000 (0.034) loss 0.3877 (0.5470) acc 90.6863 (87.8292) lr 6.9098e-04 eta 0:03:11
>>> alpha1: 0.136  alpha2: 0.050 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [33/50] batch [5/51] time 0.173 (0.465) data 0.001 (0.282) loss 0.6284 (0.5948) acc 86.4583 (86.8267) lr 6.3188e-04 eta 0:07:04
epoch [33/50] batch [10/51] time 0.188 (0.322) data 0.000 (0.141) loss 0.4933 (0.5805) acc 89.9038 (87.0975) lr 6.3188e-04 eta 0:04:52
epoch [33/50] batch [15/51] time 0.165 (0.271) data 0.000 (0.094) loss 0.5628 (0.5745) acc 87.5000 (87.2335) lr 6.3188e-04 eta 0:04:04
epoch [33/50] batch [20/51] time 0.177 (0.246) data 0.000 (0.071) loss 0.4985 (0.5756) acc 89.3617 (87.0136) lr 6.3188e-04 eta 0:03:41
epoch [33/50] batch [25/51] time 0.167 (0.231) data 0.000 (0.057) loss 0.7435 (0.5681) acc 82.6531 (87.3526) lr 6.3188e-04 eta 0:03:26
epoch [33/50] batch [30/51] time 0.177 (0.223) data 0.000 (0.047) loss 0.7093 (0.5630) acc 83.5000 (87.4097) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [35/51] time 0.189 (0.216) data 0.000 (0.040) loss 0.4121 (0.5630) acc 93.7500 (87.5775) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [40/51] time 0.164 (0.210) data 0.000 (0.035) loss 0.4057 (0.5479) acc 92.3469 (88.0348) lr 6.3188e-04 eta 0:03:04
epoch [33/50] batch [45/51] time 0.168 (0.206) data 0.000 (0.031) loss 0.6179 (0.5425) acc 85.7843 (88.1721) lr 6.3188e-04 eta 0:02:59
epoch [33/50] batch [50/51] time 0.176 (0.202) data 0.001 (0.028) loss 0.5418 (0.5456) acc 88.7255 (87.9290) lr 6.3188e-04 eta 0:02:55
>>> alpha1: 0.135  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [34/50] batch [5/51] time 0.177 (0.498) data 0.000 (0.317) loss 0.5868 (0.6086) acc 86.7647 (86.0664) lr 5.7422e-04 eta 0:07:09
epoch [34/50] batch [10/51] time 0.196 (0.336) data 0.001 (0.158) loss 0.7953 (0.5812) acc 79.6296 (87.3118) lr 5.7422e-04 eta 0:04:48
epoch [34/50] batch [15/51] time 0.171 (0.282) data 0.000 (0.107) loss 0.4721 (0.5357) acc 88.5000 (88.1267) lr 5.7422e-04 eta 0:04:00
epoch [34/50] batch [20/51] time 0.163 (0.256) data 0.000 (0.080) loss 0.5956 (0.5591) acc 87.7660 (87.7333) lr 5.7422e-04 eta 0:03:37
epoch [34/50] batch [25/51] time 0.178 (0.241) data 0.000 (0.064) loss 0.5609 (0.5451) acc 88.8889 (88.3085) lr 5.7422e-04 eta 0:03:23
epoch [34/50] batch [30/51] time 0.178 (0.230) data 0.000 (0.053) loss 0.4402 (0.5440) acc 91.9811 (88.2608) lr 5.7422e-04 eta 0:03:12
epoch [34/50] batch [35/51] time 0.166 (0.222) data 0.000 (0.046) loss 0.6280 (0.5399) acc 87.7660 (88.3015) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.040) loss 0.6209 (0.5376) acc 88.2353 (88.4204) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [45/51] time 0.171 (0.210) data 0.000 (0.036) loss 0.4020 (0.5426) acc 91.8269 (88.4193) lr 5.7422e-04 eta 0:02:52
epoch [34/50] batch [50/51] time 0.175 (0.206) data 0.000 (0.032) loss 0.5498 (0.5444) acc 88.2075 (88.2852) lr 5.7422e-04 eta 0:02:48
>>> alpha1: 0.132  alpha2: 0.045 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.35 <<<
epoch [35/50] batch [5/51] time 0.190 (0.458) data 0.000 (0.275) loss 0.4363 (0.5759) acc 85.7843 (87.9508) lr 5.1825e-04 eta 0:06:11
epoch [35/50] batch [10/51] time 0.199 (0.318) data 0.000 (0.138) loss 0.3171 (0.5455) acc 93.5185 (87.7303) lr 5.1825e-04 eta 0:04:16
epoch [35/50] batch [15/51] time 0.186 (0.271) data 0.000 (0.092) loss 0.4681 (0.5213) acc 88.1579 (88.4838) lr 5.1825e-04 eta 0:03:36
epoch [35/50] batch [20/51] time 0.185 (0.248) data 0.000 (0.069) loss 0.4089 (0.5244) acc 92.8571 (88.7404) lr 5.1825e-04 eta 0:03:17
epoch [35/50] batch [25/51] time 0.186 (0.233) data 0.000 (0.055) loss 0.5102 (0.6128) acc 88.6792 (87.2730) lr 5.1825e-04 eta 0:03:04
epoch [35/50] batch [30/51] time 0.178 (0.224) data 0.001 (0.046) loss 0.4851 (0.6487) acc 89.6226 (87.0852) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.169 (0.217) data 0.000 (0.040) loss 0.5141 (0.6300) acc 90.0000 (87.3567) lr 5.1825e-04 eta 0:02:49
epoch [35/50] batch [40/51] time 0.173 (0.212) data 0.000 (0.035) loss 0.5109 (0.6243) acc 89.4231 (87.3069) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [45/51] time 0.170 (0.207) data 0.000 (0.031) loss 0.5997 (0.6144) acc 86.2745 (87.4591) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [50/51] time 0.161 (0.203) data 0.000 (0.028) loss 0.7848 (0.6101) acc 78.7234 (87.3148) lr 5.1825e-04 eta 0:02:35
>>> alpha1: 0.129  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.33 <<<
epoch [36/50] batch [5/51] time 0.182 (0.472) data 0.000 (0.284) loss 0.6636 (0.5843) acc 80.5556 (86.8257) lr 4.6417e-04 eta 0:05:58
epoch [36/50] batch [10/51] time 0.173 (0.324) data 0.000 (0.142) loss 0.5078 (0.5467) acc 90.2174 (87.7061) lr 4.6417e-04 eta 0:04:04
epoch [36/50] batch [15/51] time 0.179 (0.274) data 0.000 (0.095) loss 0.5331 (0.5303) acc 86.4583 (88.3068) lr 4.6417e-04 eta 0:03:25
epoch [36/50] batch [20/51] time 0.192 (0.250) data 0.000 (0.071) loss 0.6714 (0.5227) acc 84.7222 (88.3065) lr 4.6417e-04 eta 0:03:06
epoch [36/50] batch [25/51] time 0.180 (0.236) data 0.000 (0.057) loss 0.4304 (0.5198) acc 86.1111 (88.4583) lr 4.6417e-04 eta 0:02:54
epoch [36/50] batch [30/51] time 0.176 (0.226) data 0.001 (0.048) loss 0.4927 (0.5173) acc 91.1765 (88.7374) lr 4.6417e-04 eta 0:02:45
epoch [36/50] batch [35/51] time 0.174 (0.218) data 0.000 (0.041) loss 0.6238 (0.5321) acc 86.5385 (88.4594) lr 4.6417e-04 eta 0:02:39
epoch [36/50] batch [40/51] time 0.166 (0.213) data 0.000 (0.036) loss 0.5035 (0.5327) acc 90.8163 (88.6044) lr 4.6417e-04 eta 0:02:34
epoch [36/50] batch [45/51] time 0.171 (0.208) data 0.000 (0.032) loss 0.6715 (0.5397) acc 84.8039 (88.3493) lr 4.6417e-04 eta 0:02:29
epoch [36/50] batch [50/51] time 0.171 (0.205) data 0.000 (0.029) loss 0.4549 (0.5346) acc 94.6078 (88.7123) lr 4.6417e-04 eta 0:02:26
>>> alpha1: 0.130  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.34 <<<
epoch [37/50] batch [5/51] time 0.184 (0.494) data 0.000 (0.314) loss 0.4177 (0.5525) acc 92.3077 (87.8744) lr 4.1221e-04 eta 0:05:50
epoch [37/50] batch [10/51] time 0.183 (0.339) data 0.000 (0.157) loss 0.3182 (0.5155) acc 92.9245 (88.7359) lr 4.1221e-04 eta 0:03:58
epoch [37/50] batch [15/51] time 0.172 (0.285) data 0.000 (0.105) loss 0.4234 (0.5142) acc 94.6078 (89.0345) lr 4.1221e-04 eta 0:03:18
epoch [37/50] batch [20/51] time 0.186 (0.257) data 0.000 (0.079) loss 0.4280 (0.4867) acc 88.1579 (89.5499) lr 4.1221e-04 eta 0:02:58
epoch [37/50] batch [25/51] time 0.187 (0.242) data 0.000 (0.063) loss 0.5742 (0.4820) acc 84.8039 (89.6213) lr 4.1221e-04 eta 0:02:46
epoch [37/50] batch [30/51] time 0.156 (0.230) data 0.000 (0.053) loss 0.7528 (0.4995) acc 84.6591 (89.3719) lr 4.1221e-04 eta 0:02:37
epoch [37/50] batch [35/51] time 0.178 (0.222) data 0.000 (0.046) loss 0.4981 (0.5105) acc 90.3061 (89.1359) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [40/51] time 0.160 (0.216) data 0.000 (0.040) loss 0.5635 (0.5113) acc 87.5000 (89.1251) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [45/51] time 0.173 (0.211) data 0.000 (0.036) loss 0.5690 (0.5072) acc 88.4615 (89.0817) lr 4.1221e-04 eta 0:02:21
epoch [37/50] batch [50/51] time 0.160 (0.207) data 0.000 (0.032) loss 0.4349 (0.5061) acc 88.0435 (89.0462) lr 4.1221e-04 eta 0:02:17
>>> alpha1: 0.128  alpha2: 0.044 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.35 <<<
epoch [38/50] batch [5/51] time 0.175 (0.524) data 0.001 (0.335) loss 0.5430 (0.5376) acc 91.8269 (90.5846) lr 3.6258e-04 eta 0:05:45
epoch [38/50] batch [10/51] time 0.178 (0.353) data 0.000 (0.168) loss 0.5153 (0.5177) acc 88.5417 (89.5543) lr 3.6258e-04 eta 0:03:50
epoch [38/50] batch [15/51] time 0.180 (0.294) data 0.000 (0.112) loss 0.4954 (0.5246) acc 88.2075 (89.2920) lr 3.6258e-04 eta 0:03:10
epoch [38/50] batch [20/51] time 0.197 (0.264) data 0.000 (0.084) loss 0.5298 (0.5340) acc 89.5000 (89.1564) lr 3.6258e-04 eta 0:02:49
epoch [38/50] batch [25/51] time 0.197 (0.247) data 0.001 (0.067) loss 0.3747 (0.5329) acc 94.1964 (88.9779) lr 3.6258e-04 eta 0:02:37
epoch [38/50] batch [30/51] time 0.182 (0.238) data 0.000 (0.057) loss 0.2710 (0.5137) acc 95.4546 (89.5361) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [35/51] time 0.197 (0.230) data 0.000 (0.049) loss 0.2680 (0.5127) acc 94.1964 (89.4089) lr 3.6258e-04 eta 0:02:24
epoch [38/50] batch [40/51] time 0.164 (0.223) data 0.000 (0.043) loss 0.6329 (0.5223) acc 88.2979 (89.1621) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [45/51] time 0.169 (0.217) data 0.000 (0.038) loss 0.4504 (0.5191) acc 95.5000 (89.3234) lr 3.6258e-04 eta 0:02:14
epoch [38/50] batch [50/51] time 0.173 (0.213) data 0.000 (0.034) loss 0.6399 (0.5243) acc 81.3726 (88.9891) lr 3.6258e-04 eta 0:02:10
>>> alpha1: 0.130  alpha2: 0.048 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.34 <<<
epoch [39/50] batch [5/51] time 0.183 (0.483) data 0.000 (0.304) loss 0.4990 (0.5488) acc 87.0000 (86.3846) lr 3.1545e-04 eta 0:04:52
epoch [39/50] batch [10/51] time 0.187 (0.329) data 0.000 (0.152) loss 0.5543 (0.5597) acc 82.5472 (87.0186) lr 3.1545e-04 eta 0:03:18
epoch [39/50] batch [15/51] time 0.179 (0.278) data 0.000 (0.101) loss 0.6014 (0.5925) acc 86.1111 (86.3003) lr 3.1545e-04 eta 0:02:46
epoch [39/50] batch [20/51] time 0.182 (0.253) data 0.000 (0.076) loss 0.4362 (0.5613) acc 94.0909 (87.0660) lr 3.1545e-04 eta 0:02:29
epoch [39/50] batch [25/51] time 0.182 (0.239) data 0.000 (0.061) loss 0.3835 (0.5285) acc 93.1818 (88.1130) lr 3.1545e-04 eta 0:02:20
epoch [39/50] batch [30/51] time 0.179 (0.228) data 0.000 (0.051) loss 0.4340 (0.5265) acc 89.3519 (88.1220) lr 3.1545e-04 eta 0:02:12
epoch [39/50] batch [35/51] time 0.179 (0.220) data 0.000 (0.044) loss 0.6114 (0.5211) acc 86.5741 (88.1640) lr 3.1545e-04 eta 0:02:07
epoch [39/50] batch [40/51] time 0.180 (0.215) data 0.000 (0.038) loss 0.5501 (0.5257) acc 87.5000 (88.0053) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [45/51] time 0.187 (0.211) data 0.000 (0.034) loss 0.6145 (0.5276) acc 84.9138 (87.9059) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.031) loss 0.4543 (0.5236) acc 92.9245 (87.9217) lr 3.1545e-04 eta 0:01:56
>>> alpha1: 0.129  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [40/50] batch [5/51] time 0.172 (0.498) data 0.002 (0.301) loss 0.5671 (0.5157) acc 82.9787 (87.7062) lr 2.7103e-04 eta 0:04:36
epoch [40/50] batch [10/51] time 0.163 (0.341) data 0.000 (0.151) loss 0.4787 (0.5477) acc 88.8298 (86.9933) lr 2.7103e-04 eta 0:03:07
epoch [40/50] batch [15/51] time 0.190 (0.289) data 0.000 (0.102) loss 0.2839 (0.5149) acc 95.9821 (88.2367) lr 2.7103e-04 eta 0:02:37
epoch [40/50] batch [20/51] time 0.177 (0.260) data 0.000 (0.076) loss 0.4735 (0.5268) acc 90.0943 (88.0516) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [25/51] time 0.163 (0.244) data 0.000 (0.061) loss 0.6258 (0.5395) acc 86.1702 (87.9358) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [30/51] time 0.175 (0.234) data 0.000 (0.051) loss 0.3649 (0.5328) acc 93.2692 (88.2919) lr 2.7103e-04 eta 0:02:04
epoch [40/50] batch [35/51] time 0.170 (0.226) data 0.000 (0.044) loss 0.5972 (0.5300) acc 84.8958 (88.1940) lr 2.7103e-04 eta 0:01:59
epoch [40/50] batch [40/51] time 0.172 (0.221) data 0.000 (0.038) loss 0.5238 (0.5217) acc 90.1961 (88.4871) lr 2.7103e-04 eta 0:01:55
epoch [40/50] batch [45/51] time 0.184 (0.216) data 0.000 (0.034) loss 0.3002 (0.5159) acc 91.5179 (88.5710) lr 2.7103e-04 eta 0:01:51
epoch [40/50] batch [50/51] time 0.182 (0.212) data 0.000 (0.031) loss 0.4139 (0.5243) acc 92.7273 (88.4945) lr 2.7103e-04 eta 0:01:48
>>> alpha1: 0.130  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.35 <<<
epoch [41/50] batch [5/51] time 0.178 (0.513) data 0.000 (0.326) loss 0.5478 (0.4964) acc 89.1509 (88.3373) lr 2.2949e-04 eta 0:04:18
epoch [41/50] batch [10/51] time 0.157 (0.348) data 0.000 (0.164) loss 0.6805 (0.4949) acc 87.7907 (89.2232) lr 2.2949e-04 eta 0:02:53
epoch [41/50] batch [15/51] time 0.178 (0.291) data 0.000 (0.109) loss 0.6782 (0.5065) acc 90.4255 (89.2249) lr 2.2949e-04 eta 0:02:23
epoch [41/50] batch [20/51] time 0.177 (0.263) data 0.000 (0.082) loss 0.3921 (0.4944) acc 89.2157 (89.2887) lr 2.2949e-04 eta 0:02:08
epoch [41/50] batch [25/51] time 0.180 (0.245) data 0.000 (0.066) loss 0.4353 (0.4875) acc 91.6667 (89.7914) lr 2.2949e-04 eta 0:01:59
epoch [41/50] batch [30/51] time 0.165 (0.233) data 0.000 (0.055) loss 0.4557 (0.4861) acc 90.1042 (89.5736) lr 2.2949e-04 eta 0:01:51
epoch [41/50] batch [35/51] time 0.174 (0.224) data 0.001 (0.047) loss 0.5718 (0.4950) acc 86.7647 (89.3034) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [40/51] time 0.173 (0.219) data 0.000 (0.041) loss 0.4877 (0.4986) acc 89.4231 (89.1610) lr 2.2949e-04 eta 0:01:42
epoch [41/50] batch [45/51] time 0.174 (0.213) data 0.000 (0.037) loss 0.5058 (0.4976) acc 89.9038 (89.1817) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [50/51] time 0.159 (0.209) data 0.000 (0.033) loss 0.5517 (0.5069) acc 84.7826 (88.8927) lr 2.2949e-04 eta 0:01:36
>>> alpha1: 0.130  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [42/50] batch [5/51] time 0.178 (0.551) data 0.000 (0.367) loss 0.5207 (0.5088) acc 86.7924 (89.2488) lr 1.9098e-04 eta 0:04:10
epoch [42/50] batch [10/51] time 0.172 (0.363) data 0.000 (0.184) loss 0.4085 (0.4701) acc 93.1373 (90.7893) lr 1.9098e-04 eta 0:02:43
epoch [42/50] batch [15/51] time 0.183 (0.300) data 0.001 (0.123) loss 0.4998 (0.4790) acc 91.3636 (90.3757) lr 1.9098e-04 eta 0:02:13
epoch [42/50] batch [20/51] time 0.195 (0.271) data 0.001 (0.092) loss 0.4653 (0.4676) acc 90.0943 (90.7403) lr 1.9098e-04 eta 0:01:58
epoch [42/50] batch [25/51] time 0.180 (0.253) data 0.000 (0.074) loss 0.6154 (0.4754) acc 80.0000 (90.0122) lr 1.9098e-04 eta 0:01:49
epoch [42/50] batch [30/51] time 0.175 (0.242) data 0.000 (0.062) loss 0.6463 (0.4862) acc 85.2041 (89.7716) lr 1.9098e-04 eta 0:01:44
epoch [42/50] batch [35/51] time 0.166 (0.232) data 0.001 (0.053) loss 0.3442 (0.4906) acc 90.2174 (89.5654) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [40/51] time 0.177 (0.226) data 0.000 (0.046) loss 0.6446 (0.4920) acc 86.3208 (89.3114) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [45/51] time 0.165 (0.220) data 0.001 (0.041) loss 0.6660 (0.4940) acc 85.1064 (89.2541) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [50/51] time 0.188 (0.215) data 0.000 (0.037) loss 0.5565 (0.4956) acc 89.9038 (89.2209) lr 1.9098e-04 eta 0:01:27
>>> alpha1: 0.130  alpha2: 0.056 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.33 <<<
epoch [43/50] batch [5/51] time 0.165 (0.491) data 0.001 (0.310) loss 0.4004 (0.4240) acc 91.6667 (92.8064) lr 1.5567e-04 eta 0:03:17
epoch [43/50] batch [10/51] time 0.193 (0.339) data 0.000 (0.155) loss 0.4280 (0.4759) acc 92.0000 (91.6840) lr 1.5567e-04 eta 0:02:14
epoch [43/50] batch [15/51] time 0.183 (0.290) data 0.001 (0.104) loss 0.4925 (0.4744) acc 91.9811 (91.6888) lr 1.5567e-04 eta 0:01:54
epoch [43/50] batch [20/51] time 0.175 (0.262) data 0.000 (0.078) loss 0.6876 (0.4948) acc 84.6939 (90.3607) lr 1.5567e-04 eta 0:01:41
epoch [43/50] batch [25/51] time 0.215 (0.250) data 0.000 (0.062) loss 0.4581 (0.4950) acc 88.2075 (90.2276) lr 1.5567e-04 eta 0:01:35
epoch [43/50] batch [30/51] time 0.179 (0.238) data 0.001 (0.052) loss 0.5165 (0.5024) acc 91.1765 (89.9527) lr 1.5567e-04 eta 0:01:29
epoch [43/50] batch [35/51] time 0.198 (0.230) data 0.000 (0.045) loss 0.5877 (0.5006) acc 87.9464 (89.8391) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [40/51] time 0.178 (0.223) data 0.000 (0.039) loss 0.3672 (0.5075) acc 91.0377 (89.3483) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [45/51] time 0.174 (0.218) data 0.000 (0.035) loss 0.4889 (0.5114) acc 86.5385 (89.0453) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [50/51] time 0.174 (0.213) data 0.000 (0.031) loss 0.4223 (0.5091) acc 92.7885 (89.0463) lr 1.5567e-04 eta 0:01:16
>>> alpha1: 0.128  alpha2: 0.052 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.33 <<<
epoch [44/50] batch [5/51] time 0.193 (0.495) data 0.000 (0.312) loss 0.4207 (0.5681) acc 91.3462 (86.3361) lr 1.2369e-04 eta 0:02:54
epoch [44/50] batch [10/51] time 0.175 (0.336) data 0.000 (0.156) loss 0.5144 (0.5419) acc 88.2353 (87.0828) lr 1.2369e-04 eta 0:01:56
epoch [44/50] batch [15/51] time 0.174 (0.283) data 0.000 (0.104) loss 0.6756 (0.5194) acc 86.7647 (88.3741) lr 1.2369e-04 eta 0:01:36
epoch [44/50] batch [20/51] time 0.196 (0.257) data 0.000 (0.078) loss 0.3127 (0.4957) acc 93.5185 (89.0085) lr 1.2369e-04 eta 0:01:26
epoch [44/50] batch [25/51] time 0.185 (0.241) data 0.001 (0.063) loss 0.4623 (0.5048) acc 91.6667 (88.7315) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [30/51] time 0.181 (0.231) data 0.000 (0.052) loss 0.5316 (0.4982) acc 88.2075 (88.8629) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [35/51] time 0.173 (0.224) data 0.000 (0.045) loss 0.3800 (0.4902) acc 94.0000 (89.2559) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [40/51] time 0.164 (0.218) data 0.000 (0.039) loss 0.8062 (0.5022) acc 86.1702 (89.1099) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.178 (0.213) data 0.000 (0.035) loss 0.3730 (0.5006) acc 92.5926 (89.1152) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [50/51] time 0.194 (0.209) data 0.000 (0.031) loss 0.3910 (0.4982) acc 91.3793 (89.2578) lr 1.2369e-04 eta 0:01:04
>>> alpha1: 0.127  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.32 <<<
epoch [45/50] batch [5/51] time 0.201 (0.527) data 0.000 (0.329) loss 0.3701 (0.4410) acc 93.9815 (89.7561) lr 9.5173e-05 eta 0:02:38
epoch [45/50] batch [10/51] time 0.185 (0.357) data 0.000 (0.164) loss 0.2972 (0.4344) acc 92.5926 (89.9759) lr 9.5173e-05 eta 0:01:45
epoch [45/50] batch [15/51] time 0.192 (0.298) data 0.001 (0.110) loss 0.2471 (0.4330) acc 97.3214 (89.8961) lr 9.5173e-05 eta 0:01:26
epoch [45/50] batch [20/51] time 0.161 (0.267) data 0.000 (0.082) loss 0.3966 (0.4417) acc 93.4783 (89.8344) lr 9.5173e-05 eta 0:01:16
epoch [45/50] batch [25/51] time 0.176 (0.247) data 0.000 (0.066) loss 0.4697 (0.4782) acc 89.0000 (89.1859) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [30/51] time 0.222 (0.238) data 0.000 (0.055) loss 0.4927 (0.4917) acc 90.5660 (88.9550) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [35/51] time 0.172 (0.230) data 0.000 (0.047) loss 0.4845 (0.4941) acc 89.5000 (89.0273) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [40/51] time 0.172 (0.222) data 0.000 (0.041) loss 0.6193 (0.4998) acc 91.1765 (89.2119) lr 9.5173e-05 eta 0:00:59
epoch [45/50] batch [45/51] time 0.174 (0.217) data 0.000 (0.037) loss 0.7810 (0.4994) acc 76.9231 (89.1326) lr 9.5173e-05 eta 0:00:56
epoch [45/50] batch [50/51] time 0.164 (0.212) data 0.000 (0.033) loss 0.5532 (0.5437) acc 89.5833 (88.6464) lr 9.5173e-05 eta 0:00:54
>>> alpha1: 0.127  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [46/50] batch [5/51] time 0.183 (0.520) data 0.002 (0.329) loss 0.5011 (0.5110) acc 90.5000 (88.4322) lr 7.0224e-05 eta 0:02:09
epoch [46/50] batch [10/51] time 0.177 (0.354) data 0.000 (0.165) loss 0.4439 (0.5083) acc 90.0000 (89.0860) lr 7.0224e-05 eta 0:01:26
epoch [46/50] batch [15/51] time 0.179 (0.298) data 0.001 (0.110) loss 0.5415 (0.4915) acc 86.7924 (89.6385) lr 7.0224e-05 eta 0:01:11
epoch [46/50] batch [20/51] time 0.179 (0.270) data 0.000 (0.082) loss 0.5169 (0.5005) acc 87.2549 (89.9180) lr 7.0224e-05 eta 0:01:03
epoch [46/50] batch [25/51] time 0.169 (0.251) data 0.000 (0.066) loss 0.5729 (0.4943) acc 88.5000 (89.8548) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [30/51] time 0.164 (0.239) data 0.000 (0.055) loss 0.5195 (0.5038) acc 91.1458 (89.4558) lr 7.0224e-05 eta 0:00:53
epoch [46/50] batch [35/51] time 0.183 (0.232) data 0.000 (0.048) loss 0.5081 (0.5034) acc 87.7451 (89.3340) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [40/51] time 0.177 (0.225) data 0.000 (0.042) loss 0.3919 (0.5004) acc 93.3962 (89.6339) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [45/51] time 0.166 (0.218) data 0.000 (0.037) loss 0.5999 (0.4947) acc 87.2449 (89.6568) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [50/51] time 0.188 (0.214) data 0.001 (0.033) loss 0.5464 (0.4998) acc 89.8148 (89.5836) lr 7.0224e-05 eta 0:00:43
>>> alpha1: 0.127  alpha2: 0.054 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [47/50] batch [5/51] time 0.176 (0.485) data 0.000 (0.305) loss 0.4203 (0.4400) acc 90.8654 (91.2241) lr 4.8943e-05 eta 0:01:36
epoch [47/50] batch [10/51] time 0.174 (0.329) data 0.000 (0.153) loss 0.4737 (0.4779) acc 90.1961 (89.9783) lr 4.8943e-05 eta 0:01:03
epoch [47/50] batch [15/51] time 0.185 (0.284) data 0.001 (0.102) loss 0.4457 (0.4808) acc 90.4546 (89.4610) lr 4.8943e-05 eta 0:00:53
epoch [47/50] batch [20/51] time 0.178 (0.257) data 0.000 (0.077) loss 0.3838 (0.4843) acc 94.8980 (89.6668) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [25/51] time 0.173 (0.242) data 0.000 (0.061) loss 0.6721 (0.4818) acc 82.8431 (89.7618) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [30/51] time 0.179 (0.232) data 0.000 (0.051) loss 0.4696 (0.4781) acc 90.3846 (89.6584) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.183 (0.226) data 0.000 (0.044) loss 0.2964 (0.5232) acc 95.2830 (89.2093) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [40/51] time 0.181 (0.220) data 0.000 (0.039) loss 0.3918 (0.5135) acc 87.7451 (89.0294) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [45/51] time 0.183 (0.216) data 0.000 (0.034) loss 0.3299 (0.5139) acc 92.4528 (88.8673) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [50/51] time 0.184 (0.212) data 0.000 (0.031) loss 0.4857 (0.5103) acc 85.6481 (88.9470) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.128  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.21 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [48/50] batch [5/51] time 0.168 (0.474) data 0.000 (0.295) loss 0.5057 (0.5728) acc 91.1458 (87.5598) lr 3.1417e-05 eta 0:01:10
epoch [48/50] batch [10/51] time 0.185 (0.331) data 0.000 (0.148) loss 0.5407 (0.5241) acc 89.4231 (88.7603) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.190 (0.283) data 0.000 (0.099) loss 0.4335 (0.4976) acc 94.2308 (89.8126) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.182 (0.257) data 0.000 (0.074) loss 0.5904 (0.5292) acc 85.8491 (88.8774) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.183 (0.244) data 0.001 (0.059) loss 0.4702 (0.5133) acc 89.7959 (89.3568) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.198 (0.234) data 0.000 (0.050) loss 0.4813 (0.5138) acc 88.6792 (89.3555) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.195 (0.229) data 0.001 (0.043) loss 0.3824 (0.5120) acc 95.4546 (89.4365) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.193 (0.222) data 0.000 (0.038) loss 0.3922 (0.5174) acc 91.5254 (89.2016) lr 3.1417e-05 eta 0:00:25
epoch [48/50] batch [45/51] time 0.175 (0.217) data 0.000 (0.034) loss 0.4320 (0.5167) acc 91.3462 (89.1592) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.183 (0.213) data 0.001 (0.030) loss 0.3118 (0.5011) acc 93.8679 (89.5925) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.127  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.34 <<<
epoch [49/50] batch [5/51] time 0.177 (0.521) data 0.000 (0.335) loss 0.5002 (0.5027) acc 89.9038 (88.1332) lr 1.7713e-05 eta 0:00:50
epoch [49/50] batch [10/51] time 0.206 (0.357) data 0.000 (0.168) loss 0.4023 (0.4771) acc 92.9245 (89.2786) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [15/51] time 0.182 (0.300) data 0.000 (0.112) loss 0.5745 (0.5194) acc 84.2593 (88.3642) lr 1.7713e-05 eta 0:00:26
epoch [49/50] batch [20/51] time 0.191 (0.274) data 0.001 (0.084) loss 0.3988 (0.5011) acc 90.3509 (89.2850) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [25/51] time 0.175 (0.255) data 0.000 (0.067) loss 0.5955 (0.5074) acc 86.9792 (88.7621) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [30/51] time 0.191 (0.243) data 0.000 (0.056) loss 0.5184 (0.5009) acc 91.5094 (89.0001) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [35/51] time 0.176 (0.235) data 0.000 (0.048) loss 0.6805 (0.5064) acc 94.8980 (89.2324) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [40/51] time 0.167 (0.228) data 0.000 (0.042) loss 0.6482 (0.5130) acc 89.7959 (89.0011) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [45/51] time 0.174 (0.221) data 0.000 (0.038) loss 0.4726 (0.5179) acc 90.8654 (88.8139) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.177 (0.217) data 0.000 (0.034) loss 0.4331 (0.5116) acc 88.2653 (88.9600) lr 1.7713e-05 eta 0:00:11
>>> alpha1: 0.127  alpha2: 0.053 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.35 <<<
epoch [50/50] batch [5/51] time 0.200 (0.513) data 0.015 (0.326) loss 0.3944 (0.4420) acc 92.7885 (90.9402) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.191 (0.352) data 0.000 (0.163) loss 0.4080 (0.5049) acc 88.0000 (89.4603) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.196 (0.296) data 0.000 (0.109) loss 0.5167 (0.5156) acc 87.9808 (89.2265) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.201 (0.270) data 0.000 (0.082) loss 0.3924 (0.4895) acc 90.7895 (89.8753) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.170 (0.251) data 0.000 (0.065) loss 0.4888 (0.4976) acc 89.3617 (89.7211) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.187 (0.241) data 0.000 (0.055) loss 0.5035 (0.4986) acc 88.1818 (89.5900) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [35/51] time 0.197 (0.233) data 0.000 (0.047) loss 0.6335 (0.4987) acc 87.5000 (89.5508) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.196 (0.228) data 0.000 (0.041) loss 0.4374 (0.4916) acc 92.1296 (89.7026) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.180 (0.223) data 0.000 (0.036) loss 0.3644 (0.4865) acc 90.0943 (89.8418) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.187 (0.218) data 0.000 (0.033) loss 0.4581 (0.4958) acc 88.3929 (89.6516) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.37, 0.28, 0.27, 0.25, 0.24, 0.24, 0.23, 0.23, 0.23, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.22, 0.22, 0.21, 0.21, 0.21, 0.21, 0.21, 0.22, 0.21, 0.22, 0.22]
* matched noise rate: [0.18, 0.1, 0.15, 0.12, 0.12, 0.11, 0.1, 0.11, 0.12, 0.12, 0.12, 0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.12, 0.14, 0.13, 0.12, 0.13, 0.12, 0.12, 0.12, 0.12, 0.11, 0.11, 0.13, 0.13, 0.13, 0.13, 0.14, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14]
* unmatched noise rate: [0.67, 0.41, 0.49, 0.44, 0.44, 0.38, 0.38, 0.38, 0.39, 0.34, 0.35, 0.36, 0.35, 0.35, 0.35, 0.34, 0.35, 0.36, 0.34, 0.33, 0.34, 0.34, 0.33, 0.35, 0.35, 0.33, 0.34, 0.35, 0.34, 0.35, 0.35, 0.33, 0.33, 0.33, 0.32, 0.34, 0.34, 0.34, 0.34, 0.35]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:04,  2.70s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.32it/s] 16%|█▌        | 4/25 [00:02<00:11,  1.88it/s] 24%|██▍       | 6/25 [00:03<00:06,  3.16it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.57it/s] 40%|████      | 10/25 [00:03<00:02,  5.97it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.27it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.41it/s] 64%|██████▍   | 16/25 [00:03<00:00,  9.31it/s] 72%|███████▏  | 18/25 [00:04<00:00,  9.72it/s] 80%|████████  | 20/25 [00:04<00:00, 10.35it/s] 88%|████████▊ | 22/25 [00:04<00:00, 10.83it/s] 96%|█████████▌| 24/25 [00:04<00:00, 11.27it/s]100%|██████████| 25/25 [00:05<00:00,  4.84it/s]
=> result
* total: 2,463
* correct: 1,916
* accuracy: 77.8%
* error: 22.2%
* macro_f1: 71.8%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 11	acc: 91.7%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 6	acc: 33.3%
* class: 2 (canterbury bells)	total: 12	correct: 0	acc: 0.0%
* class: 3 (sweet pea)	total: 17	correct: 8	acc: 47.1%
* class: 4 (english marigold)	total: 20	correct: 9	acc: 45.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 25	acc: 96.2%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 20	acc: 76.9%
* class: 11 (colt's foot)	total: 26	correct: 26	acc: 100.0%
* class: 12 (king protea)	total: 15	correct: 12	acc: 80.0%
* class: 13 (spear thistle)	total: 14	correct: 13	acc: 92.9%
* class: 14 (yellow iris)	total: 15	correct: 13	acc: 86.7%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 17	acc: 68.0%
* class: 18 (balloon flower)	total: 15	correct: 11	acc: 73.3%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 3	acc: 25.0%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 13	acc: 100.0%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 0	acc: 0.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 20	acc: 76.9%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 0	acc: 0.0%
* class: 33 (mexican aster)	total: 12	correct: 7	acc: 58.3%
* class: 34 (alpine sea holly)	total: 12	correct: 12	acc: 100.0%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 21	acc: 95.5%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 17	acc: 85.0%
* class: 40 (barbeton daisy)	total: 38	correct: 23	acc: 60.5%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 54	acc: 91.5%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 22	acc: 78.6%
* class: 50 (petunia)	total: 77	correct: 69	acc: 89.6%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 15	acc: 53.6%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 0	acc: 0.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 33	acc: 100.0%
* class: 56 (gaura)	total: 20	correct: 19	acc: 95.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 19	acc: 95.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 13	acc: 81.2%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 28	acc: 90.3%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 11	acc: 84.6%
* class: 67 (bearded iris)	total: 16	correct: 5	acc: 31.2%
* class: 68 (windflower)	total: 16	correct: 15	acc: 93.8%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 23	acc: 100.0%
* class: 71 (azalea)	total: 29	correct: 24	acc: 82.8%
* class: 72 (water lily)	total: 58	correct: 55	acc: 94.8%
* class: 73 (rose)	total: 51	correct: 48	acc: 94.1%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 30	acc: 93.8%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 38	acc: 90.5%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 48	acc: 96.0%
* class: 81 (clematis)	total: 34	correct: 34	acc: 100.0%
* class: 82 (hibiscus)	total: 39	correct: 35	acc: 89.7%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 14	acc: 77.8%
* class: 85 (tree mallow)	total: 17	correct: 14	acc: 82.4%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 34	acc: 73.9%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 23	acc: 92.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 47	acc: 95.9%
* class: 94 (bougainvillea)	total: 38	correct: 31	acc: 81.6%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 18	acc: 72.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 10	acc: 58.8%
* class: 101 (blackberry lily)	total: 14	correct: 13	acc: 92.9%
* average: 76.3%
Elapsed: 0:28:44
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '10', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_10-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 10
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.275 (1.087) data 0.000 (0.346) loss 4.8517 (4.9603) acc 0.0000 (1.2500) lr 1.0000e-05 eta 0:46:05
epoch [1/50] batch [10/51] time 0.267 (0.680) data 0.000 (0.173) loss 4.7709 (4.8210) acc 6.2500 (2.8125) lr 1.0000e-05 eta 0:28:48
epoch [1/50] batch [15/51] time 0.267 (0.543) data 0.000 (0.115) loss 4.5451 (4.7190) acc 6.2500 (3.1250) lr 1.0000e-05 eta 0:22:55
epoch [1/50] batch [20/51] time 0.267 (0.474) data 0.000 (0.087) loss 4.6314 (4.6894) acc 3.1250 (3.2812) lr 1.0000e-05 eta 0:19:58
epoch [1/50] batch [25/51] time 0.264 (0.433) data 0.000 (0.069) loss 4.4345 (4.6886) acc 3.1250 (3.1250) lr 1.0000e-05 eta 0:18:14
epoch [1/50] batch [30/51] time 0.262 (0.406) data 0.000 (0.058) loss 4.3987 (4.6639) acc 6.2500 (3.4375) lr 1.0000e-05 eta 0:17:02
epoch [1/50] batch [35/51] time 0.261 (0.386) data 0.000 (0.050) loss 4.4033 (4.6464) acc 6.2500 (3.9286) lr 1.0000e-05 eta 0:16:11
epoch [1/50] batch [40/51] time 0.258 (0.371) data 0.000 (0.043) loss 4.4950 (4.6281) acc 3.1250 (3.8281) lr 1.0000e-05 eta 0:15:30
epoch [1/50] batch [45/51] time 0.262 (0.358) data 0.000 (0.039) loss 4.5694 (4.6154) acc 0.0000 (4.0972) lr 1.0000e-05 eta 0:14:57
epoch [1/50] batch [50/51] time 0.258 (0.349) data 0.000 (0.035) loss 4.7684 (4.6108) acc 3.1250 (4.3125) lr 1.0000e-05 eta 0:14:31
epoch [2/50] batch [5/51] time 0.270 (0.552) data 0.000 (0.274) loss 4.4980 (4.8271) acc 3.1250 (5.6250) lr 2.0000e-03 eta 0:22:55
epoch [2/50] batch [10/51] time 0.262 (0.409) data 0.000 (0.137) loss 4.0552 (4.5092) acc 15.6250 (11.8750) lr 2.0000e-03 eta 0:16:57
epoch [2/50] batch [15/51] time 0.265 (0.361) data 0.000 (0.091) loss 4.3611 (4.4474) acc 18.7500 (13.5417) lr 2.0000e-03 eta 0:14:56
epoch [2/50] batch [20/51] time 0.271 (0.337) data 0.000 (0.069) loss 4.5992 (4.4662) acc 6.2500 (11.8750) lr 2.0000e-03 eta 0:13:55
epoch [2/50] batch [25/51] time 0.260 (0.322) data 0.000 (0.055) loss 4.3524 (4.4455) acc 21.8750 (12.2500) lr 2.0000e-03 eta 0:13:16
epoch [2/50] batch [30/51] time 0.263 (0.314) data 0.000 (0.046) loss 4.4590 (4.4532) acc 9.3750 (11.8750) lr 2.0000e-03 eta 0:12:55
epoch [2/50] batch [35/51] time 0.274 (0.308) data 0.000 (0.039) loss 4.2850 (4.4339) acc 15.6250 (11.6964) lr 2.0000e-03 eta 0:12:38
epoch [2/50] batch [40/51] time 0.260 (0.302) data 0.000 (0.034) loss 4.9195 (4.4389) acc 9.3750 (11.9531) lr 2.0000e-03 eta 0:12:22
epoch [2/50] batch [45/51] time 0.263 (0.297) data 0.000 (0.031) loss 4.4965 (4.4290) acc 12.5000 (12.2917) lr 2.0000e-03 eta 0:12:09
epoch [2/50] batch [50/51] time 0.261 (0.294) data 0.000 (0.028) loss 4.6119 (4.4183) acc 18.7500 (12.8125) lr 2.0000e-03 eta 0:11:59
epoch [3/50] batch [5/51] time 0.282 (0.596) data 0.000 (0.316) loss 4.1918 (4.2053) acc 25.0000 (19.3750) lr 1.9980e-03 eta 0:24:15
epoch [3/50] batch [10/51] time 0.261 (0.429) data 0.000 (0.158) loss 4.2832 (4.2853) acc 15.6250 (17.5000) lr 1.9980e-03 eta 0:17:25
epoch [3/50] batch [15/51] time 0.268 (0.376) data 0.000 (0.106) loss 4.3887 (4.2962) acc 21.8750 (17.2917) lr 1.9980e-03 eta 0:15:13
epoch [3/50] batch [20/51] time 0.261 (0.349) data 0.000 (0.079) loss 4.4955 (4.2918) acc 9.3750 (17.6562) lr 1.9980e-03 eta 0:14:06
epoch [3/50] batch [25/51] time 0.261 (0.332) data 0.000 (0.063) loss 4.0731 (4.2989) acc 21.8750 (17.3750) lr 1.9980e-03 eta 0:13:25
epoch [3/50] batch [30/51] time 0.272 (0.322) data 0.000 (0.053) loss 4.4274 (4.3138) acc 12.5000 (17.0833) lr 1.9980e-03 eta 0:12:58
epoch [3/50] batch [35/51] time 0.270 (0.314) data 0.000 (0.045) loss 4.0189 (4.3026) acc 28.1250 (17.2321) lr 1.9980e-03 eta 0:12:36
epoch [3/50] batch [40/51] time 0.258 (0.307) data 0.000 (0.040) loss 4.5250 (4.2949) acc 15.6250 (17.5781) lr 1.9980e-03 eta 0:12:19
epoch [3/50] batch [45/51] time 0.262 (0.302) data 0.000 (0.035) loss 4.4606 (4.2840) acc 9.3750 (17.8472) lr 1.9980e-03 eta 0:12:06
epoch [3/50] batch [50/51] time 0.262 (0.298) data 0.000 (0.032) loss 4.4689 (4.2833) acc 6.2500 (17.5625) lr 1.9980e-03 eta 0:11:55
epoch [4/50] batch [5/51] time 0.286 (0.580) data 0.000 (0.286) loss 4.0377 (4.0391) acc 31.2500 (25.0000) lr 1.9921e-03 eta 0:23:08
epoch [4/50] batch [10/51] time 0.271 (0.425) data 0.000 (0.143) loss 4.1206 (4.1768) acc 18.7500 (21.8750) lr 1.9921e-03 eta 0:16:53
epoch [4/50] batch [15/51] time 0.271 (0.373) data 0.000 (0.095) loss 3.7193 (4.1839) acc 34.3750 (20.8333) lr 1.9921e-03 eta 0:14:47
epoch [4/50] batch [20/51] time 0.277 (0.348) data 0.000 (0.072) loss 4.1895 (4.1857) acc 15.6250 (20.6250) lr 1.9921e-03 eta 0:13:46
epoch [4/50] batch [25/51] time 0.272 (0.332) data 0.000 (0.057) loss 4.3770 (4.2218) acc 12.5000 (19.8750) lr 1.9921e-03 eta 0:13:06
epoch [4/50] batch [30/51] time 0.261 (0.320) data 0.000 (0.048) loss 4.0839 (4.2281) acc 28.1250 (20.0000) lr 1.9921e-03 eta 0:12:38
epoch [4/50] batch [35/51] time 0.269 (0.312) data 0.000 (0.041) loss 4.4384 (4.2445) acc 15.6250 (19.5536) lr 1.9921e-03 eta 0:12:17
epoch [4/50] batch [40/51] time 0.260 (0.306) data 0.000 (0.036) loss 4.5809 (4.2609) acc 18.7500 (18.8281) lr 1.9921e-03 eta 0:12:00
epoch [4/50] batch [45/51] time 0.260 (0.301) data 0.000 (0.032) loss 4.0757 (4.2627) acc 31.2500 (18.8889) lr 1.9921e-03 eta 0:11:47
epoch [4/50] batch [50/51] time 0.259 (0.297) data 0.000 (0.029) loss 4.2030 (4.2516) acc 25.0000 (19.3125) lr 1.9921e-03 eta 0:11:36
epoch [5/50] batch [5/51] time 0.271 (0.593) data 0.000 (0.301) loss 4.2252 (4.1797) acc 28.1250 (21.2500) lr 1.9823e-03 eta 0:23:08
epoch [5/50] batch [10/51] time 0.264 (0.431) data 0.000 (0.151) loss 4.0004 (4.2004) acc 31.2500 (23.1250) lr 1.9823e-03 eta 0:16:45
epoch [5/50] batch [15/51] time 0.276 (0.376) data 0.000 (0.100) loss 4.2298 (4.2117) acc 9.3750 (21.4583) lr 1.9823e-03 eta 0:14:37
epoch [5/50] batch [20/51] time 0.278 (0.350) data 0.000 (0.075) loss 4.6078 (4.2116) acc 9.3750 (19.6875) lr 1.9823e-03 eta 0:13:33
epoch [5/50] batch [25/51] time 0.263 (0.333) data 0.000 (0.060) loss 3.9934 (4.1833) acc 18.7500 (20.0000) lr 1.9823e-03 eta 0:12:53
epoch [5/50] batch [30/51] time 0.264 (0.322) data 0.000 (0.050) loss 4.3536 (4.1686) acc 21.8750 (21.1458) lr 1.9823e-03 eta 0:12:25
epoch [5/50] batch [35/51] time 0.265 (0.314) data 0.000 (0.043) loss 4.0525 (4.2070) acc 25.0000 (20.5357) lr 1.9823e-03 eta 0:12:06
epoch [5/50] batch [40/51] time 0.264 (0.308) data 0.000 (0.038) loss 3.9122 (4.2072) acc 21.8750 (20.5469) lr 1.9823e-03 eta 0:11:50
epoch [5/50] batch [45/51] time 0.262 (0.303) data 0.000 (0.034) loss 4.6554 (4.2247) acc 21.8750 (20.9722) lr 1.9823e-03 eta 0:11:37
epoch [5/50] batch [50/51] time 0.260 (0.299) data 0.000 (0.030) loss 4.3763 (4.2271) acc 15.6250 (21.0625) lr 1.9823e-03 eta 0:11:26
epoch [6/50] batch [5/51] time 0.281 (0.603) data 0.000 (0.318) loss 4.0884 (4.1921) acc 21.8750 (21.8750) lr 1.9686e-03 eta 0:23:00
epoch [6/50] batch [10/51] time 0.260 (0.436) data 0.000 (0.159) loss 4.3433 (4.2764) acc 12.5000 (18.4375) lr 1.9686e-03 eta 0:16:36
epoch [6/50] batch [15/51] time 0.261 (0.379) data 0.000 (0.106) loss 4.2343 (4.2393) acc 28.1250 (19.5833) lr 1.9686e-03 eta 0:14:23
epoch [6/50] batch [20/51] time 0.262 (0.350) data 0.000 (0.080) loss 4.4823 (4.2397) acc 9.3750 (18.7500) lr 1.9686e-03 eta 0:13:15
epoch [6/50] batch [25/51] time 0.272 (0.334) data 0.000 (0.064) loss 4.3494 (4.2366) acc 15.6250 (19.5000) lr 1.9686e-03 eta 0:12:39
epoch [6/50] batch [30/51] time 0.262 (0.323) data 0.000 (0.054) loss 4.1275 (4.2306) acc 25.0000 (20.3125) lr 1.9686e-03 eta 0:12:12
epoch [6/50] batch [35/51] time 0.267 (0.315) data 0.000 (0.046) loss 4.2561 (4.2170) acc 18.7500 (20.7143) lr 1.9686e-03 eta 0:11:52
epoch [6/50] batch [40/51] time 0.261 (0.309) data 0.000 (0.040) loss 3.9981 (4.2025) acc 21.8750 (21.0938) lr 1.9686e-03 eta 0:11:36
epoch [6/50] batch [45/51] time 0.265 (0.304) data 0.000 (0.036) loss 4.4583 (4.2131) acc 9.3750 (20.9028) lr 1.9686e-03 eta 0:11:23
epoch [6/50] batch [50/51] time 0.264 (0.300) data 0.000 (0.032) loss 4.0889 (4.2075) acc 25.0000 (21.0000) lr 1.9686e-03 eta 0:11:12
epoch [7/50] batch [5/51] time 0.268 (0.583) data 0.000 (0.294) loss 4.4584 (4.2561) acc 12.5000 (16.2500) lr 1.9511e-03 eta 0:21:45
epoch [7/50] batch [10/51] time 0.268 (0.426) data 0.000 (0.147) loss 3.9017 (4.1665) acc 37.5000 (20.9375) lr 1.9511e-03 eta 0:15:50
epoch [7/50] batch [15/51] time 0.261 (0.372) data 0.000 (0.098) loss 4.2058 (4.2083) acc 21.8750 (21.2500) lr 1.9511e-03 eta 0:13:49
epoch [7/50] batch [20/51] time 0.264 (0.345) data 0.000 (0.074) loss 4.2506 (4.2190) acc 21.8750 (21.4062) lr 1.9511e-03 eta 0:12:46
epoch [7/50] batch [25/51] time 0.260 (0.329) data 0.000 (0.059) loss 4.1513 (4.2331) acc 21.8750 (21.1250) lr 1.9511e-03 eta 0:12:09
epoch [7/50] batch [30/51] time 0.259 (0.318) data 0.000 (0.049) loss 4.2336 (4.2249) acc 21.8750 (20.7292) lr 1.9511e-03 eta 0:11:43
epoch [7/50] batch [35/51] time 0.269 (0.310) data 0.000 (0.042) loss 3.9955 (4.2005) acc 25.0000 (20.8036) lr 1.9511e-03 eta 0:11:25
epoch [7/50] batch [40/51] time 0.260 (0.304) data 0.000 (0.037) loss 4.6595 (4.2099) acc 12.5000 (21.0938) lr 1.9511e-03 eta 0:11:10
epoch [7/50] batch [45/51] time 0.265 (0.300) data 0.000 (0.033) loss 4.0282 (4.1811) acc 15.6250 (21.3889) lr 1.9511e-03 eta 0:10:58
epoch [7/50] batch [50/51] time 0.260 (0.296) data 0.000 (0.030) loss 3.9832 (4.1777) acc 37.5000 (21.9375) lr 1.9511e-03 eta 0:10:48
epoch [8/50] batch [5/51] time 0.282 (0.617) data 0.000 (0.326) loss 4.1177 (4.2622) acc 25.0000 (22.5000) lr 1.9298e-03 eta 0:22:29
epoch [8/50] batch [10/51] time 0.264 (0.442) data 0.000 (0.163) loss 3.9243 (4.2080) acc 31.2500 (23.4375) lr 1.9298e-03 eta 0:16:04
epoch [8/50] batch [15/51] time 0.271 (0.383) data 0.000 (0.109) loss 3.8604 (4.1875) acc 21.8750 (22.2917) lr 1.9298e-03 eta 0:13:53
epoch [8/50] batch [20/51] time 0.260 (0.353) data 0.000 (0.082) loss 3.9283 (4.1898) acc 31.2500 (21.4062) lr 1.9298e-03 eta 0:12:46
epoch [8/50] batch [25/51] time 0.277 (0.336) data 0.000 (0.065) loss 4.0301 (4.1898) acc 31.2500 (21.7500) lr 1.9298e-03 eta 0:12:08
epoch [8/50] batch [30/51] time 0.269 (0.325) data 0.000 (0.055) loss 4.1636 (4.1778) acc 25.0000 (21.7708) lr 1.9298e-03 eta 0:11:42
epoch [8/50] batch [35/51] time 0.263 (0.317) data 0.000 (0.047) loss 3.9355 (4.1528) acc 21.8750 (21.9643) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [40/51] time 0.260 (0.310) data 0.000 (0.041) loss 4.5162 (4.1618) acc 6.2500 (21.5625) lr 1.9298e-03 eta 0:11:07
epoch [8/50] batch [45/51] time 0.258 (0.304) data 0.000 (0.036) loss 4.3046 (4.1539) acc 12.5000 (21.4583) lr 1.9298e-03 eta 0:10:53
epoch [8/50] batch [50/51] time 0.259 (0.300) data 0.000 (0.033) loss 4.0682 (4.1535) acc 28.1250 (21.3125) lr 1.9298e-03 eta 0:10:42
epoch [9/50] batch [5/51] time 0.284 (0.609) data 0.000 (0.325) loss 3.8551 (4.1405) acc 28.1250 (21.8750) lr 1.9048e-03 eta 0:21:42
epoch [9/50] batch [10/51] time 0.269 (0.439) data 0.000 (0.163) loss 4.1249 (4.1665) acc 21.8750 (23.7500) lr 1.9048e-03 eta 0:15:36
epoch [9/50] batch [15/51] time 0.287 (0.383) data 0.000 (0.108) loss 3.9427 (4.1450) acc 18.7500 (23.5417) lr 1.9048e-03 eta 0:13:34
epoch [9/50] batch [20/51] time 0.262 (0.356) data 0.000 (0.081) loss 3.7141 (4.1320) acc 43.7500 (23.9062) lr 1.9048e-03 eta 0:12:34
epoch [9/50] batch [25/51] time 0.261 (0.339) data 0.000 (0.065) loss 4.1783 (4.1273) acc 25.0000 (24.1250) lr 1.9048e-03 eta 0:11:56
epoch [9/50] batch [30/51] time 0.267 (0.326) data 0.000 (0.054) loss 3.7248 (4.1495) acc 25.0000 (23.1250) lr 1.9048e-03 eta 0:11:28
epoch [9/50] batch [35/51] time 0.261 (0.317) data 0.000 (0.047) loss 4.0278 (4.1427) acc 31.2500 (23.1250) lr 1.9048e-03 eta 0:11:08
epoch [9/50] batch [40/51] time 0.261 (0.310) data 0.000 (0.041) loss 4.1937 (4.1475) acc 15.6250 (22.9688) lr 1.9048e-03 eta 0:10:52
epoch [9/50] batch [45/51] time 0.259 (0.305) data 0.000 (0.036) loss 4.3267 (4.1536) acc 21.8750 (22.6389) lr 1.9048e-03 eta 0:10:38
epoch [9/50] batch [50/51] time 0.261 (0.300) data 0.000 (0.033) loss 3.8643 (4.1494) acc 37.5000 (23.0000) lr 1.9048e-03 eta 0:10:27
epoch [10/50] batch [5/51] time 0.262 (0.556) data 0.000 (0.275) loss 4.2739 (4.0584) acc 18.7500 (25.0000) lr 1.8763e-03 eta 0:19:19
epoch [10/50] batch [10/51] time 0.278 (0.412) data 0.000 (0.138) loss 3.9741 (3.9602) acc 21.8750 (26.8750) lr 1.8763e-03 eta 0:14:17
epoch [10/50] batch [15/51] time 0.261 (0.364) data 0.000 (0.092) loss 4.3376 (4.1128) acc 18.7500 (23.7500) lr 1.8763e-03 eta 0:12:35
epoch [10/50] batch [20/51] time 0.273 (0.340) data 0.000 (0.069) loss 3.9110 (4.0963) acc 31.2500 (23.9062) lr 1.8763e-03 eta 0:11:43
epoch [10/50] batch [25/51] time 0.260 (0.325) data 0.000 (0.055) loss 4.0147 (4.0602) acc 21.8750 (24.3750) lr 1.8763e-03 eta 0:11:10
epoch [10/50] batch [30/51] time 0.284 (0.315) data 0.000 (0.046) loss 3.9707 (4.0599) acc 31.2500 (23.9583) lr 1.8763e-03 eta 0:10:49
epoch [10/50] batch [35/51] time 0.274 (0.308) data 0.000 (0.039) loss 3.8291 (4.0700) acc 28.1250 (23.8393) lr 1.8763e-03 eta 0:10:33
epoch [10/50] batch [40/51] time 0.260 (0.303) data 0.000 (0.035) loss 4.6795 (4.0898) acc 15.6250 (23.6719) lr 1.8763e-03 eta 0:10:20
epoch [10/50] batch [45/51] time 0.257 (0.298) data 0.000 (0.031) loss 4.1489 (4.0983) acc 21.8750 (23.4028) lr 1.8763e-03 eta 0:10:09
epoch [10/50] batch [50/51] time 0.260 (0.294) data 0.000 (0.028) loss 4.2400 (4.1169) acc 15.6250 (22.8750) lr 1.8763e-03 eta 0:09:59
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.758  alpha2: 0.399 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.36 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.52 <<<
epoch [11/50] batch [5/51] time 0.780 (1.075) data 0.000 (0.286) loss 2.7703 (3.0272) acc 61.3636 (59.7914) lr 1.8443e-03 eta 0:36:28
epoch [11/50] batch [10/51] time 0.781 (0.882) data 0.000 (0.143) loss 3.2240 (2.9678) acc 65.2174 (59.9625) lr 1.8443e-03 eta 0:29:50
epoch [11/50] batch [15/51] time 0.168 (0.680) data 0.001 (0.095) loss 2.6788 (2.9782) acc 57.4468 (58.9372) lr 1.8443e-03 eta 0:22:56
epoch [11/50] batch [20/51] time 0.167 (0.644) data 0.000 (0.072) loss 2.2959 (2.8634) acc 78.0612 (61.0647) lr 1.8443e-03 eta 0:21:41
epoch [11/50] batch [25/51] time 0.159 (0.547) data 0.000 (0.057) loss 2.8597 (2.8335) acc 51.1111 (60.0758) lr 1.8443e-03 eta 0:18:21
epoch [11/50] batch [30/51] time 0.163 (0.483) data 0.000 (0.048) loss 2.8777 (2.8248) acc 52.8409 (59.7100) lr 1.8443e-03 eta 0:16:10
epoch [11/50] batch [35/51] time 0.159 (0.437) data 0.000 (0.041) loss 2.7263 (2.8324) acc 60.0000 (59.8900) lr 1.8443e-03 eta 0:14:35
epoch [11/50] batch [40/51] time 0.158 (0.403) data 0.000 (0.036) loss 2.5388 (2.7921) acc 63.3333 (60.8889) lr 1.8443e-03 eta 0:13:25
epoch [11/50] batch [45/51] time 0.155 (0.375) data 0.000 (0.032) loss 3.1512 (2.7965) acc 58.9286 (60.4172) lr 1.8443e-03 eta 0:12:28
epoch [11/50] batch [50/51] time 0.152 (0.354) data 0.000 (0.029) loss 2.7530 (2.7803) acc 52.9762 (60.5904) lr 1.8443e-03 eta 0:11:43
>>> alpha1: 0.692  alpha2: 0.349 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.44 <<<
epoch [12/50] batch [5/51] time 0.174 (0.421) data 0.000 (0.250) loss 1.4730 (1.7081) acc 70.4082 (71.2869) lr 1.8090e-03 eta 0:13:54
epoch [12/50] batch [10/51] time 0.176 (0.298) data 0.000 (0.125) loss 1.5455 (1.6926) acc 72.0000 (71.0357) lr 1.8090e-03 eta 0:09:50
epoch [12/50] batch [15/51] time 0.156 (0.252) data 0.000 (0.084) loss 1.6059 (1.6566) acc 62.5000 (70.0156) lr 1.8090e-03 eta 0:08:17
epoch [12/50] batch [20/51] time 0.175 (0.231) data 0.000 (0.063) loss 1.7327 (1.6714) acc 68.8775 (69.1228) lr 1.8090e-03 eta 0:07:33
epoch [12/50] batch [25/51] time 0.171 (0.219) data 0.000 (0.050) loss 1.6562 (1.6740) acc 63.5417 (68.5937) lr 1.8090e-03 eta 0:07:09
epoch [12/50] batch [30/51] time 0.180 (0.209) data 0.000 (0.042) loss 2.0897 (1.6892) acc 61.9565 (68.2625) lr 1.8090e-03 eta 0:06:49
epoch [12/50] batch [35/51] time 0.179 (0.203) data 0.000 (0.036) loss 1.6897 (1.6592) acc 70.4545 (69.0907) lr 1.8090e-03 eta 0:06:37
epoch [12/50] batch [40/51] time 0.163 (0.199) data 0.000 (0.032) loss 1.7664 (1.6530) acc 73.2143 (69.1301) lr 1.8090e-03 eta 0:06:27
epoch [12/50] batch [45/51] time 0.169 (0.196) data 0.000 (0.028) loss 1.6989 (1.6338) acc 68.6170 (69.5671) lr 1.8090e-03 eta 0:06:20
epoch [12/50] batch [50/51] time 0.164 (0.193) data 0.000 (0.025) loss 1.3513 (1.6210) acc 73.9583 (69.5313) lr 1.8090e-03 eta 0:06:13
>>> alpha1: 0.608  alpha2: 0.295 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.43 <<<
epoch [13/50] batch [5/51] time 0.163 (0.517) data 0.000 (0.339) loss 1.4270 (1.2548) acc 65.4762 (72.8446) lr 1.7705e-03 eta 0:16:39
epoch [13/50] batch [10/51] time 0.177 (0.344) data 0.000 (0.170) loss 1.2914 (1.2635) acc 75.5208 (72.7304) lr 1.7705e-03 eta 0:11:03
epoch [13/50] batch [15/51] time 0.165 (0.286) data 0.000 (0.113) loss 1.0239 (1.1976) acc 81.5217 (74.4161) lr 1.7705e-03 eta 0:09:10
epoch [13/50] batch [20/51] time 0.169 (0.256) data 0.000 (0.085) loss 0.9783 (1.2417) acc 77.0408 (73.0802) lr 1.7705e-03 eta 0:08:11
epoch [13/50] batch [25/51] time 0.741 (0.261) data 0.001 (0.068) loss 2.2171 (1.2850) acc 50.6098 (71.9379) lr 1.7705e-03 eta 0:08:18
epoch [13/50] batch [30/51] time 0.164 (0.245) data 0.000 (0.057) loss 1.0647 (1.2683) acc 75.0000 (72.2603) lr 1.7705e-03 eta 0:07:46
epoch [13/50] batch [35/51] time 0.170 (0.233) data 0.000 (0.049) loss 1.2753 (1.2639) acc 74.5000 (72.3055) lr 1.7705e-03 eta 0:07:23
epoch [13/50] batch [40/51] time 0.159 (0.224) data 0.000 (0.043) loss 1.5493 (1.2782) acc 71.7391 (72.1438) lr 1.7705e-03 eta 0:07:05
epoch [13/50] batch [45/51] time 0.157 (0.218) data 0.000 (0.038) loss 1.1903 (1.2653) acc 80.5556 (72.6870) lr 1.7705e-03 eta 0:06:51
epoch [13/50] batch [50/51] time 0.158 (0.212) data 0.000 (0.034) loss 1.1503 (1.2671) acc 82.0652 (72.5304) lr 1.7705e-03 eta 0:06:39
>>> alpha1: 0.533  alpha2: 0.249 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.27 --> matched refined noisy rate: 0.10 & unmatched refined noisy rate: 0.42 <<<
epoch [14/50] batch [5/51] time 0.158 (0.495) data 0.000 (0.322) loss 1.2894 (1.1720) acc 68.7500 (71.1194) lr 1.7290e-03 eta 0:15:31
epoch [14/50] batch [10/51] time 0.181 (0.336) data 0.000 (0.161) loss 1.3779 (1.1441) acc 72.5610 (73.6820) lr 1.7290e-03 eta 0:10:30
epoch [14/50] batch [15/51] time 0.169 (0.279) data 0.000 (0.107) loss 1.1122 (1.1501) acc 70.2128 (73.2766) lr 1.7290e-03 eta 0:08:42
epoch [14/50] batch [20/51] time 0.162 (0.288) data 0.000 (0.081) loss 1.1933 (1.1004) acc 70.1087 (74.7844) lr 1.7290e-03 eta 0:08:58
epoch [14/50] batch [25/51] time 0.164 (0.264) data 0.000 (0.065) loss 0.9294 (1.1782) acc 82.0652 (74.0032) lr 1.7290e-03 eta 0:08:11
epoch [14/50] batch [30/51] time 0.191 (0.250) data 0.000 (0.054) loss 1.1108 (1.1517) acc 78.5714 (74.5171) lr 1.7290e-03 eta 0:07:43
epoch [14/50] batch [35/51] time 0.164 (0.238) data 0.000 (0.046) loss 1.3890 (1.1550) acc 67.4419 (74.3868) lr 1.7290e-03 eta 0:07:20
epoch [14/50] batch [40/51] time 0.159 (0.248) data 0.000 (0.040) loss 1.3074 (1.1416) acc 65.0000 (74.7589) lr 1.7290e-03 eta 0:07:38
epoch [14/50] batch [45/51] time 0.169 (0.239) data 0.000 (0.036) loss 0.9809 (1.1376) acc 79.0000 (74.8763) lr 1.7290e-03 eta 0:07:19
epoch [14/50] batch [50/51] time 0.159 (0.231) data 0.000 (0.032) loss 1.2427 (1.1364) acc 66.8478 (74.8731) lr 1.7290e-03 eta 0:07:05
>>> alpha1: 0.450  alpha2: 0.200 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.43 <<<
epoch [15/50] batch [5/51] time 0.162 (0.455) data 0.000 (0.282) loss 1.1233 (1.0014) acc 71.6667 (76.5723) lr 1.6845e-03 eta 0:13:52
epoch [15/50] batch [10/51] time 0.163 (0.310) data 0.000 (0.141) loss 1.1312 (1.0116) acc 73.9362 (77.7000) lr 1.6845e-03 eta 0:09:26
epoch [15/50] batch [15/51] time 0.174 (0.265) data 0.000 (0.094) loss 0.7845 (0.9935) acc 81.7308 (78.3954) lr 1.6845e-03 eta 0:08:02
epoch [15/50] batch [20/51] time 0.161 (0.318) data 0.000 (0.071) loss 1.3336 (0.9915) acc 67.5532 (77.7383) lr 1.6845e-03 eta 0:09:37
epoch [15/50] batch [25/51] time 0.187 (0.290) data 0.001 (0.057) loss 1.3424 (0.9977) acc 70.4082 (77.6618) lr 1.6845e-03 eta 0:08:44
epoch [15/50] batch [30/51] time 0.168 (0.270) data 0.000 (0.048) loss 1.0952 (0.9940) acc 76.0204 (77.7629) lr 1.6845e-03 eta 0:08:08
epoch [15/50] batch [35/51] time 0.169 (0.257) data 0.000 (0.041) loss 1.0280 (0.9902) acc 82.1429 (78.1111) lr 1.6845e-03 eta 0:07:43
epoch [15/50] batch [40/51] time 0.166 (0.247) data 0.000 (0.036) loss 1.0123 (0.9913) acc 77.5510 (78.0701) lr 1.6845e-03 eta 0:07:23
epoch [15/50] batch [45/51] time 0.165 (0.238) data 0.000 (0.032) loss 0.8697 (0.9810) acc 75.5208 (78.2564) lr 1.6845e-03 eta 0:07:05
epoch [15/50] batch [50/51] time 0.165 (0.231) data 0.000 (0.029) loss 0.9712 (0.9707) acc 80.2083 (78.5015) lr 1.6845e-03 eta 0:06:51
>>> alpha1: 0.319  alpha2: 0.100 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.26 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [16/50] batch [5/51] time 0.182 (0.460) data 0.000 (0.283) loss 0.7404 (1.0451) acc 82.2115 (75.1982) lr 1.6374e-03 eta 0:13:39
epoch [16/50] batch [10/51] time 0.166 (0.311) data 0.000 (0.141) loss 1.0235 (0.9830) acc 74.4898 (77.3683) lr 1.6374e-03 eta 0:09:11
epoch [16/50] batch [15/51] time 0.165 (0.263) data 0.000 (0.094) loss 0.6264 (0.9026) acc 85.4167 (79.8571) lr 1.6374e-03 eta 0:07:44
epoch [16/50] batch [20/51] time 0.171 (0.240) data 0.000 (0.071) loss 0.9319 (0.8917) acc 81.3726 (79.7131) lr 1.6374e-03 eta 0:07:03
epoch [16/50] batch [25/51] time 0.175 (0.227) data 0.000 (0.057) loss 0.8749 (0.8879) acc 82.0000 (79.7640) lr 1.6374e-03 eta 0:06:39
epoch [16/50] batch [30/51] time 0.181 (0.220) data 0.000 (0.047) loss 0.7478 (0.8778) acc 84.4340 (79.8850) lr 1.6374e-03 eta 0:06:25
epoch [16/50] batch [35/51] time 0.173 (0.214) data 0.000 (0.041) loss 0.8251 (0.8827) acc 84.3750 (79.6447) lr 1.6374e-03 eta 0:06:14
epoch [16/50] batch [40/51] time 0.165 (0.208) data 0.000 (0.036) loss 0.7376 (0.8695) acc 87.5000 (79.8524) lr 1.6374e-03 eta 0:06:03
epoch [16/50] batch [45/51] time 0.164 (0.204) data 0.000 (0.032) loss 0.7846 (0.8669) acc 83.8542 (79.9536) lr 1.6374e-03 eta 0:05:54
epoch [16/50] batch [50/51] time 0.162 (0.211) data 0.000 (0.029) loss 1.0355 (0.8729) acc 72.8723 (79.7599) lr 1.6374e-03 eta 0:06:05
>>> alpha1: 0.229  alpha2: 0.034 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.43 <<<
epoch [17/50] batch [5/51] time 0.185 (0.455) data 0.000 (0.279) loss 0.7397 (0.7678) acc 85.0000 (84.5261) lr 1.5878e-03 eta 0:13:05
epoch [17/50] batch [10/51] time 0.173 (0.316) data 0.002 (0.140) loss 0.6756 (0.8226) acc 84.5000 (82.7617) lr 1.5878e-03 eta 0:09:04
epoch [17/50] batch [15/51] time 0.183 (0.271) data 0.000 (0.093) loss 0.7856 (0.8274) acc 81.6327 (82.3540) lr 1.5878e-03 eta 0:07:45
epoch [17/50] batch [20/51] time 0.178 (0.246) data 0.000 (0.070) loss 0.7250 (0.8274) acc 86.2245 (81.8975) lr 1.5878e-03 eta 0:07:02
epoch [17/50] batch [25/51] time 0.180 (0.233) data 0.000 (0.057) loss 0.9495 (0.8097) acc 75.5208 (81.7772) lr 1.5878e-03 eta 0:06:39
epoch [17/50] batch [30/51] time 0.182 (0.224) data 0.000 (0.047) loss 0.8872 (0.8156) acc 80.6122 (81.7474) lr 1.5878e-03 eta 0:06:21
epoch [17/50] batch [35/51] time 0.181 (0.218) data 0.001 (0.041) loss 0.7483 (0.8096) acc 82.2115 (81.8427) lr 1.5878e-03 eta 0:06:09
epoch [17/50] batch [40/51] time 0.150 (0.212) data 0.000 (0.036) loss 0.7768 (0.8057) acc 86.3095 (82.1386) lr 1.5878e-03 eta 0:05:58
epoch [17/50] batch [45/51] time 0.168 (0.207) data 0.000 (0.032) loss 0.7349 (0.8084) acc 79.5000 (81.8668) lr 1.5878e-03 eta 0:05:49
epoch [17/50] batch [50/51] time 0.170 (0.203) data 0.000 (0.028) loss 0.9832 (0.8217) acc 76.9608 (81.5665) lr 1.5878e-03 eta 0:05:42
>>> alpha1: 0.201  alpha2: 0.018 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [18/50] batch [5/51] time 0.164 (0.512) data 0.000 (0.335) loss 1.0257 (0.8766) acc 75.5556 (81.4790) lr 1.5358e-03 eta 0:14:19
epoch [18/50] batch [10/51] time 0.179 (0.341) data 0.000 (0.167) loss 0.8064 (0.8471) acc 81.0000 (82.4447) lr 1.5358e-03 eta 0:09:31
epoch [18/50] batch [15/51] time 0.162 (0.336) data 0.000 (0.112) loss 0.9445 (0.7849) acc 75.0000 (83.2208) lr 1.5358e-03 eta 0:09:20
epoch [18/50] batch [20/51] time 0.177 (0.294) data 0.000 (0.084) loss 0.7659 (0.8099) acc 82.3529 (82.8804) lr 1.5358e-03 eta 0:08:08
epoch [18/50] batch [25/51] time 0.169 (0.269) data 0.000 (0.067) loss 0.8419 (0.8205) acc 79.5000 (82.4288) lr 1.5358e-03 eta 0:07:26
epoch [18/50] batch [30/51] time 0.173 (0.254) data 0.000 (0.056) loss 1.0064 (0.8152) acc 72.0000 (82.2866) lr 1.5358e-03 eta 0:06:59
epoch [18/50] batch [35/51] time 0.181 (0.243) data 0.000 (0.048) loss 0.6573 (0.7988) acc 87.7273 (82.7158) lr 1.5358e-03 eta 0:06:41
epoch [18/50] batch [40/51] time 0.167 (0.235) data 0.000 (0.042) loss 0.8966 (0.8132) acc 83.5000 (82.4788) lr 1.5358e-03 eta 0:06:25
epoch [18/50] batch [45/51] time 0.163 (0.227) data 0.000 (0.037) loss 0.8277 (0.8031) acc 79.1667 (82.5413) lr 1.5358e-03 eta 0:06:11
epoch [18/50] batch [50/51] time 0.169 (0.221) data 0.000 (0.034) loss 0.7599 (0.8000) acc 83.8235 (82.7074) lr 1.5358e-03 eta 0:06:01
>>> alpha1: 0.189  alpha2: 0.017 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.42 <<<
epoch [19/50] batch [5/51] time 0.172 (0.466) data 0.000 (0.285) loss 0.6657 (0.6586) acc 82.8431 (84.3056) lr 1.4818e-03 eta 0:12:38
epoch [19/50] batch [10/51] time 0.187 (0.325) data 0.000 (0.144) loss 0.9160 (0.7774) acc 76.9608 (82.5675) lr 1.4818e-03 eta 0:08:46
epoch [19/50] batch [15/51] time 0.175 (0.274) data 0.000 (0.096) loss 2.7181 (0.8873) acc 51.0417 (81.0662) lr 1.4818e-03 eta 0:07:23
epoch [19/50] batch [20/51] time 0.192 (0.251) data 0.000 (0.072) loss 0.5538 (0.9205) acc 87.9808 (81.2517) lr 1.4818e-03 eta 0:06:44
epoch [19/50] batch [25/51] time 0.166 (0.234) data 0.000 (0.058) loss 0.8736 (0.8963) acc 79.5918 (81.2232) lr 1.4818e-03 eta 0:06:15
epoch [19/50] batch [30/51] time 0.173 (0.248) data 0.000 (0.048) loss 0.5371 (0.8774) acc 88.4615 (81.8522) lr 1.4818e-03 eta 0:06:37
epoch [19/50] batch [35/51] time 0.182 (0.238) data 0.000 (0.041) loss 0.6091 (0.8535) acc 86.8182 (82.4364) lr 1.4818e-03 eta 0:06:20
epoch [19/50] batch [40/51] time 0.156 (0.230) data 0.000 (0.036) loss 0.7516 (0.8397) acc 85.5556 (82.5029) lr 1.4818e-03 eta 0:06:06
epoch [19/50] batch [45/51] time 0.172 (0.224) data 0.000 (0.032) loss 0.8869 (0.8258) acc 78.8462 (82.7651) lr 1.4818e-03 eta 0:05:54
epoch [19/50] batch [50/51] time 0.163 (0.218) data 0.000 (0.029) loss 0.8954 (0.8259) acc 75.0000 (82.5347) lr 1.4818e-03 eta 0:05:44
>>> alpha1: 0.179  alpha2: 0.023 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.42 <<<
epoch [20/50] batch [5/51] time 0.191 (0.502) data 0.000 (0.324) loss 0.7096 (0.7843) acc 80.8824 (81.7717) lr 1.4258e-03 eta 0:13:10
epoch [20/50] batch [10/51] time 0.168 (0.337) data 0.000 (0.162) loss 0.6865 (0.7349) acc 85.5000 (83.2622) lr 1.4258e-03 eta 0:08:49
epoch [20/50] batch [15/51] time 0.168 (0.282) data 0.000 (0.108) loss 0.7512 (0.7284) acc 84.1837 (83.5761) lr 1.4258e-03 eta 0:07:21
epoch [20/50] batch [20/51] time 0.174 (0.253) data 0.001 (0.081) loss 0.9578 (0.7667) acc 82.6923 (82.4802) lr 1.4258e-03 eta 0:06:35
epoch [20/50] batch [25/51] time 0.177 (0.238) data 0.000 (0.065) loss 0.5589 (0.7542) acc 85.8491 (82.4385) lr 1.4258e-03 eta 0:06:10
epoch [20/50] batch [30/51] time 0.160 (0.227) data 0.000 (0.054) loss 0.6397 (0.7566) acc 85.3261 (82.3542) lr 1.4258e-03 eta 0:05:51
epoch [20/50] batch [35/51] time 0.191 (0.220) data 0.000 (0.047) loss 0.5904 (0.7617) acc 89.4231 (82.3959) lr 1.4258e-03 eta 0:05:39
epoch [20/50] batch [40/51] time 0.165 (0.214) data 0.000 (0.041) loss 0.8446 (0.7566) acc 81.1225 (82.4955) lr 1.4258e-03 eta 0:05:30
epoch [20/50] batch [45/51] time 0.172 (0.209) data 0.000 (0.036) loss 0.5847 (0.7386) acc 88.9423 (82.9615) lr 1.4258e-03 eta 0:05:21
epoch [20/50] batch [50/51] time 0.163 (0.205) data 0.000 (0.033) loss 0.6840 (0.7338) acc 84.3750 (82.9139) lr 1.4258e-03 eta 0:05:14
>>> alpha1: 0.170  alpha2: 0.025 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.42 <<<
epoch [21/50] batch [5/51] time 0.170 (0.539) data 0.001 (0.354) loss 0.6268 (0.7105) acc 88.2653 (85.9074) lr 1.3681e-03 eta 0:13:42
epoch [21/50] batch [10/51] time 0.187 (0.357) data 0.000 (0.177) loss 0.6432 (0.7149) acc 86.5385 (84.6141) lr 1.3681e-03 eta 0:09:02
epoch [21/50] batch [15/51] time 0.177 (0.295) data 0.000 (0.118) loss 0.7207 (0.7258) acc 87.5000 (84.0707) lr 1.3681e-03 eta 0:07:27
epoch [21/50] batch [20/51] time 0.168 (0.265) data 0.000 (0.089) loss 0.6641 (0.7308) acc 82.0000 (83.6788) lr 1.3681e-03 eta 0:06:40
epoch [21/50] batch [25/51] time 0.185 (0.249) data 0.000 (0.071) loss 0.5484 (0.7166) acc 88.4259 (84.1154) lr 1.3681e-03 eta 0:06:14
epoch [21/50] batch [30/51] time 0.188 (0.237) data 0.000 (0.059) loss 0.7486 (0.7189) acc 86.3208 (84.0180) lr 1.3681e-03 eta 0:05:55
epoch [21/50] batch [35/51] time 0.165 (0.228) data 0.000 (0.051) loss 0.5863 (0.7187) acc 89.5833 (84.0308) lr 1.3681e-03 eta 0:05:40
epoch [21/50] batch [40/51] time 0.167 (0.221) data 0.000 (0.044) loss 0.6171 (0.7141) acc 87.0000 (84.3105) lr 1.3681e-03 eta 0:05:29
epoch [21/50] batch [45/51] time 0.164 (0.215) data 0.000 (0.040) loss 0.7234 (0.7163) acc 83.6735 (84.1913) lr 1.3681e-03 eta 0:05:19
epoch [21/50] batch [50/51] time 0.174 (0.210) data 0.000 (0.036) loss 0.6131 (0.7137) acc 86.3208 (84.2238) lr 1.3681e-03 eta 0:05:10
>>> alpha1: 0.164  alpha2: 0.028 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.25 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.43 <<<
epoch [22/50] batch [5/51] time 0.197 (0.534) data 0.000 (0.337) loss 0.5090 (0.5218) acc 94.4444 (90.0007) lr 1.3090e-03 eta 0:13:07
epoch [22/50] batch [10/51] time 0.172 (0.356) data 0.000 (0.169) loss 0.8315 (0.6455) acc 79.6875 (85.6108) lr 1.3090e-03 eta 0:08:42
epoch [22/50] batch [15/51] time 0.203 (0.296) data 0.000 (0.113) loss 0.6123 (0.6590) acc 86.5741 (85.9366) lr 1.3090e-03 eta 0:07:13
epoch [22/50] batch [20/51] time 0.168 (0.265) data 0.000 (0.085) loss 0.6473 (0.6655) acc 90.5000 (85.8112) lr 1.3090e-03 eta 0:06:26
epoch [22/50] batch [25/51] time 0.159 (0.246) data 0.000 (0.068) loss 1.0106 (0.6787) acc 75.0000 (85.3711) lr 1.3090e-03 eta 0:05:57
epoch [22/50] batch [30/51] time 0.185 (0.235) data 0.015 (0.057) loss 0.7135 (0.6846) acc 83.5000 (85.0733) lr 1.3090e-03 eta 0:05:40
epoch [22/50] batch [35/51] time 0.164 (0.227) data 0.000 (0.049) loss 0.7728 (0.6871) acc 85.4167 (85.1303) lr 1.3090e-03 eta 0:05:27
epoch [22/50] batch [40/51] time 0.165 (0.220) data 0.000 (0.043) loss 0.7099 (0.6850) acc 85.2041 (85.0670) lr 1.3090e-03 eta 0:05:16
epoch [22/50] batch [45/51] time 0.171 (0.215) data 0.000 (0.038) loss 0.7219 (0.6839) acc 83.6538 (84.9559) lr 1.3090e-03 eta 0:05:07
epoch [22/50] batch [50/51] time 0.169 (0.210) data 0.000 (0.034) loss 0.6756 (0.6838) acc 86.7647 (84.8965) lr 1.3090e-03 eta 0:04:59
>>> alpha1: 0.161  alpha2: 0.031 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [23/50] batch [5/51] time 0.181 (0.505) data 0.000 (0.327) loss 0.6167 (0.6631) acc 87.0370 (85.5208) lr 1.2487e-03 eta 0:11:59
epoch [23/50] batch [10/51] time 0.168 (0.345) data 0.000 (0.164) loss 0.8249 (0.6717) acc 83.5000 (85.1579) lr 1.2487e-03 eta 0:08:09
epoch [23/50] batch [15/51] time 0.173 (0.288) data 0.000 (0.109) loss 0.5998 (0.8016) acc 84.8039 (83.6878) lr 1.2487e-03 eta 0:06:46
epoch [23/50] batch [20/51] time 0.171 (0.260) data 0.000 (0.082) loss 0.6876 (0.7662) acc 83.8235 (84.2692) lr 1.2487e-03 eta 0:06:05
epoch [23/50] batch [25/51] time 0.175 (0.244) data 0.000 (0.066) loss 0.6526 (0.7505) acc 87.2340 (84.6839) lr 1.2487e-03 eta 0:05:41
epoch [23/50] batch [30/51] time 0.169 (0.232) data 0.000 (0.055) loss 0.9343 (0.7450) acc 81.0000 (84.4396) lr 1.2487e-03 eta 0:05:24
epoch [23/50] batch [35/51] time 0.181 (0.224) data 0.000 (0.047) loss 0.8883 (0.7463) acc 82.2115 (84.2155) lr 1.2487e-03 eta 0:05:12
epoch [23/50] batch [40/51] time 0.176 (0.219) data 0.000 (0.041) loss 0.4994 (0.7292) acc 89.1509 (84.4393) lr 1.2487e-03 eta 0:05:03
epoch [23/50] batch [45/51] time 0.165 (0.213) data 0.000 (0.037) loss 0.8195 (0.7285) acc 79.5918 (84.5278) lr 1.2487e-03 eta 0:04:54
epoch [23/50] batch [50/51] time 0.169 (0.209) data 0.000 (0.033) loss 0.4542 (0.7195) acc 88.7255 (84.4418) lr 1.2487e-03 eta 0:04:48
>>> alpha1: 0.158  alpha2: 0.032 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [24/50] batch [5/51] time 0.191 (0.500) data 0.000 (0.327) loss 0.7271 (0.7285) acc 82.8431 (82.2127) lr 1.1874e-03 eta 0:11:26
epoch [24/50] batch [10/51] time 0.188 (0.336) data 0.000 (0.164) loss 0.4451 (0.6721) acc 91.2037 (83.5790) lr 1.1874e-03 eta 0:07:39
epoch [24/50] batch [15/51] time 0.165 (0.282) data 0.000 (0.109) loss 0.6854 (0.6619) acc 84.8958 (84.5189) lr 1.1874e-03 eta 0:06:24
epoch [24/50] batch [20/51] time 0.192 (0.258) data 0.000 (0.082) loss 0.6372 (0.6543) acc 84.9057 (84.8321) lr 1.1874e-03 eta 0:05:49
epoch [24/50] batch [25/51] time 0.173 (0.241) data 0.000 (0.066) loss 0.6582 (0.6472) acc 83.1731 (84.8292) lr 1.1874e-03 eta 0:05:25
epoch [24/50] batch [30/51] time 0.168 (0.230) data 0.000 (0.055) loss 0.7205 (0.6462) acc 85.5000 (85.2382) lr 1.1874e-03 eta 0:05:09
epoch [24/50] batch [35/51] time 0.194 (0.223) data 0.000 (0.047) loss 0.5457 (0.6456) acc 89.6226 (85.4689) lr 1.1874e-03 eta 0:04:59
epoch [24/50] batch [40/51] time 0.165 (0.217) data 0.000 (0.041) loss 0.5928 (0.6466) acc 88.2653 (85.3705) lr 1.1874e-03 eta 0:04:50
epoch [24/50] batch [45/51] time 0.167 (0.212) data 0.000 (0.037) loss 0.7159 (0.6377) acc 79.5000 (85.6223) lr 1.1874e-03 eta 0:04:42
epoch [24/50] batch [50/51] time 0.169 (0.208) data 0.000 (0.033) loss 0.6615 (0.6375) acc 86.2745 (85.8353) lr 1.1874e-03 eta 0:04:35
>>> alpha1: 0.155  alpha2: 0.032 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [25/50] batch [5/51] time 0.184 (0.467) data 0.000 (0.280) loss 0.5966 (0.6012) acc 84.2593 (86.4327) lr 1.1253e-03 eta 0:10:17
epoch [25/50] batch [10/51] time 0.173 (0.323) data 0.000 (0.140) loss 0.7234 (0.6399) acc 83.1731 (86.2359) lr 1.1253e-03 eta 0:07:04
epoch [25/50] batch [15/51] time 0.181 (0.273) data 0.000 (0.094) loss 0.4324 (0.6271) acc 94.8113 (86.8098) lr 1.1253e-03 eta 0:05:57
epoch [25/50] batch [20/51] time 0.185 (0.250) data 0.000 (0.070) loss 0.5036 (0.6248) acc 90.5000 (87.0829) lr 1.1253e-03 eta 0:05:27
epoch [25/50] batch [25/51] time 0.173 (0.235) data 0.000 (0.056) loss 0.7165 (0.6296) acc 86.5385 (87.0346) lr 1.1253e-03 eta 0:05:06
epoch [25/50] batch [30/51] time 0.181 (0.226) data 0.000 (0.047) loss 0.7767 (0.6393) acc 85.4546 (86.7727) lr 1.1253e-03 eta 0:04:52
epoch [25/50] batch [35/51] time 0.175 (0.219) data 0.000 (0.040) loss 0.6227 (0.6352) acc 87.5000 (86.6949) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [40/51] time 0.171 (0.213) data 0.000 (0.035) loss 0.4955 (0.6655) acc 87.5000 (86.3939) lr 1.1253e-03 eta 0:04:33
epoch [25/50] batch [45/51] time 0.171 (0.208) data 0.000 (0.031) loss 0.7751 (0.6611) acc 81.2500 (86.4472) lr 1.1253e-03 eta 0:04:26
epoch [25/50] batch [50/51] time 0.178 (0.204) data 0.000 (0.028) loss 0.4396 (0.6486) acc 91.8182 (86.6056) lr 1.1253e-03 eta 0:04:20
>>> alpha1: 0.154  alpha2: 0.034 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.41 <<<
epoch [26/50] batch [5/51] time 0.187 (0.431) data 0.001 (0.238) loss 0.6625 (0.5642) acc 87.7451 (85.7643) lr 1.0628e-03 eta 0:09:07
epoch [26/50] batch [10/51] time 0.192 (0.311) data 0.000 (0.119) loss 0.6965 (0.5877) acc 85.4546 (86.1741) lr 1.0628e-03 eta 0:06:33
epoch [26/50] batch [15/51] time 0.171 (0.266) data 0.000 (0.080) loss 0.6835 (0.6285) acc 86.1702 (85.0611) lr 1.0628e-03 eta 0:05:35
epoch [26/50] batch [20/51] time 0.177 (0.245) data 0.000 (0.060) loss 0.9186 (0.6297) acc 79.3269 (85.2202) lr 1.0628e-03 eta 0:05:07
epoch [26/50] batch [25/51] time 0.188 (0.233) data 0.000 (0.048) loss 0.5883 (0.6238) acc 89.1509 (85.9017) lr 1.0628e-03 eta 0:04:51
epoch [26/50] batch [30/51] time 0.196 (0.226) data 0.000 (0.040) loss 0.6775 (0.6109) acc 85.5000 (86.3436) lr 1.0628e-03 eta 0:04:41
epoch [26/50] batch [35/51] time 0.172 (0.219) data 0.000 (0.035) loss 0.7902 (0.6143) acc 80.0000 (86.3743) lr 1.0628e-03 eta 0:04:31
epoch [26/50] batch [40/51] time 0.164 (0.214) data 0.000 (0.030) loss 0.8981 (0.6227) acc 76.0417 (86.3245) lr 1.0628e-03 eta 0:04:24
epoch [26/50] batch [45/51] time 0.162 (0.209) data 0.000 (0.027) loss 0.5684 (0.6184) acc 89.3617 (86.3408) lr 1.0628e-03 eta 0:04:17
epoch [26/50] batch [50/51] time 0.174 (0.205) data 0.000 (0.024) loss 0.5484 (0.6127) acc 86.5385 (86.5640) lr 1.0628e-03 eta 0:04:11
>>> alpha1: 0.151  alpha2: 0.035 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [27/50] batch [5/51] time 0.161 (0.500) data 0.000 (0.317) loss 0.6595 (0.8118) acc 84.7826 (85.5361) lr 1.0000e-03 eta 0:10:09
epoch [27/50] batch [10/51] time 0.174 (0.340) data 0.000 (0.159) loss 0.5411 (0.7143) acc 87.0192 (85.6428) lr 1.0000e-03 eta 0:06:52
epoch [27/50] batch [15/51] time 0.193 (0.289) data 0.000 (0.106) loss 0.5282 (0.6658) acc 90.6863 (86.5669) lr 1.0000e-03 eta 0:05:48
epoch [27/50] batch [20/51] time 0.190 (0.261) data 0.000 (0.080) loss 0.4159 (0.6615) acc 86.5741 (85.9559) lr 1.0000e-03 eta 0:05:14
epoch [27/50] batch [25/51] time 0.184 (0.243) data 0.001 (0.064) loss 0.5593 (0.6432) acc 86.2745 (86.2453) lr 1.0000e-03 eta 0:04:51
epoch [27/50] batch [30/51] time 0.161 (0.232) data 0.000 (0.053) loss 0.6325 (0.6327) acc 82.9787 (86.2798) lr 1.0000e-03 eta 0:04:37
epoch [27/50] batch [35/51] time 0.181 (0.226) data 0.000 (0.046) loss 0.6374 (0.6243) acc 83.8235 (86.4238) lr 1.0000e-03 eta 0:04:28
epoch [27/50] batch [40/51] time 0.172 (0.219) data 0.000 (0.040) loss 0.4432 (0.6208) acc 91.3462 (86.4244) lr 1.0000e-03 eta 0:04:19
epoch [27/50] batch [45/51] time 0.166 (0.214) data 0.000 (0.035) loss 0.6002 (0.6272) acc 87.2449 (86.3849) lr 1.0000e-03 eta 0:04:11
epoch [27/50] batch [50/51] time 0.171 (0.210) data 0.000 (0.032) loss 0.5173 (0.6217) acc 87.7451 (86.3980) lr 1.0000e-03 eta 0:04:06
>>> alpha1: 0.151  alpha2: 0.042 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.11 & unmatched refined noisy rate: 0.39 <<<
epoch [28/50] batch [5/51] time 0.190 (0.467) data 0.000 (0.293) loss 0.6709 (0.5820) acc 87.0000 (88.3622) lr 9.3721e-04 eta 0:09:05
epoch [28/50] batch [10/51] time 0.172 (0.321) data 0.000 (0.147) loss 0.4569 (0.5530) acc 90.8163 (89.0906) lr 9.3721e-04 eta 0:06:12
epoch [28/50] batch [15/51] time 0.180 (0.274) data 0.001 (0.098) loss 0.5447 (0.5504) acc 88.1818 (88.9275) lr 9.3721e-04 eta 0:05:17
epoch [28/50] batch [20/51] time 0.188 (0.250) data 0.000 (0.073) loss 0.6048 (0.5592) acc 88.8889 (88.3138) lr 9.3721e-04 eta 0:04:48
epoch [28/50] batch [25/51] time 0.169 (0.234) data 0.000 (0.059) loss 0.4770 (0.5751) acc 90.6863 (87.6360) lr 9.3721e-04 eta 0:04:28
epoch [28/50] batch [30/51] time 0.186 (0.224) data 0.000 (0.049) loss 0.5426 (0.5710) acc 88.7931 (87.6750) lr 9.3721e-04 eta 0:04:16
epoch [28/50] batch [35/51] time 0.198 (0.217) data 0.000 (0.042) loss 0.5684 (0.5914) acc 90.0943 (87.1875) lr 9.3721e-04 eta 0:04:07
epoch [28/50] batch [40/51] time 0.165 (0.211) data 0.000 (0.037) loss 0.4876 (0.5829) acc 87.7551 (87.3355) lr 9.3721e-04 eta 0:03:58
epoch [28/50] batch [45/51] time 0.166 (0.206) data 0.000 (0.033) loss 0.6488 (0.5858) acc 84.6939 (87.2531) lr 9.3721e-04 eta 0:03:52
epoch [28/50] batch [50/51] time 0.171 (0.203) data 0.000 (0.029) loss 0.6386 (0.5835) acc 85.2941 (87.3971) lr 9.3721e-04 eta 0:03:47
>>> alpha1: 0.149  alpha2: 0.048 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.40 <<<
epoch [29/50] batch [5/51] time 0.165 (0.431) data 0.000 (0.252) loss 0.4722 (0.5304) acc 89.0625 (88.7113) lr 8.7467e-04 eta 0:08:01
epoch [29/50] batch [10/51] time 0.173 (0.301) data 0.000 (0.126) loss 0.6268 (0.5740) acc 85.0000 (87.4916) lr 8.7467e-04 eta 0:05:34
epoch [29/50] batch [15/51] time 0.181 (0.259) data 0.000 (0.084) loss 0.3475 (0.5663) acc 91.3636 (87.7443) lr 8.7467e-04 eta 0:04:46
epoch [29/50] batch [20/51] time 0.170 (0.240) data 0.000 (0.064) loss 0.7144 (0.5427) acc 85.7843 (88.5644) lr 8.7467e-04 eta 0:04:24
epoch [29/50] batch [25/51] time 0.188 (0.228) data 0.000 (0.051) loss 0.5943 (0.5394) acc 90.3846 (88.9434) lr 8.7467e-04 eta 0:04:09
epoch [29/50] batch [30/51] time 0.178 (0.219) data 0.000 (0.043) loss 0.6550 (0.5592) acc 81.9444 (88.4010) lr 8.7467e-04 eta 0:03:58
epoch [29/50] batch [35/51] time 0.197 (0.214) data 0.000 (0.037) loss 0.4120 (0.5558) acc 91.6667 (88.3962) lr 8.7467e-04 eta 0:03:52
epoch [29/50] batch [40/51] time 0.167 (0.208) data 0.000 (0.032) loss 0.6201 (0.5712) acc 90.5000 (88.2532) lr 8.7467e-04 eta 0:03:45
epoch [29/50] batch [45/51] time 0.172 (0.204) data 0.000 (0.029) loss 0.6247 (0.5751) acc 80.7692 (87.7615) lr 8.7467e-04 eta 0:03:40
epoch [29/50] batch [50/51] time 0.175 (0.200) data 0.000 (0.026) loss 0.6193 (0.5785) acc 87.7358 (87.5746) lr 8.7467e-04 eta 0:03:34
>>> alpha1: 0.147  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.39 <<<
epoch [30/50] batch [5/51] time 0.173 (0.484) data 0.000 (0.303) loss 0.4732 (0.5620) acc 89.2857 (87.5589) lr 8.1262e-04 eta 0:08:35
epoch [30/50] batch [10/51] time 0.169 (0.330) data 0.000 (0.152) loss 0.5047 (0.5497) acc 86.2245 (87.9105) lr 8.1262e-04 eta 0:05:49
epoch [30/50] batch [15/51] time 0.183 (0.279) data 0.000 (0.101) loss 0.4878 (0.6023) acc 91.5179 (86.5824) lr 8.1262e-04 eta 0:04:54
epoch [30/50] batch [20/51] time 0.165 (0.252) data 0.000 (0.076) loss 0.7571 (0.6043) acc 81.9149 (86.1957) lr 8.1262e-04 eta 0:04:24
epoch [30/50] batch [25/51] time 0.163 (0.236) data 0.000 (0.061) loss 0.8441 (0.6211) acc 82.9787 (85.7027) lr 8.1262e-04 eta 0:04:06
epoch [30/50] batch [30/51] time 0.178 (0.226) data 0.000 (0.051) loss 0.4234 (0.6026) acc 89.1509 (86.4040) lr 8.1262e-04 eta 0:03:55
epoch [30/50] batch [35/51] time 0.164 (0.219) data 0.000 (0.044) loss 0.6697 (0.6114) acc 85.9375 (86.2003) lr 8.1262e-04 eta 0:03:46
epoch [30/50] batch [40/51] time 0.185 (0.214) data 0.000 (0.038) loss 0.4784 (0.6013) acc 92.1053 (86.5274) lr 8.1262e-04 eta 0:03:40
epoch [30/50] batch [45/51] time 0.173 (0.209) data 0.000 (0.034) loss 0.4540 (0.5921) acc 91.3462 (86.7224) lr 8.1262e-04 eta 0:03:34
epoch [30/50] batch [50/51] time 0.162 (0.205) data 0.000 (0.031) loss 0.7645 (0.5958) acc 85.8696 (86.5018) lr 8.1262e-04 eta 0:03:29
>>> alpha1: 0.146  alpha2: 0.054 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.24 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.41 <<<
epoch [31/50] batch [5/51] time 0.193 (0.454) data 0.000 (0.262) loss 0.6254 (0.6113) acc 87.5000 (87.6214) lr 7.5131e-04 eta 0:07:41
epoch [31/50] batch [10/51] time 0.185 (0.320) data 0.000 (0.131) loss 0.4742 (0.5576) acc 90.1786 (88.7365) lr 7.5131e-04 eta 0:05:23
epoch [31/50] batch [15/51] time 0.176 (0.275) data 0.000 (0.088) loss 0.6996 (0.5456) acc 79.8077 (88.8432) lr 7.5131e-04 eta 0:04:36
epoch [31/50] batch [20/51] time 0.173 (0.250) data 0.000 (0.066) loss 0.5631 (0.5715) acc 92.2222 (88.3178) lr 7.5131e-04 eta 0:04:09
epoch [31/50] batch [25/51] time 0.166 (0.235) data 0.002 (0.053) loss 0.6889 (0.5798) acc 84.0425 (87.4647) lr 7.5131e-04 eta 0:03:54
epoch [31/50] batch [30/51] time 0.202 (0.226) data 0.000 (0.044) loss 0.5231 (0.5784) acc 84.7222 (87.0477) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [35/51] time 0.182 (0.220) data 0.000 (0.038) loss 0.5529 (0.5742) acc 88.4615 (87.0740) lr 7.5131e-04 eta 0:03:36
epoch [31/50] batch [40/51] time 0.163 (0.213) data 0.000 (0.033) loss 0.8729 (0.5834) acc 80.7292 (86.8504) lr 7.5131e-04 eta 0:03:29
epoch [31/50] batch [45/51] time 0.179 (0.226) data 0.000 (0.029) loss 0.5678 (0.5743) acc 87.5000 (87.0038) lr 7.5131e-04 eta 0:03:40
epoch [31/50] batch [50/51] time 0.167 (0.220) data 0.000 (0.026) loss 0.5279 (0.5762) acc 88.2653 (87.1748) lr 7.5131e-04 eta 0:03:33
>>> alpha1: 0.146  alpha2: 0.055 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [32/50] batch [5/51] time 0.175 (0.468) data 0.000 (0.282) loss 0.4236 (0.5211) acc 92.8571 (89.1868) lr 6.9098e-04 eta 0:07:30
epoch [32/50] batch [10/51] time 0.187 (0.327) data 0.000 (0.141) loss 0.4710 (0.5055) acc 87.9808 (88.3882) lr 6.9098e-04 eta 0:05:13
epoch [32/50] batch [15/51] time 0.169 (0.275) data 0.000 (0.094) loss 0.7040 (0.5371) acc 82.1429 (88.0133) lr 6.9098e-04 eta 0:04:22
epoch [32/50] batch [20/51] time 0.174 (0.251) data 0.000 (0.071) loss 0.5439 (0.5406) acc 88.7255 (88.0027) lr 6.9098e-04 eta 0:03:57
epoch [32/50] batch [25/51] time 0.164 (0.236) data 0.000 (0.057) loss 0.5072 (0.5527) acc 86.9792 (87.8671) lr 6.9098e-04 eta 0:03:42
epoch [32/50] batch [30/51] time 0.171 (0.226) data 0.000 (0.047) loss 0.7716 (0.5651) acc 86.2745 (88.0092) lr 6.9098e-04 eta 0:03:32
epoch [32/50] batch [35/51] time 0.170 (0.220) data 0.000 (0.040) loss 0.7077 (0.5675) acc 85.7843 (88.0456) lr 6.9098e-04 eta 0:03:25
epoch [32/50] batch [40/51] time 0.184 (0.215) data 0.000 (0.035) loss 0.3589 (0.5599) acc 94.6429 (88.2914) lr 6.9098e-04 eta 0:03:19
epoch [32/50] batch [45/51] time 0.170 (0.210) data 0.000 (0.032) loss 0.5574 (0.5556) acc 85.7843 (88.2577) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [50/51] time 0.184 (0.207) data 0.000 (0.028) loss 0.5765 (0.5631) acc 84.4340 (87.9795) lr 6.9098e-04 eta 0:03:09
>>> alpha1: 0.142  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.38 <<<
epoch [33/50] batch [5/51] time 0.167 (0.450) data 0.001 (0.271) loss 0.7475 (0.6050) acc 84.3750 (88.2579) lr 6.3188e-04 eta 0:06:50
epoch [33/50] batch [10/51] time 0.188 (0.313) data 0.001 (0.136) loss 0.6263 (0.6449) acc 87.5000 (86.5661) lr 6.3188e-04 eta 0:04:44
epoch [33/50] batch [15/51] time 0.194 (0.270) data 0.000 (0.090) loss 0.6285 (0.6193) acc 85.3774 (86.5253) lr 6.3188e-04 eta 0:04:03
epoch [33/50] batch [20/51] time 0.192 (0.248) data 0.000 (0.068) loss 0.3290 (0.5824) acc 93.7500 (87.2699) lr 6.3188e-04 eta 0:03:43
epoch [33/50] batch [25/51] time 0.199 (0.235) data 0.000 (0.054) loss 0.5609 (0.5685) acc 87.7193 (87.4593) lr 6.3188e-04 eta 0:03:29
epoch [33/50] batch [30/51] time 0.189 (0.227) data 0.000 (0.045) loss 0.6610 (0.5862) acc 87.2642 (87.1166) lr 6.3188e-04 eta 0:03:21
epoch [33/50] batch [35/51] time 0.203 (0.221) data 0.000 (0.039) loss 0.4814 (0.5818) acc 91.0377 (87.2463) lr 6.3188e-04 eta 0:03:14
epoch [33/50] batch [40/51] time 0.179 (0.216) data 0.000 (0.034) loss 0.6942 (0.5824) acc 87.7273 (87.2216) lr 6.3188e-04 eta 0:03:09
epoch [33/50] batch [45/51] time 0.178 (0.211) data 0.000 (0.030) loss 0.5997 (0.5801) acc 85.6481 (87.2052) lr 6.3188e-04 eta 0:03:04
epoch [33/50] batch [50/51] time 0.168 (0.208) data 0.000 (0.027) loss 0.5034 (0.5733) acc 90.5000 (87.4314) lr 6.3188e-04 eta 0:03:00
>>> alpha1: 0.139  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.40 <<<
epoch [34/50] batch [5/51] time 0.182 (0.486) data 0.000 (0.292) loss 0.4441 (0.5319) acc 90.2778 (88.4590) lr 5.7422e-04 eta 0:06:58
epoch [34/50] batch [10/51] time 0.168 (0.331) data 0.000 (0.146) loss 0.7265 (0.5651) acc 84.5000 (87.6526) lr 5.7422e-04 eta 0:04:43
epoch [34/50] batch [15/51] time 0.181 (0.282) data 0.000 (0.098) loss 0.7016 (0.5668) acc 87.5000 (87.6746) lr 5.7422e-04 eta 0:04:00
epoch [34/50] batch [20/51] time 0.182 (0.257) data 0.000 (0.073) loss 0.5198 (0.5538) acc 88.4259 (87.9817) lr 5.7422e-04 eta 0:03:37
epoch [34/50] batch [25/51] time 0.175 (0.241) data 0.000 (0.059) loss 0.7977 (0.5608) acc 85.6383 (87.9346) lr 5.7422e-04 eta 0:03:22
epoch [34/50] batch [30/51] time 0.186 (0.230) data 0.000 (0.049) loss 0.6041 (0.5612) acc 90.3846 (87.7989) lr 5.7422e-04 eta 0:03:12
epoch [34/50] batch [35/51] time 0.177 (0.224) data 0.000 (0.042) loss 0.7272 (0.5631) acc 83.1633 (87.7497) lr 5.7422e-04 eta 0:03:06
epoch [34/50] batch [40/51] time 0.170 (0.217) data 0.000 (0.037) loss 0.4728 (0.5604) acc 92.1569 (88.0118) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [45/51] time 0.160 (0.212) data 0.000 (0.033) loss 0.6484 (0.5565) acc 79.8913 (87.9236) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [50/51] time 0.168 (0.208) data 0.000 (0.029) loss 0.5539 (0.5534) acc 87.0000 (88.0208) lr 5.7422e-04 eta 0:02:49
>>> alpha1: 0.137  alpha2: 0.050 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.39 <<<
epoch [35/50] batch [5/51] time 0.166 (0.464) data 0.000 (0.286) loss 0.7346 (0.6201) acc 84.8958 (85.8908) lr 5.1825e-04 eta 0:06:16
epoch [35/50] batch [10/51] time 0.184 (0.323) data 0.001 (0.143) loss 0.3975 (0.5502) acc 94.5455 (88.2713) lr 5.1825e-04 eta 0:04:20
epoch [35/50] batch [15/51] time 0.198 (0.274) data 0.000 (0.096) loss 0.3863 (0.5243) acc 93.2692 (89.1592) lr 5.1825e-04 eta 0:03:39
epoch [35/50] batch [20/51] time 0.179 (0.250) data 0.000 (0.072) loss 0.3215 (0.5132) acc 92.4528 (89.1001) lr 5.1825e-04 eta 0:03:18
epoch [35/50] batch [25/51] time 0.198 (0.236) data 0.001 (0.057) loss 0.3907 (0.5170) acc 93.7500 (88.8407) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [30/51] time 0.184 (0.225) data 0.000 (0.048) loss 0.7129 (0.5383) acc 80.2885 (88.4223) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.199 (0.220) data 0.000 (0.041) loss 0.4686 (0.5379) acc 90.9091 (88.2998) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [40/51] time 0.171 (0.234) data 0.000 (0.036) loss 0.6439 (0.5359) acc 86.5000 (88.3290) lr 5.1825e-04 eta 0:03:01
epoch [35/50] batch [45/51] time 0.162 (0.228) data 0.000 (0.032) loss 0.4895 (0.5379) acc 89.8936 (88.3273) lr 5.1825e-04 eta 0:02:55
epoch [35/50] batch [50/51] time 0.170 (0.222) data 0.000 (0.029) loss 0.6186 (0.5341) acc 82.8431 (88.3250) lr 5.1825e-04 eta 0:02:50
>>> alpha1: 0.136  alpha2: 0.051 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.39 <<<
epoch [36/50] batch [5/51] time 0.177 (0.448) data 0.000 (0.273) loss 0.5650 (0.5725) acc 87.7358 (88.1519) lr 4.6417e-04 eta 0:05:40
epoch [36/50] batch [10/51] time 0.169 (0.312) data 0.001 (0.136) loss 0.4096 (0.5686) acc 91.5000 (88.7566) lr 4.6417e-04 eta 0:03:55
epoch [36/50] batch [15/51] time 0.177 (0.268) data 0.000 (0.091) loss 0.6141 (0.5440) acc 85.0962 (88.7569) lr 4.6417e-04 eta 0:03:21
epoch [36/50] batch [20/51] time 0.172 (0.246) data 0.000 (0.068) loss 0.6483 (0.5399) acc 88.2653 (88.8877) lr 4.6417e-04 eta 0:03:03
epoch [36/50] batch [25/51] time 0.170 (0.234) data 0.000 (0.055) loss 0.6946 (0.5552) acc 77.5510 (88.2056) lr 4.6417e-04 eta 0:02:53
epoch [36/50] batch [30/51] time 0.173 (0.225) data 0.000 (0.046) loss 0.6112 (0.5649) acc 85.2041 (87.7433) lr 4.6417e-04 eta 0:02:45
epoch [36/50] batch [35/51] time 0.171 (0.219) data 0.000 (0.039) loss 0.5090 (0.5486) acc 91.1765 (88.2219) lr 4.6417e-04 eta 0:02:39
epoch [36/50] batch [40/51] time 0.186 (0.214) data 0.000 (0.034) loss 0.3624 (0.5417) acc 92.6724 (88.4149) lr 4.6417e-04 eta 0:02:34
epoch [36/50] batch [45/51] time 0.178 (0.209) data 0.001 (0.031) loss 0.3979 (0.5518) acc 93.5185 (88.2291) lr 4.6417e-04 eta 0:02:30
epoch [36/50] batch [50/51] time 0.171 (0.205) data 0.000 (0.028) loss 0.4002 (0.5462) acc 89.2157 (88.3250) lr 4.6417e-04 eta 0:02:26
>>> alpha1: 0.133  alpha2: 0.049 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.38 <<<
epoch [37/50] batch [5/51] time 0.204 (0.496) data 0.001 (0.300) loss 0.5864 (0.4940) acc 85.7759 (88.9829) lr 4.1221e-04 eta 0:05:51
epoch [37/50] batch [10/51] time 0.194 (0.345) data 0.000 (0.150) loss 0.5042 (0.5237) acc 90.3846 (88.4312) lr 4.1221e-04 eta 0:04:02
epoch [37/50] batch [15/51] time 0.169 (0.287) data 0.000 (0.100) loss 0.5385 (0.5258) acc 91.0000 (88.3202) lr 4.1221e-04 eta 0:03:20
epoch [37/50] batch [20/51] time 0.180 (0.259) data 0.000 (0.075) loss 0.4157 (0.5131) acc 92.9245 (88.3124) lr 4.1221e-04 eta 0:02:59
epoch [37/50] batch [25/51] time 0.177 (0.244) data 0.000 (0.060) loss 0.4950 (0.5197) acc 93.0000 (88.1799) lr 4.1221e-04 eta 0:02:47
epoch [37/50] batch [30/51] time 0.170 (0.232) data 0.000 (0.050) loss 0.5677 (0.5213) acc 89.5000 (88.2511) lr 4.1221e-04 eta 0:02:38
epoch [37/50] batch [35/51] time 0.178 (0.224) data 0.000 (0.043) loss 0.6478 (0.5278) acc 81.7308 (87.9701) lr 4.1221e-04 eta 0:02:32
epoch [37/50] batch [40/51] time 0.174 (0.218) data 0.000 (0.038) loss 0.5570 (0.5302) acc 88.4615 (87.8705) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [45/51] time 0.174 (0.214) data 0.000 (0.034) loss 0.5521 (0.5228) acc 88.4615 (88.0980) lr 4.1221e-04 eta 0:02:23
epoch [37/50] batch [50/51] time 0.160 (0.210) data 0.000 (0.030) loss 0.7386 (0.5307) acc 86.6667 (88.0374) lr 4.1221e-04 eta 0:02:19
>>> alpha1: 0.134  alpha2: 0.050 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.39 <<<
epoch [38/50] batch [5/51] time 0.181 (0.472) data 0.000 (0.291) loss 0.6135 (0.5254) acc 89.9038 (89.4205) lr 3.6258e-04 eta 0:05:10
epoch [38/50] batch [10/51] time 0.172 (0.326) data 0.000 (0.146) loss 0.4612 (0.4945) acc 85.7843 (89.7898) lr 3.6258e-04 eta 0:03:32
epoch [38/50] batch [15/51] time 0.187 (0.275) data 0.000 (0.097) loss 0.5514 (0.5087) acc 86.1607 (89.2229) lr 3.6258e-04 eta 0:02:58
epoch [38/50] batch [20/51] time 0.178 (0.251) data 0.000 (0.073) loss 0.4088 (0.5160) acc 93.3962 (89.0669) lr 3.6258e-04 eta 0:02:41
epoch [38/50] batch [25/51] time 0.165 (0.237) data 0.000 (0.059) loss 0.6737 (0.5161) acc 82.2917 (88.9857) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [30/51] time 0.179 (0.226) data 0.000 (0.049) loss 0.4796 (0.5161) acc 90.5660 (88.8691) lr 3.6258e-04 eta 0:02:23
epoch [38/50] batch [35/51] time 0.196 (0.220) data 0.000 (0.042) loss 0.5974 (0.5180) acc 84.2391 (88.8773) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [40/51] time 0.175 (0.214) data 0.000 (0.037) loss 0.6318 (0.5173) acc 85.0962 (88.8693) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [45/51] time 0.173 (0.210) data 0.001 (0.033) loss 0.4930 (0.5146) acc 84.8958 (88.8451) lr 3.6258e-04 eta 0:02:10
epoch [38/50] batch [50/51] time 0.177 (0.207) data 0.000 (0.030) loss 0.4098 (0.5112) acc 90.5660 (88.9194) lr 3.6258e-04 eta 0:02:07
>>> alpha1: 0.137  alpha2: 0.056 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.39 <<<
epoch [39/50] batch [5/51] time 0.188 (0.457) data 0.000 (0.272) loss 0.3467 (0.4059) acc 89.8148 (91.2514) lr 3.1545e-04 eta 0:04:37
epoch [39/50] batch [10/51] time 0.168 (0.320) data 0.000 (0.136) loss 0.6334 (0.4641) acc 87.5000 (89.8992) lr 3.1545e-04 eta 0:03:12
epoch [39/50] batch [15/51] time 0.169 (0.273) data 0.000 (0.091) loss 0.6154 (0.4860) acc 88.2653 (89.7244) lr 3.1545e-04 eta 0:02:42
epoch [39/50] batch [20/51] time 0.169 (0.249) data 0.000 (0.068) loss 0.5658 (0.4957) acc 87.0000 (89.5915) lr 3.1545e-04 eta 0:02:27
epoch [39/50] batch [25/51] time 0.187 (0.235) data 0.001 (0.055) loss 0.6414 (0.5088) acc 87.9808 (89.2350) lr 3.1545e-04 eta 0:02:18
epoch [39/50] batch [30/51] time 0.195 (0.228) data 0.000 (0.046) loss 0.6196 (0.5447) acc 84.6154 (89.1991) lr 3.1545e-04 eta 0:02:12
epoch [39/50] batch [35/51] time 0.184 (0.243) data 0.000 (0.039) loss 0.4841 (0.5399) acc 91.6667 (89.1796) lr 3.1545e-04 eta 0:02:20
epoch [39/50] batch [40/51] time 0.178 (0.234) data 0.000 (0.034) loss 0.4737 (0.5484) acc 89.2157 (88.6149) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [45/51] time 0.179 (0.228) data 0.000 (0.030) loss 0.5471 (0.5449) acc 88.4259 (88.6343) lr 3.1545e-04 eta 0:02:09
epoch [39/50] batch [50/51] time 0.169 (0.223) data 0.000 (0.027) loss 0.6920 (0.5471) acc 82.1429 (88.4213) lr 3.1545e-04 eta 0:02:05
>>> alpha1: 0.137  alpha2: 0.056 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [40/50] batch [5/51] time 0.186 (0.508) data 0.000 (0.306) loss 0.4970 (0.4887) acc 89.9038 (89.9942) lr 2.7103e-04 eta 0:04:42
epoch [40/50] batch [10/51] time 0.174 (0.345) data 0.000 (0.153) loss 0.5484 (0.5132) acc 92.3469 (90.4170) lr 2.7103e-04 eta 0:03:10
epoch [40/50] batch [15/51] time 0.181 (0.286) data 0.000 (0.102) loss 0.5754 (0.5341) acc 87.5000 (88.8340) lr 2.7103e-04 eta 0:02:36
epoch [40/50] batch [20/51] time 0.195 (0.259) data 0.000 (0.077) loss 0.2389 (0.5249) acc 96.4912 (89.1617) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [25/51] time 0.179 (0.242) data 0.000 (0.061) loss 0.3405 (0.5369) acc 94.9074 (89.0566) lr 2.7103e-04 eta 0:02:09
epoch [40/50] batch [30/51] time 0.189 (0.233) data 0.000 (0.051) loss 0.2937 (0.5260) acc 94.7368 (89.2597) lr 2.7103e-04 eta 0:02:03
epoch [40/50] batch [35/51] time 0.167 (0.226) data 0.000 (0.044) loss 0.6659 (0.5302) acc 85.1064 (88.7938) lr 2.7103e-04 eta 0:01:58
epoch [40/50] batch [40/51] time 0.172 (0.219) data 0.000 (0.039) loss 0.4283 (0.5297) acc 90.8654 (88.7346) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [45/51] time 0.169 (0.214) data 0.001 (0.034) loss 0.5682 (0.5390) acc 86.0000 (88.3800) lr 2.7103e-04 eta 0:01:50
epoch [40/50] batch [50/51] time 0.168 (0.210) data 0.000 (0.031) loss 0.3076 (0.5272) acc 95.0000 (88.7443) lr 2.7103e-04 eta 0:01:47
>>> alpha1: 0.136  alpha2: 0.056 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [41/50] batch [5/51] time 0.171 (0.441) data 0.000 (0.254) loss 0.3611 (0.4101) acc 92.5000 (90.8308) lr 2.2949e-04 eta 0:03:42
epoch [41/50] batch [10/51] time 0.180 (0.312) data 0.000 (0.127) loss 0.6381 (0.5224) acc 84.7222 (89.3982) lr 2.2949e-04 eta 0:02:36
epoch [41/50] batch [15/51] time 0.172 (0.266) data 0.000 (0.085) loss 0.5751 (0.5281) acc 90.5000 (89.1760) lr 2.2949e-04 eta 0:02:11
epoch [41/50] batch [20/51] time 0.171 (0.244) data 0.000 (0.064) loss 0.5834 (0.5204) acc 89.7059 (89.2568) lr 2.2949e-04 eta 0:01:59
epoch [41/50] batch [25/51] time 0.187 (0.231) data 0.001 (0.051) loss 0.2833 (0.5262) acc 93.0556 (88.7918) lr 2.2949e-04 eta 0:01:51
epoch [41/50] batch [30/51] time 0.173 (0.223) data 0.000 (0.043) loss 0.4842 (0.5216) acc 92.7885 (88.9731) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [35/51] time 0.178 (0.216) data 0.000 (0.037) loss 0.7106 (0.5384) acc 86.7647 (88.5718) lr 2.2949e-04 eta 0:01:42
epoch [41/50] batch [40/51] time 0.178 (0.211) data 0.000 (0.032) loss 0.4819 (0.5300) acc 90.2778 (88.6114) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [45/51] time 0.162 (0.206) data 0.000 (0.029) loss 0.8034 (0.5325) acc 85.6383 (88.7450) lr 2.2949e-04 eta 0:01:35
epoch [41/50] batch [50/51] time 0.174 (0.203) data 0.000 (0.026) loss 0.5603 (0.5423) acc 89.4231 (88.3724) lr 2.2949e-04 eta 0:01:33
>>> alpha1: 0.136  alpha2: 0.061 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [42/50] batch [5/51] time 0.194 (0.472) data 0.000 (0.282) loss 0.4814 (0.5685) acc 90.1961 (88.6552) lr 1.9098e-04 eta 0:03:34
epoch [42/50] batch [10/51] time 0.172 (0.327) data 0.000 (0.141) loss 0.2841 (0.4627) acc 94.6078 (91.3447) lr 1.9098e-04 eta 0:02:26
epoch [42/50] batch [15/51] time 0.175 (0.278) data 0.000 (0.094) loss 0.3293 (0.4629) acc 95.4082 (90.7186) lr 1.9098e-04 eta 0:02:03
epoch [42/50] batch [20/51] time 0.194 (0.254) data 0.000 (0.071) loss 0.4005 (0.4631) acc 94.0000 (90.4791) lr 1.9098e-04 eta 0:01:51
epoch [42/50] batch [25/51] time 0.179 (0.240) data 0.000 (0.057) loss 0.5847 (0.4754) acc 88.7755 (90.0991) lr 1.9098e-04 eta 0:01:44
epoch [42/50] batch [30/51] time 0.177 (0.231) data 0.000 (0.047) loss 0.6536 (0.4978) acc 88.0208 (89.6480) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [35/51] time 0.180 (0.224) data 0.000 (0.041) loss 0.4253 (0.5003) acc 88.6364 (89.5935) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [40/51] time 0.166 (0.217) data 0.000 (0.036) loss 0.4366 (0.5036) acc 89.0000 (89.4783) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [45/51] time 0.188 (0.213) data 0.000 (0.032) loss 0.4665 (0.5035) acc 90.5172 (89.1774) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [50/51] time 0.164 (0.209) data 0.000 (0.028) loss 0.6843 (0.5156) acc 85.4167 (89.0261) lr 1.9098e-04 eta 0:01:25
>>> alpha1: 0.135  alpha2: 0.061 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.37 <<<
epoch [43/50] batch [5/51] time 0.187 (0.467) data 0.001 (0.283) loss 0.7932 (0.5157) acc 77.0833 (88.9812) lr 1.5567e-04 eta 0:03:08
epoch [43/50] batch [10/51] time 0.183 (0.327) data 0.000 (0.142) loss 0.5458 (0.5005) acc 91.6667 (89.7480) lr 1.5567e-04 eta 0:02:10
epoch [43/50] batch [15/51] time 0.175 (0.276) data 0.000 (0.095) loss 0.4474 (0.5125) acc 94.6078 (89.2464) lr 1.5567e-04 eta 0:01:48
epoch [43/50] batch [20/51] time 0.171 (0.251) data 0.000 (0.071) loss 0.5923 (0.5215) acc 87.2549 (88.9885) lr 1.5567e-04 eta 0:01:37
epoch [43/50] batch [25/51] time 0.170 (0.237) data 0.000 (0.057) loss 0.2957 (0.5232) acc 91.6667 (88.8673) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.184 (0.228) data 0.000 (0.047) loss 0.3496 (0.5271) acc 93.8679 (89.0733) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [35/51] time 0.196 (0.222) data 0.000 (0.041) loss 0.7304 (0.5369) acc 82.6923 (88.6612) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.190 (0.217) data 0.000 (0.036) loss 0.4429 (0.5334) acc 91.5179 (88.8055) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.178 (0.213) data 0.000 (0.032) loss 0.4607 (0.5264) acc 88.6792 (88.9972) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [50/51] time 0.170 (0.209) data 0.000 (0.029) loss 0.4405 (0.5273) acc 91.6667 (88.8123) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.133  alpha2: 0.059 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.36 <<<
epoch [44/50] batch [5/51] time 0.183 (0.439) data 0.000 (0.257) loss 0.6018 (0.5150) acc 88.5417 (88.2073) lr 1.2369e-04 eta 0:02:34
epoch [44/50] batch [10/51] time 0.177 (0.313) data 0.001 (0.131) loss 0.5646 (0.5041) acc 92.3469 (89.6950) lr 1.2369e-04 eta 0:01:48
epoch [44/50] batch [15/51] time 0.200 (0.272) data 0.000 (0.087) loss 0.4474 (0.5082) acc 90.4546 (89.7158) lr 1.2369e-04 eta 0:01:32
epoch [44/50] batch [20/51] time 0.207 (0.250) data 0.000 (0.066) loss 0.3447 (0.5157) acc 91.5179 (89.3615) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [25/51] time 0.189 (0.237) data 0.000 (0.052) loss 0.7067 (0.5176) acc 87.0370 (89.4843) lr 1.2369e-04 eta 0:01:18
epoch [44/50] batch [30/51] time 0.181 (0.229) data 0.000 (0.044) loss 0.2911 (0.5132) acc 93.8775 (89.3642) lr 1.2369e-04 eta 0:01:14
epoch [44/50] batch [35/51] time 0.192 (0.222) data 0.001 (0.038) loss 0.4385 (0.5257) acc 87.0370 (88.7726) lr 1.2369e-04 eta 0:01:11
epoch [44/50] batch [40/51] time 0.163 (0.216) data 0.000 (0.033) loss 0.6766 (0.5183) acc 84.8958 (88.9458) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [45/51] time 0.162 (0.211) data 0.000 (0.029) loss 0.4430 (0.5213) acc 89.8936 (88.8546) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [50/51] time 0.179 (0.207) data 0.000 (0.026) loss 0.4996 (0.5160) acc 87.0370 (88.8325) lr 1.2369e-04 eta 0:01:03
>>> alpha1: 0.133  alpha2: 0.059 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [45/50] batch [5/51] time 0.200 (0.463) data 0.000 (0.281) loss 0.4566 (0.5436) acc 89.5000 (87.7643) lr 9.5173e-05 eta 0:02:19
epoch [45/50] batch [10/51] time 0.196 (0.324) data 0.000 (0.141) loss 0.5895 (0.5465) acc 88.9423 (87.7587) lr 9.5173e-05 eta 0:01:35
epoch [45/50] batch [15/51] time 0.184 (0.277) data 0.000 (0.094) loss 0.6153 (0.5508) acc 87.2642 (87.7937) lr 9.5173e-05 eta 0:01:20
epoch [45/50] batch [20/51] time 0.195 (0.254) data 0.000 (0.070) loss 0.4969 (0.5238) acc 88.7255 (88.4552) lr 9.5173e-05 eta 0:01:12
epoch [45/50] batch [25/51] time 0.194 (0.239) data 0.000 (0.056) loss 0.4940 (0.5151) acc 86.2245 (88.7541) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [30/51] time 0.160 (0.228) data 0.001 (0.047) loss 0.7151 (0.5197) acc 86.0465 (88.7327) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [35/51] time 0.218 (0.222) data 0.000 (0.040) loss 0.5277 (0.5187) acc 87.9630 (88.7227) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.164 (0.216) data 0.000 (0.035) loss 0.6566 (0.5235) acc 87.5000 (88.6774) lr 9.5173e-05 eta 0:00:57
epoch [45/50] batch [45/51] time 0.167 (0.211) data 0.000 (0.031) loss 0.3979 (0.5117) acc 91.8367 (88.8892) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.167 (0.208) data 0.000 (0.028) loss 0.5501 (0.5060) acc 87.0000 (89.0065) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.133  alpha2: 0.060 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.37 <<<
epoch [46/50] batch [5/51] time 0.228 (0.485) data 0.000 (0.277) loss 0.3516 (0.4293) acc 91.1765 (90.7197) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [10/51] time 0.171 (0.330) data 0.001 (0.139) loss 0.5556 (0.4427) acc 88.2653 (89.9659) lr 7.0224e-05 eta 0:01:20
epoch [46/50] batch [15/51] time 0.183 (0.281) data 0.000 (0.093) loss 0.3131 (0.4465) acc 95.0000 (90.2607) lr 7.0224e-05 eta 0:01:07
epoch [46/50] batch [20/51] time 0.188 (0.254) data 0.000 (0.069) loss 0.5611 (0.4594) acc 88.8889 (90.1915) lr 7.0224e-05 eta 0:00:59
epoch [46/50] batch [25/51] time 0.179 (0.240) data 0.000 (0.056) loss 0.6211 (0.4736) acc 85.8491 (89.7499) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.170 (0.230) data 0.000 (0.046) loss 0.7151 (0.4874) acc 84.1837 (89.6667) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [35/51] time 0.184 (0.223) data 0.000 (0.040) loss 0.4335 (0.4896) acc 91.8182 (89.5580) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [40/51] time 0.178 (0.217) data 0.000 (0.035) loss 0.5588 (0.4979) acc 85.3774 (89.2937) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [45/51] time 0.174 (0.212) data 0.001 (0.031) loss 0.5975 (0.4988) acc 85.0962 (89.1445) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.174 (0.208) data 0.000 (0.028) loss 0.5392 (0.5015) acc 89.4231 (89.0992) lr 7.0224e-05 eta 0:00:42
>>> alpha1: 0.133  alpha2: 0.057 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.38 <<<
epoch [47/50] batch [5/51] time 0.185 (0.470) data 0.000 (0.286) loss 0.4264 (0.5678) acc 93.6274 (89.2508) lr 4.8943e-05 eta 0:01:33
epoch [47/50] batch [10/51] time 0.174 (0.326) data 0.000 (0.143) loss 0.4500 (0.5263) acc 91.8269 (89.2466) lr 4.8943e-05 eta 0:01:03
epoch [47/50] batch [15/51] time 0.183 (0.279) data 0.000 (0.096) loss 0.3765 (0.4855) acc 91.0377 (90.0352) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [20/51] time 0.180 (0.254) data 0.000 (0.072) loss 0.6161 (0.5229) acc 80.6604 (88.4434) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.165 (0.239) data 0.000 (0.057) loss 0.4576 (0.5133) acc 88.8298 (88.7302) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [30/51] time 0.207 (0.231) data 0.000 (0.048) loss 0.4126 (0.5100) acc 92.9245 (88.7674) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.187 (0.225) data 0.000 (0.041) loss 0.4382 (0.4962) acc 90.3846 (89.0859) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [40/51] time 0.180 (0.219) data 0.000 (0.036) loss 0.4427 (0.4961) acc 88.8889 (88.9793) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.176 (0.214) data 0.000 (0.032) loss 0.5214 (0.4899) acc 88.6792 (89.1861) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.182 (0.210) data 0.000 (0.029) loss 0.4579 (0.4900) acc 87.5000 (89.1674) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.133  alpha2: 0.058 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.38 <<<
epoch [48/50] batch [5/51] time 0.175 (0.493) data 0.000 (0.312) loss 0.5987 (0.5776) acc 83.5000 (87.9900) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [10/51] time 0.191 (0.336) data 0.000 (0.156) loss 0.5763 (0.5801) acc 87.5000 (87.7000) lr 3.1417e-05 eta 0:00:48
epoch [48/50] batch [15/51] time 0.185 (0.285) data 0.000 (0.104) loss 0.4708 (0.5332) acc 91.6667 (88.5375) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.182 (0.258) data 0.000 (0.078) loss 0.4684 (0.5151) acc 89.0909 (88.8728) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.195 (0.243) data 0.000 (0.063) loss 0.3374 (0.5000) acc 93.5185 (89.3470) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.167 (0.232) data 0.000 (0.052) loss 0.3975 (0.4842) acc 89.2857 (89.5082) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.173 (0.224) data 0.000 (0.045) loss 0.4908 (0.4909) acc 91.3462 (89.4541) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.185 (0.218) data 0.000 (0.039) loss 0.3401 (0.4924) acc 94.2982 (89.2724) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.170 (0.213) data 0.000 (0.035) loss 0.7347 (0.4965) acc 81.3726 (89.0551) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.172 (0.209) data 0.000 (0.031) loss 0.6012 (0.4981) acc 88.0000 (89.1547) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.132  alpha2: 0.059 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.22 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.38 <<<
epoch [49/50] batch [5/51] time 0.180 (0.487) data 0.000 (0.307) loss 0.4527 (0.5540) acc 89.8148 (87.3457) lr 1.7713e-05 eta 0:00:47
epoch [49/50] batch [10/51] time 0.181 (0.333) data 0.000 (0.154) loss 0.4173 (0.4981) acc 93.6274 (89.1424) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.168 (0.280) data 0.000 (0.102) loss 0.4923 (0.4838) acc 87.5000 (89.6273) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [20/51] time 0.222 (0.257) data 0.000 (0.077) loss 0.4069 (0.4929) acc 89.1667 (89.5844) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [25/51] time 0.167 (0.241) data 0.000 (0.062) loss 0.5053 (0.4850) acc 90.8163 (90.0200) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.179 (0.231) data 0.000 (0.051) loss 0.4236 (0.4864) acc 86.3208 (89.5649) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [35/51] time 0.164 (0.223) data 0.000 (0.044) loss 0.4014 (0.4831) acc 93.7500 (89.8023) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [40/51] time 0.181 (0.219) data 0.000 (0.039) loss 0.4557 (0.4886) acc 88.6364 (89.6185) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.180 (0.214) data 0.000 (0.034) loss 0.3474 (0.4878) acc 95.3704 (89.7794) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.181 (0.210) data 0.000 (0.031) loss 0.3407 (0.4781) acc 93.9815 (90.1816) lr 1.7713e-05 eta 0:00:10
>>> alpha1: 0.133  alpha2: 0.058 <<<
>>> noisy rate: 0.62 --> refined noisy rate: 0.23 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.38 <<<
epoch [50/50] batch [5/51] time 0.206 (0.511) data 0.013 (0.316) loss 0.5403 (0.5326) acc 89.8148 (88.5034) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.186 (0.349) data 0.000 (0.158) loss 0.4502 (0.4671) acc 88.3929 (89.6860) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.179 (0.294) data 0.000 (0.105) loss 0.4718 (0.4549) acc 89.4231 (89.8138) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.183 (0.264) data 0.000 (0.079) loss 0.4523 (0.4616) acc 89.7321 (90.0955) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.192 (0.249) data 0.000 (0.063) loss 0.6408 (0.4904) acc 83.0189 (89.2763) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.179 (0.238) data 0.000 (0.053) loss 0.7923 (0.5019) acc 81.8627 (89.0419) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.180 (0.230) data 0.000 (0.045) loss 0.7344 (0.5070) acc 83.8235 (88.9916) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.181 (0.223) data 0.000 (0.040) loss 0.4710 (0.5064) acc 90.9091 (89.0485) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.187 (0.218) data 0.000 (0.035) loss 0.6469 (0.5131) acc 81.6964 (88.6956) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.172 (0.214) data 0.000 (0.032) loss 0.5283 (0.5095) acc 86.0577 (88.7241) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_10FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.36, 0.3, 0.28, 0.27, 0.26, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.24, 0.24, 0.23, 0.23, 0.24, 0.23, 0.24, 0.24, 0.24, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.22, 0.23]
* matched noise rate: [0.14, 0.13, 0.11, 0.1, 0.11, 0.12, 0.12, 0.11, 0.11, 0.13, 0.12, 0.13, 0.12, 0.11, 0.12, 0.11, 0.12, 0.11, 0.12, 0.12, 0.12, 0.13, 0.14, 0.13, 0.13, 0.13, 0.14, 0.13, 0.13, 0.13, 0.13, 0.14, 0.14, 0.14, 0.13, 0.13, 0.14, 0.14, 0.13, 0.14]
* unmatched noise rate: [0.52, 0.44, 0.43, 0.42, 0.43, 0.41, 0.43, 0.42, 0.42, 0.42, 0.42, 0.43, 0.41, 0.41, 0.41, 0.41, 0.41, 0.39, 0.4, 0.39, 0.41, 0.38, 0.38, 0.4, 0.39, 0.39, 0.38, 0.39, 0.39, 0.38, 0.38, 0.37, 0.37, 0.36, 0.38, 0.37, 0.38, 0.38, 0.38, 0.38]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:03,  2.65s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.34it/s] 16%|█▌        | 4/25 [00:02<00:11,  1.90it/s] 20%|██        | 5/25 [00:03<00:07,  2.57it/s] 28%|██▊       | 7/25 [00:03<00:04,  4.17it/s] 36%|███▌      | 9/25 [00:03<00:02,  5.72it/s] 44%|████▍     | 11/25 [00:03<00:01,  7.13it/s] 52%|█████▏    | 13/25 [00:03<00:01,  8.20it/s] 60%|██████    | 15/25 [00:03<00:01,  9.21it/s] 68%|██████▊   | 17/25 [00:04<00:00,  8.88it/s] 76%|███████▌  | 19/25 [00:04<00:00,  9.74it/s] 84%|████████▍ | 21/25 [00:04<00:00, 10.42it/s] 92%|█████████▏| 23/25 [00:04<00:00, 10.95it/s]100%|██████████| 25/25 [00:05<00:00,  7.61it/s]100%|██████████| 25/25 [00:05<00:00,  4.77it/s]
=> result
* total: 2,463
* correct: 1,910
* accuracy: 77.5%
* error: 22.5%
* macro_f1: 72.0%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 0	acc: 0.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 0	acc: 0.0%
* class: 2 (canterbury bells)	total: 12	correct: 2	acc: 16.7%
* class: 3 (sweet pea)	total: 17	correct: 3	acc: 17.6%
* class: 4 (english marigold)	total: 20	correct: 13	acc: 65.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 10	acc: 83.3%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 10	acc: 71.4%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 24	acc: 92.3%
* class: 11 (colt's foot)	total: 26	correct: 17	acc: 65.4%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 16	acc: 64.0%
* class: 18 (balloon flower)	total: 15	correct: 13	acc: 86.7%
* class: 19 (giant white arum lily)	total: 17	correct: 15	acc: 88.2%
* class: 20 (fire lily)	total: 12	correct: 10	acc: 83.3%
* class: 21 (pincushion flower)	total: 17	correct: 17	acc: 100.0%
* class: 22 (fritillary)	total: 27	correct: 23	acc: 85.2%
* class: 23 (red ginger)	total: 13	correct: 10	acc: 76.9%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 9	acc: 75.0%
* class: 27 (stemless gentian)	total: 20	correct: 18	acc: 90.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 21	acc: 80.8%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 11	acc: 78.6%
* class: 32 (love in the mist)	total: 14	correct: 12	acc: 85.7%
* class: 33 (mexican aster)	total: 12	correct: 10	acc: 83.3%
* class: 34 (alpine sea holly)	total: 12	correct: 8	acc: 66.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 20	acc: 90.9%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 17	acc: 85.0%
* class: 40 (barbeton daisy)	total: 38	correct: 16	acc: 42.1%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 28	acc: 100.0%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 54	acc: 91.5%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 72	acc: 93.5%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 22	acc: 78.6%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 0	acc: 0.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 18	acc: 90.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 16	acc: 100.0%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 0	acc: 0.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 12	acc: 92.3%
* class: 67 (bearded iris)	total: 16	correct: 6	acc: 37.5%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 21	acc: 91.3%
* class: 71 (azalea)	total: 29	correct: 25	acc: 86.2%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 29	acc: 90.6%
* class: 76 (passion flower)	total: 75	correct: 74	acc: 98.7%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 30	acc: 88.2%
* class: 82 (hibiscus)	total: 39	correct: 37	acc: 94.9%
* class: 83 (columbine)	total: 26	correct: 22	acc: 84.6%
* class: 84 (desert-rose)	total: 18	correct: 18	acc: 100.0%
* class: 85 (tree mallow)	total: 17	correct: 17	acc: 100.0%
* class: 86 (magnolia)	total: 18	correct: 18	acc: 100.0%
* class: 87 (cyclamen)	total: 46	correct: 39	acc: 84.8%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 19	acc: 76.0%
* class: 90 (hippeastrum)	total: 23	correct: 19	acc: 82.6%
* class: 91 (bee balm)	total: 20	correct: 20	acc: 100.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 49	acc: 100.0%
* class: 94 (bougainvillea)	total: 38	correct: 34	acc: 89.5%
* class: 95 (camellia)	total: 27	correct: 23	acc: 85.2%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 22	acc: 88.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 10	acc: 58.8%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 76.3%
Elapsed: 0:28:16
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
resume: 
root: ./data
seed: 1
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_12-seed_1.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.274 (1.098) data 0.000 (0.353) loss 5.0413 (4.9100) acc 0.0000 (4.3750) lr 1.0000e-05 eta 0:46:34
epoch [1/50] batch [10/51] time 0.259 (0.681) data 0.000 (0.176) loss 4.6637 (4.8263) acc 0.0000 (3.1250) lr 1.0000e-05 eta 0:28:48
epoch [1/50] batch [15/51] time 0.269 (0.542) data 0.000 (0.118) loss 5.0367 (4.8427) acc 0.0000 (2.7083) lr 1.0000e-05 eta 0:22:55
epoch [1/50] batch [20/51] time 0.263 (0.473) data 0.000 (0.088) loss 4.6799 (4.7576) acc 12.5000 (3.5938) lr 1.0000e-05 eta 0:19:55
epoch [1/50] batch [25/51] time 0.270 (0.432) data 0.000 (0.071) loss 4.7400 (4.7532) acc 0.0000 (3.5000) lr 1.0000e-05 eta 0:18:11
epoch [1/50] batch [30/51] time 0.262 (0.405) data 0.000 (0.059) loss 4.7075 (4.7319) acc 9.3750 (4.0625) lr 1.0000e-05 eta 0:16:59
epoch [1/50] batch [35/51] time 0.260 (0.385) data 0.000 (0.051) loss 4.6027 (4.7081) acc 9.3750 (4.7321) lr 1.0000e-05 eta 0:16:08
epoch [1/50] batch [40/51] time 0.260 (0.370) data 0.000 (0.044) loss 4.7721 (4.6989) acc 3.1250 (4.6875) lr 1.0000e-05 eta 0:15:27
epoch [1/50] batch [45/51] time 0.260 (0.357) data 0.000 (0.039) loss 4.8492 (4.6873) acc 3.1250 (4.8611) lr 1.0000e-05 eta 0:14:55
epoch [1/50] batch [50/51] time 0.259 (0.348) data 0.000 (0.035) loss 4.5443 (4.6818) acc 6.2500 (4.7500) lr 1.0000e-05 eta 0:14:29
epoch [2/50] batch [5/51] time 0.277 (0.575) data 0.000 (0.287) loss 4.5565 (4.6426) acc 0.0000 (1.2500) lr 2.0000e-03 eta 0:23:54
epoch [2/50] batch [10/51] time 0.267 (0.424) data 0.000 (0.144) loss 4.7193 (4.6300) acc 6.2500 (5.0000) lr 2.0000e-03 eta 0:17:35
epoch [2/50] batch [15/51] time 0.262 (0.373) data 0.000 (0.096) loss 4.3975 (4.6073) acc 6.2500 (3.9583) lr 2.0000e-03 eta 0:15:26
epoch [2/50] batch [20/51] time 0.262 (0.346) data 0.000 (0.072) loss 4.2078 (4.5639) acc 21.8750 (5.3125) lr 2.0000e-03 eta 0:14:17
epoch [2/50] batch [25/51] time 0.283 (0.330) data 0.000 (0.058) loss 4.6084 (4.5729) acc 3.1250 (5.1250) lr 2.0000e-03 eta 0:13:37
epoch [2/50] batch [30/51] time 0.273 (0.320) data 0.000 (0.048) loss 4.3966 (4.5700) acc 18.7500 (5.5208) lr 2.0000e-03 eta 0:13:10
epoch [2/50] batch [35/51] time 0.268 (0.312) data 0.000 (0.041) loss 4.3237 (4.5608) acc 15.6250 (5.8036) lr 2.0000e-03 eta 0:12:49
epoch [2/50] batch [40/51] time 0.261 (0.306) data 0.000 (0.036) loss 4.4539 (4.5560) acc 3.1250 (6.0156) lr 2.0000e-03 eta 0:12:32
epoch [2/50] batch [45/51] time 0.260 (0.301) data 0.001 (0.032) loss 4.5767 (4.5609) acc 3.1250 (5.9028) lr 2.0000e-03 eta 0:12:18
epoch [2/50] batch [50/51] time 0.258 (0.297) data 0.000 (0.029) loss 4.5904 (4.5597) acc 15.6250 (6.3125) lr 2.0000e-03 eta 0:12:06
epoch [3/50] batch [5/51] time 0.288 (0.602) data 0.000 (0.305) loss 4.4240 (4.4444) acc 15.6250 (12.5000) lr 1.9980e-03 eta 0:24:30
epoch [3/50] batch [10/51] time 0.260 (0.433) data 0.000 (0.153) loss 4.3189 (4.4806) acc 12.5000 (11.5625) lr 1.9980e-03 eta 0:17:34
epoch [3/50] batch [15/51] time 0.267 (0.377) data 0.000 (0.102) loss 4.4341 (4.4655) acc 6.2500 (10.6250) lr 1.9980e-03 eta 0:15:16
epoch [3/50] batch [20/51] time 0.268 (0.349) data 0.000 (0.076) loss 4.4661 (4.4675) acc 6.2500 (10.1562) lr 1.9980e-03 eta 0:14:07
epoch [3/50] batch [25/51] time 0.260 (0.332) data 0.000 (0.061) loss 4.5236 (4.4655) acc 9.3750 (10.1250) lr 1.9980e-03 eta 0:13:25
epoch [3/50] batch [30/51] time 0.271 (0.322) data 0.000 (0.051) loss 4.4620 (4.4677) acc 6.2500 (9.5833) lr 1.9980e-03 eta 0:12:58
epoch [3/50] batch [35/51] time 0.265 (0.314) data 0.000 (0.044) loss 4.6513 (4.4736) acc 9.3750 (9.3750) lr 1.9980e-03 eta 0:12:36
epoch [3/50] batch [40/51] time 0.260 (0.307) data 0.000 (0.038) loss 4.3380 (4.4714) acc 12.5000 (9.2969) lr 1.9980e-03 eta 0:12:19
epoch [3/50] batch [45/51] time 0.261 (0.302) data 0.000 (0.034) loss 4.3827 (4.4741) acc 6.2500 (9.3056) lr 1.9980e-03 eta 0:12:05
epoch [3/50] batch [50/51] time 0.258 (0.298) data 0.000 (0.031) loss 4.5867 (4.4739) acc 0.0000 (9.3125) lr 1.9980e-03 eta 0:11:53
epoch [4/50] batch [5/51] time 0.278 (0.548) data 0.000 (0.260) loss 4.2785 (4.4520) acc 15.6250 (11.8750) lr 1.9921e-03 eta 0:21:50
epoch [4/50] batch [10/51] time 0.259 (0.408) data 0.000 (0.130) loss 4.4528 (4.4846) acc 3.1250 (8.7500) lr 1.9921e-03 eta 0:16:13
epoch [4/50] batch [15/51] time 0.268 (0.361) data 0.000 (0.087) loss 4.3215 (4.4499) acc 12.5000 (8.7500) lr 1.9921e-03 eta 0:14:21
epoch [4/50] batch [20/51] time 0.260 (0.337) data 0.000 (0.065) loss 4.6399 (4.4580) acc 12.5000 (9.8438) lr 1.9921e-03 eta 0:13:21
epoch [4/50] batch [25/51] time 0.259 (0.323) data 0.000 (0.052) loss 4.4443 (4.4421) acc 9.3750 (11.0000) lr 1.9921e-03 eta 0:12:45
epoch [4/50] batch [30/51] time 0.273 (0.313) data 0.000 (0.043) loss 4.6112 (4.4634) acc 3.1250 (9.8958) lr 1.9921e-03 eta 0:12:20
epoch [4/50] batch [35/51] time 0.274 (0.307) data 0.000 (0.037) loss 4.1891 (4.4386) acc 31.2500 (11.2500) lr 1.9921e-03 eta 0:12:04
epoch [4/50] batch [40/51] time 0.260 (0.301) data 0.000 (0.033) loss 4.4496 (4.4488) acc 12.5000 (10.8594) lr 1.9921e-03 eta 0:11:50
epoch [4/50] batch [45/51] time 0.258 (0.297) data 0.000 (0.029) loss 4.5462 (4.4575) acc 3.1250 (10.9028) lr 1.9921e-03 eta 0:11:37
epoch [4/50] batch [50/51] time 0.262 (0.293) data 0.000 (0.026) loss 4.5646 (4.4622) acc 6.2500 (10.5000) lr 1.9921e-03 eta 0:11:27
epoch [5/50] batch [5/51] time 0.270 (0.628) data 0.000 (0.353) loss 4.2604 (4.4118) acc 12.5000 (11.2500) lr 1.9823e-03 eta 0:24:31
epoch [5/50] batch [10/51] time 0.261 (0.448) data 0.000 (0.177) loss 4.3253 (4.4613) acc 12.5000 (10.6250) lr 1.9823e-03 eta 0:17:27
epoch [5/50] batch [15/51] time 0.264 (0.388) data 0.000 (0.118) loss 4.2967 (4.4266) acc 12.5000 (11.4583) lr 1.9823e-03 eta 0:15:04
epoch [5/50] batch [20/51] time 0.260 (0.357) data 0.000 (0.088) loss 4.3483 (4.4493) acc 12.5000 (11.4062) lr 1.9823e-03 eta 0:13:50
epoch [5/50] batch [25/51] time 0.270 (0.338) data 0.000 (0.071) loss 4.4288 (4.4320) acc 9.3750 (11.8750) lr 1.9823e-03 eta 0:13:05
epoch [5/50] batch [30/51] time 0.273 (0.326) data 0.000 (0.059) loss 4.3829 (4.4143) acc 9.3750 (12.0833) lr 1.9823e-03 eta 0:12:34
epoch [5/50] batch [35/51] time 0.269 (0.317) data 0.000 (0.051) loss 4.5655 (4.4327) acc 6.2500 (11.6071) lr 1.9823e-03 eta 0:12:13
epoch [5/50] batch [40/51] time 0.256 (0.310) data 0.000 (0.044) loss 4.5703 (4.4254) acc 6.2500 (11.7969) lr 1.9823e-03 eta 0:11:55
epoch [5/50] batch [45/51] time 0.256 (0.304) data 0.000 (0.039) loss 4.5938 (4.4376) acc 0.0000 (11.4583) lr 1.9823e-03 eta 0:11:39
epoch [5/50] batch [50/51] time 0.256 (0.299) data 0.000 (0.035) loss 4.2599 (4.4362) acc 21.8750 (11.1250) lr 1.9823e-03 eta 0:11:27
epoch [6/50] batch [5/51] time 0.268 (0.546) data 0.000 (0.249) loss 4.4532 (4.3590) acc 3.1250 (11.2500) lr 1.9686e-03 eta 0:20:50
epoch [6/50] batch [10/51] time 0.258 (0.405) data 0.000 (0.125) loss 4.5684 (4.4101) acc 12.5000 (10.6250) lr 1.9686e-03 eta 0:15:26
epoch [6/50] batch [15/51] time 0.258 (0.358) data 0.000 (0.083) loss 4.2556 (4.3848) acc 6.2500 (10.0000) lr 1.9686e-03 eta 0:13:36
epoch [6/50] batch [20/51] time 0.264 (0.334) data 0.000 (0.062) loss 4.4578 (4.3913) acc 9.3750 (10.4688) lr 1.9686e-03 eta 0:12:40
epoch [6/50] batch [25/51] time 0.262 (0.320) data 0.000 (0.050) loss 4.6314 (4.3894) acc 3.1250 (11.2500) lr 1.9686e-03 eta 0:12:05
epoch [6/50] batch [30/51] time 0.270 (0.310) data 0.000 (0.042) loss 4.5087 (4.4129) acc 12.5000 (10.6250) lr 1.9686e-03 eta 0:11:42
epoch [6/50] batch [35/51] time 0.265 (0.304) data 0.000 (0.036) loss 4.3431 (4.4110) acc 6.2500 (10.8929) lr 1.9686e-03 eta 0:11:26
epoch [6/50] batch [40/51] time 0.256 (0.298) data 0.000 (0.031) loss 4.4024 (4.4083) acc 15.6250 (11.4844) lr 1.9686e-03 eta 0:11:12
epoch [6/50] batch [45/51] time 0.256 (0.294) data 0.000 (0.028) loss 4.5000 (4.4151) acc 9.3750 (11.2500) lr 1.9686e-03 eta 0:11:00
epoch [6/50] batch [50/51] time 0.257 (0.290) data 0.000 (0.025) loss 4.5983 (4.4279) acc 15.6250 (11.1875) lr 1.9686e-03 eta 0:10:50
epoch [7/50] batch [5/51] time 0.258 (0.612) data 0.000 (0.332) loss 4.1652 (4.3018) acc 18.7500 (13.7500) lr 1.9511e-03 eta 0:22:50
epoch [7/50] batch [10/51] time 0.259 (0.439) data 0.000 (0.166) loss 4.4459 (4.3731) acc 12.5000 (12.8125) lr 1.9511e-03 eta 0:16:20
epoch [7/50] batch [15/51] time 0.279 (0.380) data 0.000 (0.111) loss 4.2268 (4.3819) acc 15.6250 (12.5000) lr 1.9511e-03 eta 0:14:07
epoch [7/50] batch [20/51] time 0.273 (0.352) data 0.000 (0.083) loss 4.3730 (4.3673) acc 12.5000 (12.0312) lr 1.9511e-03 eta 0:13:02
epoch [7/50] batch [25/51] time 0.258 (0.335) data 0.000 (0.067) loss 4.3612 (4.3603) acc 15.6250 (12.2500) lr 1.9511e-03 eta 0:12:22
epoch [7/50] batch [30/51] time 0.258 (0.322) data 0.000 (0.056) loss 4.2935 (4.3704) acc 15.6250 (12.6042) lr 1.9511e-03 eta 0:11:54
epoch [7/50] batch [35/51] time 0.258 (0.314) data 0.000 (0.048) loss 4.3535 (4.3766) acc 9.3750 (12.4107) lr 1.9511e-03 eta 0:11:33
epoch [7/50] batch [40/51] time 0.256 (0.307) data 0.000 (0.042) loss 4.4831 (4.3813) acc 12.5000 (12.5781) lr 1.9511e-03 eta 0:11:16
epoch [7/50] batch [45/51] time 0.255 (0.301) data 0.000 (0.037) loss 4.4735 (4.3957) acc 6.2500 (12.0139) lr 1.9511e-03 eta 0:11:02
epoch [7/50] batch [50/51] time 0.256 (0.297) data 0.000 (0.033) loss 4.4913 (4.3977) acc 9.3750 (11.9375) lr 1.9511e-03 eta 0:10:50
epoch [8/50] batch [5/51] time 0.269 (0.510) data 0.000 (0.220) loss 4.2855 (4.3737) acc 12.5000 (11.8750) lr 1.9298e-03 eta 0:18:36
epoch [8/50] batch [10/51] time 0.260 (0.388) data 0.000 (0.110) loss 4.2758 (4.3621) acc 12.5000 (11.5625) lr 1.9298e-03 eta 0:14:07
epoch [8/50] batch [15/51] time 0.258 (0.348) data 0.000 (0.074) loss 4.3743 (4.3304) acc 9.3750 (12.0833) lr 1.9298e-03 eta 0:12:37
epoch [8/50] batch [20/51] time 0.258 (0.327) data 0.000 (0.055) loss 4.5717 (4.3680) acc 6.2500 (11.5625) lr 1.9298e-03 eta 0:11:49
epoch [8/50] batch [25/51] time 0.266 (0.315) data 0.000 (0.044) loss 4.3094 (4.3544) acc 9.3750 (12.2500) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [30/51] time 0.268 (0.307) data 0.000 (0.037) loss 4.5179 (4.3488) acc 3.1250 (12.6042) lr 1.9298e-03 eta 0:11:03
epoch [8/50] batch [35/51] time 0.273 (0.301) data 0.000 (0.032) loss 4.3350 (4.3485) acc 15.6250 (12.8571) lr 1.9298e-03 eta 0:10:50
epoch [8/50] batch [40/51] time 0.259 (0.296) data 0.000 (0.028) loss 4.3748 (4.3448) acc 12.5000 (13.0469) lr 1.9298e-03 eta 0:10:37
epoch [8/50] batch [45/51] time 0.257 (0.292) data 0.000 (0.025) loss 4.1845 (4.3553) acc 15.6250 (12.6389) lr 1.9298e-03 eta 0:10:26
epoch [8/50] batch [50/51] time 0.258 (0.288) data 0.000 (0.022) loss 4.4211 (4.3713) acc 9.3750 (12.3125) lr 1.9298e-03 eta 0:10:17
epoch [9/50] batch [5/51] time 0.281 (0.591) data 0.000 (0.297) loss 4.4977 (4.3241) acc 9.3750 (11.8750) lr 1.9048e-03 eta 0:21:02
epoch [9/50] batch [10/51] time 0.260 (0.427) data 0.000 (0.149) loss 4.3268 (4.3103) acc 12.5000 (13.4375) lr 1.9048e-03 eta 0:15:10
epoch [9/50] batch [15/51] time 0.267 (0.374) data 0.000 (0.099) loss 4.4943 (4.3464) acc 12.5000 (12.5000) lr 1.9048e-03 eta 0:13:14
epoch [9/50] batch [20/51] time 0.268 (0.346) data 0.000 (0.074) loss 4.1274 (4.3578) acc 18.7500 (12.0312) lr 1.9048e-03 eta 0:12:14
epoch [9/50] batch [25/51] time 0.284 (0.330) data 0.000 (0.060) loss 4.6030 (4.3646) acc 9.3750 (11.6250) lr 1.9048e-03 eta 0:11:38
epoch [9/50] batch [30/51] time 0.272 (0.320) data 0.000 (0.050) loss 4.2305 (4.3478) acc 9.3750 (12.2917) lr 1.9048e-03 eta 0:11:15
epoch [9/50] batch [35/51] time 0.261 (0.312) data 0.000 (0.043) loss 4.4613 (4.3495) acc 9.3750 (12.3214) lr 1.9048e-03 eta 0:10:56
epoch [9/50] batch [40/51] time 0.256 (0.305) data 0.000 (0.037) loss 4.6167 (4.3619) acc 9.3750 (12.4219) lr 1.9048e-03 eta 0:10:40
epoch [9/50] batch [45/51] time 0.256 (0.300) data 0.000 (0.033) loss 4.4181 (4.3708) acc 9.3750 (12.6389) lr 1.9048e-03 eta 0:10:28
epoch [9/50] batch [50/51] time 0.256 (0.295) data 0.000 (0.030) loss 4.2191 (4.3756) acc 21.8750 (12.5625) lr 1.9048e-03 eta 0:10:17
epoch [10/50] batch [5/51] time 0.273 (0.539) data 0.000 (0.243) loss 4.3474 (4.2685) acc 12.5000 (14.3750) lr 1.8763e-03 eta 0:18:45
epoch [10/50] batch [10/51] time 0.258 (0.400) data 0.000 (0.122) loss 4.5273 (4.3126) acc 6.2500 (11.8750) lr 1.8763e-03 eta 0:13:51
epoch [10/50] batch [15/51] time 0.260 (0.354) data 0.000 (0.081) loss 4.2632 (4.2593) acc 18.7500 (13.7500) lr 1.8763e-03 eta 0:12:15
epoch [10/50] batch [20/51] time 0.258 (0.332) data 0.000 (0.061) loss 4.3426 (4.2901) acc 15.6250 (13.5938) lr 1.8763e-03 eta 0:11:27
epoch [10/50] batch [25/51] time 0.259 (0.318) data 0.000 (0.049) loss 4.4906 (4.3001) acc 9.3750 (13.5000) lr 1.8763e-03 eta 0:10:56
epoch [10/50] batch [30/51] time 0.258 (0.309) data 0.000 (0.041) loss 4.3941 (4.3249) acc 9.3750 (13.1250) lr 1.8763e-03 eta 0:10:36
epoch [10/50] batch [35/51] time 0.270 (0.302) data 0.000 (0.035) loss 4.0938 (4.3235) acc 9.3750 (12.3214) lr 1.8763e-03 eta 0:10:21
epoch [10/50] batch [40/51] time 0.256 (0.297) data 0.000 (0.031) loss 4.3004 (4.3242) acc 18.7500 (12.8125) lr 1.8763e-03 eta 0:10:08
epoch [10/50] batch [45/51] time 0.256 (0.292) data 0.000 (0.027) loss 4.6187 (4.3397) acc 6.2500 (12.4306) lr 1.8763e-03 eta 0:09:57
epoch [10/50] batch [50/51] time 0.257 (0.289) data 0.000 (0.024) loss 4.2560 (4.3452) acc 15.6250 (12.6875) lr 1.8763e-03 eta 0:09:49
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/tensorboard)
>>> alpha1: 0.885  alpha2: 0.435 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.59 --> matched refined noisy rate: 0.38 & unmatched refined noisy rate: 0.92 <<<
epoch [11/50] batch [5/51] time 0.176 (0.912) data 0.000 (0.331) loss 3.8298 (3.8890) acc 41.9811 (32.6351) lr 1.8443e-03 eta 0:30:56
epoch [11/50] batch [10/51] time 0.893 (0.747) data 0.000 (0.166) loss 3.3460 (3.7227) acc 44.5455 (37.8328) lr 1.8443e-03 eta 0:25:16
epoch [11/50] batch [15/51] time 0.865 (0.693) data 0.000 (0.111) loss 4.0301 (3.7283) acc 29.5000 (37.6301) lr 1.8443e-03 eta 0:23:23
epoch [11/50] batch [20/51] time 0.174 (0.602) data 0.000 (0.083) loss 3.5305 (3.6748) acc 41.3462 (38.4630) lr 1.8443e-03 eta 0:20:15
epoch [11/50] batch [25/51] time 0.995 (0.581) data 0.000 (0.066) loss 3.4728 (3.6575) acc 47.4138 (40.1796) lr 1.8443e-03 eta 0:19:30
epoch [11/50] batch [30/51] time 0.176 (0.514) data 0.000 (0.055) loss 3.1542 (3.6349) acc 52.4038 (40.5998) lr 1.8443e-03 eta 0:17:12
epoch [11/50] batch [35/51] time 0.174 (0.466) data 0.000 (0.048) loss 3.7454 (3.6424) acc 30.5000 (40.1954) lr 1.8443e-03 eta 0:15:33
epoch [11/50] batch [40/51] time 0.176 (0.430) data 0.000 (0.042) loss 3.3403 (3.6343) acc 53.7736 (40.6793) lr 1.8443e-03 eta 0:14:20
epoch [11/50] batch [45/51] time 0.168 (0.401) data 0.000 (0.037) loss 3.1269 (3.6224) acc 40.0000 (40.3177) lr 1.8443e-03 eta 0:13:20
epoch [11/50] batch [50/51] time 0.165 (0.378) data 0.000 (0.033) loss 3.7057 (3.6091) acc 36.7347 (40.4510) lr 1.8443e-03 eta 0:12:32
>>> alpha1: 0.835  alpha2: 0.411 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.45 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.57 <<<
epoch [12/50] batch [5/51] time 0.142 (0.960) data 0.001 (0.311) loss 2.4978 (2.5415) acc 54.6053 (53.5290) lr 1.8090e-03 eta 0:31:44
epoch [12/50] batch [10/51] time 0.153 (0.659) data 0.000 (0.156) loss 1.9776 (2.3668) acc 61.6279 (58.6796) lr 1.8090e-03 eta 0:21:44
epoch [12/50] batch [15/51] time 0.164 (0.491) data 0.000 (0.104) loss 2.1437 (2.3501) acc 52.6596 (59.0286) lr 1.8090e-03 eta 0:16:08
epoch [12/50] batch [20/51] time 0.153 (0.459) data 0.000 (0.078) loss 2.1301 (2.3243) acc 63.3721 (59.0294) lr 1.8090e-03 eta 0:15:03
epoch [12/50] batch [25/51] time 0.156 (0.399) data 0.000 (0.062) loss 2.6008 (2.3557) acc 53.5714 (58.3186) lr 1.8090e-03 eta 0:13:02
epoch [12/50] batch [30/51] time 0.649 (0.374) data 0.000 (0.052) loss 2.1042 (2.3566) acc 66.4634 (57.9138) lr 1.8090e-03 eta 0:12:12
epoch [12/50] batch [35/51] time 0.142 (0.342) data 0.000 (0.045) loss 2.3697 (2.3469) acc 54.6053 (57.8432) lr 1.8090e-03 eta 0:11:07
epoch [12/50] batch [40/51] time 0.155 (0.318) data 0.000 (0.039) loss 2.5907 (2.3453) acc 42.6136 (57.6763) lr 1.8090e-03 eta 0:10:20
epoch [12/50] batch [45/51] time 0.152 (0.299) data 0.000 (0.035) loss 2.3974 (2.3347) acc 45.8333 (57.7913) lr 1.8090e-03 eta 0:09:42
epoch [12/50] batch [50/51] time 0.151 (0.284) data 0.000 (0.031) loss 2.2233 (2.3277) acc 54.7619 (57.7954) lr 1.8090e-03 eta 0:09:11
>>> alpha1: 0.776  alpha2: 0.366 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.65 <<<
epoch [13/50] batch [5/51] time 0.173 (0.529) data 0.000 (0.346) loss 1.5802 (1.7559) acc 71.5000 (63.5228) lr 1.7705e-03 eta 0:17:02
epoch [13/50] batch [10/51] time 0.172 (0.354) data 0.000 (0.173) loss 1.8304 (1.7127) acc 59.3137 (66.8887) lr 1.7705e-03 eta 0:11:22
epoch [13/50] batch [15/51] time 0.184 (0.297) data 0.000 (0.116) loss 1.6784 (1.6905) acc 68.9815 (66.7120) lr 1.7705e-03 eta 0:09:30
epoch [13/50] batch [20/51] time 0.199 (0.267) data 0.015 (0.087) loss 1.5776 (1.6989) acc 66.2037 (65.6001) lr 1.7705e-03 eta 0:08:32
epoch [13/50] batch [25/51] time 0.182 (0.251) data 0.000 (0.070) loss 1.5113 (1.6733) acc 78.7037 (66.8251) lr 1.7705e-03 eta 0:08:00
epoch [13/50] batch [30/51] time 0.170 (0.239) data 0.000 (0.058) loss 1.6308 (1.6683) acc 63.0000 (66.9840) lr 1.7705e-03 eta 0:07:35
epoch [13/50] batch [35/51] time 0.177 (0.230) data 0.000 (0.050) loss 1.4269 (1.6496) acc 70.1923 (67.2751) lr 1.7705e-03 eta 0:07:18
epoch [13/50] batch [40/51] time 0.173 (0.223) data 0.000 (0.044) loss 1.8744 (1.6610) acc 56.7308 (66.4950) lr 1.7705e-03 eta 0:07:03
epoch [13/50] batch [45/51] time 0.159 (0.218) data 0.000 (0.039) loss 2.0630 (1.6721) acc 42.3913 (65.5893) lr 1.7705e-03 eta 0:06:51
epoch [13/50] batch [50/51] time 0.173 (0.213) data 0.000 (0.035) loss 1.7315 (1.6654) acc 63.9423 (65.6264) lr 1.7705e-03 eta 0:06:42
>>> alpha1: 0.701  alpha2: 0.328 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.36 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.60 <<<
epoch [14/50] batch [5/51] time 0.179 (0.445) data 0.000 (0.265) loss 1.2786 (1.3406) acc 70.7547 (73.9101) lr 1.7290e-03 eta 0:13:57
epoch [14/50] batch [10/51] time 0.171 (0.314) data 0.000 (0.133) loss 1.2869 (1.4162) acc 72.0000 (71.7850) lr 1.7290e-03 eta 0:09:48
epoch [14/50] batch [15/51] time 0.182 (0.270) data 0.000 (0.088) loss 1.2803 (1.4017) acc 73.1818 (71.5176) lr 1.7290e-03 eta 0:08:24
epoch [14/50] batch [20/51] time 0.166 (0.246) data 0.000 (0.066) loss 1.5156 (1.4193) acc 68.3673 (71.2345) lr 1.7290e-03 eta 0:07:38
epoch [14/50] batch [25/51] time 0.189 (0.233) data 0.000 (0.053) loss 1.4572 (1.4067) acc 61.7924 (70.4853) lr 1.7290e-03 eta 0:07:14
epoch [14/50] batch [30/51] time 0.186 (0.224) data 0.001 (0.044) loss 1.4483 (1.4050) acc 66.8269 (70.1440) lr 1.7290e-03 eta 0:06:56
epoch [14/50] batch [35/51] time 0.180 (0.218) data 0.000 (0.038) loss 1.0533 (1.3852) acc 75.9434 (71.0258) lr 1.7290e-03 eta 0:06:43
epoch [14/50] batch [40/51] time 0.163 (0.213) data 0.000 (0.033) loss 1.8441 (1.3905) acc 56.5217 (71.0622) lr 1.7290e-03 eta 0:06:33
epoch [14/50] batch [45/51] time 0.163 (0.209) data 0.000 (0.030) loss 1.5993 (1.3817) acc 66.4894 (71.2184) lr 1.7290e-03 eta 0:06:24
epoch [14/50] batch [50/51] time 0.178 (0.205) data 0.000 (0.027) loss 1.2635 (1.3872) acc 75.9434 (71.0563) lr 1.7290e-03 eta 0:06:16
>>> alpha1: 0.652  alpha2: 0.297 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.36 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.60 <<<
epoch [15/50] batch [5/51] time 0.174 (0.498) data 0.001 (0.308) loss 1.3689 (1.1460) acc 69.1176 (75.4134) lr 1.6845e-03 eta 0:15:11
epoch [15/50] batch [10/51] time 0.179 (0.341) data 0.000 (0.154) loss 1.0844 (1.1426) acc 86.1111 (76.9845) lr 1.6845e-03 eta 0:10:22
epoch [15/50] batch [15/51] time 0.169 (0.286) data 0.000 (0.103) loss 1.2777 (1.1761) acc 72.0000 (74.9940) lr 1.6845e-03 eta 0:08:39
epoch [15/50] batch [20/51] time 0.178 (0.258) data 0.000 (0.077) loss 1.2859 (1.1976) acc 73.5849 (74.7943) lr 1.6845e-03 eta 0:07:48
epoch [15/50] batch [25/51] time 0.174 (0.242) data 0.000 (0.062) loss 1.1476 (1.2079) acc 74.0000 (74.0340) lr 1.6845e-03 eta 0:07:18
epoch [15/50] batch [30/51] time 0.172 (0.232) data 0.000 (0.052) loss 1.6512 (1.2276) acc 68.3673 (73.7088) lr 1.6845e-03 eta 0:06:59
epoch [15/50] batch [35/51] time 0.173 (0.225) data 0.000 (0.044) loss 1.4343 (1.2509) acc 64.5000 (73.1449) lr 1.6845e-03 eta 0:06:44
epoch [15/50] batch [40/51] time 0.171 (0.218) data 0.000 (0.039) loss 1.3831 (1.2486) acc 65.8654 (72.8886) lr 1.6845e-03 eta 0:06:32
epoch [15/50] batch [45/51] time 0.185 (0.214) data 0.000 (0.034) loss 1.0489 (1.2543) acc 80.4545 (72.6623) lr 1.6845e-03 eta 0:06:22
epoch [15/50] batch [50/51] time 0.162 (0.209) data 0.000 (0.031) loss 1.5500 (1.2639) acc 67.7083 (72.3192) lr 1.6845e-03 eta 0:06:13
>>> alpha1: 0.572  alpha2: 0.226 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.52 <<<
epoch [16/50] batch [5/51] time 0.165 (0.440) data 0.000 (0.265) loss 1.3697 (1.1099) acc 72.8723 (78.3911) lr 1.6374e-03 eta 0:13:02
epoch [16/50] batch [10/51] time 0.188 (0.308) data 0.000 (0.134) loss 1.0823 (1.1929) acc 79.3269 (74.0456) lr 1.6374e-03 eta 0:09:06
epoch [16/50] batch [15/51] time 0.190 (0.263) data 0.000 (0.090) loss 0.9614 (1.1827) acc 80.8824 (74.2561) lr 1.6374e-03 eta 0:07:45
epoch [16/50] batch [20/51] time 0.167 (0.239) data 0.000 (0.067) loss 1.1945 (1.1784) acc 68.3673 (74.0326) lr 1.6374e-03 eta 0:07:01
epoch [16/50] batch [25/51] time 0.171 (0.224) data 0.000 (0.054) loss 1.2926 (1.1837) acc 66.6667 (73.3067) lr 1.6374e-03 eta 0:06:34
epoch [16/50] batch [30/51] time 0.173 (0.215) data 0.000 (0.045) loss 1.2402 (1.1877) acc 70.6731 (73.1214) lr 1.6374e-03 eta 0:06:17
epoch [16/50] batch [35/51] time 0.190 (0.210) data 0.000 (0.039) loss 1.0418 (1.1751) acc 80.2885 (73.5359) lr 1.6374e-03 eta 0:06:07
epoch [16/50] batch [40/51] time 0.166 (0.205) data 0.000 (0.034) loss 0.8755 (1.1568) acc 84.6939 (74.2684) lr 1.6374e-03 eta 0:05:58
epoch [16/50] batch [45/51] time 0.151 (0.200) data 0.000 (0.030) loss 1.1793 (1.1623) acc 77.3810 (74.1521) lr 1.6374e-03 eta 0:05:48
epoch [16/50] batch [50/51] time 0.161 (0.196) data 0.000 (0.027) loss 1.1185 (1.1819) acc 77.1277 (73.6218) lr 1.6374e-03 eta 0:05:40
>>> alpha1: 0.401  alpha2: 0.132 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.34 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [17/50] batch [5/51] time 0.173 (0.431) data 0.000 (0.259) loss 1.2744 (1.1184) acc 73.2955 (77.3785) lr 1.5878e-03 eta 0:12:25
epoch [17/50] batch [10/51] time 0.175 (0.305) data 0.000 (0.131) loss 1.1975 (1.0801) acc 75.5102 (77.0654) lr 1.5878e-03 eta 0:08:46
epoch [17/50] batch [15/51] time 0.163 (0.259) data 0.000 (0.088) loss 0.9963 (1.0685) acc 80.3191 (77.3158) lr 1.5878e-03 eta 0:07:26
epoch [17/50] batch [20/51] time 0.165 (0.236) data 0.000 (0.066) loss 0.8874 (1.0413) acc 77.0833 (77.6445) lr 1.5878e-03 eta 0:06:44
epoch [17/50] batch [25/51] time 0.168 (0.222) data 0.000 (0.053) loss 0.8436 (1.0357) acc 85.0000 (77.7948) lr 1.5878e-03 eta 0:06:18
epoch [17/50] batch [30/51] time 0.173 (0.213) data 0.000 (0.044) loss 1.0571 (1.0112) acc 72.3958 (77.9560) lr 1.5878e-03 eta 0:06:03
epoch [17/50] batch [35/51] time 0.178 (0.208) data 0.000 (0.038) loss 0.6765 (1.0016) acc 87.2549 (77.8071) lr 1.5878e-03 eta 0:05:53
epoch [17/50] batch [40/51] time 0.178 (0.203) data 0.000 (0.033) loss 0.8697 (1.0031) acc 81.4815 (77.8437) lr 1.5878e-03 eta 0:05:44
epoch [17/50] batch [45/51] time 0.161 (0.199) data 0.000 (0.029) loss 1.0285 (1.0169) acc 81.9149 (77.4187) lr 1.5878e-03 eta 0:05:35
epoch [17/50] batch [50/51] time 0.165 (0.195) data 0.000 (0.026) loss 1.0329 (1.0161) acc 76.0204 (77.4270) lr 1.5878e-03 eta 0:05:28
>>> alpha1: 0.301  alpha2: 0.076 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.34 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.50 <<<
epoch [18/50] batch [5/51] time 0.187 (0.500) data 0.000 (0.326) loss 1.0608 (0.8365) acc 80.5000 (82.3909) lr 1.5358e-03 eta 0:13:59
epoch [18/50] batch [10/51] time 0.183 (0.340) data 0.000 (0.163) loss 0.6665 (0.8206) acc 88.2353 (82.2379) lr 1.5358e-03 eta 0:09:29
epoch [18/50] batch [15/51] time 0.189 (0.287) data 0.000 (0.109) loss 1.2012 (0.8924) acc 69.1176 (79.1773) lr 1.5358e-03 eta 0:07:58
epoch [18/50] batch [20/51] time 0.172 (0.257) data 0.000 (0.082) loss 1.0551 (0.8907) acc 76.5000 (79.3616) lr 1.5358e-03 eta 0:07:07
epoch [18/50] batch [25/51] time 0.185 (0.240) data 0.001 (0.065) loss 0.7375 (0.8893) acc 82.2115 (79.3755) lr 1.5358e-03 eta 0:06:37
epoch [18/50] batch [30/51] time 0.174 (0.228) data 0.000 (0.055) loss 0.7555 (0.8888) acc 79.7872 (79.3036) lr 1.5358e-03 eta 0:06:17
epoch [18/50] batch [35/51] time 0.170 (0.220) data 0.000 (0.047) loss 0.8743 (0.8859) acc 79.8913 (79.0238) lr 1.5358e-03 eta 0:06:02
epoch [18/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.041) loss 0.9274 (0.8885) acc 79.9020 (79.1160) lr 1.5358e-03 eta 0:05:51
epoch [18/50] batch [45/51] time 0.170 (0.209) data 0.000 (0.036) loss 0.9618 (0.8983) acc 76.4706 (78.6942) lr 1.5358e-03 eta 0:05:41
epoch [18/50] batch [50/51] time 0.154 (0.204) data 0.000 (0.033) loss 0.8977 (0.9065) acc 78.9773 (78.2602) lr 1.5358e-03 eta 0:05:32
>>> alpha1: 0.263  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.49 <<<
epoch [19/50] batch [5/51] time 0.167 (0.498) data 0.001 (0.321) loss 1.0040 (0.9310) acc 73.9362 (80.2670) lr 1.4818e-03 eta 0:13:30
epoch [19/50] batch [10/51] time 0.174 (0.334) data 0.000 (0.161) loss 0.8316 (0.8768) acc 76.4706 (79.2682) lr 1.4818e-03 eta 0:09:01
epoch [19/50] batch [15/51] time 0.166 (0.280) data 0.000 (0.107) loss 0.7040 (0.9104) acc 81.6327 (78.6016) lr 1.4818e-03 eta 0:07:32
epoch [19/50] batch [20/51] time 0.172 (0.255) data 0.000 (0.081) loss 0.7921 (0.8886) acc 81.3726 (79.4370) lr 1.4818e-03 eta 0:06:51
epoch [19/50] batch [25/51] time 0.174 (0.238) data 0.000 (0.065) loss 0.7424 (0.8814) acc 83.1731 (79.6368) lr 1.4818e-03 eta 0:06:21
epoch [19/50] batch [30/51] time 0.169 (0.227) data 0.000 (0.054) loss 0.5704 (0.8881) acc 91.3265 (79.6238) lr 1.4818e-03 eta 0:06:03
epoch [19/50] batch [35/51] time 0.193 (0.219) data 0.000 (0.046) loss 0.6544 (0.8813) acc 87.0192 (80.1205) lr 1.4818e-03 eta 0:05:49
epoch [19/50] batch [40/51] time 0.166 (0.212) data 0.000 (0.041) loss 0.8576 (0.8883) acc 77.5510 (79.9825) lr 1.4818e-03 eta 0:05:38
epoch [19/50] batch [45/51] time 0.156 (0.206) data 0.000 (0.036) loss 1.0113 (0.8996) acc 73.8889 (79.3772) lr 1.4818e-03 eta 0:05:27
epoch [19/50] batch [50/51] time 0.164 (0.217) data 0.000 (0.033) loss 0.6550 (0.8946) acc 85.9375 (79.4525) lr 1.4818e-03 eta 0:05:42
>>> alpha1: 0.241  alpha2: 0.045 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.51 <<<
epoch [20/50] batch [5/51] time 0.168 (0.484) data 0.000 (0.304) loss 0.8117 (0.8498) acc 78.5000 (81.2418) lr 1.4258e-03 eta 0:12:42
epoch [20/50] batch [10/51] time 0.185 (0.328) data 0.000 (0.152) loss 0.5716 (0.7966) acc 85.4167 (80.7787) lr 1.4258e-03 eta 0:08:34
epoch [20/50] batch [15/51] time 0.165 (0.276) data 0.000 (0.102) loss 0.7331 (0.8028) acc 83.1633 (81.3381) lr 1.4258e-03 eta 0:07:12
epoch [20/50] batch [20/51] time 0.161 (0.249) data 0.000 (0.076) loss 0.9095 (0.8210) acc 75.5319 (80.8818) lr 1.4258e-03 eta 0:06:28
epoch [20/50] batch [25/51] time 0.173 (0.235) data 0.000 (0.061) loss 0.8932 (0.8139) acc 79.3269 (81.1537) lr 1.4258e-03 eta 0:06:05
epoch [20/50] batch [30/51] time 0.167 (0.225) data 0.000 (0.051) loss 0.7171 (0.8077) acc 82.6531 (81.2366) lr 1.4258e-03 eta 0:05:49
epoch [20/50] batch [35/51] time 0.188 (0.218) data 0.000 (0.044) loss 0.7457 (0.8084) acc 86.1702 (80.9345) lr 1.4258e-03 eta 0:05:36
epoch [20/50] batch [40/51] time 0.170 (0.212) data 0.000 (0.038) loss 0.8625 (0.8214) acc 80.8824 (80.6654) lr 1.4258e-03 eta 0:05:26
epoch [20/50] batch [45/51] time 0.177 (0.207) data 0.000 (0.034) loss 0.7199 (0.8182) acc 79.2453 (80.5698) lr 1.4258e-03 eta 0:05:18
epoch [20/50] batch [50/51] time 0.163 (0.203) data 0.000 (0.031) loss 0.8240 (0.8107) acc 80.2083 (80.7047) lr 1.4258e-03 eta 0:05:11
>>> alpha1: 0.225  alpha2: 0.040 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.51 <<<
epoch [21/50] batch [5/51] time 0.162 (0.437) data 0.001 (0.257) loss 1.1095 (0.7934) acc 75.0000 (82.3484) lr 1.3681e-03 eta 0:11:06
epoch [21/50] batch [10/51] time 0.176 (0.308) data 0.000 (0.129) loss 0.7094 (0.7549) acc 85.3774 (82.9623) lr 1.3681e-03 eta 0:07:48
epoch [21/50] batch [15/51] time 0.171 (0.267) data 0.000 (0.086) loss 0.6301 (0.7448) acc 86.5000 (83.8662) lr 1.3681e-03 eta 0:06:43
epoch [21/50] batch [20/51] time 0.170 (0.244) data 0.000 (0.064) loss 0.7790 (0.7602) acc 80.8824 (83.1902) lr 1.3681e-03 eta 0:06:08
epoch [21/50] batch [25/51] time 0.171 (0.231) data 0.000 (0.052) loss 0.8948 (0.7838) acc 82.8431 (82.7619) lr 1.3681e-03 eta 0:05:47
epoch [21/50] batch [30/51] time 0.186 (0.221) data 0.000 (0.043) loss 0.7315 (0.7723) acc 81.8627 (82.8594) lr 1.3681e-03 eta 0:05:31
epoch [21/50] batch [35/51] time 0.165 (0.214) data 0.000 (0.037) loss 0.7974 (0.7668) acc 80.6122 (82.7868) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [40/51] time 0.164 (0.209) data 0.000 (0.032) loss 0.8608 (0.7850) acc 81.1225 (82.2692) lr 1.3681e-03 eta 0:05:11
epoch [21/50] batch [45/51] time 0.160 (0.204) data 0.000 (0.029) loss 0.9819 (0.7839) acc 77.6596 (82.4215) lr 1.3681e-03 eta 0:05:02
epoch [21/50] batch [50/51] time 0.171 (0.200) data 0.000 (0.026) loss 0.6003 (0.7732) acc 84.1346 (82.7782) lr 1.3681e-03 eta 0:04:56
>>> alpha1: 0.216  alpha2: 0.040 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [22/50] batch [5/51] time 0.167 (0.448) data 0.000 (0.271) loss 0.7709 (0.7860) acc 83.6735 (83.3736) lr 1.3090e-03 eta 0:11:00
epoch [22/50] batch [10/51] time 0.171 (0.309) data 0.000 (0.135) loss 0.7646 (0.7898) acc 82.0000 (82.2743) lr 1.3090e-03 eta 0:07:34
epoch [22/50] batch [15/51] time 0.176 (0.267) data 0.000 (0.090) loss 0.7297 (0.7449) acc 81.5000 (82.9374) lr 1.3090e-03 eta 0:06:30
epoch [22/50] batch [20/51] time 0.181 (0.244) data 0.000 (0.068) loss 0.6777 (0.7399) acc 85.4546 (83.1256) lr 1.3090e-03 eta 0:05:56
epoch [22/50] batch [25/51] time 0.188 (0.232) data 0.000 (0.054) loss 0.9226 (0.7672) acc 76.4151 (81.8237) lr 1.3090e-03 eta 0:05:36
epoch [22/50] batch [30/51] time 0.164 (0.224) data 0.000 (0.045) loss 0.8855 (0.7854) acc 82.9787 (81.7386) lr 1.3090e-03 eta 0:05:23
epoch [22/50] batch [35/51] time 0.176 (0.216) data 0.000 (0.039) loss 0.6830 (0.7857) acc 88.9423 (81.8213) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [40/51] time 0.170 (0.211) data 0.000 (0.034) loss 0.9734 (0.7927) acc 72.0588 (81.7580) lr 1.3090e-03 eta 0:05:03
epoch [22/50] batch [45/51] time 0.166 (0.207) data 0.000 (0.031) loss 0.8466 (0.7913) acc 82.1429 (81.9580) lr 1.3090e-03 eta 0:04:56
epoch [22/50] batch [50/51] time 0.169 (0.204) data 0.000 (0.028) loss 0.6399 (0.7832) acc 85.0000 (82.0618) lr 1.3090e-03 eta 0:04:51
>>> alpha1: 0.204  alpha2: 0.037 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [23/50] batch [5/51] time 0.170 (0.475) data 0.000 (0.299) loss 0.5204 (1.0606) acc 94.3878 (80.0216) lr 1.2487e-03 eta 0:11:16
epoch [23/50] batch [10/51] time 0.168 (0.326) data 0.000 (0.150) loss 0.9230 (0.9071) acc 82.1429 (82.1950) lr 1.2487e-03 eta 0:07:42
epoch [23/50] batch [15/51] time 0.161 (0.276) data 0.000 (0.100) loss 0.8703 (0.8483) acc 77.7174 (81.6371) lr 1.2487e-03 eta 0:06:30
epoch [23/50] batch [20/51] time 0.169 (0.251) data 0.000 (0.075) loss 0.8906 (0.8432) acc 77.0000 (81.0195) lr 1.2487e-03 eta 0:05:53
epoch [23/50] batch [25/51] time 0.171 (0.236) data 0.000 (0.060) loss 0.5375 (0.8107) acc 87.7451 (81.2100) lr 1.2487e-03 eta 0:05:30
epoch [23/50] batch [30/51] time 0.171 (0.227) data 0.000 (0.050) loss 0.9384 (0.7995) acc 80.8824 (81.4224) lr 1.2487e-03 eta 0:05:17
epoch [23/50] batch [35/51] time 0.179 (0.220) data 0.000 (0.043) loss 0.7457 (0.8077) acc 85.7143 (81.4770) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [40/51] time 0.168 (0.214) data 0.000 (0.038) loss 0.6890 (0.7878) acc 82.5000 (81.8366) lr 1.2487e-03 eta 0:04:56
epoch [23/50] batch [45/51] time 0.161 (0.209) data 0.000 (0.033) loss 1.2142 (0.7969) acc 73.9362 (81.6676) lr 1.2487e-03 eta 0:04:48
epoch [23/50] batch [50/51] time 0.182 (0.206) data 0.000 (0.030) loss 0.5794 (0.7841) acc 86.1607 (81.8949) lr 1.2487e-03 eta 0:04:43
>>> alpha1: 0.195  alpha2: 0.034 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [24/50] batch [5/51] time 0.213 (0.460) data 0.000 (0.266) loss 0.5730 (0.7830) acc 85.3774 (84.6960) lr 1.1874e-03 eta 0:10:31
epoch [24/50] batch [10/51] time 0.171 (0.320) data 0.000 (0.133) loss 1.0954 (0.8385) acc 69.0217 (81.4100) lr 1.1874e-03 eta 0:07:17
epoch [24/50] batch [15/51] time 0.166 (0.273) data 0.000 (0.089) loss 0.7300 (0.8023) acc 85.1064 (82.0279) lr 1.1874e-03 eta 0:06:11
epoch [24/50] batch [20/51] time 0.178 (0.249) data 0.000 (0.067) loss 0.7061 (0.7749) acc 83.6735 (82.8280) lr 1.1874e-03 eta 0:05:38
epoch [24/50] batch [25/51] time 0.187 (0.236) data 0.000 (0.053) loss 0.5572 (0.7606) acc 87.2549 (82.7066) lr 1.1874e-03 eta 0:05:19
epoch [24/50] batch [30/51] time 0.181 (0.227) data 0.000 (0.045) loss 0.9411 (0.7717) acc 78.2407 (82.4914) lr 1.1874e-03 eta 0:05:06
epoch [24/50] batch [35/51] time 0.178 (0.220) data 0.000 (0.038) loss 0.6932 (0.7600) acc 87.5000 (82.8693) lr 1.1874e-03 eta 0:04:54
epoch [24/50] batch [40/51] time 0.170 (0.214) data 0.000 (0.034) loss 0.8660 (0.7572) acc 77.5000 (82.8872) lr 1.1874e-03 eta 0:04:46
epoch [24/50] batch [45/51] time 0.167 (0.210) data 0.000 (0.030) loss 0.4979 (0.7527) acc 89.7959 (82.9174) lr 1.1874e-03 eta 0:04:39
epoch [24/50] batch [50/51] time 0.169 (0.206) data 0.000 (0.027) loss 0.6224 (0.7427) acc 84.5000 (83.2823) lr 1.1874e-03 eta 0:04:33
>>> alpha1: 0.187  alpha2: 0.034 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.50 <<<
epoch [25/50] batch [5/51] time 0.183 (0.523) data 0.001 (0.337) loss 0.8192 (0.7207) acc 79.9020 (80.9699) lr 1.1253e-03 eta 0:11:31
epoch [25/50] batch [10/51] time 0.180 (0.352) data 0.000 (0.169) loss 0.6910 (0.6970) acc 83.7963 (82.6555) lr 1.1253e-03 eta 0:07:42
epoch [25/50] batch [15/51] time 0.160 (0.293) data 0.000 (0.113) loss 0.6724 (0.6893) acc 84.7826 (83.8046) lr 1.1253e-03 eta 0:06:24
epoch [25/50] batch [20/51] time 0.179 (0.264) data 0.000 (0.084) loss 0.6739 (0.6947) acc 80.3922 (83.1768) lr 1.1253e-03 eta 0:05:44
epoch [25/50] batch [25/51] time 0.167 (0.247) data 0.000 (0.068) loss 0.7180 (0.6947) acc 80.6122 (83.3802) lr 1.1253e-03 eta 0:05:21
epoch [25/50] batch [30/51] time 0.185 (0.236) data 0.000 (0.056) loss 0.6133 (0.6843) acc 83.3333 (83.5394) lr 1.1253e-03 eta 0:05:05
epoch [25/50] batch [35/51] time 0.177 (0.228) data 0.000 (0.048) loss 0.9565 (0.6909) acc 76.4151 (83.3141) lr 1.1253e-03 eta 0:04:54
epoch [25/50] batch [40/51] time 0.172 (0.222) data 0.000 (0.043) loss 0.4723 (0.6799) acc 90.1961 (83.5752) lr 1.1253e-03 eta 0:04:45
epoch [25/50] batch [45/51] time 0.167 (0.216) data 0.000 (0.038) loss 0.8327 (0.6890) acc 76.5306 (83.3183) lr 1.1253e-03 eta 0:04:36
epoch [25/50] batch [50/51] time 0.170 (0.212) data 0.000 (0.034) loss 0.6739 (0.6982) acc 92.1569 (83.4330) lr 1.1253e-03 eta 0:04:29
>>> alpha1: 0.187  alpha2: 0.038 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.52 <<<
epoch [26/50] batch [5/51] time 0.169 (0.479) data 0.000 (0.302) loss 0.5959 (0.6539) acc 84.5000 (87.4629) lr 1.0628e-03 eta 0:10:08
epoch [26/50] batch [10/51] time 0.181 (0.329) data 0.000 (0.151) loss 0.7545 (0.6961) acc 83.1818 (85.3619) lr 1.0628e-03 eta 0:06:55
epoch [26/50] batch [15/51] time 0.163 (0.279) data 0.000 (0.101) loss 0.7305 (0.6906) acc 82.6087 (84.3625) lr 1.0628e-03 eta 0:05:51
epoch [26/50] batch [20/51] time 0.191 (0.255) data 0.000 (0.076) loss 0.7521 (0.6870) acc 81.4655 (84.0313) lr 1.0628e-03 eta 0:05:19
epoch [26/50] batch [25/51] time 0.168 (0.240) data 0.000 (0.061) loss 0.5251 (0.7451) acc 87.2449 (83.7713) lr 1.0628e-03 eta 0:04:59
epoch [26/50] batch [30/51] time 0.170 (0.229) data 0.000 (0.051) loss 0.6206 (0.7411) acc 85.0000 (83.6936) lr 1.0628e-03 eta 0:04:44
epoch [26/50] batch [35/51] time 0.166 (0.222) data 0.000 (0.043) loss 0.8954 (0.7438) acc 75.5208 (83.8604) lr 1.0628e-03 eta 0:04:35
epoch [26/50] batch [40/51] time 0.165 (0.217) data 0.000 (0.038) loss 0.7621 (0.7451) acc 78.6458 (83.7348) lr 1.0628e-03 eta 0:04:28
epoch [26/50] batch [45/51] time 0.181 (0.213) data 0.000 (0.034) loss 0.5692 (0.7357) acc 80.4545 (83.4841) lr 1.0628e-03 eta 0:04:21
epoch [26/50] batch [50/51] time 0.163 (0.208) data 0.000 (0.030) loss 0.9221 (0.7535) acc 76.0417 (83.2526) lr 1.0628e-03 eta 0:04:15
>>> alpha1: 0.178  alpha2: 0.036 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.50 <<<
epoch [27/50] batch [5/51] time 0.184 (0.528) data 0.000 (0.346) loss 0.5580 (0.6223) acc 88.6364 (87.7496) lr 1.0000e-03 eta 0:10:43
epoch [27/50] batch [10/51] time 0.180 (0.352) data 0.001 (0.173) loss 0.5604 (0.6255) acc 87.9630 (86.6659) lr 1.0000e-03 eta 0:07:07
epoch [27/50] batch [15/51] time 0.178 (0.293) data 0.000 (0.115) loss 0.5486 (0.6313) acc 89.4231 (85.9865) lr 1.0000e-03 eta 0:05:54
epoch [27/50] batch [20/51] time 0.185 (0.264) data 0.000 (0.087) loss 0.4615 (0.6403) acc 94.2982 (86.2042) lr 1.0000e-03 eta 0:05:18
epoch [27/50] batch [25/51] time 0.199 (0.248) data 0.000 (0.069) loss 0.6669 (0.6531) acc 85.7143 (85.8065) lr 1.0000e-03 eta 0:04:57
epoch [27/50] batch [30/51] time 0.169 (0.236) data 0.000 (0.058) loss 0.8386 (0.6688) acc 79.1667 (85.0135) lr 1.0000e-03 eta 0:04:42
epoch [27/50] batch [35/51] time 0.173 (0.228) data 0.000 (0.050) loss 0.7606 (0.6538) acc 80.8824 (85.2630) lr 1.0000e-03 eta 0:04:31
epoch [27/50] batch [40/51] time 0.166 (0.221) data 0.000 (0.043) loss 0.7695 (0.6515) acc 85.7143 (85.4092) lr 1.0000e-03 eta 0:04:22
epoch [27/50] batch [45/51] time 0.178 (0.216) data 0.000 (0.039) loss 0.7394 (0.6544) acc 76.8519 (85.1483) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [50/51] time 0.176 (0.212) data 0.000 (0.035) loss 0.7369 (0.6499) acc 83.9623 (85.1246) lr 1.0000e-03 eta 0:04:08
>>> alpha1: 0.179  alpha2: 0.040 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [28/50] batch [5/51] time 0.186 (0.527) data 0.000 (0.337) loss 0.8547 (0.7197) acc 82.4468 (84.4483) lr 9.3721e-04 eta 0:10:15
epoch [28/50] batch [10/51] time 0.171 (0.352) data 0.000 (0.169) loss 0.6990 (0.7031) acc 84.8039 (84.3848) lr 9.3721e-04 eta 0:06:49
epoch [28/50] batch [15/51] time 0.201 (0.299) data 0.000 (0.113) loss 0.5944 (0.6453) acc 86.5741 (85.8310) lr 9.3721e-04 eta 0:05:45
epoch [28/50] batch [20/51] time 0.181 (0.268) data 0.001 (0.085) loss 0.7529 (0.6637) acc 82.8431 (84.9755) lr 9.3721e-04 eta 0:05:09
epoch [28/50] batch [25/51] time 0.178 (0.250) data 0.000 (0.068) loss 0.8959 (0.6745) acc 78.0000 (84.4373) lr 9.3721e-04 eta 0:04:46
epoch [28/50] batch [30/51] time 0.186 (0.239) data 0.000 (0.057) loss 0.4850 (0.6600) acc 94.8113 (85.1558) lr 9.3721e-04 eta 0:04:32
epoch [28/50] batch [35/51] time 0.182 (0.230) data 0.000 (0.048) loss 0.4706 (0.6501) acc 90.4546 (85.2672) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [40/51] time 0.171 (0.224) data 0.000 (0.042) loss 0.6372 (0.6558) acc 83.8235 (84.9520) lr 9.3721e-04 eta 0:04:13
epoch [28/50] batch [45/51] time 0.189 (0.219) data 0.000 (0.038) loss 0.9133 (0.6561) acc 73.6607 (85.0809) lr 9.3721e-04 eta 0:04:07
epoch [28/50] batch [50/51] time 0.174 (0.215) data 0.001 (0.034) loss 0.8243 (0.6528) acc 79.8077 (85.2065) lr 9.3721e-04 eta 0:04:01
>>> alpha1: 0.175  alpha2: 0.042 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.32 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [29/50] batch [5/51] time 0.192 (0.430) data 0.000 (0.253) loss 0.7224 (0.6131) acc 81.4815 (86.2491) lr 8.7467e-04 eta 0:08:00
epoch [29/50] batch [10/51] time 0.168 (0.303) data 0.000 (0.126) loss 0.9835 (0.6848) acc 79.5000 (85.0577) lr 8.7467e-04 eta 0:05:37
epoch [29/50] batch [15/51] time 0.176 (0.261) data 0.000 (0.084) loss 0.6493 (0.6830) acc 82.6531 (84.0501) lr 8.7467e-04 eta 0:04:49
epoch [29/50] batch [20/51] time 0.183 (0.241) data 0.000 (0.063) loss 0.4715 (0.6746) acc 87.7451 (83.8207) lr 8.7467e-04 eta 0:04:26
epoch [29/50] batch [25/51] time 0.188 (0.229) data 0.000 (0.051) loss 0.4809 (0.6545) acc 87.5000 (84.7755) lr 8.7467e-04 eta 0:04:11
epoch [29/50] batch [30/51] time 0.171 (0.221) data 0.000 (0.042) loss 0.5665 (0.6484) acc 89.2157 (84.9750) lr 8.7467e-04 eta 0:04:01
epoch [29/50] batch [35/51] time 0.179 (0.214) data 0.000 (0.036) loss 0.6918 (0.6528) acc 87.0370 (84.6152) lr 8.7467e-04 eta 0:03:52
epoch [29/50] batch [40/51] time 0.173 (0.210) data 0.000 (0.032) loss 0.9793 (0.6587) acc 80.2885 (84.6698) lr 8.7467e-04 eta 0:03:47
epoch [29/50] batch [45/51] time 0.164 (0.206) data 0.000 (0.028) loss 0.8284 (0.6588) acc 80.7292 (84.7984) lr 8.7467e-04 eta 0:03:41
epoch [29/50] batch [50/51] time 0.172 (0.203) data 0.000 (0.026) loss 0.6799 (0.6630) acc 83.8235 (84.5890) lr 8.7467e-04 eta 0:03:37
>>> alpha1: 0.173  alpha2: 0.044 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.50 <<<
epoch [30/50] batch [5/51] time 0.171 (0.502) data 0.000 (0.315) loss 0.5266 (0.5970) acc 86.7647 (86.4325) lr 8.1262e-04 eta 0:08:55
epoch [30/50] batch [10/51] time 0.190 (0.340) data 0.001 (0.158) loss 0.6010 (0.6153) acc 88.9423 (85.7215) lr 8.1262e-04 eta 0:06:00
epoch [30/50] batch [15/51] time 0.202 (0.289) data 0.000 (0.105) loss 0.6417 (0.6108) acc 85.3448 (86.1569) lr 8.1262e-04 eta 0:05:04
epoch [30/50] batch [20/51] time 0.183 (0.261) data 0.000 (0.079) loss 0.6164 (0.6090) acc 88.4259 (86.3150) lr 8.1262e-04 eta 0:04:33
epoch [30/50] batch [25/51] time 0.197 (0.245) data 0.000 (0.063) loss 0.5264 (0.6076) acc 87.9808 (86.5184) lr 8.1262e-04 eta 0:04:16
epoch [30/50] batch [30/51] time 0.171 (0.235) data 0.000 (0.053) loss 0.6531 (0.5961) acc 83.3333 (86.5089) lr 8.1262e-04 eta 0:04:04
epoch [30/50] batch [35/51] time 0.200 (0.228) data 0.000 (0.045) loss 0.8598 (0.6193) acc 85.0000 (85.9364) lr 8.1262e-04 eta 0:03:56
epoch [30/50] batch [40/51] time 0.184 (0.222) data 0.001 (0.040) loss 0.5703 (0.6230) acc 85.2679 (85.8158) lr 8.1262e-04 eta 0:03:48
epoch [30/50] batch [45/51] time 0.172 (0.216) data 0.000 (0.035) loss 0.8337 (0.6277) acc 79.4118 (85.6472) lr 8.1262e-04 eta 0:03:42
epoch [30/50] batch [50/51] time 0.157 (0.212) data 0.000 (0.032) loss 0.6609 (0.6368) acc 88.6905 (85.6462) lr 8.1262e-04 eta 0:03:36
>>> alpha1: 0.171  alpha2: 0.047 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.49 <<<
epoch [31/50] batch [5/51] time 0.188 (0.485) data 0.001 (0.304) loss 0.9515 (0.6276) acc 78.9216 (87.1000) lr 7.5131e-04 eta 0:08:12
epoch [31/50] batch [10/51] time 0.184 (0.335) data 0.000 (0.152) loss 0.4203 (0.5881) acc 91.5179 (87.6331) lr 7.5131e-04 eta 0:05:38
epoch [31/50] batch [15/51] time 0.170 (0.283) data 0.000 (0.101) loss 0.6423 (0.5827) acc 86.7647 (87.6583) lr 7.5131e-04 eta 0:04:44
epoch [31/50] batch [20/51] time 0.183 (0.257) data 0.000 (0.076) loss 0.5603 (0.5878) acc 89.2857 (87.4530) lr 7.5131e-04 eta 0:04:17
epoch [31/50] batch [25/51] time 0.178 (0.242) data 0.000 (0.061) loss 0.7893 (0.5912) acc 79.7170 (86.6196) lr 7.5131e-04 eta 0:04:00
epoch [31/50] batch [30/51] time 0.185 (0.231) data 0.000 (0.051) loss 0.8469 (0.6040) acc 73.0000 (86.0658) lr 7.5131e-04 eta 0:03:48
epoch [31/50] batch [35/51] time 0.166 (0.223) data 0.000 (0.044) loss 0.8053 (0.6006) acc 78.0612 (86.0275) lr 7.5131e-04 eta 0:03:39
epoch [31/50] batch [40/51] time 0.178 (0.217) data 0.000 (0.038) loss 0.6662 (0.6174) acc 82.8704 (85.7019) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [45/51] time 0.181 (0.212) data 0.000 (0.034) loss 0.4659 (0.6135) acc 90.1786 (85.9003) lr 7.5131e-04 eta 0:03:27
epoch [31/50] batch [50/51] time 0.172 (0.209) data 0.000 (0.030) loss 0.4725 (0.6125) acc 90.8654 (85.9783) lr 7.5131e-04 eta 0:03:22
>>> alpha1: 0.170  alpha2: 0.053 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [32/50] batch [5/51] time 0.181 (0.507) data 0.000 (0.324) loss 0.5391 (0.5017) acc 88.5000 (90.3848) lr 6.9098e-04 eta 0:08:08
epoch [32/50] batch [10/51] time 0.164 (0.344) data 0.000 (0.162) loss 0.6076 (0.5295) acc 90.6250 (89.1581) lr 6.9098e-04 eta 0:05:29
epoch [32/50] batch [15/51] time 0.171 (0.288) data 0.000 (0.108) loss 0.5335 (0.5558) acc 83.8889 (87.8213) lr 6.9098e-04 eta 0:04:35
epoch [32/50] batch [20/51] time 0.171 (0.261) data 0.000 (0.081) loss 0.5796 (0.5573) acc 85.7843 (87.6278) lr 6.9098e-04 eta 0:04:07
epoch [32/50] batch [25/51] time 0.178 (0.245) data 0.000 (0.065) loss 0.5695 (0.5555) acc 87.2549 (87.8152) lr 6.9098e-04 eta 0:03:51
epoch [32/50] batch [30/51] time 0.175 (0.234) data 0.001 (0.054) loss 0.8886 (0.5805) acc 81.6327 (87.3333) lr 6.9098e-04 eta 0:03:39
epoch [32/50] batch [35/51] time 0.180 (0.227) data 0.000 (0.046) loss 0.6431 (0.5831) acc 84.7222 (87.0229) lr 6.9098e-04 eta 0:03:32
epoch [32/50] batch [40/51] time 0.161 (0.220) data 0.000 (0.041) loss 0.5738 (0.5917) acc 86.9565 (86.7095) lr 6.9098e-04 eta 0:03:24
epoch [32/50] batch [45/51] time 0.167 (0.215) data 0.000 (0.036) loss 0.7054 (0.5900) acc 82.6531 (86.8884) lr 6.9098e-04 eta 0:03:18
epoch [32/50] batch [50/51] time 0.169 (0.211) data 0.000 (0.033) loss 0.5674 (0.5950) acc 86.5000 (86.7420) lr 6.9098e-04 eta 0:03:13
>>> alpha1: 0.168  alpha2: 0.054 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [33/50] batch [5/51] time 0.189 (0.482) data 0.000 (0.296) loss 0.4326 (0.4546) acc 90.0862 (90.0692) lr 6.3188e-04 eta 0:07:20
epoch [33/50] batch [10/51] time 0.179 (0.328) data 0.000 (0.148) loss 0.4842 (0.5590) acc 88.8889 (86.6737) lr 6.3188e-04 eta 0:04:57
epoch [33/50] batch [15/51] time 0.179 (0.279) data 0.000 (0.099) loss 0.7049 (0.5806) acc 85.0000 (86.8414) lr 6.3188e-04 eta 0:04:11
epoch [33/50] batch [20/51] time 0.186 (0.256) data 0.000 (0.074) loss 0.6637 (0.6038) acc 87.5000 (86.4784) lr 6.3188e-04 eta 0:03:49
epoch [33/50] batch [25/51] time 0.167 (0.241) data 0.000 (0.059) loss 0.6346 (0.6118) acc 84.8958 (86.4587) lr 6.3188e-04 eta 0:03:34
epoch [33/50] batch [30/51] time 0.178 (0.231) data 0.000 (0.050) loss 0.5389 (0.6056) acc 89.1509 (86.3690) lr 6.3188e-04 eta 0:03:24
epoch [33/50] batch [35/51] time 0.177 (0.224) data 0.000 (0.042) loss 0.5578 (0.6054) acc 87.2642 (86.3998) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [40/51] time 0.176 (0.218) data 0.000 (0.037) loss 0.6201 (0.6133) acc 89.6226 (86.3128) lr 6.3188e-04 eta 0:03:11
epoch [33/50] batch [45/51] time 0.168 (0.213) data 0.000 (0.033) loss 0.5211 (0.6017) acc 90.0000 (86.7742) lr 6.3188e-04 eta 0:03:05
epoch [33/50] batch [50/51] time 0.168 (0.209) data 0.000 (0.030) loss 0.7166 (0.6021) acc 81.2500 (86.6591) lr 6.3188e-04 eta 0:03:01
>>> alpha1: 0.165  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.49 <<<
epoch [34/50] batch [5/51] time 0.195 (0.504) data 0.000 (0.318) loss 0.5558 (0.6202) acc 86.6071 (84.5466) lr 5.7422e-04 eta 0:07:14
epoch [34/50] batch [10/51] time 0.180 (0.349) data 0.000 (0.159) loss 0.5085 (0.6032) acc 86.3208 (85.5597) lr 5.7422e-04 eta 0:04:59
epoch [34/50] batch [15/51] time 0.177 (0.293) data 0.000 (0.106) loss 0.5893 (0.6231) acc 87.0000 (85.3624) lr 5.7422e-04 eta 0:04:09
epoch [34/50] batch [20/51] time 0.189 (0.265) data 0.001 (0.080) loss 0.7628 (0.6076) acc 85.5769 (86.1255) lr 5.7422e-04 eta 0:03:44
epoch [34/50] batch [25/51] time 0.181 (0.249) data 0.000 (0.064) loss 0.4790 (0.5944) acc 89.5000 (86.7888) lr 5.7422e-04 eta 0:03:29
epoch [34/50] batch [30/51] time 0.183 (0.237) data 0.000 (0.053) loss 0.6508 (0.5958) acc 83.9286 (86.6359) lr 5.7422e-04 eta 0:03:18
epoch [34/50] batch [35/51] time 0.170 (0.228) data 0.000 (0.046) loss 0.6527 (0.6055) acc 81.5000 (86.4529) lr 5.7422e-04 eta 0:03:10
epoch [34/50] batch [40/51] time 0.182 (0.222) data 0.000 (0.040) loss 0.4381 (0.5966) acc 90.7407 (86.6381) lr 5.7422e-04 eta 0:03:03
epoch [34/50] batch [45/51] time 0.175 (0.217) data 0.000 (0.036) loss 0.4838 (0.5910) acc 85.5769 (86.7016) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [50/51] time 0.168 (0.212) data 0.000 (0.032) loss 0.5972 (0.5927) acc 88.7755 (86.5970) lr 5.7422e-04 eta 0:02:53
>>> alpha1: 0.163  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [35/50] batch [5/51] time 0.164 (0.522) data 0.000 (0.345) loss 0.6132 (0.5777) acc 80.3191 (85.2385) lr 5.1825e-04 eta 0:07:03
epoch [35/50] batch [10/51] time 0.185 (0.353) data 0.000 (0.173) loss 0.5076 (0.6143) acc 89.9038 (86.7259) lr 5.1825e-04 eta 0:04:44
epoch [35/50] batch [15/51] time 0.172 (0.297) data 0.000 (0.115) loss 0.6284 (0.5984) acc 85.2941 (86.6652) lr 5.1825e-04 eta 0:03:57
epoch [35/50] batch [20/51] time 0.180 (0.267) data 0.000 (0.087) loss 0.6223 (0.6006) acc 85.1852 (86.5570) lr 5.1825e-04 eta 0:03:32
epoch [35/50] batch [25/51] time 0.186 (0.248) data 0.000 (0.069) loss 0.5391 (0.6278) acc 87.5000 (86.3693) lr 5.1825e-04 eta 0:03:16
epoch [35/50] batch [30/51] time 0.183 (0.238) data 0.000 (0.058) loss 0.3087 (0.6137) acc 93.6364 (86.7445) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [35/51] time 0.175 (0.229) data 0.000 (0.050) loss 0.5714 (0.6201) acc 84.1346 (86.4245) lr 5.1825e-04 eta 0:02:58
epoch [35/50] batch [40/51] time 0.165 (0.223) data 0.000 (0.043) loss 0.4921 (0.6080) acc 91.1458 (86.7334) lr 5.1825e-04 eta 0:02:53
epoch [35/50] batch [45/51] time 0.180 (0.218) data 0.000 (0.039) loss 0.4572 (0.6020) acc 92.5926 (86.7964) lr 5.1825e-04 eta 0:02:47
epoch [35/50] batch [50/51] time 0.174 (0.214) data 0.000 (0.035) loss 0.7106 (0.5993) acc 80.7692 (86.6159) lr 5.1825e-04 eta 0:02:43
>>> alpha1: 0.162  alpha2: 0.060 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.46 <<<
epoch [36/50] batch [5/51] time 0.179 (0.455) data 0.000 (0.266) loss 0.6844 (0.5064) acc 82.5000 (88.9596) lr 4.6417e-04 eta 0:05:45
epoch [36/50] batch [10/51] time 0.180 (0.317) data 0.000 (0.133) loss 0.5091 (0.4991) acc 89.8148 (88.9994) lr 4.6417e-04 eta 0:03:59
epoch [36/50] batch [15/51] time 0.192 (0.272) data 0.000 (0.089) loss 0.4081 (0.5063) acc 90.5660 (88.6378) lr 4.6417e-04 eta 0:03:24
epoch [36/50] batch [20/51] time 0.172 (0.250) data 0.000 (0.067) loss 0.5036 (0.5325) acc 88.2353 (87.7401) lr 4.6417e-04 eta 0:03:06
epoch [36/50] batch [25/51] time 0.177 (0.237) data 0.000 (0.053) loss 0.5207 (0.5285) acc 89.1509 (87.9084) lr 4.6417e-04 eta 0:02:55
epoch [36/50] batch [30/51] time 0.185 (0.227) data 0.000 (0.045) loss 0.6885 (0.5392) acc 85.2941 (87.6508) lr 4.6417e-04 eta 0:02:46
epoch [36/50] batch [35/51] time 0.169 (0.220) data 0.000 (0.038) loss 0.6137 (0.5464) acc 84.0000 (87.6614) lr 4.6417e-04 eta 0:02:40
epoch [36/50] batch [40/51] time 0.179 (0.215) data 0.000 (0.033) loss 0.6531 (0.5568) acc 87.0370 (87.4510) lr 4.6417e-04 eta 0:02:35
epoch [36/50] batch [45/51] time 0.179 (0.210) data 0.000 (0.030) loss 0.4352 (0.5642) acc 93.1373 (87.3691) lr 4.6417e-04 eta 0:02:31
epoch [36/50] batch [50/51] time 0.189 (0.208) data 0.000 (0.027) loss 0.5141 (0.5609) acc 90.1786 (87.4395) lr 4.6417e-04 eta 0:02:28
>>> alpha1: 0.161  alpha2: 0.060 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [37/50] batch [5/51] time 0.183 (0.531) data 0.000 (0.341) loss 0.5851 (0.5958) acc 86.8182 (87.0899) lr 4.1221e-04 eta 0:06:16
epoch [37/50] batch [10/51] time 0.184 (0.356) data 0.000 (0.171) loss 0.3955 (0.5984) acc 92.5000 (86.1586) lr 4.1221e-04 eta 0:04:10
epoch [37/50] batch [15/51] time 0.174 (0.297) data 0.001 (0.114) loss 0.4188 (0.6081) acc 90.3846 (86.2189) lr 4.1221e-04 eta 0:03:27
epoch [37/50] batch [20/51] time 0.162 (0.267) data 0.000 (0.085) loss 0.5572 (0.6086) acc 88.3333 (86.5516) lr 4.1221e-04 eta 0:03:05
epoch [37/50] batch [25/51] time 0.182 (0.250) data 0.001 (0.068) loss 0.6172 (0.6078) acc 85.7843 (86.5627) lr 4.1221e-04 eta 0:02:51
epoch [37/50] batch [30/51] time 0.163 (0.237) data 0.000 (0.057) loss 0.5475 (0.6056) acc 92.0213 (86.9988) lr 4.1221e-04 eta 0:02:41
epoch [37/50] batch [35/51] time 0.215 (0.229) data 0.000 (0.049) loss 0.7420 (0.6017) acc 86.1607 (86.8563) lr 4.1221e-04 eta 0:02:35
epoch [37/50] batch [40/51] time 0.178 (0.223) data 0.000 (0.043) loss 0.2902 (0.5893) acc 94.4444 (87.1698) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [45/51] time 0.178 (0.218) data 0.000 (0.038) loss 0.5459 (0.5882) acc 86.7647 (87.2030) lr 4.1221e-04 eta 0:02:25
epoch [37/50] batch [50/51] time 0.169 (0.213) data 0.001 (0.034) loss 0.5402 (0.5819) acc 89.5000 (87.0923) lr 4.1221e-04 eta 0:02:21
>>> alpha1: 0.164  alpha2: 0.064 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.46 <<<
epoch [38/50] batch [5/51] time 0.182 (0.463) data 0.001 (0.280) loss 0.4459 (0.5980) acc 87.5000 (86.5108) lr 3.6258e-04 eta 0:05:04
epoch [38/50] batch [10/51] time 0.181 (0.322) data 0.000 (0.140) loss 0.4036 (0.5755) acc 92.2727 (87.8631) lr 3.6258e-04 eta 0:03:30
epoch [38/50] batch [15/51] time 0.201 (0.275) data 0.000 (0.093) loss 0.4768 (0.5512) acc 87.2807 (88.3351) lr 3.6258e-04 eta 0:02:58
epoch [38/50] batch [20/51] time 0.188 (0.252) data 0.000 (0.070) loss 0.6133 (0.5545) acc 81.9444 (87.9307) lr 3.6258e-04 eta 0:02:41
epoch [38/50] batch [25/51] time 0.170 (0.237) data 0.000 (0.056) loss 0.4510 (0.5491) acc 89.5000 (88.1983) lr 3.6258e-04 eta 0:02:31
epoch [38/50] batch [30/51] time 0.177 (0.226) data 0.000 (0.047) loss 0.6428 (0.5657) acc 89.6226 (87.6254) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.184 (0.219) data 0.001 (0.040) loss 0.7502 (0.5778) acc 89.0000 (87.4777) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [40/51] time 0.161 (0.214) data 0.000 (0.035) loss 0.3342 (0.5597) acc 96.8085 (87.9380) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [45/51] time 0.177 (0.209) data 0.000 (0.031) loss 0.6306 (0.5623) acc 85.3774 (87.8149) lr 3.6258e-04 eta 0:02:09
epoch [38/50] batch [50/51] time 0.173 (0.205) data 0.000 (0.028) loss 0.4370 (0.5596) acc 90.3846 (87.6283) lr 3.6258e-04 eta 0:02:05
>>> alpha1: 0.163  alpha2: 0.064 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [39/50] batch [5/51] time 0.172 (0.487) data 0.000 (0.310) loss 0.4821 (0.5770) acc 88.2353 (85.3248) lr 3.1545e-04 eta 0:04:55
epoch [39/50] batch [10/51] time 0.167 (0.334) data 0.001 (0.155) loss 0.7158 (0.6013) acc 83.1633 (86.2537) lr 3.1545e-04 eta 0:03:20
epoch [39/50] batch [15/51] time 0.165 (0.280) data 0.001 (0.104) loss 0.5658 (0.5886) acc 88.5417 (87.0103) lr 3.1545e-04 eta 0:02:47
epoch [39/50] batch [20/51] time 0.174 (0.256) data 0.000 (0.078) loss 0.6188 (0.5557) acc 83.1731 (87.5142) lr 3.1545e-04 eta 0:02:31
epoch [39/50] batch [25/51] time 0.173 (0.241) data 0.001 (0.062) loss 0.5999 (0.5592) acc 88.5000 (87.6632) lr 3.1545e-04 eta 0:02:21
epoch [39/50] batch [30/51] time 0.201 (0.232) data 0.000 (0.052) loss 0.5879 (0.5675) acc 86.7924 (87.2934) lr 3.1545e-04 eta 0:02:14
epoch [39/50] batch [35/51] time 0.182 (0.226) data 0.000 (0.045) loss 0.5046 (0.5679) acc 87.9808 (87.1345) lr 3.1545e-04 eta 0:02:10
epoch [39/50] batch [40/51] time 0.174 (0.220) data 0.000 (0.039) loss 0.4965 (0.5646) acc 87.9808 (87.1334) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [45/51] time 0.177 (0.215) data 0.000 (0.035) loss 0.4938 (0.5676) acc 88.2075 (87.0523) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [50/51] time 0.181 (0.211) data 0.001 (0.031) loss 0.6315 (0.5658) acc 93.6274 (87.0964) lr 3.1545e-04 eta 0:01:58
>>> alpha1: 0.164  alpha2: 0.069 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [40/50] batch [5/51] time 0.182 (0.470) data 0.000 (0.290) loss 0.4281 (0.4533) acc 90.4546 (89.5772) lr 2.7103e-04 eta 0:04:21
epoch [40/50] batch [10/51] time 0.186 (0.328) data 0.000 (0.145) loss 0.4488 (0.4972) acc 90.8654 (88.9412) lr 2.7103e-04 eta 0:03:00
epoch [40/50] batch [15/51] time 0.205 (0.279) data 0.000 (0.097) loss 0.6500 (0.5359) acc 83.9623 (88.2662) lr 2.7103e-04 eta 0:02:32
epoch [40/50] batch [20/51] time 0.168 (0.254) data 0.000 (0.073) loss 0.5209 (0.5353) acc 90.7609 (88.5109) lr 2.7103e-04 eta 0:02:17
epoch [40/50] batch [25/51] time 0.186 (0.240) data 0.000 (0.058) loss 0.5568 (0.5343) acc 87.9808 (88.8054) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [30/51] time 0.179 (0.230) data 0.000 (0.049) loss 0.6006 (0.5351) acc 90.3061 (88.7220) lr 2.7103e-04 eta 0:02:02
epoch [40/50] batch [35/51] time 0.178 (0.222) data 0.000 (0.042) loss 0.4976 (0.5384) acc 89.9038 (88.4738) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [40/51] time 0.176 (0.217) data 0.001 (0.037) loss 0.6191 (0.5357) acc 87.5000 (88.6202) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [45/51] time 0.181 (0.213) data 0.000 (0.033) loss 0.5701 (0.5383) acc 84.2593 (88.3069) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.181 (0.209) data 0.000 (0.029) loss 0.6008 (0.5449) acc 88.4259 (88.2534) lr 2.7103e-04 eta 0:01:46
>>> alpha1: 0.163  alpha2: 0.068 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [41/50] batch [5/51] time 0.177 (0.464) data 0.000 (0.266) loss 0.5818 (0.5812) acc 87.9808 (87.4604) lr 2.2949e-04 eta 0:03:54
epoch [41/50] batch [10/51] time 0.178 (0.323) data 0.000 (0.133) loss 0.4161 (0.5345) acc 89.1509 (88.7487) lr 2.2949e-04 eta 0:02:41
epoch [41/50] batch [15/51] time 0.181 (0.273) data 0.000 (0.089) loss 0.6972 (0.5536) acc 87.9808 (88.0436) lr 2.2949e-04 eta 0:02:15
epoch [41/50] batch [20/51] time 0.173 (0.249) data 0.000 (0.067) loss 0.6034 (0.5531) acc 85.0000 (88.4229) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [25/51] time 0.186 (0.236) data 0.013 (0.054) loss 0.6961 (0.5511) acc 78.0612 (88.1654) lr 2.2949e-04 eta 0:01:54
epoch [41/50] batch [30/51] time 0.171 (0.227) data 0.000 (0.045) loss 0.6133 (0.5690) acc 90.1961 (87.6548) lr 2.2949e-04 eta 0:01:48
epoch [41/50] batch [35/51] time 0.176 (0.219) data 0.000 (0.039) loss 0.7316 (0.5676) acc 80.8824 (87.5388) lr 2.2949e-04 eta 0:01:44
epoch [41/50] batch [40/51] time 0.176 (0.214) data 0.000 (0.034) loss 0.5840 (0.5689) acc 89.1509 (87.4602) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [45/51] time 0.174 (0.210) data 0.000 (0.030) loss 0.5202 (0.5609) acc 85.5769 (87.5937) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [50/51] time 0.172 (0.206) data 0.000 (0.027) loss 0.7416 (0.5611) acc 86.7647 (87.6224) lr 2.2949e-04 eta 0:01:34
>>> alpha1: 0.160  alpha2: 0.064 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.46 <<<
epoch [42/50] batch [5/51] time 0.206 (0.460) data 0.015 (0.274) loss 0.4087 (0.4433) acc 94.4444 (90.0676) lr 1.9098e-04 eta 0:03:28
epoch [42/50] batch [10/51] time 0.182 (0.325) data 0.000 (0.137) loss 0.6289 (0.4826) acc 83.0189 (88.5002) lr 1.9098e-04 eta 0:02:25
epoch [42/50] batch [15/51] time 0.174 (0.276) data 0.000 (0.092) loss 0.5819 (0.5498) acc 85.7143 (87.9376) lr 1.9098e-04 eta 0:02:02
epoch [42/50] batch [20/51] time 0.180 (0.254) data 0.001 (0.069) loss 0.6049 (0.5511) acc 84.4340 (87.7418) lr 1.9098e-04 eta 0:01:51
epoch [42/50] batch [25/51] time 0.177 (0.240) data 0.000 (0.055) loss 0.4781 (0.5401) acc 87.7358 (87.8792) lr 1.9098e-04 eta 0:01:43
epoch [42/50] batch [30/51] time 0.200 (0.229) data 0.000 (0.046) loss 0.6448 (0.5400) acc 87.0192 (88.0048) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [35/51] time 0.202 (0.223) data 0.000 (0.039) loss 0.6005 (0.5495) acc 85.6481 (87.5496) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [40/51] time 0.177 (0.218) data 0.000 (0.035) loss 0.4852 (0.5458) acc 88.6792 (87.9705) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [45/51] time 0.177 (0.213) data 0.000 (0.031) loss 0.5653 (0.5517) acc 87.7358 (87.7530) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [50/51] time 0.171 (0.209) data 0.000 (0.028) loss 0.9066 (0.5567) acc 78.5000 (87.5506) lr 1.9098e-04 eta 0:01:25
>>> alpha1: 0.158  alpha2: 0.062 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [43/50] batch [5/51] time 0.187 (0.472) data 0.000 (0.286) loss 0.5945 (0.5058) acc 87.9808 (89.8555) lr 1.5567e-04 eta 0:03:10
epoch [43/50] batch [10/51] time 0.176 (0.325) data 0.000 (0.143) loss 0.4409 (0.5284) acc 92.1569 (88.8031) lr 1.5567e-04 eta 0:02:09
epoch [43/50] batch [15/51] time 0.182 (0.275) data 0.000 (0.096) loss 0.5313 (0.5321) acc 89.1509 (88.7129) lr 1.5567e-04 eta 0:01:48
epoch [43/50] batch [20/51] time 0.190 (0.250) data 0.000 (0.072) loss 0.5250 (0.5532) acc 90.0943 (87.9833) lr 1.5567e-04 eta 0:01:37
epoch [43/50] batch [25/51] time 0.186 (0.238) data 0.000 (0.057) loss 0.6157 (0.5603) acc 81.6038 (87.4016) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.187 (0.227) data 0.000 (0.048) loss 0.4146 (0.5549) acc 92.3077 (87.7526) lr 1.5567e-04 eta 0:01:25
epoch [43/50] batch [35/51] time 0.198 (0.220) data 0.000 (0.041) loss 0.4604 (0.5582) acc 92.3077 (87.5387) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.174 (0.215) data 0.001 (0.036) loss 0.4721 (0.5516) acc 94.2308 (87.8887) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.174 (0.211) data 0.000 (0.032) loss 0.5515 (0.5454) acc 88.5000 (88.2485) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.168 (0.207) data 0.000 (0.029) loss 0.5578 (0.5431) acc 85.5000 (88.2000) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.155  alpha2: 0.062 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.48 <<<
epoch [44/50] batch [5/51] time 0.184 (0.419) data 0.000 (0.239) loss 0.6128 (0.6145) acc 87.2340 (85.8271) lr 1.2369e-04 eta 0:02:27
epoch [44/50] batch [10/51] time 0.180 (0.302) data 0.000 (0.121) loss 0.4311 (0.6023) acc 86.4583 (86.0428) lr 1.2369e-04 eta 0:01:44
epoch [44/50] batch [15/51] time 0.172 (0.262) data 0.000 (0.081) loss 0.4684 (0.5900) acc 89.7059 (87.0450) lr 1.2369e-04 eta 0:01:29
epoch [44/50] batch [20/51] time 0.186 (0.242) data 0.000 (0.061) loss 0.5555 (0.5731) acc 85.5000 (87.1549) lr 1.2369e-04 eta 0:01:21
epoch [44/50] batch [25/51] time 0.170 (0.230) data 0.000 (0.049) loss 0.5029 (0.5458) acc 87.0000 (87.9178) lr 1.2369e-04 eta 0:01:16
epoch [44/50] batch [30/51] time 0.182 (0.222) data 0.000 (0.041) loss 0.6402 (0.5295) acc 85.5000 (88.2159) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [35/51] time 0.180 (0.216) data 0.000 (0.035) loss 0.4994 (0.5211) acc 89.0000 (88.2345) lr 1.2369e-04 eta 0:01:09
epoch [44/50] batch [40/51] time 0.173 (0.211) data 0.000 (0.031) loss 0.4022 (0.5170) acc 91.8269 (88.5939) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [45/51] time 0.177 (0.207) data 0.000 (0.027) loss 0.4625 (0.5094) acc 87.7358 (88.8067) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [50/51] time 0.178 (0.204) data 0.000 (0.024) loss 0.6438 (0.5111) acc 82.2115 (88.8155) lr 1.2369e-04 eta 0:01:02
>>> alpha1: 0.152  alpha2: 0.059 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [45/50] batch [5/51] time 0.182 (0.489) data 0.000 (0.297) loss 0.4805 (0.5315) acc 92.1569 (87.6804) lr 9.5173e-05 eta 0:02:27
epoch [45/50] batch [10/51] time 0.182 (0.335) data 0.000 (0.149) loss 0.5811 (0.5393) acc 89.1509 (88.1636) lr 9.5173e-05 eta 0:01:39
epoch [45/50] batch [15/51] time 0.181 (0.284) data 0.000 (0.099) loss 0.6392 (0.5450) acc 84.6154 (88.2826) lr 9.5173e-05 eta 0:01:22
epoch [45/50] batch [20/51] time 0.172 (0.258) data 0.000 (0.075) loss 0.4516 (0.5261) acc 88.2353 (88.6858) lr 9.5173e-05 eta 0:01:13
epoch [45/50] batch [25/51] time 0.172 (0.243) data 0.000 (0.060) loss 0.5770 (0.5182) acc 87.2549 (88.9039) lr 9.5173e-05 eta 0:01:08
epoch [45/50] batch [30/51] time 0.186 (0.231) data 0.000 (0.050) loss 0.6365 (0.5315) acc 85.6481 (88.4539) lr 9.5173e-05 eta 0:01:03
epoch [45/50] batch [35/51] time 0.198 (0.224) data 0.000 (0.043) loss 0.3239 (0.5238) acc 93.6364 (88.5313) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.188 (0.219) data 0.001 (0.037) loss 0.4577 (0.5222) acc 92.5439 (88.7751) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.180 (0.214) data 0.000 (0.033) loss 0.4841 (0.5236) acc 87.9630 (88.7653) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.182 (0.210) data 0.001 (0.030) loss 0.3543 (0.5262) acc 93.6364 (88.7885) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.151  alpha2: 0.058 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.48 <<<
epoch [46/50] batch [5/51] time 0.182 (0.494) data 0.000 (0.302) loss 0.5449 (0.5471) acc 84.2593 (87.0668) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [10/51] time 0.167 (0.335) data 0.000 (0.151) loss 0.6975 (0.5623) acc 83.8542 (87.5183) lr 7.0224e-05 eta 0:01:22
epoch [46/50] batch [15/51] time 0.187 (0.284) data 0.000 (0.101) loss 0.3982 (0.5594) acc 89.9123 (88.0585) lr 7.0224e-05 eta 0:01:08
epoch [46/50] batch [20/51] time 0.192 (0.260) data 0.001 (0.076) loss 0.3789 (0.5596) acc 93.1373 (87.8834) lr 7.0224e-05 eta 0:01:01
epoch [46/50] batch [25/51] time 0.213 (0.244) data 0.000 (0.061) loss 0.5134 (0.5717) acc 88.2075 (87.8237) lr 7.0224e-05 eta 0:00:56
epoch [46/50] batch [30/51] time 0.183 (0.234) data 0.000 (0.051) loss 0.6372 (0.5612) acc 85.8491 (88.1954) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [35/51] time 0.207 (0.227) data 0.007 (0.044) loss 0.4230 (0.5487) acc 92.5439 (88.5005) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [40/51] time 0.188 (0.221) data 0.000 (0.038) loss 0.4375 (0.5365) acc 90.6250 (88.5516) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [45/51] time 0.180 (0.216) data 0.000 (0.034) loss 0.6869 (0.5403) acc 87.5000 (88.4228) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [50/51] time 0.173 (0.212) data 0.000 (0.031) loss 0.3958 (0.5333) acc 92.7885 (88.6204) lr 7.0224e-05 eta 0:00:43
>>> alpha1: 0.151  alpha2: 0.060 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.48 <<<
epoch [47/50] batch [5/51] time 0.180 (0.460) data 0.000 (0.268) loss 0.5859 (0.5035) acc 87.0192 (90.1471) lr 4.8943e-05 eta 0:01:31
epoch [47/50] batch [10/51] time 0.178 (0.324) data 0.000 (0.134) loss 0.4654 (0.4998) acc 92.9245 (90.5399) lr 4.8943e-05 eta 0:01:02
epoch [47/50] batch [15/51] time 0.176 (0.277) data 0.001 (0.089) loss 0.5558 (0.5199) acc 88.9423 (89.6997) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [20/51] time 0.191 (0.255) data 0.000 (0.068) loss 0.4259 (0.5092) acc 92.7885 (89.6520) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [25/51] time 0.197 (0.243) data 0.001 (0.054) loss 0.4871 (0.5152) acc 89.5000 (89.2156) lr 4.8943e-05 eta 0:00:43
epoch [47/50] batch [30/51] time 0.188 (0.233) data 0.001 (0.045) loss 0.4036 (0.5131) acc 90.6863 (89.0358) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.186 (0.225) data 0.000 (0.039) loss 0.6685 (0.5272) acc 83.7963 (88.7052) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [40/51] time 0.189 (0.219) data 0.000 (0.034) loss 0.4725 (0.5258) acc 89.5455 (88.6983) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [45/51] time 0.160 (0.213) data 0.000 (0.030) loss 0.5967 (0.5280) acc 88.0435 (88.6637) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [50/51] time 0.172 (0.210) data 0.000 (0.027) loss 0.4903 (0.5233) acc 87.2549 (88.7099) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.153  alpha2: 0.065 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [48/50] batch [5/51] time 0.169 (0.470) data 0.000 (0.286) loss 0.4402 (0.4235) acc 89.7959 (90.8652) lr 3.1417e-05 eta 0:01:09
epoch [48/50] batch [10/51] time 0.193 (0.333) data 0.000 (0.144) loss 0.4054 (0.4483) acc 90.3509 (89.8649) lr 3.1417e-05 eta 0:00:47
epoch [48/50] batch [15/51] time 0.193 (0.285) data 0.001 (0.096) loss 0.5345 (0.4574) acc 88.4615 (89.6104) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [20/51] time 0.175 (0.260) data 0.000 (0.072) loss 0.5383 (0.4736) acc 84.5000 (89.3484) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.185 (0.245) data 0.000 (0.058) loss 0.3774 (0.4862) acc 93.9815 (89.5175) lr 3.1417e-05 eta 0:00:31
epoch [48/50] batch [30/51] time 0.178 (0.234) data 0.000 (0.048) loss 0.8011 (0.5119) acc 85.5000 (88.8170) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.188 (0.226) data 0.001 (0.041) loss 0.4951 (0.5190) acc 89.0000 (88.5898) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.179 (0.221) data 0.000 (0.036) loss 0.7092 (0.5242) acc 86.3208 (88.5665) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.167 (0.215) data 0.000 (0.032) loss 0.4043 (0.5193) acc 93.3673 (88.8400) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.181 (0.211) data 0.000 (0.029) loss 0.4274 (0.5180) acc 92.2727 (88.9091) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.152  alpha2: 0.062 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.46 <<<
epoch [49/50] batch [5/51] time 0.167 (0.570) data 0.000 (0.382) loss 0.5537 (0.5082) acc 89.7959 (90.1071) lr 1.7713e-05 eta 0:00:55
epoch [49/50] batch [10/51] time 0.178 (0.373) data 0.000 (0.191) loss 0.3901 (0.5163) acc 93.3962 (89.2369) lr 1.7713e-05 eta 0:00:34
epoch [49/50] batch [15/51] time 0.172 (0.308) data 0.000 (0.128) loss 0.5610 (0.5488) acc 84.0425 (87.8435) lr 1.7713e-05 eta 0:00:26
epoch [49/50] batch [20/51] time 0.218 (0.279) data 0.000 (0.096) loss 0.5460 (0.5401) acc 88.5965 (88.6174) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [25/51] time 0.178 (0.260) data 0.000 (0.077) loss 0.5284 (0.5343) acc 86.0000 (88.3106) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [30/51] time 0.176 (0.247) data 0.000 (0.064) loss 0.5744 (0.5390) acc 86.5000 (88.3615) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [35/51] time 0.180 (0.238) data 0.000 (0.055) loss 0.6549 (0.5428) acc 87.2642 (88.0888) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [40/51] time 0.171 (0.230) data 0.000 (0.048) loss 0.5690 (0.5460) acc 86.7647 (88.1037) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [45/51] time 0.169 (0.223) data 0.000 (0.043) loss 0.6070 (0.5446) acc 83.0000 (87.9161) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.182 (0.219) data 0.000 (0.038) loss 0.3378 (0.5331) acc 93.3962 (88.3134) lr 1.7713e-05 eta 0:00:11
>>> alpha1: 0.154  alpha2: 0.066 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.47 <<<
epoch [50/50] batch [5/51] time 0.181 (0.460) data 0.000 (0.272) loss 0.4883 (0.5314) acc 87.0370 (87.6311) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [10/51] time 0.186 (0.317) data 0.000 (0.136) loss 0.4604 (0.5451) acc 92.7273 (87.7963) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [15/51] time 0.184 (0.323) data 0.000 (0.091) loss 0.5185 (0.5062) acc 91.2037 (89.4231) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [20/51] time 0.178 (0.287) data 0.000 (0.068) loss 0.3963 (0.5091) acc 91.6667 (89.0979) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.169 (0.295) data 0.000 (0.055) loss 0.4848 (0.5149) acc 93.6170 (89.0469) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [30/51] time 0.185 (0.278) data 0.000 (0.046) loss 0.3808 (0.5123) acc 93.1373 (89.3473) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [35/51] time 0.190 (0.265) data 0.000 (0.039) loss 0.4383 (0.5097) acc 88.8393 (89.2477) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [40/51] time 0.165 (0.253) data 0.000 (0.034) loss 0.4766 (0.5138) acc 90.8163 (89.2145) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.166 (0.244) data 0.000 (0.031) loss 0.4525 (0.5218) acc 88.7755 (88.9597) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.182 (0.237) data 0.000 (0.027) loss 0.4632 (0.5207) acc 88.6364 (88.8968) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed1/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.59, 0.45, 0.39, 0.36, 0.36, 0.35, 0.34, 0.34, 0.33, 0.33, 0.33, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3]
* matched noise rate: [0.38, 0.18, 0.23, 0.21, 0.21, 0.17, 0.17, 0.19, 0.19, 0.19, 0.19, 0.2, 0.2, 0.2, 0.19, 0.2, 0.19, 0.2, 0.2, 0.2, 0.21, 0.2, 0.2, 0.21, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.21, 0.2, 0.21, 0.2, 0.2, 0.21, 0.2, 0.21, 0.2]
* unmatched noise rate: [0.92, 0.57, 0.65, 0.6, 0.6, 0.52, 0.5, 0.5, 0.49, 0.51, 0.51, 0.5, 0.49, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.49, 0.48, 0.48, 0.49, 0.48, 0.46, 0.45, 0.46, 0.47, 0.47, 0.47, 0.46, 0.47, 0.48, 0.48, 0.48, 0.48, 0.47, 0.46, 0.47]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:03,  2.64s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.35it/s] 20%|██        | 5/25 [00:02<00:08,  2.48it/s] 24%|██▍       | 6/25 [00:03<00:06,  3.04it/s] 32%|███▏      | 8/25 [00:03<00:03,  4.49it/s] 40%|████      | 10/25 [00:03<00:02,  5.94it/s] 48%|████▊     | 12/25 [00:03<00:01,  7.27it/s] 56%|█████▌    | 14/25 [00:03<00:01,  8.44it/s] 64%|██████▍   | 16/25 [00:03<00:00,  9.39it/s] 72%|███████▏  | 18/25 [00:04<00:00,  8.79it/s] 80%|████████  | 20/25 [00:04<00:00,  9.57it/s] 88%|████████▊ | 22/25 [00:04<00:00, 10.27it/s] 96%|█████████▌| 24/25 [00:04<00:00, 10.75it/s]100%|██████████| 25/25 [00:05<00:00,  4.74it/s]
=> result
* total: 2,463
* correct: 1,651
* accuracy: 67.0%
* error: 33.0%
* macro_f1: 63.7%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 4	acc: 22.2%
* class: 2 (canterbury bells)	total: 12	correct: 2	acc: 16.7%
* class: 3 (sweet pea)	total: 17	correct: 8	acc: 47.1%
* class: 4 (english marigold)	total: 20	correct: 4	acc: 20.0%
* class: 5 (tiger lily)	total: 14	correct: 12	acc: 85.7%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 8	acc: 57.1%
* class: 9 (globe thistle)	total: 14	correct: 13	acc: 92.9%
* class: 10 (snapdragon)	total: 26	correct: 19	acc: 73.1%
* class: 11 (colt's foot)	total: 26	correct: 0	acc: 0.0%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 13	acc: 86.7%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 25	acc: 96.2%
* class: 17 (peruvian lily)	total: 25	correct: 16	acc: 64.0%
* class: 18 (balloon flower)	total: 15	correct: 0	acc: 0.0%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 10	acc: 83.3%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 2	acc: 15.4%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 0	acc: 0.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 22	acc: 95.7%
* class: 29 (sweet william)	total: 26	correct: 19	acc: 73.1%
* class: 30 (carnation)	total: 16	correct: 10	acc: 62.5%
* class: 31 (garden phlox)	total: 14	correct: 11	acc: 78.6%
* class: 32 (love in the mist)	total: 14	correct: 4	acc: 28.6%
* class: 33 (mexican aster)	total: 12	correct: 10	acc: 83.3%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 6	acc: 27.3%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 15	acc: 88.2%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 7	acc: 35.0%
* class: 40 (barbeton daisy)	total: 38	correct: 22	acc: 57.9%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 27	acc: 96.4%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 0	acc: 0.0%
* class: 46 (marigold)	total: 20	correct: 17	acc: 85.0%
* class: 47 (buttercup)	total: 21	correct: 21	acc: 100.0%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 8	acc: 10.4%
* class: 51 (wild pansy)	total: 26	correct: 17	acc: 65.4%
* class: 52 (primula)	total: 28	correct: 20	acc: 71.4%
* class: 53 (sunflower)	total: 19	correct: 18	acc: 94.7%
* class: 54 (pelargonium)	total: 21	correct: 0	acc: 0.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 19	acc: 95.0%
* class: 57 (geranium)	total: 34	correct: 33	acc: 97.1%
* class: 58 (orange dahlia)	total: 20	correct: 19	acc: 95.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 15	acc: 93.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 15	acc: 93.8%
* class: 64 (californian poppy)	total: 31	correct: 27	acc: 87.1%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 9	acc: 69.2%
* class: 67 (bearded iris)	total: 16	correct: 12	acc: 75.0%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 23	acc: 100.0%
* class: 71 (azalea)	total: 29	correct: 24	acc: 82.8%
* class: 72 (water lily)	total: 58	correct: 57	acc: 98.3%
* class: 73 (rose)	total: 51	correct: 49	acc: 96.1%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 23	acc: 71.9%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 38	acc: 90.5%
* class: 78 (toad lily)	total: 13	correct: 10	acc: 76.9%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 33	acc: 97.1%
* class: 82 (hibiscus)	total: 39	correct: 39	acc: 100.0%
* class: 83 (columbine)	total: 26	correct: 24	acc: 92.3%
* class: 84 (desert-rose)	total: 18	correct: 12	acc: 66.7%
* class: 85 (tree mallow)	total: 17	correct: 10	acc: 58.8%
* class: 86 (magnolia)	total: 18	correct: 13	acc: 72.2%
* class: 87 (cyclamen)	total: 46	correct: 36	acc: 78.3%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 15	acc: 60.0%
* class: 90 (hippeastrum)	total: 23	correct: 17	acc: 73.9%
* class: 91 (bee balm)	total: 20	correct: 17	acc: 85.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 48	acc: 98.0%
* class: 94 (bougainvillea)	total: 38	correct: 26	acc: 68.4%
* class: 95 (camellia)	total: 27	correct: 21	acc: 77.8%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 16	acc: 64.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 3	acc: 17.6%
* class: 101 (blackberry lily)	total: 14	correct: 11	acc: 78.6%
* average: 68.0%
Elapsed: 0:28:14
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
resume: 
root: ./data
seed: 2
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_12-seed_2.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.272 (1.128) data 0.000 (0.397) loss 4.7611 (4.9715) acc 6.2500 (2.5000) lr 1.0000e-05 eta 0:47:50
epoch [1/50] batch [10/51] time 0.266 (0.696) data 0.000 (0.199) loss 4.7935 (4.9435) acc 3.1250 (3.7500) lr 1.0000e-05 eta 0:29:27
epoch [1/50] batch [15/51] time 0.261 (0.553) data 0.000 (0.133) loss 4.6305 (4.9298) acc 0.0000 (3.5417) lr 1.0000e-05 eta 0:23:20
epoch [1/50] batch [20/51] time 0.272 (0.482) data 0.000 (0.100) loss 4.6898 (4.8931) acc 6.2500 (3.7500) lr 1.0000e-05 eta 0:20:18
epoch [1/50] batch [25/51] time 0.259 (0.438) data 0.000 (0.080) loss 4.9039 (4.8650) acc 0.0000 (3.8750) lr 1.0000e-05 eta 0:18:26
epoch [1/50] batch [30/51] time 0.271 (0.410) data 0.000 (0.066) loss 4.8267 (4.8513) acc 6.2500 (4.1667) lr 1.0000e-05 eta 0:17:13
epoch [1/50] batch [35/51] time 0.260 (0.390) data 0.000 (0.057) loss 4.6171 (4.8437) acc 6.2500 (4.0179) lr 1.0000e-05 eta 0:16:19
epoch [1/50] batch [40/51] time 0.260 (0.374) data 0.000 (0.050) loss 4.5122 (4.8262) acc 6.2500 (3.9844) lr 1.0000e-05 eta 0:15:38
epoch [1/50] batch [45/51] time 0.257 (0.361) data 0.000 (0.044) loss 4.7275 (4.8086) acc 3.1250 (3.9583) lr 1.0000e-05 eta 0:15:04
epoch [1/50] batch [50/51] time 0.260 (0.351) data 0.000 (0.040) loss 4.7317 (4.8015) acc 3.1250 (3.8750) lr 1.0000e-05 eta 0:14:37
epoch [2/50] batch [5/51] time 0.260 (0.609) data 0.000 (0.334) loss 4.4840 (4.5435) acc 6.2500 (3.1250) lr 2.0000e-03 eta 0:25:19
epoch [2/50] batch [10/51] time 0.279 (0.440) data 0.000 (0.167) loss 4.6302 (4.5444) acc 6.2500 (6.2500) lr 2.0000e-03 eta 0:18:13
epoch [2/50] batch [15/51] time 0.260 (0.384) data 0.000 (0.112) loss 4.6577 (4.5328) acc 6.2500 (6.6667) lr 2.0000e-03 eta 0:15:53
epoch [2/50] batch [20/51] time 0.274 (0.355) data 0.000 (0.084) loss 4.2674 (4.5226) acc 18.7500 (7.3438) lr 2.0000e-03 eta 0:14:40
epoch [2/50] batch [25/51] time 0.269 (0.338) data 0.000 (0.067) loss 4.3519 (4.5091) acc 18.7500 (8.6250) lr 2.0000e-03 eta 0:13:55
epoch [2/50] batch [30/51] time 0.271 (0.326) data 0.000 (0.056) loss 4.6144 (4.5193) acc 9.3750 (8.6458) lr 2.0000e-03 eta 0:13:25
epoch [2/50] batch [35/51] time 0.270 (0.318) data 0.000 (0.048) loss 4.4929 (4.5231) acc 6.2500 (8.3929) lr 2.0000e-03 eta 0:13:03
epoch [2/50] batch [40/51] time 0.257 (0.311) data 0.000 (0.042) loss 4.4776 (4.5214) acc 9.3750 (8.9062) lr 2.0000e-03 eta 0:12:44
epoch [2/50] batch [45/51] time 0.258 (0.305) data 0.000 (0.037) loss 4.4537 (4.5195) acc 9.3750 (8.8194) lr 2.0000e-03 eta 0:12:28
epoch [2/50] batch [50/51] time 0.259 (0.300) data 0.000 (0.034) loss 4.6618 (4.5198) acc 6.2500 (8.9375) lr 2.0000e-03 eta 0:12:15
epoch [3/50] batch [5/51] time 0.269 (0.588) data 0.000 (0.314) loss 4.3520 (4.4761) acc 18.7500 (13.1250) lr 1.9980e-03 eta 0:23:57
epoch [3/50] batch [10/51] time 0.280 (0.427) data 0.000 (0.157) loss 4.5149 (4.5170) acc 9.3750 (10.3125) lr 1.9980e-03 eta 0:17:22
epoch [3/50] batch [15/51] time 0.285 (0.373) data 0.000 (0.105) loss 4.5604 (4.5082) acc 6.2500 (11.0417) lr 1.9980e-03 eta 0:15:08
epoch [3/50] batch [20/51] time 0.275 (0.346) data 0.000 (0.079) loss 4.5617 (4.5242) acc 12.5000 (10.4688) lr 1.9980e-03 eta 0:14:00
epoch [3/50] batch [25/51] time 0.270 (0.331) data 0.000 (0.063) loss 4.7124 (4.5090) acc 6.2500 (10.3750) lr 1.9980e-03 eta 0:13:22
epoch [3/50] batch [30/51] time 0.273 (0.321) data 0.000 (0.053) loss 4.1740 (4.4949) acc 25.0000 (11.2500) lr 1.9980e-03 eta 0:12:57
epoch [3/50] batch [35/51] time 0.259 (0.314) data 0.000 (0.045) loss 4.4386 (4.4895) acc 9.3750 (11.2500) lr 1.9980e-03 eta 0:12:37
epoch [3/50] batch [40/51] time 0.259 (0.307) data 0.000 (0.039) loss 4.3990 (4.4813) acc 9.3750 (11.4844) lr 1.9980e-03 eta 0:12:19
epoch [3/50] batch [45/51] time 0.262 (0.302) data 0.000 (0.035) loss 4.4523 (4.4743) acc 12.5000 (11.5972) lr 1.9980e-03 eta 0:12:06
epoch [3/50] batch [50/51] time 0.260 (0.298) data 0.000 (0.032) loss 4.2365 (4.4732) acc 18.7500 (11.5000) lr 1.9980e-03 eta 0:11:54
epoch [4/50] batch [5/51] time 0.286 (0.606) data 0.000 (0.312) loss 4.5289 (4.4477) acc 3.1250 (10.6250) lr 1.9921e-03 eta 0:24:09
epoch [4/50] batch [10/51] time 0.288 (0.438) data 0.000 (0.156) loss 4.4910 (4.4544) acc 12.5000 (12.8125) lr 1.9921e-03 eta 0:17:26
epoch [4/50] batch [15/51] time 0.265 (0.380) data 0.000 (0.104) loss 4.5020 (4.4571) acc 15.6250 (11.8750) lr 1.9921e-03 eta 0:15:05
epoch [4/50] batch [20/51] time 0.260 (0.352) data 0.000 (0.078) loss 4.4318 (4.4597) acc 9.3750 (11.0938) lr 1.9921e-03 eta 0:13:55
epoch [4/50] batch [25/51] time 0.262 (0.334) data 0.000 (0.063) loss 4.3060 (4.4430) acc 9.3750 (12.2500) lr 1.9921e-03 eta 0:13:12
epoch [4/50] batch [30/51] time 0.259 (0.323) data 0.000 (0.052) loss 4.6418 (4.4426) acc 12.5000 (12.3958) lr 1.9921e-03 eta 0:12:44
epoch [4/50] batch [35/51] time 0.260 (0.314) data 0.000 (0.045) loss 4.5620 (4.4551) acc 9.3750 (11.8750) lr 1.9921e-03 eta 0:12:22
epoch [4/50] batch [40/51] time 0.261 (0.308) data 0.000 (0.039) loss 4.5382 (4.4701) acc 12.5000 (11.7188) lr 1.9921e-03 eta 0:12:05
epoch [4/50] batch [45/51] time 0.261 (0.303) data 0.000 (0.035) loss 4.3756 (4.4742) acc 18.7500 (11.5278) lr 1.9921e-03 eta 0:11:51
epoch [4/50] batch [50/51] time 0.258 (0.298) data 0.000 (0.031) loss 4.2423 (4.4675) acc 18.7500 (11.6250) lr 1.9921e-03 eta 0:11:39
epoch [5/50] batch [5/51] time 0.274 (0.627) data 0.000 (0.326) loss 4.7211 (4.4802) acc 12.5000 (14.3750) lr 1.9823e-03 eta 0:24:27
epoch [5/50] batch [10/51] time 0.263 (0.451) data 0.000 (0.163) loss 4.3535 (4.5168) acc 12.5000 (12.1875) lr 1.9823e-03 eta 0:17:33
epoch [5/50] batch [15/51] time 0.275 (0.390) data 0.000 (0.109) loss 4.3747 (4.4965) acc 21.8750 (12.2917) lr 1.9823e-03 eta 0:15:09
epoch [5/50] batch [20/51] time 0.263 (0.360) data 0.000 (0.082) loss 4.4352 (4.4898) acc 18.7500 (11.7188) lr 1.9823e-03 eta 0:13:56
epoch [5/50] batch [25/51] time 0.263 (0.340) data 0.000 (0.065) loss 4.5588 (4.4878) acc 9.3750 (12.1250) lr 1.9823e-03 eta 0:13:10
epoch [5/50] batch [30/51] time 0.285 (0.329) data 0.000 (0.055) loss 4.3769 (4.4721) acc 12.5000 (11.8750) lr 1.9823e-03 eta 0:12:41
epoch [5/50] batch [35/51] time 0.278 (0.320) data 0.000 (0.047) loss 4.3822 (4.4503) acc 15.6250 (12.6786) lr 1.9823e-03 eta 0:12:19
epoch [5/50] batch [40/51] time 0.260 (0.313) data 0.000 (0.041) loss 4.4130 (4.4460) acc 9.3750 (13.0469) lr 1.9823e-03 eta 0:12:01
epoch [5/50] batch [45/51] time 0.257 (0.307) data 0.000 (0.037) loss 4.6364 (4.4548) acc 6.2500 (12.8472) lr 1.9823e-03 eta 0:11:45
epoch [5/50] batch [50/51] time 0.260 (0.302) data 0.000 (0.033) loss 4.4584 (4.4585) acc 9.3750 (12.4375) lr 1.9823e-03 eta 0:11:32
epoch [6/50] batch [5/51] time 0.298 (0.615) data 0.000 (0.316) loss 4.4559 (4.4746) acc 12.5000 (10.6250) lr 1.9686e-03 eta 0:23:29
epoch [6/50] batch [10/51] time 0.272 (0.442) data 0.000 (0.158) loss 4.4320 (4.4361) acc 18.7500 (12.8125) lr 1.9686e-03 eta 0:16:50
epoch [6/50] batch [15/51] time 0.267 (0.387) data 0.000 (0.105) loss 4.1897 (4.4312) acc 15.6250 (11.8750) lr 1.9686e-03 eta 0:14:41
epoch [6/50] batch [20/51] time 0.265 (0.358) data 0.000 (0.079) loss 4.1914 (4.3863) acc 6.2500 (12.9688) lr 1.9686e-03 eta 0:13:34
epoch [6/50] batch [25/51] time 0.262 (0.341) data 0.000 (0.063) loss 4.4459 (4.4094) acc 9.3750 (12.2500) lr 1.9686e-03 eta 0:12:55
epoch [6/50] batch [30/51] time 0.274 (0.329) data 0.000 (0.053) loss 4.6496 (4.4158) acc 3.1250 (11.9792) lr 1.9686e-03 eta 0:12:24
epoch [6/50] batch [35/51] time 0.276 (0.320) data 0.000 (0.045) loss 4.5652 (4.4213) acc 6.2500 (12.0536) lr 1.9686e-03 eta 0:12:03
epoch [6/50] batch [40/51] time 0.260 (0.313) data 0.000 (0.040) loss 4.5845 (4.4150) acc 9.3750 (12.0312) lr 1.9686e-03 eta 0:11:45
epoch [6/50] batch [45/51] time 0.259 (0.307) data 0.000 (0.035) loss 4.6188 (4.4224) acc 9.3750 (11.7361) lr 1.9686e-03 eta 0:11:30
epoch [6/50] batch [50/51] time 0.258 (0.302) data 0.000 (0.032) loss 4.2417 (4.4246) acc 25.0000 (11.9375) lr 1.9686e-03 eta 0:11:17
epoch [7/50] batch [5/51] time 0.264 (0.565) data 0.000 (0.288) loss 4.4790 (4.3891) acc 9.3750 (12.5000) lr 1.9511e-03 eta 0:21:04
epoch [7/50] batch [10/51] time 0.264 (0.417) data 0.000 (0.144) loss 4.4713 (4.3670) acc 9.3750 (12.8125) lr 1.9511e-03 eta 0:15:32
epoch [7/50] batch [15/51] time 0.268 (0.367) data 0.000 (0.096) loss 4.5423 (4.4028) acc 12.5000 (12.7083) lr 1.9511e-03 eta 0:13:37
epoch [7/50] batch [20/51] time 0.260 (0.341) data 0.000 (0.072) loss 4.4026 (4.4223) acc 15.6250 (12.5000) lr 1.9511e-03 eta 0:12:37
epoch [7/50] batch [25/51] time 0.261 (0.325) data 0.000 (0.058) loss 4.5888 (4.4493) acc 6.2500 (11.5000) lr 1.9511e-03 eta 0:12:01
epoch [7/50] batch [30/51] time 0.272 (0.316) data 0.000 (0.048) loss 4.5403 (4.4505) acc 6.2500 (11.2500) lr 1.9511e-03 eta 0:11:39
epoch [7/50] batch [35/51] time 0.266 (0.310) data 0.001 (0.041) loss 4.1982 (4.4382) acc 18.7500 (11.6071) lr 1.9511e-03 eta 0:11:23
epoch [7/50] batch [40/51] time 0.259 (0.304) data 0.000 (0.036) loss 4.4774 (4.4385) acc 9.3750 (11.6406) lr 1.9511e-03 eta 0:11:09
epoch [7/50] batch [45/51] time 0.260 (0.299) data 0.000 (0.032) loss 4.2666 (4.4262) acc 12.5000 (11.8750) lr 1.9511e-03 eta 0:10:57
epoch [7/50] batch [50/51] time 0.260 (0.295) data 0.000 (0.029) loss 4.1840 (4.4146) acc 18.7500 (12.1250) lr 1.9511e-03 eta 0:10:47
epoch [8/50] batch [5/51] time 0.259 (0.568) data 0.000 (0.298) loss 4.6538 (4.3607) acc 9.3750 (15.6250) lr 1.9298e-03 eta 0:20:43
epoch [8/50] batch [10/51] time 0.274 (0.418) data 0.000 (0.149) loss 4.3607 (4.3913) acc 15.6250 (13.7500) lr 1.9298e-03 eta 0:15:12
epoch [8/50] batch [15/51] time 0.260 (0.367) data 0.000 (0.099) loss 4.3739 (4.3915) acc 3.1250 (12.7083) lr 1.9298e-03 eta 0:13:20
epoch [8/50] batch [20/51] time 0.260 (0.341) data 0.000 (0.075) loss 4.4744 (4.4034) acc 12.5000 (12.6562) lr 1.9298e-03 eta 0:12:21
epoch [8/50] batch [25/51] time 0.277 (0.327) data 0.000 (0.060) loss 4.6430 (4.3983) acc 9.3750 (12.7500) lr 1.9298e-03 eta 0:11:48
epoch [8/50] batch [30/51] time 0.263 (0.317) data 0.000 (0.050) loss 4.3546 (4.3958) acc 15.6250 (12.9167) lr 1.9298e-03 eta 0:11:24
epoch [8/50] batch [35/51] time 0.276 (0.311) data 0.000 (0.043) loss 4.6194 (4.3781) acc 3.1250 (13.2143) lr 1.9298e-03 eta 0:11:10
epoch [8/50] batch [40/51] time 0.259 (0.304) data 0.000 (0.037) loss 4.5253 (4.3959) acc 6.2500 (12.9688) lr 1.9298e-03 eta 0:10:54
epoch [8/50] batch [45/51] time 0.261 (0.299) data 0.000 (0.033) loss 4.3741 (4.3992) acc 3.1250 (12.4306) lr 1.9298e-03 eta 0:10:42
epoch [8/50] batch [50/51] time 0.264 (0.295) data 0.000 (0.030) loss 4.5620 (4.4001) acc 6.2500 (12.5000) lr 1.9298e-03 eta 0:10:33
epoch [9/50] batch [5/51] time 0.286 (0.560) data 0.000 (0.282) loss 4.4416 (4.2815) acc 12.5000 (14.3750) lr 1.9048e-03 eta 0:19:56
epoch [9/50] batch [10/51] time 0.265 (0.414) data 0.000 (0.141) loss 4.2328 (4.2907) acc 12.5000 (13.7500) lr 1.9048e-03 eta 0:14:42
epoch [9/50] batch [15/51] time 0.259 (0.364) data 0.000 (0.094) loss 3.9214 (4.3037) acc 21.8750 (13.9583) lr 1.9048e-03 eta 0:12:53
epoch [9/50] batch [20/51] time 0.268 (0.341) data 0.000 (0.071) loss 4.5008 (4.3388) acc 6.2500 (12.8125) lr 1.9048e-03 eta 0:12:02
epoch [9/50] batch [25/51] time 0.268 (0.326) data 0.000 (0.057) loss 4.2849 (4.3335) acc 9.3750 (13.0000) lr 1.9048e-03 eta 0:11:30
epoch [9/50] batch [30/51] time 0.269 (0.316) data 0.000 (0.047) loss 4.3488 (4.3555) acc 18.7500 (12.6042) lr 1.9048e-03 eta 0:11:07
epoch [9/50] batch [35/51] time 0.261 (0.309) data 0.000 (0.040) loss 4.2861 (4.3634) acc 28.1250 (13.0357) lr 1.9048e-03 eta 0:10:51
epoch [9/50] batch [40/51] time 0.263 (0.304) data 0.000 (0.035) loss 4.2983 (4.3651) acc 15.6250 (12.7344) lr 1.9048e-03 eta 0:10:38
epoch [9/50] batch [45/51] time 0.261 (0.299) data 0.000 (0.032) loss 4.3040 (4.3734) acc 12.5000 (12.7083) lr 1.9048e-03 eta 0:10:26
epoch [9/50] batch [50/51] time 0.257 (0.295) data 0.000 (0.028) loss 4.5217 (4.3756) acc 15.6250 (12.6250) lr 1.9048e-03 eta 0:10:16
epoch [10/50] batch [5/51] time 0.306 (0.614) data 0.001 (0.324) loss 4.2028 (4.3343) acc 12.5000 (13.1250) lr 1.8763e-03 eta 0:21:20
epoch [10/50] batch [10/51] time 0.265 (0.442) data 0.000 (0.162) loss 4.4935 (4.3215) acc 18.7500 (12.1875) lr 1.8763e-03 eta 0:15:19
epoch [10/50] batch [15/51] time 0.262 (0.383) data 0.000 (0.108) loss 4.2537 (4.3141) acc 18.7500 (12.5000) lr 1.8763e-03 eta 0:13:15
epoch [10/50] batch [20/51] time 0.272 (0.356) data 0.000 (0.081) loss 4.3690 (4.3182) acc 9.3750 (12.5000) lr 1.8763e-03 eta 0:12:16
epoch [10/50] batch [25/51] time 0.261 (0.338) data 0.000 (0.065) loss 4.4337 (4.3349) acc 12.5000 (12.3750) lr 1.8763e-03 eta 0:11:38
epoch [10/50] batch [30/51] time 0.264 (0.327) data 0.000 (0.054) loss 4.5717 (4.3476) acc 9.3750 (12.3958) lr 1.8763e-03 eta 0:11:13
epoch [10/50] batch [35/51] time 0.268 (0.318) data 0.000 (0.046) loss 4.4005 (4.3477) acc 15.6250 (12.3214) lr 1.8763e-03 eta 0:10:53
epoch [10/50] batch [40/51] time 0.259 (0.311) data 0.000 (0.041) loss 4.3402 (4.3503) acc 12.5000 (12.1875) lr 1.8763e-03 eta 0:10:37
epoch [10/50] batch [45/51] time 0.259 (0.305) data 0.000 (0.036) loss 4.2337 (4.3518) acc 9.3750 (12.2917) lr 1.8763e-03 eta 0:10:24
epoch [10/50] batch [50/51] time 0.259 (0.300) data 0.000 (0.033) loss 4.2741 (4.3572) acc 15.6250 (12.3750) lr 1.8763e-03 eta 0:10:13
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/tensorboard)
>>> alpha1: 0.843  alpha2: 0.405 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.54 --> matched refined noisy rate: 0.34 & unmatched refined noisy rate: 0.89 <<<
epoch [11/50] batch [5/51] time 0.183 (1.097) data 0.000 (0.254) loss 3.4969 (3.5872) acc 47.1698 (46.6245) lr 1.8443e-03 eta 0:37:12
epoch [11/50] batch [10/51] time 0.168 (0.841) data 0.000 (0.127) loss 3.9533 (3.5140) acc 30.0000 (45.5211) lr 1.8443e-03 eta 0:28:26
epoch [11/50] batch [15/51] time 0.747 (0.752) data 0.000 (0.085) loss 3.2025 (3.5660) acc 36.4130 (43.4287) lr 1.8443e-03 eta 0:25:23
epoch [11/50] batch [20/51] time 0.194 (0.608) data 0.000 (0.064) loss 3.2297 (3.5126) acc 55.9091 (44.9595) lr 1.8443e-03 eta 0:20:28
epoch [11/50] batch [25/51] time 0.177 (0.551) data 0.000 (0.051) loss 3.6772 (3.4883) acc 37.7358 (43.9171) lr 1.8443e-03 eta 0:18:29
epoch [11/50] batch [30/51] time 0.205 (0.490) data 0.000 (0.043) loss 3.1732 (3.4746) acc 52.1930 (44.3379) lr 1.8443e-03 eta 0:16:24
epoch [11/50] batch [35/51] time 0.184 (0.447) data 0.000 (0.037) loss 3.3635 (3.4501) acc 49.0566 (44.1095) lr 1.8443e-03 eta 0:14:56
epoch [11/50] batch [40/51] time 0.181 (0.414) data 0.000 (0.032) loss 3.2499 (3.4523) acc 50.0000 (44.1710) lr 1.8443e-03 eta 0:13:47
epoch [11/50] batch [45/51] time 0.166 (0.387) data 0.000 (0.028) loss 3.5847 (3.4342) acc 31.1224 (44.4166) lr 1.8443e-03 eta 0:12:51
epoch [11/50] batch [50/51] time 0.168 (0.380) data 0.000 (0.026) loss 3.2857 (3.4309) acc 55.5000 (44.6909) lr 1.8443e-03 eta 0:12:36
>>> alpha1: 0.820  alpha2: 0.394 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.41 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.54 <<<
epoch [12/50] batch [5/51] time 0.165 (0.864) data 0.000 (0.336) loss 1.7962 (2.2421) acc 75.5208 (62.7675) lr 1.8090e-03 eta 0:28:34
epoch [12/50] batch [10/51] time 0.156 (0.564) data 0.000 (0.168) loss 2.1487 (2.2833) acc 52.2727 (59.0977) lr 1.8090e-03 eta 0:18:35
epoch [12/50] batch [15/51] time 0.159 (0.548) data 0.000 (0.112) loss 2.1855 (2.2340) acc 64.4445 (59.7058) lr 1.8090e-03 eta 0:18:01
epoch [12/50] batch [20/51] time 0.150 (0.481) data 0.000 (0.084) loss 2.3072 (2.2477) acc 66.4634 (60.1356) lr 1.8090e-03 eta 0:15:47
epoch [12/50] batch [25/51] time 0.182 (0.416) data 0.000 (0.067) loss 1.7519 (2.2222) acc 70.4082 (60.9236) lr 1.8090e-03 eta 0:13:36
epoch [12/50] batch [30/51] time 0.163 (0.373) data 0.000 (0.056) loss 2.4252 (2.2348) acc 58.3333 (60.7393) lr 1.8090e-03 eta 0:12:10
epoch [12/50] batch [35/51] time 0.153 (0.342) data 0.000 (0.048) loss 1.9425 (2.2532) acc 76.8293 (60.2773) lr 1.8090e-03 eta 0:11:08
epoch [12/50] batch [40/51] time 0.143 (0.318) data 0.000 (0.042) loss 1.8753 (2.2348) acc 71.1538 (60.9304) lr 1.8090e-03 eta 0:10:20
epoch [12/50] batch [45/51] time 0.152 (0.300) data 0.000 (0.038) loss 2.3904 (2.2243) acc 64.5349 (61.3649) lr 1.8090e-03 eta 0:09:42
epoch [12/50] batch [50/51] time 0.154 (0.285) data 0.000 (0.034) loss 1.7328 (2.1965) acc 76.1364 (61.9691) lr 1.8090e-03 eta 0:09:12
>>> alpha1: 0.751  alpha2: 0.346 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.66 <<<
epoch [13/50] batch [5/51] time 0.181 (0.495) data 0.000 (0.317) loss 1.5338 (1.7823) acc 68.1818 (62.7663) lr 1.7705e-03 eta 0:15:57
epoch [13/50] batch [10/51] time 0.193 (0.339) data 0.000 (0.159) loss 1.1337 (1.6621) acc 77.2727 (66.9800) lr 1.7705e-03 eta 0:10:53
epoch [13/50] batch [15/51] time 0.178 (0.285) data 0.000 (0.106) loss 1.4063 (1.5948) acc 68.9815 (68.5284) lr 1.7705e-03 eta 0:09:08
epoch [13/50] batch [20/51] time 0.177 (0.258) data 0.000 (0.080) loss 1.4980 (1.6056) acc 68.8679 (67.9342) lr 1.7705e-03 eta 0:08:14
epoch [13/50] batch [25/51] time 0.187 (0.243) data 0.000 (0.064) loss 1.5369 (1.5895) acc 63.6364 (67.9641) lr 1.7705e-03 eta 0:07:45
epoch [13/50] batch [30/51] time 0.180 (0.253) data 0.000 (0.053) loss 1.6296 (1.5925) acc 59.5455 (68.0447) lr 1.7705e-03 eta 0:08:03
epoch [13/50] batch [35/51] time 0.189 (0.243) data 0.000 (0.046) loss 1.5645 (1.5937) acc 69.0000 (67.8992) lr 1.7705e-03 eta 0:07:41
epoch [13/50] batch [40/51] time 0.174 (0.234) data 0.000 (0.040) loss 2.0166 (1.6104) acc 58.1731 (67.2546) lr 1.7705e-03 eta 0:07:23
epoch [13/50] batch [45/51] time 0.170 (0.227) data 0.000 (0.035) loss 1.7432 (1.6040) acc 60.2941 (67.0264) lr 1.7705e-03 eta 0:07:09
epoch [13/50] batch [50/51] time 0.180 (0.222) data 0.000 (0.032) loss 1.7467 (1.5966) acc 65.7407 (67.4684) lr 1.7705e-03 eta 0:06:58
>>> alpha1: 0.661  alpha2: 0.299 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.60 <<<
epoch [14/50] batch [5/51] time 0.176 (0.484) data 0.000 (0.301) loss 1.4363 (1.3058) acc 65.5660 (72.3346) lr 1.7290e-03 eta 0:15:11
epoch [14/50] batch [10/51] time 0.172 (0.330) data 0.000 (0.150) loss 1.1260 (1.3415) acc 76.4423 (70.1525) lr 1.7290e-03 eta 0:10:19
epoch [14/50] batch [15/51] time 0.181 (0.279) data 0.000 (0.100) loss 1.0787 (1.2863) acc 82.7273 (71.9435) lr 1.7290e-03 eta 0:08:41
epoch [14/50] batch [20/51] time 0.186 (0.254) data 0.001 (0.075) loss 1.3215 (1.2737) acc 66.1765 (72.3471) lr 1.7290e-03 eta 0:07:53
epoch [14/50] batch [25/51] time 0.169 (0.239) data 0.000 (0.060) loss 1.1616 (1.2640) acc 73.5000 (72.8032) lr 1.7290e-03 eta 0:07:24
epoch [14/50] batch [30/51] time 0.179 (0.229) data 0.000 (0.050) loss 1.3808 (1.2875) acc 69.8113 (72.0050) lr 1.7290e-03 eta 0:07:05
epoch [14/50] batch [35/51] time 0.189 (0.222) data 0.000 (0.043) loss 1.2636 (1.3018) acc 68.5345 (71.5439) lr 1.7290e-03 eta 0:06:51
epoch [14/50] batch [40/51] time 0.165 (0.215) data 0.000 (0.038) loss 1.1599 (1.3076) acc 75.0000 (71.2146) lr 1.7290e-03 eta 0:06:37
epoch [14/50] batch [45/51] time 0.170 (0.211) data 0.000 (0.034) loss 1.2140 (1.3054) acc 78.9216 (71.3633) lr 1.7290e-03 eta 0:06:28
epoch [14/50] batch [50/51] time 0.173 (0.207) data 0.000 (0.030) loss 1.7870 (1.3165) acc 61.0577 (71.1800) lr 1.7290e-03 eta 0:06:20
>>> alpha1: 0.600  alpha2: 0.265 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.60 <<<
epoch [15/50] batch [5/51] time 0.183 (0.449) data 0.000 (0.264) loss 0.9296 (1.1503) acc 82.2727 (74.8182) lr 1.6845e-03 eta 0:13:42
epoch [15/50] batch [10/51] time 0.192 (0.317) data 0.000 (0.132) loss 1.4127 (1.2227) acc 67.9825 (72.4395) lr 1.6845e-03 eta 0:09:39
epoch [15/50] batch [15/51] time 0.170 (0.271) data 0.000 (0.088) loss 1.3960 (1.2344) acc 68.5000 (71.9909) lr 1.6845e-03 eta 0:08:13
epoch [15/50] batch [20/51] time 0.173 (0.247) data 0.000 (0.066) loss 1.3005 (1.2407) acc 64.9038 (72.4540) lr 1.6845e-03 eta 0:07:28
epoch [15/50] batch [25/51] time 0.172 (0.233) data 0.001 (0.053) loss 1.0561 (1.2404) acc 78.1250 (72.2452) lr 1.6845e-03 eta 0:07:01
epoch [15/50] batch [30/51] time 0.178 (0.225) data 0.000 (0.044) loss 0.8804 (1.2084) acc 85.6481 (72.9779) lr 1.6845e-03 eta 0:06:45
epoch [15/50] batch [35/51] time 0.181 (0.218) data 0.000 (0.038) loss 0.8498 (1.2049) acc 82.5472 (73.3583) lr 1.6845e-03 eta 0:06:32
epoch [15/50] batch [40/51] time 0.184 (0.214) data 0.000 (0.033) loss 1.2652 (1.2083) acc 70.0893 (73.3036) lr 1.6845e-03 eta 0:06:23
epoch [15/50] batch [45/51] time 0.176 (0.210) data 0.000 (0.029) loss 1.0523 (1.2012) acc 76.8868 (73.6432) lr 1.6845e-03 eta 0:06:15
epoch [15/50] batch [50/51] time 0.170 (0.206) data 0.000 (0.027) loss 1.2805 (1.1963) acc 66.6667 (73.6575) lr 1.6845e-03 eta 0:06:08
>>> alpha1: 0.444  alpha2: 0.175 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.46 <<<
epoch [16/50] batch [5/51] time 0.165 (0.522) data 0.000 (0.353) loss 0.7947 (1.0267) acc 86.4583 (78.6866) lr 1.6374e-03 eta 0:15:28
epoch [16/50] batch [10/51] time 0.165 (0.342) data 0.000 (0.177) loss 0.9640 (1.0190) acc 80.8511 (78.8926) lr 1.6374e-03 eta 0:10:06
epoch [16/50] batch [15/51] time 0.171 (0.282) data 0.000 (0.118) loss 0.8290 (0.9941) acc 81.8627 (79.1940) lr 1.6374e-03 eta 0:08:19
epoch [16/50] batch [20/51] time 0.158 (0.255) data 0.000 (0.088) loss 1.2649 (0.9793) acc 70.5556 (78.8794) lr 1.6374e-03 eta 0:07:29
epoch [16/50] batch [25/51] time 0.183 (0.237) data 0.001 (0.071) loss 0.9606 (0.9733) acc 77.0833 (78.8959) lr 1.6374e-03 eta 0:06:56
epoch [16/50] batch [30/51] time 0.161 (0.226) data 0.001 (0.059) loss 0.8660 (0.9808) acc 81.6667 (78.2955) lr 1.6374e-03 eta 0:06:36
epoch [16/50] batch [35/51] time 0.173 (0.217) data 0.000 (0.051) loss 1.0620 (1.0075) acc 73.0000 (77.8354) lr 1.6374e-03 eta 0:06:19
epoch [16/50] batch [40/51] time 0.161 (0.212) data 0.000 (0.044) loss 0.8985 (1.0077) acc 81.9149 (77.9781) lr 1.6374e-03 eta 0:06:09
epoch [16/50] batch [45/51] time 0.163 (0.206) data 0.000 (0.039) loss 0.8186 (0.9944) acc 83.8542 (78.2885) lr 1.6374e-03 eta 0:05:58
epoch [16/50] batch [50/51] time 0.155 (0.201) data 0.000 (0.036) loss 1.1246 (1.0036) acc 78.4091 (78.1556) lr 1.6374e-03 eta 0:05:48
>>> alpha1: 0.316  alpha2: 0.092 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.13 & unmatched refined noisy rate: 0.47 <<<
epoch [17/50] batch [5/51] time 0.159 (0.554) data 0.000 (0.379) loss 0.9607 (0.9205) acc 77.2222 (78.3344) lr 1.5878e-03 eta 0:15:57
epoch [17/50] batch [10/51] time 0.182 (0.360) data 0.000 (0.191) loss 1.1506 (1.0018) acc 77.0833 (77.2336) lr 1.5878e-03 eta 0:10:20
epoch [17/50] batch [15/51] time 0.156 (0.295) data 0.000 (0.128) loss 1.0466 (0.9597) acc 71.5909 (77.7387) lr 1.5878e-03 eta 0:08:27
epoch [17/50] batch [20/51] time 0.159 (0.264) data 0.000 (0.096) loss 0.9013 (0.9296) acc 83.1522 (79.0910) lr 1.5878e-03 eta 0:07:32
epoch [17/50] batch [25/51] time 0.168 (0.245) data 0.000 (0.077) loss 0.6346 (0.9072) acc 87.5000 (79.2334) lr 1.5878e-03 eta 0:06:58
epoch [17/50] batch [30/51] time 0.169 (0.231) data 0.000 (0.064) loss 0.7663 (0.9275) acc 83.8542 (79.0894) lr 1.5878e-03 eta 0:06:33
epoch [17/50] batch [35/51] time 0.167 (0.222) data 0.000 (0.055) loss 1.0939 (0.9249) acc 75.0000 (78.9885) lr 1.5878e-03 eta 0:06:16
epoch [17/50] batch [40/51] time 0.158 (0.214) data 0.000 (0.048) loss 1.3053 (0.9388) acc 70.0000 (78.7228) lr 1.5878e-03 eta 0:06:03
epoch [17/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.043) loss 0.9370 (0.9311) acc 76.5000 (79.0467) lr 1.5878e-03 eta 0:05:52
epoch [17/50] batch [50/51] time 0.173 (0.205) data 0.000 (0.038) loss 0.7341 (0.9292) acc 79.8077 (78.8726) lr 1.5878e-03 eta 0:05:45
>>> alpha1: 0.243  alpha2: 0.051 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.46 <<<
epoch [18/50] batch [5/51] time 0.172 (0.498) data 0.000 (0.328) loss 0.7528 (0.7971) acc 85.3261 (84.2629) lr 1.5358e-03 eta 0:13:54
epoch [18/50] batch [10/51] time 0.170 (0.334) data 0.000 (0.164) loss 1.0047 (0.7904) acc 72.5000 (82.8488) lr 1.5358e-03 eta 0:09:18
epoch [18/50] batch [15/51] time 0.179 (0.281) data 0.001 (0.109) loss 0.7182 (0.7875) acc 83.1522 (83.0438) lr 1.5358e-03 eta 0:07:48
epoch [18/50] batch [20/51] time 0.171 (0.253) data 0.000 (0.082) loss 0.7655 (0.8175) acc 85.4167 (82.0753) lr 1.5358e-03 eta 0:07:00
epoch [18/50] batch [25/51] time 0.164 (0.236) data 0.000 (0.066) loss 1.1909 (0.8300) acc 71.6667 (81.6571) lr 1.5358e-03 eta 0:06:30
epoch [18/50] batch [30/51] time 0.160 (0.225) data 0.000 (0.055) loss 0.9432 (0.8240) acc 80.9783 (81.7286) lr 1.5358e-03 eta 0:06:12
epoch [18/50] batch [35/51] time 0.210 (0.219) data 0.014 (0.048) loss 0.8464 (0.8211) acc 81.6038 (81.6083) lr 1.5358e-03 eta 0:06:00
epoch [18/50] batch [40/51] time 0.178 (0.213) data 0.000 (0.042) loss 0.8569 (0.8331) acc 78.4314 (81.1552) lr 1.5358e-03 eta 0:05:50
epoch [18/50] batch [45/51] time 0.168 (0.208) data 0.000 (0.037) loss 0.9364 (0.8369) acc 76.0204 (81.1433) lr 1.5358e-03 eta 0:05:41
epoch [18/50] batch [50/51] time 0.159 (0.204) data 0.000 (0.033) loss 0.9288 (0.8413) acc 76.0870 (81.1179) lr 1.5358e-03 eta 0:05:32
>>> alpha1: 0.217  alpha2: 0.039 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.12 & unmatched refined noisy rate: 0.47 <<<
epoch [19/50] batch [5/51] time 0.174 (0.496) data 0.000 (0.314) loss 1.1735 (0.9080) acc 71.9388 (77.4322) lr 1.4818e-03 eta 0:13:27
epoch [19/50] batch [10/51] time 0.157 (0.332) data 0.000 (0.157) loss 0.7738 (0.8527) acc 84.8837 (79.8944) lr 1.4818e-03 eta 0:08:58
epoch [19/50] batch [15/51] time 0.173 (0.277) data 0.000 (0.105) loss 0.6398 (0.8621) acc 88.9423 (80.2257) lr 1.4818e-03 eta 0:07:28
epoch [19/50] batch [20/51] time 0.172 (0.250) data 0.000 (0.079) loss 0.7578 (0.8603) acc 85.1064 (80.6982) lr 1.4818e-03 eta 0:06:42
epoch [19/50] batch [25/51] time 0.168 (0.235) data 0.000 (0.063) loss 0.6794 (0.8594) acc 85.2041 (80.7663) lr 1.4818e-03 eta 0:06:17
epoch [19/50] batch [30/51] time 0.183 (0.225) data 0.000 (0.053) loss 0.6657 (0.8303) acc 86.7647 (81.2483) lr 1.4818e-03 eta 0:06:01
epoch [19/50] batch [35/51] time 0.170 (0.218) data 0.000 (0.045) loss 0.3472 (0.8057) acc 94.5000 (82.0533) lr 1.4818e-03 eta 0:05:47
epoch [19/50] batch [40/51] time 0.168 (0.211) data 0.000 (0.040) loss 0.6667 (0.7928) acc 86.0000 (82.4226) lr 1.4818e-03 eta 0:05:36
epoch [19/50] batch [45/51] time 0.162 (0.206) data 0.000 (0.035) loss 0.7121 (0.7981) acc 78.1915 (82.1972) lr 1.4818e-03 eta 0:05:26
epoch [19/50] batch [50/51] time 0.161 (0.202) data 0.000 (0.032) loss 0.9446 (0.8010) acc 76.6304 (82.0024) lr 1.4818e-03 eta 0:05:19
>>> alpha1: 0.198  alpha2: 0.036 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.14 & unmatched refined noisy rate: 0.50 <<<
epoch [20/50] batch [5/51] time 0.179 (0.530) data 0.000 (0.348) loss 0.7810 (0.7474) acc 81.9444 (82.2307) lr 1.4258e-03 eta 0:13:55
epoch [20/50] batch [10/51] time 0.167 (0.353) data 0.000 (0.174) loss 0.7895 (0.7450) acc 77.5510 (83.0467) lr 1.4258e-03 eta 0:09:14
epoch [20/50] batch [15/51] time 0.171 (0.292) data 0.000 (0.116) loss 0.6080 (0.7124) acc 86.2745 (83.8739) lr 1.4258e-03 eta 0:07:37
epoch [20/50] batch [20/51] time 0.157 (0.261) data 0.000 (0.087) loss 1.0379 (0.7254) acc 78.8889 (83.6729) lr 1.4258e-03 eta 0:06:47
epoch [20/50] batch [25/51] time 0.179 (0.245) data 0.000 (0.070) loss 0.6945 (0.7216) acc 84.0000 (83.7242) lr 1.4258e-03 eta 0:06:21
epoch [20/50] batch [30/51] time 0.181 (0.235) data 0.000 (0.058) loss 0.7006 (0.7310) acc 82.0000 (83.5741) lr 1.4258e-03 eta 0:06:04
epoch [20/50] batch [35/51] time 0.175 (0.227) data 0.000 (0.050) loss 0.7482 (0.7326) acc 83.5000 (83.5432) lr 1.4258e-03 eta 0:05:50
epoch [20/50] batch [40/51] time 0.165 (0.220) data 0.000 (0.044) loss 1.0151 (0.7546) acc 69.2708 (82.7471) lr 1.4258e-03 eta 0:05:39
epoch [20/50] batch [45/51] time 0.166 (0.214) data 0.000 (0.039) loss 0.6474 (0.7544) acc 89.2857 (82.8312) lr 1.4258e-03 eta 0:05:28
epoch [20/50] batch [50/51] time 0.161 (0.209) data 0.000 (0.035) loss 1.0472 (0.7594) acc 73.9362 (82.4069) lr 1.4258e-03 eta 0:05:20
>>> alpha1: 0.184  alpha2: 0.034 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.15 & unmatched refined noisy rate: 0.50 <<<
epoch [21/50] batch [5/51] time 0.191 (0.490) data 0.000 (0.312) loss 0.6815 (0.8645) acc 81.4815 (82.9540) lr 1.3681e-03 eta 0:12:27
epoch [21/50] batch [10/51] time 0.178 (0.337) data 0.000 (0.156) loss 0.7132 (0.8414) acc 79.2453 (82.3145) lr 1.3681e-03 eta 0:08:32
epoch [21/50] batch [15/51] time 0.181 (0.284) data 0.000 (0.104) loss 0.6669 (0.7821) acc 84.2593 (83.3675) lr 1.3681e-03 eta 0:07:10
epoch [21/50] batch [20/51] time 0.164 (0.256) data 0.000 (0.078) loss 0.8540 (0.7685) acc 76.0417 (83.3671) lr 1.3681e-03 eta 0:06:26
epoch [21/50] batch [25/51] time 0.191 (0.240) data 0.000 (0.063) loss 0.5907 (0.7644) acc 87.7273 (83.3613) lr 1.3681e-03 eta 0:06:01
epoch [21/50] batch [30/51] time 0.172 (0.230) data 0.000 (0.052) loss 0.6543 (0.7556) acc 88.2653 (83.2609) lr 1.3681e-03 eta 0:05:45
epoch [21/50] batch [35/51] time 0.191 (0.222) data 0.000 (0.045) loss 0.8113 (0.7561) acc 80.1020 (83.5927) lr 1.3681e-03 eta 0:05:32
epoch [21/50] batch [40/51] time 0.164 (0.217) data 0.000 (0.039) loss 0.8112 (0.7537) acc 85.9375 (83.8166) lr 1.3681e-03 eta 0:05:22
epoch [21/50] batch [45/51] time 0.167 (0.212) data 0.001 (0.035) loss 0.6444 (0.7607) acc 83.1633 (83.4517) lr 1.3681e-03 eta 0:05:14
epoch [21/50] batch [50/51] time 0.180 (0.208) data 0.000 (0.031) loss 0.6588 (0.7557) acc 81.8182 (83.3554) lr 1.3681e-03 eta 0:05:07
>>> alpha1: 0.177  alpha2: 0.035 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.49 <<<
epoch [22/50] batch [5/51] time 0.173 (0.476) data 0.000 (0.296) loss 0.6552 (0.7597) acc 81.6327 (82.8976) lr 1.3090e-03 eta 0:11:40
epoch [22/50] batch [10/51] time 0.192 (0.329) data 0.000 (0.148) loss 0.8105 (0.7360) acc 83.9286 (83.3056) lr 1.3090e-03 eta 0:08:02
epoch [22/50] batch [15/51] time 0.179 (0.277) data 0.000 (0.099) loss 0.5601 (0.7179) acc 87.5000 (84.1575) lr 1.3090e-03 eta 0:06:45
epoch [22/50] batch [20/51] time 0.185 (0.252) data 0.000 (0.074) loss 0.6766 (0.7256) acc 84.6491 (84.1133) lr 1.3090e-03 eta 0:06:07
epoch [22/50] batch [25/51] time 0.172 (0.236) data 0.000 (0.059) loss 0.8116 (0.7083) acc 86.2245 (84.3311) lr 1.3090e-03 eta 0:05:43
epoch [22/50] batch [30/51] time 0.187 (0.228) data 0.000 (0.049) loss 0.6355 (0.7159) acc 84.1346 (83.9895) lr 1.3090e-03 eta 0:05:29
epoch [22/50] batch [35/51] time 0.166 (0.221) data 0.000 (0.042) loss 0.8512 (0.7279) acc 80.3191 (83.6317) lr 1.3090e-03 eta 0:05:18
epoch [22/50] batch [40/51] time 0.168 (0.215) data 0.000 (0.037) loss 0.7623 (0.7193) acc 83.5000 (83.9501) lr 1.3090e-03 eta 0:05:08
epoch [22/50] batch [45/51] time 0.179 (0.210) data 0.000 (0.033) loss 0.8652 (0.7233) acc 79.5455 (83.5468) lr 1.3090e-03 eta 0:05:00
epoch [22/50] batch [50/51] time 0.182 (0.206) data 0.000 (0.030) loss 0.6554 (0.7291) acc 85.7143 (83.4086) lr 1.3090e-03 eta 0:04:54
>>> alpha1: 0.168  alpha2: 0.036 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [23/50] batch [5/51] time 0.167 (0.520) data 0.000 (0.334) loss 0.7788 (0.5934) acc 80.2083 (88.8068) lr 1.2487e-03 eta 0:12:19
epoch [23/50] batch [10/51] time 0.176 (0.351) data 0.000 (0.167) loss 0.6116 (0.6554) acc 85.7843 (86.7165) lr 1.2487e-03 eta 0:08:17
epoch [23/50] batch [15/51] time 0.174 (0.296) data 0.000 (0.111) loss 0.6866 (0.7769) acc 83.6538 (85.1236) lr 1.2487e-03 eta 0:06:57
epoch [23/50] batch [20/51] time 0.178 (0.266) data 0.000 (0.084) loss 0.7153 (0.7673) acc 83.9623 (84.3925) lr 1.2487e-03 eta 0:06:14
epoch [23/50] batch [25/51] time 0.193 (0.248) data 0.000 (0.067) loss 0.4780 (0.7486) acc 89.7959 (84.4942) lr 1.2487e-03 eta 0:05:48
epoch [23/50] batch [30/51] time 0.189 (0.237) data 0.000 (0.056) loss 0.8674 (0.7409) acc 81.7708 (84.5992) lr 1.2487e-03 eta 0:05:31
epoch [23/50] batch [35/51] time 0.177 (0.229) data 0.000 (0.048) loss 0.7373 (0.7449) acc 85.8491 (84.6120) lr 1.2487e-03 eta 0:05:18
epoch [23/50] batch [40/51] time 1.319 (0.250) data 0.000 (0.042) loss 0.6823 (0.7465) acc 83.4746 (84.2717) lr 1.2487e-03 eta 0:05:47
epoch [23/50] batch [45/51] time 0.184 (0.243) data 0.000 (0.037) loss 0.6252 (0.7503) acc 85.9091 (84.0234) lr 1.2487e-03 eta 0:05:35
epoch [23/50] batch [50/51] time 0.171 (0.236) data 0.000 (0.034) loss 0.7211 (0.7454) acc 83.8235 (83.9444) lr 1.2487e-03 eta 0:05:25
>>> alpha1: 0.164  alpha2: 0.040 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [24/50] batch [5/51] time 0.182 (0.502) data 0.000 (0.317) loss 0.5358 (0.6385) acc 88.1818 (87.1802) lr 1.1874e-03 eta 0:11:28
epoch [24/50] batch [10/51] time 0.171 (0.340) data 0.000 (0.158) loss 0.7364 (0.6495) acc 84.3137 (86.2387) lr 1.1874e-03 eta 0:07:44
epoch [24/50] batch [15/51] time 0.190 (0.288) data 0.000 (0.106) loss 0.7931 (0.6644) acc 84.6154 (86.2963) lr 1.1874e-03 eta 0:06:31
epoch [24/50] batch [20/51] time 0.196 (0.262) data 0.000 (0.079) loss 0.8344 (0.6886) acc 81.6038 (85.2309) lr 1.1874e-03 eta 0:05:55
epoch [24/50] batch [25/51] time 0.187 (0.246) data 0.010 (0.064) loss 0.5293 (0.6821) acc 89.4231 (85.2521) lr 1.1874e-03 eta 0:05:32
epoch [24/50] batch [30/51] time 0.170 (0.235) data 0.000 (0.053) loss 0.7605 (0.6755) acc 87.7451 (85.2703) lr 1.1874e-03 eta 0:05:16
epoch [24/50] batch [35/51] time 0.181 (0.227) data 0.000 (0.046) loss 0.5589 (0.6756) acc 84.2593 (85.1705) lr 1.1874e-03 eta 0:05:04
epoch [24/50] batch [40/51] time 0.185 (0.221) data 0.000 (0.040) loss 0.6999 (0.6847) acc 79.1667 (84.4472) lr 1.1874e-03 eta 0:04:55
epoch [24/50] batch [45/51] time 0.170 (0.215) data 0.000 (0.036) loss 1.0820 (0.6928) acc 71.5000 (84.0957) lr 1.1874e-03 eta 0:04:47
epoch [24/50] batch [50/51] time 0.175 (0.212) data 0.000 (0.032) loss 0.7253 (0.6883) acc 84.6154 (84.4074) lr 1.1874e-03 eta 0:04:40
>>> alpha1: 0.160  alpha2: 0.042 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [25/50] batch [5/51] time 0.186 (0.457) data 0.001 (0.267) loss 0.8992 (0.7813) acc 82.8704 (82.6837) lr 1.1253e-03 eta 0:10:04
epoch [25/50] batch [10/51] time 0.180 (0.318) data 0.000 (0.133) loss 0.8334 (0.7279) acc 80.5556 (83.0923) lr 1.1253e-03 eta 0:06:59
epoch [25/50] batch [15/51] time 0.199 (0.272) data 0.000 (0.089) loss 0.5308 (0.6892) acc 87.5000 (84.4899) lr 1.1253e-03 eta 0:05:55
epoch [25/50] batch [20/51] time 0.195 (0.248) data 0.000 (0.067) loss 0.5433 (0.6854) acc 88.2075 (84.7028) lr 1.1253e-03 eta 0:05:24
epoch [25/50] batch [25/51] time 0.192 (0.236) data 0.000 (0.054) loss 0.6215 (0.6715) acc 86.0577 (84.6514) lr 1.1253e-03 eta 0:05:07
epoch [25/50] batch [30/51] time 0.182 (0.229) data 0.000 (0.045) loss 0.6101 (0.6642) acc 88.2353 (84.7307) lr 1.1253e-03 eta 0:04:56
epoch [25/50] batch [35/51] time 0.185 (0.222) data 0.000 (0.038) loss 0.6702 (0.6715) acc 79.9107 (84.4219) lr 1.1253e-03 eta 0:04:46
epoch [25/50] batch [40/51] time 0.172 (0.216) data 0.000 (0.034) loss 0.5836 (0.6670) acc 88.4615 (84.4476) lr 1.1253e-03 eta 0:04:38
epoch [25/50] batch [45/51] time 0.175 (0.212) data 0.000 (0.030) loss 0.6809 (0.6660) acc 83.9623 (84.3727) lr 1.1253e-03 eta 0:04:31
epoch [25/50] batch [50/51] time 0.167 (0.208) data 0.000 (0.027) loss 0.7574 (0.6597) acc 83.6735 (84.6210) lr 1.1253e-03 eta 0:04:25
>>> alpha1: 0.155  alpha2: 0.042 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.46 <<<
epoch [26/50] batch [5/51] time 0.177 (0.461) data 0.000 (0.278) loss 0.7364 (0.6035) acc 87.2642 (86.6886) lr 1.0628e-03 eta 0:09:45
epoch [26/50] batch [10/51] time 0.167 (0.323) data 0.000 (0.140) loss 0.5899 (0.6503) acc 88.2653 (85.6837) lr 1.0628e-03 eta 0:06:48
epoch [26/50] batch [15/51] time 0.188 (0.277) data 0.000 (0.094) loss 0.7549 (0.6692) acc 83.4906 (84.4682) lr 1.0628e-03 eta 0:05:49
epoch [26/50] batch [20/51] time 0.170 (0.253) data 0.000 (0.070) loss 0.5051 (0.6687) acc 92.1569 (84.9720) lr 1.0628e-03 eta 0:05:17
epoch [26/50] batch [25/51] time 0.175 (0.238) data 0.001 (0.056) loss 0.6704 (0.6699) acc 81.3726 (84.7350) lr 1.0628e-03 eta 0:04:57
epoch [26/50] batch [30/51] time 0.181 (0.228) data 0.000 (0.047) loss 0.7141 (0.6891) acc 78.6364 (84.0127) lr 1.0628e-03 eta 0:04:44
epoch [26/50] batch [35/51] time 0.191 (0.222) data 0.000 (0.040) loss 0.8502 (0.6934) acc 77.2727 (83.9597) lr 1.0628e-03 eta 0:04:34
epoch [26/50] batch [40/51] time 0.169 (0.215) data 0.000 (0.035) loss 0.7277 (0.6811) acc 84.0000 (84.4358) lr 1.0628e-03 eta 0:04:25
epoch [26/50] batch [45/51] time 0.158 (0.210) data 0.000 (0.031) loss 0.8864 (0.6833) acc 74.4445 (84.2805) lr 1.0628e-03 eta 0:04:18
epoch [26/50] batch [50/51] time 0.171 (0.206) data 0.000 (0.028) loss 0.7444 (0.6858) acc 81.8627 (84.4110) lr 1.0628e-03 eta 0:04:12
>>> alpha1: 0.150  alpha2: 0.044 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [27/50] batch [5/51] time 0.200 (0.471) data 0.001 (0.282) loss 0.6106 (0.9291) acc 91.6667 (82.3460) lr 1.0000e-03 eta 0:09:34
epoch [27/50] batch [10/51] time 0.178 (0.323) data 0.000 (0.141) loss 0.6102 (0.7977) acc 85.6481 (83.2705) lr 1.0000e-03 eta 0:06:32
epoch [27/50] batch [15/51] time 0.171 (0.272) data 0.000 (0.094) loss 0.6010 (0.7315) acc 87.2549 (84.5286) lr 1.0000e-03 eta 0:05:29
epoch [27/50] batch [20/51] time 0.180 (0.249) data 0.000 (0.071) loss 0.5072 (0.7177) acc 90.7407 (84.4606) lr 1.0000e-03 eta 0:04:59
epoch [27/50] batch [25/51] time 0.182 (0.236) data 0.000 (0.057) loss 0.4883 (0.7066) acc 90.8654 (84.5960) lr 1.0000e-03 eta 0:04:42
epoch [27/50] batch [30/51] time 0.173 (0.227) data 0.000 (0.047) loss 0.7291 (0.6903) acc 83.6538 (85.3311) lr 1.0000e-03 eta 0:04:30
epoch [27/50] batch [35/51] time 0.189 (0.221) data 0.000 (0.041) loss 0.5888 (0.6823) acc 83.1818 (85.2645) lr 1.0000e-03 eta 0:04:22
epoch [27/50] batch [40/51] time 0.165 (0.215) data 0.000 (0.035) loss 0.7841 (0.6833) acc 84.3750 (85.2059) lr 1.0000e-03 eta 0:04:14
epoch [27/50] batch [45/51] time 0.162 (0.210) data 0.000 (0.032) loss 0.7161 (0.6828) acc 85.6383 (85.0845) lr 1.0000e-03 eta 0:04:07
epoch [27/50] batch [50/51] time 0.160 (0.206) data 0.000 (0.028) loss 0.8274 (0.6791) acc 78.2609 (85.1444) lr 1.0000e-03 eta 0:04:01
>>> alpha1: 0.149  alpha2: 0.046 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.47 <<<
epoch [28/50] batch [5/51] time 0.183 (0.496) data 0.000 (0.304) loss 0.7078 (0.5899) acc 79.6875 (86.7453) lr 9.3721e-04 eta 0:09:39
epoch [28/50] batch [10/51] time 0.158 (0.337) data 0.000 (0.152) loss 0.8085 (0.6093) acc 80.0000 (85.7693) lr 9.3721e-04 eta 0:06:32
epoch [28/50] batch [15/51] time 0.159 (0.281) data 0.000 (0.102) loss 0.4844 (0.6004) acc 91.6667 (86.5089) lr 9.3721e-04 eta 0:05:25
epoch [28/50] batch [20/51] time 0.173 (0.254) data 0.000 (0.077) loss 0.7879 (0.6230) acc 79.0816 (85.8943) lr 9.3721e-04 eta 0:04:53
epoch [28/50] batch [25/51] time 0.181 (0.238) data 0.000 (0.062) loss 0.5211 (0.6176) acc 93.0556 (86.0335) lr 9.3721e-04 eta 0:04:33
epoch [28/50] batch [30/51] time 0.186 (0.228) data 0.000 (0.051) loss 0.6195 (0.6163) acc 84.9057 (85.8345) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [35/51] time 0.174 (0.221) data 0.000 (0.044) loss 0.6711 (0.6051) acc 84.8039 (86.2606) lr 9.3721e-04 eta 0:04:11
epoch [28/50] batch [40/51] time 0.164 (0.215) data 0.000 (0.039) loss 0.6528 (0.6053) acc 84.3750 (86.3723) lr 9.3721e-04 eta 0:04:03
epoch [28/50] batch [45/51] time 0.169 (0.210) data 0.000 (0.034) loss 0.9009 (0.6067) acc 78.4314 (86.2002) lr 9.3721e-04 eta 0:03:56
epoch [28/50] batch [50/51] time 0.165 (0.206) data 0.001 (0.031) loss 0.6438 (0.6105) acc 86.4583 (86.2037) lr 9.3721e-04 eta 0:03:51
>>> alpha1: 0.144  alpha2: 0.043 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.49 <<<
epoch [29/50] batch [5/51] time 0.183 (0.466) data 0.000 (0.282) loss 0.4171 (0.5818) acc 89.8148 (86.8578) lr 8.7467e-04 eta 0:08:40
epoch [29/50] batch [10/51] time 0.171 (0.320) data 0.000 (0.141) loss 0.6016 (0.6053) acc 81.8627 (85.9575) lr 8.7467e-04 eta 0:05:56
epoch [29/50] batch [15/51] time 0.174 (0.271) data 0.001 (0.094) loss 0.5369 (0.5965) acc 87.9808 (86.5491) lr 8.7467e-04 eta 0:04:59
epoch [29/50] batch [20/51] time 0.182 (0.246) data 0.000 (0.071) loss 0.7089 (0.6079) acc 87.7451 (86.0808) lr 8.7467e-04 eta 0:04:31
epoch [29/50] batch [25/51] time 0.173 (0.232) data 0.000 (0.057) loss 0.6369 (0.6192) acc 85.0962 (85.6061) lr 8.7467e-04 eta 0:04:14
epoch [29/50] batch [30/51] time 0.173 (0.223) data 0.002 (0.047) loss 0.6731 (0.6332) acc 84.3750 (85.4045) lr 8.7467e-04 eta 0:04:03
epoch [29/50] batch [35/51] time 0.203 (0.216) data 0.000 (0.041) loss 0.7251 (0.6381) acc 78.2407 (85.0735) lr 8.7467e-04 eta 0:03:55
epoch [29/50] batch [40/51] time 0.162 (0.211) data 0.000 (0.036) loss 0.7846 (0.6410) acc 81.9149 (85.0467) lr 8.7467e-04 eta 0:03:48
epoch [29/50] batch [45/51] time 0.169 (0.207) data 0.000 (0.032) loss 0.5114 (0.6350) acc 88.2353 (85.2581) lr 8.7467e-04 eta 0:03:42
epoch [29/50] batch [50/51] time 0.178 (0.203) data 0.000 (0.028) loss 0.4324 (0.6229) acc 90.7407 (85.6877) lr 8.7467e-04 eta 0:03:37
>>> alpha1: 0.142  alpha2: 0.045 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [30/50] batch [5/51] time 0.178 (0.491) data 0.000 (0.304) loss 0.5727 (0.5547) acc 90.1961 (88.9082) lr 8.1262e-04 eta 0:08:43
epoch [30/50] batch [10/51] time 0.180 (0.338) data 0.000 (0.152) loss 0.5972 (0.5649) acc 84.8039 (87.5025) lr 8.1262e-04 eta 0:05:58
epoch [30/50] batch [15/51] time 0.182 (0.283) data 0.000 (0.101) loss 0.4755 (0.5606) acc 91.1765 (86.9444) lr 8.1262e-04 eta 0:04:59
epoch [30/50] batch [20/51] time 0.190 (0.258) data 0.000 (0.076) loss 0.6203 (0.5937) acc 85.6481 (86.3137) lr 8.1262e-04 eta 0:04:30
epoch [30/50] batch [25/51] time 0.177 (0.241) data 0.000 (0.061) loss 0.6277 (0.5727) acc 87.2642 (86.7719) lr 8.1262e-04 eta 0:04:11
epoch [30/50] batch [30/51] time 0.179 (0.230) data 0.000 (0.051) loss 0.8376 (0.5881) acc 75.0000 (86.2557) lr 8.1262e-04 eta 0:03:59
epoch [30/50] batch [35/51] time 0.195 (0.224) data 0.000 (0.044) loss 0.6412 (0.5827) acc 81.0185 (86.2648) lr 8.1262e-04 eta 0:03:51
epoch [30/50] batch [40/51] time 0.166 (0.218) data 0.000 (0.038) loss 0.6613 (0.5908) acc 85.7143 (86.1464) lr 8.1262e-04 eta 0:03:44
epoch [30/50] batch [45/51] time 0.179 (0.213) data 0.000 (0.034) loss 0.7921 (0.5941) acc 86.5741 (86.2895) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [50/51] time 0.169 (0.209) data 0.000 (0.031) loss 0.6579 (0.6036) acc 80.5000 (85.9781) lr 8.1262e-04 eta 0:03:33
>>> alpha1: 0.140  alpha2: 0.046 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [31/50] batch [5/51] time 0.181 (0.466) data 0.001 (0.278) loss 0.4610 (0.5681) acc 86.1111 (85.9407) lr 7.5131e-04 eta 0:07:52
epoch [31/50] batch [10/51] time 0.195 (0.325) data 0.000 (0.139) loss 0.5303 (0.5595) acc 86.8182 (86.5579) lr 7.5131e-04 eta 0:05:28
epoch [31/50] batch [15/51] time 0.187 (0.282) data 0.001 (0.093) loss 0.8011 (0.5964) acc 80.8824 (85.6661) lr 7.5131e-04 eta 0:04:43
epoch [31/50] batch [20/51] time 0.178 (0.258) data 0.000 (0.070) loss 0.5392 (0.5867) acc 85.7843 (86.1016) lr 7.5131e-04 eta 0:04:18
epoch [31/50] batch [25/51] time 0.178 (0.245) data 0.000 (0.056) loss 0.8013 (0.5888) acc 85.0000 (86.5668) lr 7.5131e-04 eta 0:04:03
epoch [31/50] batch [30/51] time 0.197 (0.235) data 0.000 (0.047) loss 0.5449 (0.5857) acc 86.8182 (86.5447) lr 7.5131e-04 eta 0:03:52
epoch [31/50] batch [35/51] time 0.196 (0.227) data 0.000 (0.040) loss 0.6036 (0.5896) acc 86.5000 (86.4992) lr 7.5131e-04 eta 0:03:43
epoch [31/50] batch [40/51] time 0.177 (0.220) data 0.000 (0.035) loss 0.6569 (0.5938) acc 86.1111 (86.4145) lr 7.5131e-04 eta 0:03:35
epoch [31/50] batch [45/51] time 0.182 (0.215) data 0.000 (0.031) loss 0.5547 (0.6092) acc 87.0536 (86.1070) lr 7.5131e-04 eta 0:03:29
epoch [31/50] batch [50/51] time 0.181 (0.211) data 0.000 (0.028) loss 0.5557 (0.6049) acc 89.0909 (86.3151) lr 7.5131e-04 eta 0:03:24
>>> alpha1: 0.136  alpha2: 0.045 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.16 & unmatched refined noisy rate: 0.49 <<<
epoch [32/50] batch [5/51] time 0.176 (0.506) data 0.000 (0.319) loss 0.6795 (0.5003) acc 85.7143 (90.4677) lr 6.9098e-04 eta 0:08:07
epoch [32/50] batch [10/51] time 0.176 (0.343) data 0.000 (0.161) loss 0.5161 (0.5469) acc 87.5000 (88.6043) lr 6.9098e-04 eta 0:05:28
epoch [32/50] batch [15/51] time 0.168 (0.289) data 0.001 (0.107) loss 0.5780 (0.5398) acc 81.6327 (88.3597) lr 6.9098e-04 eta 0:04:35
epoch [32/50] batch [20/51] time 0.186 (0.264) data 0.000 (0.081) loss 0.6476 (0.5523) acc 86.7647 (88.0729) lr 6.9098e-04 eta 0:04:10
epoch [32/50] batch [25/51] time 0.188 (0.247) data 0.001 (0.065) loss 0.3441 (0.5546) acc 93.6274 (88.0434) lr 6.9098e-04 eta 0:03:53
epoch [32/50] batch [30/51] time 0.190 (0.236) data 0.000 (0.054) loss 0.3573 (0.5521) acc 94.3878 (88.2375) lr 6.9098e-04 eta 0:03:41
epoch [32/50] batch [35/51] time 0.183 (0.229) data 0.000 (0.046) loss 0.7793 (0.5740) acc 83.6364 (87.5694) lr 6.9098e-04 eta 0:03:33
epoch [32/50] batch [40/51] time 0.164 (0.222) data 0.000 (0.040) loss 0.9264 (0.5886) acc 76.0417 (87.0380) lr 6.9098e-04 eta 0:03:26
epoch [32/50] batch [45/51] time 0.176 (0.217) data 0.000 (0.036) loss 0.5363 (0.5844) acc 88.2075 (86.9236) lr 6.9098e-04 eta 0:03:20
epoch [32/50] batch [50/51] time 0.170 (0.213) data 0.000 (0.032) loss 0.7103 (0.5853) acc 83.8235 (86.6964) lr 6.9098e-04 eta 0:03:15
>>> alpha1: 0.136  alpha2: 0.046 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.51 <<<
epoch [33/50] batch [5/51] time 0.170 (0.514) data 0.000 (0.322) loss 0.6678 (0.6739) acc 88.0208 (86.1166) lr 6.3188e-04 eta 0:07:49
epoch [33/50] batch [10/51] time 0.178 (0.346) data 0.001 (0.161) loss 0.4998 (0.6230) acc 87.2642 (86.6146) lr 6.3188e-04 eta 0:05:14
epoch [33/50] batch [15/51] time 0.168 (0.290) data 0.000 (0.108) loss 0.5091 (0.6125) acc 88.2653 (86.7748) lr 6.3188e-04 eta 0:04:21
epoch [33/50] batch [20/51] time 0.180 (0.263) data 0.000 (0.081) loss 0.4751 (0.5982) acc 86.7924 (86.2012) lr 6.3188e-04 eta 0:03:56
epoch [33/50] batch [25/51] time 0.187 (0.247) data 0.000 (0.065) loss 0.5021 (0.5940) acc 90.3509 (86.3813) lr 6.3188e-04 eta 0:03:40
epoch [33/50] batch [30/51] time 0.168 (0.235) data 0.000 (0.054) loss 0.5456 (0.5939) acc 89.0000 (86.5794) lr 6.3188e-04 eta 0:03:29
epoch [33/50] batch [35/51] time 0.211 (0.229) data 0.000 (0.046) loss 0.5861 (0.5908) acc 86.0577 (86.8947) lr 6.3188e-04 eta 0:03:22
epoch [33/50] batch [40/51] time 0.171 (0.223) data 0.000 (0.041) loss 0.4762 (0.5800) acc 91.6667 (86.9737) lr 6.3188e-04 eta 0:03:15
epoch [33/50] batch [45/51] time 0.173 (0.218) data 0.000 (0.036) loss 0.6054 (0.5743) acc 83.1731 (87.0977) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [50/51] time 0.182 (0.214) data 0.000 (0.033) loss 0.5953 (0.5763) acc 86.3208 (87.0194) lr 6.3188e-04 eta 0:03:05
>>> alpha1: 0.137  alpha2: 0.048 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [34/50] batch [5/51] time 0.179 (0.473) data 0.000 (0.291) loss 0.7095 (0.6577) acc 78.7736 (84.3753) lr 5.7422e-04 eta 0:06:47
epoch [34/50] batch [10/51] time 0.199 (0.328) data 0.000 (0.146) loss 0.5239 (0.5912) acc 89.4737 (85.9428) lr 5.7422e-04 eta 0:04:41
epoch [34/50] batch [15/51] time 0.203 (0.278) data 0.000 (0.097) loss 0.6344 (0.6038) acc 86.1607 (86.2022) lr 5.7422e-04 eta 0:03:57
epoch [34/50] batch [20/51] time 0.167 (0.252) data 0.000 (0.073) loss 0.3826 (0.5973) acc 94.3878 (86.2360) lr 5.7422e-04 eta 0:03:33
epoch [34/50] batch [25/51] time 0.181 (0.238) data 0.000 (0.058) loss 0.5463 (0.5828) acc 88.7755 (86.4826) lr 5.7422e-04 eta 0:03:20
epoch [34/50] batch [30/51] time 0.177 (0.229) data 0.000 (0.049) loss 0.7617 (0.5872) acc 84.4340 (86.7540) lr 5.7422e-04 eta 0:03:11
epoch [34/50] batch [35/51] time 0.195 (0.222) data 0.000 (0.042) loss 0.7580 (0.5841) acc 84.0000 (86.9935) lr 5.7422e-04 eta 0:03:04
epoch [34/50] batch [40/51] time 0.172 (0.217) data 0.000 (0.037) loss 0.6217 (0.5795) acc 82.6531 (86.9441) lr 5.7422e-04 eta 0:02:59
epoch [34/50] batch [45/51] time 0.179 (0.212) data 0.000 (0.033) loss 0.5460 (0.5734) acc 88.8889 (87.0515) lr 5.7422e-04 eta 0:02:54
epoch [34/50] batch [50/51] time 0.182 (0.209) data 0.000 (0.029) loss 0.4533 (0.5674) acc 91.0714 (87.1697) lr 5.7422e-04 eta 0:02:50
>>> alpha1: 0.136  alpha2: 0.047 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [35/50] batch [5/51] time 0.181 (0.441) data 0.000 (0.254) loss 0.5121 (0.6164) acc 87.9808 (87.4010) lr 5.1825e-04 eta 0:05:57
epoch [35/50] batch [10/51] time 0.190 (0.316) data 0.000 (0.127) loss 0.5449 (0.5744) acc 87.0370 (88.1724) lr 5.1825e-04 eta 0:04:14
epoch [35/50] batch [15/51] time 0.178 (0.270) data 0.000 (0.085) loss 0.6924 (0.6043) acc 82.4074 (86.9134) lr 5.1825e-04 eta 0:03:36
epoch [35/50] batch [20/51] time 0.179 (0.247) data 0.000 (0.064) loss 0.7180 (0.5855) acc 88.8889 (87.1652) lr 5.1825e-04 eta 0:03:16
epoch [35/50] batch [25/51] time 0.173 (0.232) data 0.000 (0.051) loss 0.6845 (0.6695) acc 86.5385 (86.2116) lr 5.1825e-04 eta 0:03:03
epoch [35/50] batch [30/51] time 0.189 (0.225) data 0.000 (0.043) loss 0.4818 (0.6887) acc 89.1509 (86.1399) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [35/51] time 0.182 (0.219) data 0.000 (0.037) loss 0.4804 (0.6674) acc 95.1923 (86.5126) lr 5.1825e-04 eta 0:02:51
epoch [35/50] batch [40/51] time 0.185 (0.214) data 0.000 (0.032) loss 0.3954 (0.6499) acc 92.1296 (86.7520) lr 5.1825e-04 eta 0:02:46
epoch [35/50] batch [45/51] time 0.175 (0.209) data 0.000 (0.028) loss 0.5333 (0.6429) acc 92.4528 (86.8899) lr 5.1825e-04 eta 0:02:41
epoch [35/50] batch [50/51] time 0.169 (0.206) data 0.001 (0.026) loss 0.8902 (0.6419) acc 76.5957 (86.7189) lr 5.1825e-04 eta 0:02:37
>>> alpha1: 0.136  alpha2: 0.049 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.47 <<<
epoch [36/50] batch [5/51] time 0.176 (0.473) data 0.000 (0.287) loss 0.4036 (0.5338) acc 91.1765 (89.6381) lr 4.6417e-04 eta 0:05:59
epoch [36/50] batch [10/51] time 0.178 (0.326) data 0.000 (0.144) loss 0.4609 (0.5500) acc 89.1509 (88.9498) lr 4.6417e-04 eta 0:04:06
epoch [36/50] batch [15/51] time 0.192 (0.277) data 0.000 (0.096) loss 0.5968 (0.5481) acc 86.6379 (89.1599) lr 4.6417e-04 eta 0:03:27
epoch [36/50] batch [20/51] time 0.187 (0.254) data 0.000 (0.072) loss 0.7964 (0.5429) acc 80.8036 (88.8503) lr 4.6417e-04 eta 0:03:09
epoch [36/50] batch [25/51] time 0.180 (0.239) data 0.000 (0.058) loss 0.3645 (0.5580) acc 89.6226 (88.2191) lr 4.6417e-04 eta 0:02:56
epoch [36/50] batch [30/51] time 0.190 (0.231) data 0.000 (0.048) loss 0.4021 (0.5493) acc 94.5000 (88.5966) lr 4.6417e-04 eta 0:02:49
epoch [36/50] batch [35/51] time 0.187 (0.224) data 0.000 (0.041) loss 0.6093 (0.5747) acc 81.9444 (87.6376) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [40/51] time 0.173 (0.219) data 0.000 (0.036) loss 0.6374 (0.5744) acc 88.4615 (87.7465) lr 4.6417e-04 eta 0:02:38
epoch [36/50] batch [45/51] time 0.171 (0.214) data 0.000 (0.032) loss 0.4687 (0.5744) acc 89.2857 (87.8459) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [50/51] time 0.170 (0.210) data 0.000 (0.029) loss 0.5745 (0.5758) acc 83.5000 (87.6741) lr 4.6417e-04 eta 0:02:30
>>> alpha1: 0.138  alpha2: 0.050 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [37/50] batch [5/51] time 0.186 (0.546) data 0.000 (0.355) loss 0.5222 (0.6145) acc 91.2037 (85.8471) lr 4.1221e-04 eta 0:06:27
epoch [37/50] batch [10/51] time 0.191 (0.364) data 0.000 (0.178) loss 0.4782 (0.5458) acc 89.5000 (88.1829) lr 4.1221e-04 eta 0:04:16
epoch [37/50] batch [15/51] time 0.185 (0.307) data 0.000 (0.119) loss 0.6695 (0.5604) acc 85.5263 (88.0056) lr 4.1221e-04 eta 0:03:34
epoch [37/50] batch [20/51] time 0.204 (0.277) data 0.000 (0.089) loss 0.6703 (0.5518) acc 83.9286 (88.1122) lr 4.1221e-04 eta 0:03:12
epoch [37/50] batch [25/51] time 0.192 (0.258) data 0.000 (0.071) loss 0.7050 (0.5662) acc 86.3208 (87.8791) lr 4.1221e-04 eta 0:02:57
epoch [37/50] batch [30/51] time 0.183 (0.247) data 0.001 (0.059) loss 0.5395 (0.5641) acc 87.0192 (87.8543) lr 4.1221e-04 eta 0:02:48
epoch [37/50] batch [35/51] time 0.168 (0.237) data 0.000 (0.051) loss 0.6010 (0.5636) acc 84.8958 (87.7449) lr 4.1221e-04 eta 0:02:40
epoch [37/50] batch [40/51] time 0.173 (0.230) data 0.000 (0.045) loss 0.5791 (0.5619) acc 88.4615 (87.8391) lr 4.1221e-04 eta 0:02:34
epoch [37/50] batch [45/51] time 0.179 (0.224) data 0.000 (0.040) loss 0.4447 (0.5643) acc 89.8148 (87.7999) lr 4.1221e-04 eta 0:02:29
epoch [37/50] batch [50/51] time 0.170 (0.219) data 0.000 (0.036) loss 0.4596 (0.5538) acc 87.0000 (87.9139) lr 4.1221e-04 eta 0:02:25
>>> alpha1: 0.134  alpha2: 0.052 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [38/50] batch [5/51] time 0.182 (0.427) data 0.000 (0.243) loss 0.4174 (0.5916) acc 92.2727 (86.0305) lr 3.6258e-04 eta 0:04:41
epoch [38/50] batch [10/51] time 0.168 (0.304) data 0.000 (0.122) loss 0.5698 (0.5567) acc 88.8298 (87.9305) lr 3.6258e-04 eta 0:03:18
epoch [38/50] batch [15/51] time 0.195 (0.264) data 0.000 (0.081) loss 0.4080 (0.5560) acc 92.5439 (87.7327) lr 3.6258e-04 eta 0:02:50
epoch [38/50] batch [20/51] time 0.193 (0.246) data 0.000 (0.061) loss 0.5626 (0.5319) acc 85.6383 (88.3432) lr 3.6258e-04 eta 0:02:38
epoch [38/50] batch [25/51] time 0.181 (0.233) data 0.000 (0.049) loss 0.5410 (0.5117) acc 88.8889 (88.5982) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [30/51] time 0.203 (0.225) data 0.000 (0.041) loss 0.6009 (0.5224) acc 86.5385 (88.3355) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [35/51] time 0.204 (0.220) data 0.000 (0.035) loss 0.5122 (0.5303) acc 86.8421 (88.1978) lr 3.6258e-04 eta 0:02:18
epoch [38/50] batch [40/51] time 0.181 (0.217) data 0.000 (0.031) loss 0.4007 (0.5323) acc 90.8654 (87.9866) lr 3.6258e-04 eta 0:02:15
epoch [38/50] batch [45/51] time 0.171 (0.212) data 0.000 (0.027) loss 0.5460 (0.5419) acc 89.7059 (87.8146) lr 3.6258e-04 eta 0:02:11
epoch [38/50] batch [50/51] time 0.164 (0.209) data 0.000 (0.025) loss 0.6116 (0.5392) acc 82.8125 (87.8572) lr 3.6258e-04 eta 0:02:07
>>> alpha1: 0.134  alpha2: 0.054 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.49 <<<
epoch [39/50] batch [5/51] time 0.184 (0.479) data 0.000 (0.285) loss 0.4128 (0.4814) acc 92.9245 (89.7577) lr 3.1545e-04 eta 0:04:50
epoch [39/50] batch [10/51] time 0.184 (0.329) data 0.000 (0.143) loss 0.4977 (0.5283) acc 87.7273 (88.6816) lr 3.1545e-04 eta 0:03:17
epoch [39/50] batch [15/51] time 0.169 (0.280) data 0.000 (0.095) loss 0.6744 (0.5576) acc 83.5000 (87.8356) lr 3.1545e-04 eta 0:02:47
epoch [39/50] batch [20/51] time 0.186 (0.256) data 0.000 (0.072) loss 0.4993 (0.5490) acc 87.7358 (87.9849) lr 3.1545e-04 eta 0:02:31
epoch [39/50] batch [25/51] time 0.183 (0.241) data 0.000 (0.057) loss 0.5223 (0.5452) acc 91.1765 (87.8658) lr 3.1545e-04 eta 0:02:21
epoch [39/50] batch [30/51] time 0.196 (0.234) data 0.000 (0.048) loss 0.7222 (0.5510) acc 86.7924 (88.1342) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [35/51] time 0.185 (0.226) data 0.000 (0.041) loss 0.5999 (0.5605) acc 89.0909 (87.9586) lr 3.1545e-04 eta 0:02:10
epoch [39/50] batch [40/51] time 0.183 (0.219) data 0.000 (0.036) loss 0.4958 (0.5533) acc 90.0000 (88.1649) lr 3.1545e-04 eta 0:02:05
epoch [39/50] batch [45/51] time 0.170 (0.215) data 0.000 (0.032) loss 0.6245 (0.5537) acc 83.8235 (87.9348) lr 3.1545e-04 eta 0:02:01
epoch [39/50] batch [50/51] time 0.178 (0.211) data 0.000 (0.029) loss 0.3101 (0.5443) acc 94.9074 (88.2952) lr 3.1545e-04 eta 0:01:58
>>> alpha1: 0.136  alpha2: 0.057 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [40/50] batch [5/51] time 0.162 (0.483) data 0.001 (0.300) loss 0.8195 (0.5533) acc 82.0652 (87.9335) lr 2.7103e-04 eta 0:04:28
epoch [40/50] batch [10/51] time 0.184 (0.334) data 0.000 (0.150) loss 0.5016 (0.5375) acc 89.3519 (87.7076) lr 2.7103e-04 eta 0:03:03
epoch [40/50] batch [15/51] time 0.188 (0.282) data 0.000 (0.100) loss 0.5170 (0.5403) acc 87.7193 (87.5231) lr 2.7103e-04 eta 0:02:33
epoch [40/50] batch [20/51] time 0.174 (0.256) data 0.000 (0.075) loss 0.7800 (0.5726) acc 82.6923 (86.9466) lr 2.7103e-04 eta 0:02:18
epoch [40/50] batch [25/51] time 0.189 (0.240) data 0.000 (0.060) loss 0.5763 (0.5671) acc 90.6863 (86.9042) lr 2.7103e-04 eta 0:02:08
epoch [40/50] batch [30/51] time 0.167 (0.230) data 0.001 (0.050) loss 0.6721 (0.5660) acc 86.4583 (87.4959) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [35/51] time 0.173 (0.222) data 0.000 (0.043) loss 0.5420 (0.5619) acc 88.9423 (87.6269) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [40/51] time 0.181 (0.217) data 0.000 (0.038) loss 0.4153 (0.5484) acc 89.0909 (88.0434) lr 2.7103e-04 eta 0:01:52
epoch [40/50] batch [45/51] time 0.187 (0.213) data 0.000 (0.034) loss 0.5320 (0.5412) acc 89.6552 (88.2482) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [50/51] time 0.180 (0.208) data 0.000 (0.030) loss 0.4643 (0.5461) acc 88.4259 (88.1157) lr 2.7103e-04 eta 0:01:46
>>> alpha1: 0.136  alpha2: 0.058 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.48 <<<
epoch [41/50] batch [5/51] time 0.189 (0.473) data 0.000 (0.288) loss 0.4100 (0.4753) acc 91.0377 (89.0639) lr 2.2949e-04 eta 0:03:58
epoch [41/50] batch [10/51] time 0.189 (0.325) data 0.000 (0.144) loss 0.5447 (0.5472) acc 91.0377 (88.3264) lr 2.2949e-04 eta 0:02:42
epoch [41/50] batch [15/51] time 0.172 (0.277) data 0.000 (0.096) loss 0.5691 (0.5499) acc 83.8235 (88.1706) lr 2.2949e-04 eta 0:02:17
epoch [41/50] batch [20/51] time 0.165 (0.252) data 0.000 (0.072) loss 0.6066 (0.5434) acc 87.2340 (88.1155) lr 2.2949e-04 eta 0:02:03
epoch [41/50] batch [25/51] time 0.173 (0.238) data 0.000 (0.058) loss 0.2948 (0.5251) acc 94.1176 (88.7797) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [30/51] time 0.177 (0.228) data 0.000 (0.048) loss 0.5868 (0.5231) acc 84.9057 (88.6480) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [35/51] time 0.189 (0.221) data 0.000 (0.041) loss 0.4507 (0.5180) acc 90.0943 (88.8537) lr 2.2949e-04 eta 0:01:44
epoch [41/50] batch [40/51] time 0.168 (0.216) data 0.000 (0.036) loss 0.6525 (0.5182) acc 86.0000 (88.8667) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [45/51] time 0.170 (0.212) data 0.000 (0.032) loss 0.6355 (0.5256) acc 85.7843 (88.5515) lr 2.2949e-04 eta 0:01:38
epoch [41/50] batch [50/51] time 0.167 (0.208) data 0.000 (0.029) loss 0.5253 (0.5265) acc 91.0000 (88.3940) lr 2.2949e-04 eta 0:01:35
>>> alpha1: 0.134  alpha2: 0.058 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.47 <<<
epoch [42/50] batch [5/51] time 0.199 (0.460) data 0.000 (0.271) loss 0.6304 (0.5957) acc 86.7924 (87.4502) lr 1.9098e-04 eta 0:03:28
epoch [42/50] batch [10/51] time 0.174 (0.320) data 0.000 (0.136) loss 0.5117 (0.5358) acc 89.0000 (88.8149) lr 1.9098e-04 eta 0:02:23
epoch [42/50] batch [15/51] time 0.174 (0.272) data 0.000 (0.090) loss 0.6441 (0.5496) acc 87.0192 (88.1885) lr 1.9098e-04 eta 0:02:00
epoch [42/50] batch [20/51] time 0.171 (0.251) data 0.000 (0.068) loss 0.5970 (0.5398) acc 85.7843 (88.5766) lr 1.9098e-04 eta 0:01:50
epoch [42/50] batch [25/51] time 0.183 (0.236) data 0.000 (0.054) loss 0.8782 (0.5374) acc 80.2885 (88.3548) lr 1.9098e-04 eta 0:01:42
epoch [42/50] batch [30/51] time 0.200 (0.229) data 0.001 (0.045) loss 0.5476 (0.5330) acc 91.8269 (88.3746) lr 1.9098e-04 eta 0:01:38
epoch [42/50] batch [35/51] time 0.183 (0.223) data 0.000 (0.039) loss 0.7798 (0.5452) acc 81.3726 (87.8844) lr 1.9098e-04 eta 0:01:34
epoch [42/50] batch [40/51] time 0.175 (0.218) data 0.000 (0.034) loss 0.5107 (0.5362) acc 85.5769 (88.1249) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [45/51] time 0.178 (0.213) data 0.000 (0.030) loss 0.4385 (0.5405) acc 92.9245 (88.0605) lr 1.9098e-04 eta 0:01:28
epoch [42/50] batch [50/51] time 0.169 (0.209) data 0.000 (0.027) loss 0.4887 (0.5365) acc 90.0000 (88.0909) lr 1.9098e-04 eta 0:01:25
>>> alpha1: 0.133  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.47 <<<
epoch [43/50] batch [5/51] time 0.175 (0.456) data 0.000 (0.272) loss 0.4978 (0.4763) acc 87.9808 (89.1646) lr 1.5567e-04 eta 0:03:03
epoch [43/50] batch [10/51] time 0.174 (0.316) data 0.000 (0.136) loss 0.4411 (0.5032) acc 93.7500 (89.7048) lr 1.5567e-04 eta 0:02:05
epoch [43/50] batch [15/51] time 0.167 (0.268) data 0.000 (0.091) loss 0.7191 (0.5476) acc 80.1020 (88.1780) lr 1.5567e-04 eta 0:01:45
epoch [43/50] batch [20/51] time 0.192 (0.248) data 0.000 (0.068) loss 0.4251 (0.5194) acc 87.0192 (88.8655) lr 1.5567e-04 eta 0:01:36
epoch [43/50] batch [25/51] time 0.199 (0.236) data 0.000 (0.055) loss 0.4688 (0.5066) acc 90.1961 (89.2277) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [30/51] time 0.185 (0.228) data 0.000 (0.046) loss 0.5474 (0.5077) acc 87.0192 (89.1034) lr 1.5567e-04 eta 0:01:26
epoch [43/50] batch [35/51] time 0.163 (0.220) data 0.000 (0.039) loss 0.6546 (0.5243) acc 82.4468 (88.7320) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [40/51] time 0.189 (0.216) data 0.000 (0.034) loss 0.7024 (0.5248) acc 87.0690 (88.8760) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.182 (0.212) data 0.001 (0.030) loss 0.6412 (0.5285) acc 85.6481 (88.6406) lr 1.5567e-04 eta 0:01:16
epoch [43/50] batch [50/51] time 0.184 (0.208) data 0.000 (0.027) loss 0.5098 (0.5250) acc 85.2679 (88.7274) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.132  alpha2: 0.057 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.48 <<<
epoch [44/50] batch [5/51] time 0.166 (0.517) data 0.000 (0.333) loss 0.4422 (0.4848) acc 91.1458 (89.1563) lr 1.2369e-04 eta 0:03:01
epoch [44/50] batch [10/51] time 0.201 (0.355) data 0.001 (0.167) loss 0.4790 (0.4826) acc 88.2075 (89.3655) lr 1.2369e-04 eta 0:02:03
epoch [44/50] batch [15/51] time 0.184 (0.298) data 0.000 (0.111) loss 0.5108 (0.4901) acc 85.8491 (88.8019) lr 1.2369e-04 eta 0:01:41
epoch [44/50] batch [20/51] time 0.207 (0.271) data 0.000 (0.084) loss 0.3815 (0.4756) acc 92.5439 (89.4015) lr 1.2369e-04 eta 0:01:31
epoch [44/50] batch [25/51] time 0.197 (0.254) data 0.000 (0.067) loss 0.6787 (0.5039) acc 89.4231 (88.9713) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [30/51] time 0.189 (0.242) data 0.000 (0.056) loss 0.4351 (0.5032) acc 90.5660 (88.8806) lr 1.2369e-04 eta 0:01:19
epoch [44/50] batch [35/51] time 0.179 (0.234) data 0.000 (0.048) loss 0.8650 (0.5208) acc 81.2500 (88.6716) lr 1.2369e-04 eta 0:01:15
epoch [44/50] batch [40/51] time 0.174 (0.227) data 0.000 (0.042) loss 0.4249 (0.5153) acc 90.8654 (88.7945) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [45/51] time 0.170 (0.221) data 0.000 (0.037) loss 0.6649 (0.5240) acc 85.0000 (88.6676) lr 1.2369e-04 eta 0:01:09
epoch [44/50] batch [50/51] time 0.192 (0.218) data 0.001 (0.034) loss 0.4215 (0.5243) acc 88.5593 (88.5402) lr 1.2369e-04 eta 0:01:06
>>> alpha1: 0.130  alpha2: 0.055 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.48 <<<
epoch [45/50] batch [5/51] time 0.177 (0.467) data 0.000 (0.269) loss 0.4644 (0.4708) acc 88.9423 (89.3309) lr 9.5173e-05 eta 0:02:20
epoch [45/50] batch [10/51] time 0.176 (0.325) data 0.000 (0.134) loss 0.5987 (0.5229) acc 87.7358 (88.7007) lr 9.5173e-05 eta 0:01:36
epoch [45/50] batch [15/51] time 0.197 (0.279) data 0.000 (0.090) loss 0.4584 (0.5137) acc 90.1961 (88.7208) lr 9.5173e-05 eta 0:01:21
epoch [45/50] batch [20/51] time 0.169 (0.255) data 0.001 (0.067) loss 0.5443 (0.4963) acc 87.7660 (89.1626) lr 9.5173e-05 eta 0:01:12
epoch [45/50] batch [25/51] time 0.191 (0.241) data 0.000 (0.054) loss 0.2916 (0.5003) acc 94.6429 (89.0430) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [30/51] time 0.182 (0.232) data 0.000 (0.045) loss 0.5095 (0.4903) acc 91.5094 (89.4770) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [35/51] time 0.184 (0.224) data 0.000 (0.039) loss 0.4868 (0.4955) acc 89.1509 (89.3157) lr 9.5173e-05 eta 0:01:00
epoch [45/50] batch [40/51] time 0.176 (0.218) data 0.000 (0.034) loss 0.4834 (0.4924) acc 90.0943 (89.5226) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.160 (0.213) data 0.000 (0.030) loss 0.6498 (0.5017) acc 82.9787 (89.0738) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.168 (0.209) data 0.000 (0.027) loss 0.6161 (0.5447) acc 88.0000 (88.5379) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.130  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.49 <<<
epoch [46/50] batch [5/51] time 0.182 (0.487) data 0.000 (0.299) loss 0.4201 (0.4719) acc 94.5455 (89.1374) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [10/51] time 0.167 (0.337) data 0.002 (0.150) loss 0.5312 (0.4572) acc 89.0625 (89.9383) lr 7.0224e-05 eta 0:01:22
epoch [46/50] batch [15/51] time 0.177 (0.285) data 0.000 (0.100) loss 0.4897 (0.4863) acc 90.0943 (89.4916) lr 7.0224e-05 eta 0:01:08
epoch [46/50] batch [20/51] time 0.187 (0.259) data 0.000 (0.075) loss 0.4756 (0.4939) acc 90.3846 (89.3951) lr 7.0224e-05 eta 0:01:00
epoch [46/50] batch [25/51] time 0.182 (0.242) data 0.000 (0.060) loss 0.4297 (0.5000) acc 90.0000 (89.3880) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [30/51] time 0.175 (0.232) data 0.000 (0.050) loss 0.4885 (0.4950) acc 90.1961 (89.5321) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [35/51] time 0.188 (0.224) data 0.000 (0.043) loss 0.4450 (0.4957) acc 92.1296 (89.5246) lr 7.0224e-05 eta 0:00:49
epoch [46/50] batch [40/51] time 0.184 (0.219) data 0.000 (0.038) loss 0.5477 (0.5000) acc 89.7321 (89.5568) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [45/51] time 0.173 (0.214) data 0.000 (0.033) loss 0.4565 (0.4996) acc 92.3077 (89.6162) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [50/51] time 0.175 (0.210) data 0.000 (0.030) loss 0.6732 (0.5067) acc 78.8462 (89.3001) lr 7.0224e-05 eta 0:00:43
>>> alpha1: 0.131  alpha2: 0.055 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.50 <<<
epoch [47/50] batch [5/51] time 0.173 (0.493) data 0.000 (0.303) loss 0.4754 (0.4326) acc 89.4231 (89.7937) lr 4.8943e-05 eta 0:01:38
epoch [47/50] batch [10/51] time 0.183 (0.334) data 0.000 (0.152) loss 0.5321 (0.4915) acc 88.3929 (88.7578) lr 4.8943e-05 eta 0:01:04
epoch [47/50] batch [15/51] time 0.179 (0.287) data 0.001 (0.101) loss 0.7531 (0.5172) acc 86.1111 (87.5862) lr 4.8943e-05 eta 0:00:54
epoch [47/50] batch [20/51] time 0.198 (0.262) data 0.000 (0.076) loss 0.5452 (0.5280) acc 87.5000 (87.6681) lr 4.8943e-05 eta 0:00:48
epoch [47/50] batch [25/51] time 0.186 (0.247) data 0.000 (0.061) loss 0.4822 (0.5029) acc 88.6792 (88.4147) lr 4.8943e-05 eta 0:00:44
epoch [47/50] batch [30/51] time 0.173 (0.235) data 0.000 (0.051) loss 0.4484 (0.5159) acc 87.0192 (87.8754) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [35/51] time 0.190 (0.228) data 0.000 (0.044) loss 0.5730 (0.5517) acc 84.3750 (87.5234) lr 4.8943e-05 eta 0:00:38
epoch [47/50] batch [40/51] time 0.180 (0.223) data 0.000 (0.038) loss 0.5949 (0.5462) acc 89.6226 (87.8287) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [45/51] time 0.174 (0.218) data 0.000 (0.034) loss 0.7076 (0.5493) acc 79.8077 (87.5621) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [50/51] time 0.177 (0.214) data 0.000 (0.031) loss 0.3649 (0.5441) acc 94.8113 (87.8970) lr 4.8943e-05 eta 0:00:32
>>> alpha1: 0.131  alpha2: 0.058 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.51 <<<
epoch [48/50] batch [5/51] time 0.173 (0.469) data 0.000 (0.283) loss 0.4035 (0.5140) acc 92.0000 (88.6082) lr 3.1417e-05 eta 0:01:09
epoch [48/50] batch [10/51] time 0.193 (0.328) data 0.000 (0.142) loss 0.6108 (0.5195) acc 89.6226 (89.0024) lr 3.1417e-05 eta 0:00:46
epoch [48/50] batch [15/51] time 0.187 (0.281) data 0.000 (0.094) loss 0.4238 (0.5201) acc 89.8148 (88.6831) lr 3.1417e-05 eta 0:00:38
epoch [48/50] batch [20/51] time 0.186 (0.256) data 0.000 (0.071) loss 0.6638 (0.5189) acc 87.0370 (88.6752) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [25/51] time 0.178 (0.242) data 0.000 (0.057) loss 0.5281 (0.4967) acc 89.5833 (89.3875) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [30/51] time 0.191 (0.233) data 0.000 (0.047) loss 0.3598 (0.4897) acc 93.3962 (89.6270) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [35/51] time 0.184 (0.226) data 0.000 (0.041) loss 0.4034 (0.4961) acc 93.6274 (89.5605) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [40/51] time 0.179 (0.221) data 0.000 (0.036) loss 0.5148 (0.4965) acc 89.8148 (89.3189) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [45/51] time 0.172 (0.216) data 0.000 (0.032) loss 0.3764 (0.4967) acc 93.2692 (89.2451) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [50/51] time 0.178 (0.212) data 0.000 (0.029) loss 0.2363 (0.4906) acc 92.5926 (89.2738) lr 3.1417e-05 eta 0:00:21
>>> alpha1: 0.131  alpha2: 0.056 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.17 & unmatched refined noisy rate: 0.51 <<<
epoch [49/50] batch [5/51] time 0.192 (0.490) data 0.001 (0.307) loss 0.5539 (0.5373) acc 89.7059 (87.7614) lr 1.7713e-05 eta 0:00:47
epoch [49/50] batch [10/51] time 0.202 (0.334) data 0.000 (0.153) loss 0.3107 (0.4825) acc 93.1818 (90.3433) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [15/51] time 0.180 (0.281) data 0.000 (0.102) loss 0.5579 (0.5322) acc 83.6538 (88.5173) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [20/51] time 0.180 (0.257) data 0.000 (0.078) loss 0.3893 (0.5118) acc 91.6667 (89.1157) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [25/51] time 0.178 (0.243) data 0.000 (0.062) loss 0.4209 (0.5132) acc 92.4528 (88.9781) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [30/51] time 0.190 (0.266) data 0.000 (0.052) loss 0.4743 (0.5231) acc 88.8393 (88.7560) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [35/51] time 0.166 (0.253) data 0.000 (0.044) loss 0.4005 (0.5147) acc 91.6667 (88.9641) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [40/51] time 0.167 (0.244) data 0.000 (0.039) loss 0.5877 (0.5213) acc 85.7143 (88.8062) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [45/51] time 0.159 (0.235) data 0.000 (0.035) loss 0.5421 (0.5175) acc 83.1522 (88.8577) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [50/51] time 0.171 (0.230) data 0.000 (0.031) loss 0.4440 (0.5081) acc 88.4615 (88.9871) lr 1.7713e-05 eta 0:00:11
>>> alpha1: 0.131  alpha2: 0.057 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.50 <<<
epoch [50/50] batch [5/51] time 0.171 (0.505) data 0.000 (0.319) loss 0.5228 (0.5330) acc 88.7255 (89.4571) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [10/51] time 0.177 (0.343) data 0.000 (0.160) loss 0.5653 (0.5424) acc 89.6226 (89.5910) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [15/51] time 0.194 (0.290) data 0.000 (0.107) loss 0.4705 (0.5158) acc 86.5741 (89.5194) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.186 (0.262) data 0.000 (0.080) loss 0.5151 (0.5150) acc 89.4737 (89.3714) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.169 (0.246) data 0.000 (0.064) loss 0.4897 (0.5248) acc 91.0000 (89.3081) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.216 (0.235) data 0.000 (0.053) loss 0.4246 (0.5323) acc 91.3462 (89.2698) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/51] time 0.179 (0.227) data 0.000 (0.046) loss 0.6070 (0.5376) acc 88.4259 (89.1164) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.177 (0.221) data 0.000 (0.041) loss 0.5038 (0.5276) acc 85.3774 (89.1416) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.182 (0.216) data 0.000 (0.036) loss 0.3413 (0.5202) acc 91.5179 (89.2977) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.170 (0.212) data 0.000 (0.032) loss 0.6074 (0.5240) acc 88.7255 (89.2144) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed2/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.54, 0.41, 0.37, 0.35, 0.33, 0.31, 0.3, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.28, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.28, 0.29, 0.29, 0.29, 0.29, 0.29, 0.28, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29, 0.29]
* matched noise rate: [0.34, 0.13, 0.21, 0.19, 0.18, 0.13, 0.13, 0.12, 0.12, 0.14, 0.15, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.16, 0.16, 0.17, 0.17, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.17, 0.17, 0.17, 0.17, 0.18]
* unmatched noise rate: [0.89, 0.54, 0.66, 0.6, 0.6, 0.46, 0.47, 0.46, 0.47, 0.5, 0.5, 0.49, 0.5, 0.49, 0.49, 0.46, 0.48, 0.47, 0.49, 0.5, 0.49, 0.49, 0.51, 0.5, 0.49, 0.47, 0.5, 0.49, 0.49, 0.49, 0.48, 0.47, 0.47, 0.48, 0.48, 0.49, 0.5, 0.51, 0.51, 0.5]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:03,  2.64s/it] 12%|█▏        | 3/25 [00:02<00:16,  1.34it/s] 20%|██        | 5/25 [00:02<00:08,  2.46it/s] 28%|██▊       | 7/25 [00:03<00:04,  3.68it/s] 36%|███▌      | 9/25 [00:03<00:03,  5.00it/s] 44%|████▍     | 11/25 [00:03<00:02,  6.32it/s] 52%|█████▏    | 13/25 [00:03<00:01,  7.55it/s] 60%|██████    | 15/25 [00:03<00:01,  8.63it/s] 68%|██████▊   | 17/25 [00:04<00:01,  7.10it/s] 76%|███████▌  | 19/25 [00:04<00:00,  8.19it/s] 84%|████████▍ | 21/25 [00:04<00:00,  9.14it/s] 92%|█████████▏| 23/25 [00:04<00:00,  9.95it/s]100%|██████████| 25/25 [00:05<00:00,  7.22it/s]100%|██████████| 25/25 [00:05<00:00,  4.73it/s]
=> result
* total: 2,463
* correct: 1,760
* accuracy: 71.5%
* error: 28.5%
* macro_f1: 66.8%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 16	acc: 88.9%
* class: 2 (canterbury bells)	total: 12	correct: 0	acc: 0.0%
* class: 3 (sweet pea)	total: 17	correct: 8	acc: 47.1%
* class: 4 (english marigold)	total: 20	correct: 9	acc: 45.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 11	acc: 91.7%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 14	acc: 100.0%
* class: 9 (globe thistle)	total: 14	correct: 12	acc: 85.7%
* class: 10 (snapdragon)	total: 26	correct: 19	acc: 73.1%
* class: 11 (colt's foot)	total: 26	correct: 0	acc: 0.0%
* class: 12 (king protea)	total: 15	correct: 15	acc: 100.0%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 14	acc: 93.3%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 17	acc: 68.0%
* class: 18 (balloon flower)	total: 15	correct: 2	acc: 13.3%
* class: 19 (giant white arum lily)	total: 17	correct: 13	acc: 76.5%
* class: 20 (fire lily)	total: 12	correct: 12	acc: 100.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 6	acc: 46.2%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 13	acc: 100.0%
* class: 26 (prince of wales feathers)	total: 12	correct: 0	acc: 0.0%
* class: 27 (stemless gentian)	total: 20	correct: 19	acc: 95.0%
* class: 28 (artichoke)	total: 23	correct: 23	acc: 100.0%
* class: 29 (sweet william)	total: 26	correct: 19	acc: 73.1%
* class: 30 (carnation)	total: 16	correct: 12	acc: 75.0%
* class: 31 (garden phlox)	total: 14	correct: 13	acc: 92.9%
* class: 32 (love in the mist)	total: 14	correct: 11	acc: 78.6%
* class: 33 (mexican aster)	total: 12	correct: 12	acc: 100.0%
* class: 34 (alpine sea holly)	total: 12	correct: 11	acc: 91.7%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 10	acc: 45.5%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 16	acc: 94.1%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 18	acc: 90.0%
* class: 40 (barbeton daisy)	total: 38	correct: 19	acc: 50.0%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 27	acc: 96.4%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 55	acc: 93.2%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 20	acc: 95.2%
* class: 48 (oxeye daisy)	total: 15	correct: 15	acc: 100.0%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 0	acc: 0.0%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 11	acc: 39.3%
* class: 53 (sunflower)	total: 19	correct: 18	acc: 94.7%
* class: 54 (pelargonium)	total: 21	correct: 0	acc: 0.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 20	acc: 100.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 32	acc: 97.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 11	acc: 68.8%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 0	acc: 0.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 7	acc: 43.8%
* class: 68 (windflower)	total: 16	correct: 16	acc: 100.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 22	acc: 95.7%
* class: 71 (azalea)	total: 29	correct: 25	acc: 86.2%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 50	acc: 98.0%
* class: 74 (thorn apple)	total: 36	correct: 0	acc: 0.0%
* class: 75 (morning glory)	total: 32	correct: 30	acc: 93.8%
* class: 76 (passion flower)	total: 75	correct: 73	acc: 97.3%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 30	acc: 93.8%
* class: 80 (frangipani)	total: 50	correct: 49	acc: 98.0%
* class: 81 (clematis)	total: 34	correct: 32	acc: 94.1%
* class: 82 (hibiscus)	total: 39	correct: 39	acc: 100.0%
* class: 83 (columbine)	total: 26	correct: 19	acc: 73.1%
* class: 84 (desert-rose)	total: 18	correct: 0	acc: 0.0%
* class: 85 (tree mallow)	total: 17	correct: 10	acc: 58.8%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 40	acc: 87.0%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 20	acc: 80.0%
* class: 90 (hippeastrum)	total: 23	correct: 15	acc: 65.2%
* class: 91 (bee balm)	total: 20	correct: 18	acc: 90.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 48	acc: 98.0%
* class: 94 (bougainvillea)	total: 38	correct: 26	acc: 68.4%
* class: 95 (camellia)	total: 27	correct: 22	acc: 81.5%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 17	acc: 68.0%
* class: 98 (bromelia)	total: 18	correct: 18	acc: 100.0%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 8	acc: 47.1%
* class: 101 (blackberry lily)	total: 14	correct: 14	acc: 100.0%
* average: 72.2%
Elapsed: 0:28:18
Run this job and save the output to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/DPL/rn50.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.DPL.N_CTX', '16', 'TRAINER.DPL.CSC', 'False', 'TRAINER.DPL.GCE', 'False', 'TRAINER.DPL.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_FP', '12', 'DATASET.FP_TYPE', 'symflip', 'DATASET.NUM_SHOTS', '16']
output_dir: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
resume: 
root: ./data
seed: 3
source_domains: None
target_domains: None
trainer: DPL
transforms: None
************
** Config **
************
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-83-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.3
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.24.3           py38h14f4228_0  
[conda] numpy-base                1.24.3           py38h31eccc5_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py38_cu113    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
        Pillow (9.4.0)

Loading trainer: DPL
Loading dataset: OxfordFlowers
Reading split from /data1/zhli/dpl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data1/zhli/dpl/data/oxford_flowers/split_fewshot/shots_16_symflip/fp_12-seed_3.pkl
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
* Using custom transform for testing
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      408
# test     2,463
---------  -------------
Building transform_train
DATALOADER:
  K: 4
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  FP_TYPE: symflip
  NAME: OxfordFlowers
  NUM_FP: 12
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ./data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_val
  NO_TEST: False
  PER_CLASS_RESULT: True
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  DPL:
    ALPHA: 1.0
    ANALYSIS: True
    AVERAGE_LOSS: True
    AVERAGE_MATCH: True
    BETA: 0.001
    BETA1: 0.2
    BETA2: 0.8
    BLIP_PATH: ./checkpoints/model_base_retrieval_coco.pth
    CLASS_TOKEN_POSITION: end
    CO_LAMBDA: 0.5
    CSC: False
    CTX_INIT: 
    GCE: False
    N_CTX: 16
    PREC: fp32
    SAVE_INTERVAL: 10
    TEMP: 2
    WARMUP_EPOCH: 10
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: DPL
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Loading CLIP (backbone: RN50)
loading pretrained model from the directory: ./checkpoints/model_base_retrieval_coco.pth
reshape position embedding from 576 to 196
load checkpoint from ./checkpoints/model_base_retrieval_coco.pth
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
Start WarmUp
/data1/zhli/dpl/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/50] batch [5/51] time 0.268 (1.055) data 0.000 (0.279) loss 5.0188 (4.9900) acc 0.0000 (3.1250) lr 1.0000e-05 eta 0:44:45
epoch [1/50] batch [10/51] time 0.270 (0.662) data 0.000 (0.140) loss 4.6256 (4.9018) acc 3.1250 (2.5000) lr 1.0000e-05 eta 0:28:01
epoch [1/50] batch [15/51] time 0.266 (0.529) data 0.000 (0.093) loss 4.6795 (4.8138) acc 0.0000 (2.0833) lr 1.0000e-05 eta 0:22:20
epoch [1/50] batch [20/51] time 0.261 (0.462) data 0.000 (0.070) loss 4.6882 (4.7894) acc 0.0000 (2.1875) lr 1.0000e-05 eta 0:19:30
epoch [1/50] batch [25/51] time 0.268 (0.423) data 0.000 (0.056) loss 4.5312 (4.7823) acc 0.0000 (2.0000) lr 1.0000e-05 eta 0:17:48
epoch [1/50] batch [30/51] time 0.269 (0.397) data 0.000 (0.047) loss 4.5715 (4.7673) acc 3.1250 (1.9792) lr 1.0000e-05 eta 0:16:39
epoch [1/50] batch [35/51] time 0.261 (0.378) data 0.000 (0.040) loss 4.7720 (4.7478) acc 0.0000 (2.4107) lr 1.0000e-05 eta 0:15:51
epoch [1/50] batch [40/51] time 0.256 (0.363) data 0.000 (0.035) loss 4.7686 (4.7306) acc 0.0000 (2.3438) lr 1.0000e-05 eta 0:15:11
epoch [1/50] batch [45/51] time 0.257 (0.351) data 0.000 (0.031) loss 4.7527 (4.7192) acc 3.1250 (2.6389) lr 1.0000e-05 eta 0:14:39
epoch [1/50] batch [50/51] time 0.257 (0.342) data 0.000 (0.028) loss 4.7338 (4.7093) acc 3.1250 (2.8125) lr 1.0000e-05 eta 0:14:14
epoch [2/50] batch [5/51] time 0.268 (0.545) data 0.000 (0.269) loss 4.4983 (4.5660) acc 6.2500 (6.8750) lr 2.0000e-03 eta 0:22:39
epoch [2/50] batch [10/51] time 0.278 (0.405) data 0.000 (0.134) loss 4.6688 (4.5475) acc 9.3750 (7.8125) lr 2.0000e-03 eta 0:16:49
epoch [2/50] batch [15/51] time 0.267 (0.359) data 0.000 (0.090) loss 4.5281 (4.5370) acc 3.1250 (7.9167) lr 2.0000e-03 eta 0:14:51
epoch [2/50] batch [20/51] time 0.273 (0.335) data 0.014 (0.068) loss 4.6399 (4.5450) acc 3.1250 (7.5000) lr 2.0000e-03 eta 0:13:49
epoch [2/50] batch [25/51] time 0.258 (0.320) data 0.000 (0.054) loss 4.5043 (4.5428) acc 12.5000 (7.7500) lr 2.0000e-03 eta 0:13:10
epoch [2/50] batch [30/51] time 0.258 (0.310) data 0.000 (0.045) loss 4.5286 (4.5309) acc 6.2500 (7.8125) lr 2.0000e-03 eta 0:12:45
epoch [2/50] batch [35/51] time 0.259 (0.303) data 0.000 (0.039) loss 4.3333 (4.5171) acc 18.7500 (7.9464) lr 2.0000e-03 eta 0:12:25
epoch [2/50] batch [40/51] time 0.257 (0.297) data 0.000 (0.034) loss 4.6794 (4.5212) acc 9.3750 (8.2812) lr 2.0000e-03 eta 0:12:11
epoch [2/50] batch [45/51] time 0.257 (0.293) data 0.000 (0.030) loss 4.5865 (4.5210) acc 6.2500 (8.2639) lr 2.0000e-03 eta 0:11:58
epoch [2/50] batch [50/51] time 0.256 (0.289) data 0.000 (0.027) loss 4.5179 (4.5219) acc 9.3750 (8.5625) lr 2.0000e-03 eta 0:11:48
epoch [3/50] batch [5/51] time 0.260 (0.565) data 0.000 (0.277) loss 4.4345 (4.4175) acc 12.5000 (10.6250) lr 1.9980e-03 eta 0:23:00
epoch [3/50] batch [10/51] time 0.259 (0.413) data 0.000 (0.139) loss 4.4247 (4.4592) acc 15.6250 (10.6250) lr 1.9980e-03 eta 0:16:47
epoch [3/50] batch [15/51] time 0.262 (0.363) data 0.000 (0.092) loss 4.3711 (4.4659) acc 18.7500 (11.2500) lr 1.9980e-03 eta 0:14:43
epoch [3/50] batch [20/51] time 0.262 (0.337) data 0.000 (0.069) loss 4.4638 (4.4853) acc 15.6250 (11.0938) lr 1.9980e-03 eta 0:13:38
epoch [3/50] batch [25/51] time 0.257 (0.322) data 0.000 (0.056) loss 4.2747 (4.4790) acc 18.7500 (11.0000) lr 1.9980e-03 eta 0:13:00
epoch [3/50] batch [30/51] time 0.284 (0.313) data 0.000 (0.046) loss 4.3557 (4.4775) acc 9.3750 (10.8333) lr 1.9980e-03 eta 0:12:36
epoch [3/50] batch [35/51] time 0.268 (0.306) data 0.000 (0.040) loss 4.2426 (4.4757) acc 18.7500 (11.0714) lr 1.9980e-03 eta 0:12:18
epoch [3/50] batch [40/51] time 0.256 (0.300) data 0.000 (0.035) loss 4.5633 (4.4691) acc 6.2500 (11.4844) lr 1.9980e-03 eta 0:12:02
epoch [3/50] batch [45/51] time 0.257 (0.295) data 0.000 (0.031) loss 4.5390 (4.4637) acc 9.3750 (11.8750) lr 1.9980e-03 eta 0:11:49
epoch [3/50] batch [50/51] time 0.258 (0.291) data 0.000 (0.028) loss 4.5409 (4.4730) acc 6.2500 (11.5000) lr 1.9980e-03 eta 0:11:38
epoch [4/50] batch [5/51] time 0.260 (0.553) data 0.000 (0.280) loss 4.4036 (4.3764) acc 21.8750 (16.2500) lr 1.9921e-03 eta 0:22:02
epoch [4/50] batch [10/51] time 0.265 (0.409) data 0.000 (0.140) loss 4.3277 (4.4039) acc 15.6250 (13.7500) lr 1.9921e-03 eta 0:16:15
epoch [4/50] batch [15/51] time 0.266 (0.360) data 0.000 (0.093) loss 4.0945 (4.4302) acc 31.2500 (13.7500) lr 1.9921e-03 eta 0:14:17
epoch [4/50] batch [20/51] time 0.298 (0.339) data 0.000 (0.070) loss 4.4786 (4.4207) acc 12.5000 (13.5938) lr 1.9921e-03 eta 0:13:24
epoch [4/50] batch [25/51] time 0.271 (0.324) data 0.000 (0.056) loss 4.4292 (4.4164) acc 9.3750 (13.5000) lr 1.9921e-03 eta 0:12:47
epoch [4/50] batch [30/51] time 0.258 (0.313) data 0.000 (0.047) loss 4.3423 (4.4204) acc 21.8750 (13.6458) lr 1.9921e-03 eta 0:12:21
epoch [4/50] batch [35/51] time 0.262 (0.306) data 0.000 (0.040) loss 4.6866 (4.4378) acc 3.1250 (13.0357) lr 1.9921e-03 eta 0:12:03
epoch [4/50] batch [40/51] time 0.258 (0.300) data 0.000 (0.035) loss 4.6590 (4.4455) acc 6.2500 (12.6562) lr 1.9921e-03 eta 0:11:47
epoch [4/50] batch [45/51] time 0.257 (0.296) data 0.000 (0.031) loss 4.4936 (4.4512) acc 3.1250 (12.0139) lr 1.9921e-03 eta 0:11:35
epoch [4/50] batch [50/51] time 0.257 (0.292) data 0.000 (0.028) loss 4.5947 (4.4565) acc 6.2500 (11.8125) lr 1.9921e-03 eta 0:11:24
epoch [5/50] batch [5/51] time 0.259 (0.615) data 0.000 (0.344) loss 4.4053 (4.3378) acc 12.5000 (16.8750) lr 1.9823e-03 eta 0:23:59
epoch [5/50] batch [10/51] time 0.259 (0.439) data 0.000 (0.172) loss 4.3553 (4.3675) acc 15.6250 (15.0000) lr 1.9823e-03 eta 0:17:05
epoch [5/50] batch [15/51] time 0.258 (0.381) data 0.000 (0.115) loss 4.6801 (4.4164) acc 3.1250 (13.1250) lr 1.9823e-03 eta 0:14:48
epoch [5/50] batch [20/51] time 0.260 (0.352) data 0.000 (0.086) loss 4.5539 (4.4331) acc 6.2500 (11.5625) lr 1.9823e-03 eta 0:13:38
epoch [5/50] batch [25/51] time 0.259 (0.335) data 0.000 (0.069) loss 4.4726 (4.4299) acc 18.7500 (12.2500) lr 1.9823e-03 eta 0:12:56
epoch [5/50] batch [30/51] time 0.284 (0.324) data 0.000 (0.057) loss 4.2145 (4.4098) acc 15.6250 (13.1250) lr 1.9823e-03 eta 0:12:29
epoch [5/50] batch [35/51] time 0.259 (0.315) data 0.000 (0.049) loss 4.2241 (4.4395) acc 25.0000 (12.8571) lr 1.9823e-03 eta 0:12:06
epoch [5/50] batch [40/51] time 0.257 (0.308) data 0.000 (0.043) loss 4.4240 (4.4388) acc 15.6250 (12.8125) lr 1.9823e-03 eta 0:11:49
epoch [5/50] batch [45/51] time 0.258 (0.302) data 0.000 (0.038) loss 4.3315 (4.4353) acc 9.3750 (12.7083) lr 1.9823e-03 eta 0:11:35
epoch [5/50] batch [50/51] time 0.257 (0.298) data 0.000 (0.035) loss 4.5942 (4.4377) acc 9.3750 (12.6875) lr 1.9823e-03 eta 0:11:23
epoch [6/50] batch [5/51] time 0.272 (0.550) data 0.000 (0.271) loss 4.3060 (4.4376) acc 18.7500 (10.0000) lr 1.9686e-03 eta 0:20:58
epoch [6/50] batch [10/51] time 0.260 (0.406) data 0.000 (0.136) loss 4.3313 (4.4629) acc 21.8750 (9.6875) lr 1.9686e-03 eta 0:15:26
epoch [6/50] batch [15/51] time 0.274 (0.358) data 0.000 (0.091) loss 4.4946 (4.4691) acc 12.5000 (10.6250) lr 1.9686e-03 eta 0:13:35
epoch [6/50] batch [20/51] time 0.258 (0.333) data 0.000 (0.068) loss 4.4546 (4.4756) acc 9.3750 (10.3125) lr 1.9686e-03 eta 0:12:37
epoch [6/50] batch [25/51] time 0.259 (0.319) data 0.000 (0.054) loss 4.5529 (4.4677) acc 3.1250 (10.2500) lr 1.9686e-03 eta 0:12:03
epoch [6/50] batch [30/51] time 0.259 (0.310) data 0.000 (0.045) loss 4.2279 (4.4441) acc 21.8750 (11.1458) lr 1.9686e-03 eta 0:11:42
epoch [6/50] batch [35/51] time 0.261 (0.303) data 0.000 (0.039) loss 4.3258 (4.4375) acc 15.6250 (11.6071) lr 1.9686e-03 eta 0:11:25
epoch [6/50] batch [40/51] time 0.256 (0.297) data 0.000 (0.034) loss 4.3366 (4.4172) acc 15.6250 (12.1094) lr 1.9686e-03 eta 0:11:10
epoch [6/50] batch [45/51] time 0.258 (0.293) data 0.000 (0.030) loss 4.5871 (4.4217) acc 3.1250 (11.8056) lr 1.9686e-03 eta 0:10:59
epoch [6/50] batch [50/51] time 0.256 (0.289) data 0.000 (0.027) loss 4.4419 (4.4168) acc 6.2500 (12.0000) lr 1.9686e-03 eta 0:10:49
epoch [7/50] batch [5/51] time 0.287 (0.598) data 0.000 (0.314) loss 4.5162 (4.4919) acc 6.2500 (8.7500) lr 1.9511e-03 eta 0:22:17
epoch [7/50] batch [10/51] time 0.258 (0.432) data 0.000 (0.157) loss 4.2450 (4.4516) acc 18.7500 (10.3125) lr 1.9511e-03 eta 0:16:04
epoch [7/50] batch [15/51] time 0.258 (0.376) data 0.000 (0.105) loss 4.5830 (4.4390) acc 6.2500 (11.6667) lr 1.9511e-03 eta 0:13:57
epoch [7/50] batch [20/51] time 0.285 (0.349) data 0.000 (0.079) loss 4.3738 (4.4383) acc 15.6250 (11.7188) lr 1.9511e-03 eta 0:12:56
epoch [7/50] batch [25/51] time 0.266 (0.333) data 0.000 (0.063) loss 4.1162 (4.4021) acc 18.7500 (12.1250) lr 1.9511e-03 eta 0:12:18
epoch [7/50] batch [30/51] time 0.261 (0.322) data 0.000 (0.053) loss 4.5624 (4.4063) acc 6.2500 (11.8750) lr 1.9511e-03 eta 0:11:53
epoch [7/50] batch [35/51] time 0.266 (0.314) data 0.000 (0.045) loss 4.3043 (4.4156) acc 18.7500 (11.7857) lr 1.9511e-03 eta 0:11:33
epoch [7/50] batch [40/51] time 0.258 (0.307) data 0.000 (0.040) loss 4.6290 (4.4152) acc 3.1250 (11.7969) lr 1.9511e-03 eta 0:11:17
epoch [7/50] batch [45/51] time 0.257 (0.302) data 0.000 (0.035) loss 4.3790 (4.4080) acc 3.1250 (11.6667) lr 1.9511e-03 eta 0:11:03
epoch [7/50] batch [50/51] time 0.256 (0.297) data 0.000 (0.032) loss 4.3006 (4.4092) acc 18.7500 (11.9375) lr 1.9511e-03 eta 0:10:52
epoch [8/50] batch [5/51] time 0.264 (0.529) data 0.000 (0.243) loss 4.2976 (4.4385) acc 12.5000 (9.3750) lr 1.9298e-03 eta 0:19:17
epoch [8/50] batch [10/51] time 0.262 (0.397) data 0.000 (0.122) loss 4.3713 (4.4233) acc 15.6250 (10.9375) lr 1.9298e-03 eta 0:14:27
epoch [8/50] batch [15/51] time 0.259 (0.352) data 0.000 (0.081) loss 4.1770 (4.3716) acc 15.6250 (13.3333) lr 1.9298e-03 eta 0:12:45
epoch [8/50] batch [20/51] time 0.257 (0.329) data 0.000 (0.061) loss 4.2079 (4.3608) acc 12.5000 (12.8125) lr 1.9298e-03 eta 0:11:54
epoch [8/50] batch [25/51] time 0.272 (0.316) data 0.000 (0.049) loss 4.2460 (4.3826) acc 21.8750 (12.6250) lr 1.9298e-03 eta 0:11:24
epoch [8/50] batch [30/51] time 0.264 (0.307) data 0.000 (0.041) loss 4.3978 (4.4006) acc 12.5000 (12.0833) lr 1.9298e-03 eta 0:11:04
epoch [8/50] batch [35/51] time 0.260 (0.302) data 0.000 (0.035) loss 4.4461 (4.4023) acc 9.3750 (12.1429) lr 1.9298e-03 eta 0:10:51
epoch [8/50] batch [40/51] time 0.257 (0.297) data 0.000 (0.031) loss 4.4227 (4.3933) acc 9.3750 (12.4219) lr 1.9298e-03 eta 0:10:39
epoch [8/50] batch [45/51] time 0.258 (0.293) data 0.000 (0.027) loss 4.4112 (4.3825) acc 12.5000 (12.4306) lr 1.9298e-03 eta 0:10:28
epoch [8/50] batch [50/51] time 0.260 (0.289) data 0.000 (0.025) loss 4.3173 (4.3828) acc 6.2500 (12.2500) lr 1.9298e-03 eta 0:10:19
epoch [9/50] batch [5/51] time 0.268 (0.553) data 0.000 (0.261) loss 4.2580 (4.3265) acc 12.5000 (13.1250) lr 1.9048e-03 eta 0:19:42
epoch [9/50] batch [10/51] time 0.265 (0.407) data 0.000 (0.131) loss 4.3807 (4.3288) acc 9.3750 (13.7500) lr 1.9048e-03 eta 0:14:28
epoch [9/50] batch [15/51] time 0.258 (0.361) data 0.000 (0.087) loss 4.6129 (4.3443) acc 9.3750 (11.8750) lr 1.9048e-03 eta 0:12:46
epoch [9/50] batch [20/51] time 0.266 (0.337) data 0.000 (0.065) loss 4.1717 (4.3515) acc 21.8750 (12.6562) lr 1.9048e-03 eta 0:11:54
epoch [9/50] batch [25/51] time 0.261 (0.321) data 0.000 (0.052) loss 4.5584 (4.3525) acc 15.6250 (12.5000) lr 1.9048e-03 eta 0:11:19
epoch [9/50] batch [30/51] time 0.272 (0.312) data 0.000 (0.044) loss 4.2357 (4.3587) acc 12.5000 (12.2917) lr 1.9048e-03 eta 0:10:59
epoch [9/50] batch [35/51] time 0.261 (0.305) data 0.000 (0.038) loss 4.3844 (4.3672) acc 6.2500 (11.7857) lr 1.9048e-03 eta 0:10:42
epoch [9/50] batch [40/51] time 0.259 (0.300) data 0.000 (0.033) loss 4.5861 (4.3682) acc 3.1250 (11.7188) lr 1.9048e-03 eta 0:10:29
epoch [9/50] batch [45/51] time 0.256 (0.295) data 0.000 (0.029) loss 4.3550 (4.3803) acc 12.5000 (11.8750) lr 1.9048e-03 eta 0:10:17
epoch [9/50] batch [50/51] time 0.256 (0.291) data 0.000 (0.026) loss 3.9826 (4.3712) acc 21.8750 (12.2500) lr 1.9048e-03 eta 0:10:08
epoch [10/50] batch [5/51] time 0.268 (0.537) data 0.000 (0.270) loss 4.3051 (4.1887) acc 15.6250 (14.3750) lr 1.8763e-03 eta 0:18:39
epoch [10/50] batch [10/51] time 0.257 (0.400) data 0.000 (0.135) loss 4.3608 (4.2371) acc 12.5000 (15.3125) lr 1.8763e-03 eta 0:13:52
epoch [10/50] batch [15/51] time 0.258 (0.353) data 0.000 (0.090) loss 4.4653 (4.3032) acc 15.6250 (14.1667) lr 1.8763e-03 eta 0:12:13
epoch [10/50] batch [20/51] time 0.268 (0.331) data 0.000 (0.068) loss 4.4891 (4.3298) acc 12.5000 (13.5938) lr 1.8763e-03 eta 0:11:24
epoch [10/50] batch [25/51] time 0.265 (0.318) data 0.000 (0.054) loss 4.3061 (4.3153) acc 12.5000 (13.8750) lr 1.8763e-03 eta 0:10:56
epoch [10/50] batch [30/51] time 0.275 (0.309) data 0.000 (0.045) loss 4.2492 (4.3199) acc 6.2500 (13.2292) lr 1.8763e-03 eta 0:10:36
epoch [10/50] batch [35/51] time 0.257 (0.302) data 0.000 (0.039) loss 4.4834 (4.3403) acc 18.7500 (13.0357) lr 1.8763e-03 eta 0:10:20
epoch [10/50] batch [40/51] time 0.256 (0.296) data 0.000 (0.034) loss 4.3442 (4.3406) acc 3.1250 (12.5781) lr 1.8763e-03 eta 0:10:07
epoch [10/50] batch [45/51] time 0.256 (0.292) data 0.000 (0.030) loss 4.4327 (4.3378) acc 12.5000 (12.7083) lr 1.8763e-03 eta 0:09:56
epoch [10/50] batch [50/51] time 0.256 (0.288) data 0.000 (0.027) loss 4.2808 (4.3413) acc 15.6250 (12.6250) lr 1.8763e-03 eta 0:09:47
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/tensorboard)
>>> alpha1: 0.779  alpha2: 0.374 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.53 --> matched refined noisy rate: 0.28 & unmatched refined noisy rate: 0.81 <<<
epoch [11/50] batch [5/51] time 0.840 (0.955) data 0.000 (0.286) loss 3.1764 (3.4586) acc 53.3654 (42.4625) lr 1.8443e-03 eta 0:32:23
epoch [11/50] batch [10/51] time 0.175 (0.630) data 0.000 (0.143) loss 3.3841 (3.3923) acc 46.6346 (43.3392) lr 1.8443e-03 eta 0:21:18
epoch [11/50] batch [15/51] time 0.169 (0.596) data 0.000 (0.095) loss 3.1278 (3.3819) acc 41.8367 (44.1913) lr 1.8443e-03 eta 0:20:07
epoch [11/50] batch [20/51] time 0.863 (0.560) data 0.001 (0.072) loss 2.9403 (3.2989) acc 56.3726 (46.3528) lr 1.8443e-03 eta 0:18:50
epoch [11/50] batch [25/51] time 0.189 (0.482) data 0.000 (0.057) loss 3.1679 (3.2972) acc 54.0000 (46.8266) lr 1.8443e-03 eta 0:16:10
epoch [11/50] batch [30/51] time 0.169 (0.429) data 0.000 (0.048) loss 3.2002 (3.2692) acc 54.5000 (47.6771) lr 1.8443e-03 eta 0:14:23
epoch [11/50] batch [35/51] time 0.179 (0.410) data 0.000 (0.041) loss 3.3492 (3.2849) acc 37.5000 (47.1448) lr 1.8443e-03 eta 0:13:41
epoch [11/50] batch [40/51] time 0.177 (0.398) data 0.000 (0.036) loss 3.3656 (3.2635) acc 45.2830 (47.7377) lr 1.8443e-03 eta 0:13:16
epoch [11/50] batch [45/51] time 0.155 (0.373) data 0.000 (0.032) loss 3.4856 (3.2623) acc 43.1818 (47.7654) lr 1.8443e-03 eta 0:12:23
epoch [11/50] batch [50/51] time 0.164 (0.367) data 0.000 (0.029) loss 2.8897 (3.2386) acc 53.6458 (48.4749) lr 1.8443e-03 eta 0:12:09
>>> alpha1: 0.768  alpha2: 0.359 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.43 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.59 <<<
epoch [12/50] batch [5/51] time 0.164 (0.581) data 0.000 (0.291) loss 2.0885 (2.1666) acc 69.7917 (67.9977) lr 1.8090e-03 eta 0:19:12
epoch [12/50] batch [10/51] time 0.162 (0.373) data 0.000 (0.146) loss 2.6401 (2.1786) acc 48.7805 (63.0606) lr 1.8090e-03 eta 0:12:17
epoch [12/50] batch [15/51] time 0.164 (0.300) data 0.000 (0.097) loss 2.0107 (2.1883) acc 57.2222 (59.9415) lr 1.8090e-03 eta 0:09:52
epoch [12/50] batch [20/51] time 0.158 (0.294) data 0.000 (0.073) loss 2.5303 (2.2191) acc 59.4444 (58.7421) lr 1.8090e-03 eta 0:09:38
epoch [12/50] batch [25/51] time 0.163 (0.266) data 0.000 (0.058) loss 1.8841 (2.2295) acc 62.7660 (58.4504) lr 1.8090e-03 eta 0:08:43
epoch [12/50] batch [30/51] time 0.158 (0.267) data 0.000 (0.049) loss 2.0320 (2.2213) acc 67.7778 (58.6423) lr 1.8090e-03 eta 0:08:42
epoch [12/50] batch [35/51] time 0.156 (0.252) data 0.000 (0.042) loss 2.0087 (2.1804) acc 70.3488 (59.5288) lr 1.8090e-03 eta 0:08:11
epoch [12/50] batch [40/51] time 0.155 (0.240) data 0.000 (0.037) loss 2.1765 (2.1806) acc 60.7955 (59.9247) lr 1.8090e-03 eta 0:07:48
epoch [12/50] batch [45/51] time 0.163 (0.231) data 0.000 (0.033) loss 1.7060 (2.1622) acc 72.3958 (60.4841) lr 1.8090e-03 eta 0:07:29
epoch [12/50] batch [50/51] time 0.158 (0.223) data 0.000 (0.029) loss 1.9147 (2.1484) acc 55.5556 (60.6996) lr 1.8090e-03 eta 0:07:13
>>> alpha1: 0.703  alpha2: 0.315 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.39 --> matched refined noisy rate: 0.26 & unmatched refined noisy rate: 0.61 <<<
epoch [13/50] batch [5/51] time 0.168 (0.522) data 0.000 (0.343) loss 1.5031 (1.5273) acc 72.5000 (69.3191) lr 1.7705e-03 eta 0:16:49
epoch [13/50] batch [10/51] time 0.169 (0.422) data 0.000 (0.172) loss 1.2755 (1.4939) acc 76.5000 (69.1364) lr 1.7705e-03 eta 0:13:34
epoch [13/50] batch [15/51] time 0.170 (0.341) data 0.000 (0.114) loss 1.6075 (1.4699) acc 62.7451 (69.6119) lr 1.7705e-03 eta 0:10:55
epoch [13/50] batch [20/51] time 0.183 (0.299) data 0.000 (0.086) loss 1.5843 (1.4977) acc 62.0536 (68.8098) lr 1.7705e-03 eta 0:09:33
epoch [13/50] batch [25/51] time 0.191 (0.276) data 0.000 (0.069) loss 1.7330 (1.5414) acc 57.2917 (67.7205) lr 1.7705e-03 eta 0:08:47
epoch [13/50] batch [30/51] time 0.194 (0.261) data 0.000 (0.057) loss 1.8747 (1.5497) acc 56.6327 (67.7469) lr 1.7705e-03 eta 0:08:17
epoch [13/50] batch [35/51] time 0.182 (0.250) data 0.000 (0.049) loss 1.5162 (1.5491) acc 61.3208 (68.0017) lr 1.7705e-03 eta 0:07:55
epoch [13/50] batch [40/51] time 0.177 (0.241) data 0.000 (0.043) loss 1.3427 (1.5481) acc 72.6852 (67.6342) lr 1.7705e-03 eta 0:07:36
epoch [13/50] batch [45/51] time 0.170 (0.233) data 0.000 (0.038) loss 1.7342 (1.5399) acc 61.7647 (67.9154) lr 1.7705e-03 eta 0:07:21
epoch [13/50] batch [50/51] time 0.173 (0.227) data 0.000 (0.035) loss 1.6124 (1.5383) acc 61.5385 (68.1137) lr 1.7705e-03 eta 0:07:08
>>> alpha1: 0.655  alpha2: 0.286 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.37 --> matched refined noisy rate: 0.23 & unmatched refined noisy rate: 0.58 <<<
epoch [14/50] batch [5/51] time 0.174 (0.485) data 0.000 (0.305) loss 1.3533 (1.3403) acc 72.1154 (72.2464) lr 1.7290e-03 eta 0:15:12
epoch [14/50] batch [10/51] time 0.179 (0.332) data 0.000 (0.154) loss 1.2709 (1.3645) acc 72.5490 (71.3569) lr 1.7290e-03 eta 0:10:22
epoch [14/50] batch [15/51] time 0.200 (0.282) data 0.001 (0.103) loss 1.2993 (1.3305) acc 68.1373 (71.7858) lr 1.7290e-03 eta 0:08:48
epoch [14/50] batch [20/51] time 0.173 (0.255) data 0.000 (0.077) loss 1.3506 (1.3141) acc 65.3846 (71.7991) lr 1.7290e-03 eta 0:07:56
epoch [14/50] batch [25/51] time 0.181 (0.240) data 0.000 (0.062) loss 1.3965 (1.4007) acc 69.4444 (69.5242) lr 1.7290e-03 eta 0:07:27
epoch [14/50] batch [30/51] time 0.938 (0.255) data 0.000 (0.052) loss 1.2208 (1.3696) acc 75.8772 (70.1043) lr 1.7290e-03 eta 0:07:53
epoch [14/50] batch [35/51] time 0.185 (0.244) data 0.000 (0.044) loss 1.3910 (1.3618) acc 70.0000 (70.2537) lr 1.7290e-03 eta 0:07:32
epoch [14/50] batch [40/51] time 0.177 (0.236) data 0.000 (0.039) loss 1.1565 (1.3477) acc 75.9434 (70.6342) lr 1.7290e-03 eta 0:07:15
epoch [14/50] batch [45/51] time 0.171 (0.229) data 0.000 (0.035) loss 1.5078 (1.3409) acc 75.4902 (71.0945) lr 1.7290e-03 eta 0:07:01
epoch [14/50] batch [50/51] time 0.173 (0.223) data 0.000 (0.031) loss 1.4601 (1.3567) acc 62.9808 (70.4175) lr 1.7290e-03 eta 0:06:49
>>> alpha1: 0.608  alpha2: 0.260 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.35 --> matched refined noisy rate: 0.24 & unmatched refined noisy rate: 0.58 <<<
epoch [15/50] batch [5/51] time 0.179 (0.642) data 0.001 (0.298) loss 1.1879 (1.1527) acc 73.6111 (73.1291) lr 1.6845e-03 eta 0:19:35
epoch [15/50] batch [10/51] time 0.169 (0.408) data 0.000 (0.149) loss 1.3350 (1.1734) acc 69.0000 (73.5391) lr 1.6845e-03 eta 0:12:24
epoch [15/50] batch [15/51] time 0.187 (0.334) data 0.000 (0.100) loss 1.1095 (1.1473) acc 79.0909 (75.7182) lr 1.6845e-03 eta 0:10:08
epoch [15/50] batch [20/51] time 0.171 (0.295) data 0.000 (0.075) loss 1.3613 (1.1871) acc 73.0392 (74.5696) lr 1.6845e-03 eta 0:08:55
epoch [15/50] batch [25/51] time 0.182 (0.273) data 0.000 (0.060) loss 1.1846 (1.1884) acc 76.8182 (74.3232) lr 1.6845e-03 eta 0:08:13
epoch [15/50] batch [30/51] time 0.175 (0.257) data 0.000 (0.050) loss 1.3463 (1.1980) acc 67.7885 (73.9103) lr 1.6845e-03 eta 0:07:44
epoch [15/50] batch [35/51] time 0.185 (0.247) data 0.000 (0.043) loss 1.0881 (1.2016) acc 78.1250 (73.8701) lr 1.6845e-03 eta 0:07:24
epoch [15/50] batch [40/51] time 0.173 (0.237) data 0.000 (0.038) loss 1.3498 (1.2149) acc 75.4808 (73.3460) lr 1.6845e-03 eta 0:07:06
epoch [15/50] batch [45/51] time 0.182 (0.230) data 0.000 (0.033) loss 1.4686 (1.2178) acc 56.2500 (73.1518) lr 1.6845e-03 eta 0:06:52
epoch [15/50] batch [50/51] time 0.182 (0.225) data 0.000 (0.030) loss 1.3327 (1.2108) acc 70.9821 (73.4625) lr 1.6845e-03 eta 0:06:42
>>> alpha1: 0.537  alpha2: 0.206 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.33 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.53 <<<
epoch [16/50] batch [5/51] time 0.172 (0.498) data 0.000 (0.313) loss 0.8379 (1.0941) acc 80.3922 (73.0994) lr 1.6374e-03 eta 0:14:46
epoch [16/50] batch [10/51] time 0.180 (0.337) data 0.000 (0.157) loss 1.1249 (1.0685) acc 73.5000 (74.6803) lr 1.6374e-03 eta 0:09:58
epoch [16/50] batch [15/51] time 0.207 (0.287) data 0.000 (0.105) loss 1.4101 (1.0847) acc 64.5455 (74.9581) lr 1.6374e-03 eta 0:08:27
epoch [16/50] batch [20/51] time 0.174 (0.260) data 0.000 (0.079) loss 0.9321 (1.0511) acc 80.7692 (75.7161) lr 1.6374e-03 eta 0:07:38
epoch [16/50] batch [25/51] time 0.173 (0.244) data 0.000 (0.063) loss 1.0115 (1.0790) acc 73.0769 (75.1623) lr 1.6374e-03 eta 0:07:08
epoch [16/50] batch [30/51] time 0.179 (0.233) data 0.000 (0.052) loss 0.9778 (1.0656) acc 80.4545 (75.8562) lr 1.6374e-03 eta 0:06:48
epoch [16/50] batch [35/51] time 0.165 (0.224) data 0.000 (0.045) loss 1.2430 (1.0784) acc 71.9388 (75.5991) lr 1.6374e-03 eta 0:06:32
epoch [16/50] batch [40/51] time 0.162 (0.218) data 0.000 (0.039) loss 1.1050 (1.0941) acc 78.1250 (75.4032) lr 1.6374e-03 eta 0:06:19
epoch [16/50] batch [45/51] time 0.178 (0.213) data 0.000 (0.035) loss 0.9389 (1.0958) acc 80.0000 (75.3415) lr 1.6374e-03 eta 0:06:10
epoch [16/50] batch [50/51] time 0.159 (0.208) data 0.000 (0.032) loss 1.1236 (1.1099) acc 68.6170 (74.6659) lr 1.6374e-03 eta 0:06:00
>>> alpha1: 0.363  alpha2: 0.117 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.31 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.49 <<<
epoch [17/50] batch [5/51] time 0.191 (0.517) data 0.000 (0.341) loss 0.8017 (0.8895) acc 85.9091 (80.9092) lr 1.5878e-03 eta 0:14:53
epoch [17/50] batch [10/51] time 0.168 (0.344) data 0.000 (0.170) loss 0.8820 (0.8734) acc 76.5000 (80.7879) lr 1.5878e-03 eta 0:09:53
epoch [17/50] batch [15/51] time 0.168 (0.288) data 0.000 (0.114) loss 1.2716 (0.9239) acc 67.3469 (79.5134) lr 1.5878e-03 eta 0:08:15
epoch [17/50] batch [20/51] time 0.180 (0.298) data 0.000 (0.085) loss 1.1222 (0.9319) acc 81.8627 (78.9180) lr 1.5878e-03 eta 0:08:31
epoch [17/50] batch [25/51] time 0.165 (0.273) data 0.000 (0.068) loss 1.2634 (0.9374) acc 64.0625 (78.7346) lr 1.5878e-03 eta 0:07:45
epoch [17/50] batch [30/51] time 0.182 (0.256) data 0.000 (0.057) loss 1.2663 (0.9616) acc 72.0588 (77.7668) lr 1.5878e-03 eta 0:07:17
epoch [17/50] batch [35/51] time 0.186 (0.246) data 0.000 (0.049) loss 0.7754 (0.9466) acc 83.3333 (77.9783) lr 1.5878e-03 eta 0:06:58
epoch [17/50] batch [40/51] time 0.169 (0.237) data 0.000 (0.043) loss 0.8258 (0.9450) acc 81.0000 (78.2671) lr 1.5878e-03 eta 0:06:41
epoch [17/50] batch [45/51] time 0.181 (0.230) data 0.000 (0.038) loss 0.8282 (0.9502) acc 82.2727 (78.1395) lr 1.5878e-03 eta 0:06:29
epoch [17/50] batch [50/51] time 0.166 (0.225) data 0.000 (0.034) loss 0.7396 (0.9482) acc 82.6531 (77.9692) lr 1.5878e-03 eta 0:06:18
>>> alpha1: 0.305  alpha2: 0.089 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [18/50] batch [5/51] time 0.170 (0.496) data 0.000 (0.313) loss 0.9330 (0.7897) acc 76.0000 (81.5795) lr 1.5358e-03 eta 0:13:52
epoch [18/50] batch [10/51] time 0.174 (0.336) data 0.000 (0.157) loss 0.9309 (0.8255) acc 80.5000 (81.7556) lr 1.5358e-03 eta 0:09:21
epoch [18/50] batch [15/51] time 0.178 (0.282) data 0.000 (0.105) loss 1.2113 (0.8318) acc 71.0000 (81.5642) lr 1.5358e-03 eta 0:07:50
epoch [18/50] batch [20/51] time 0.174 (0.255) data 0.000 (0.079) loss 0.7115 (0.8236) acc 84.6154 (81.5842) lr 1.5358e-03 eta 0:07:03
epoch [18/50] batch [25/51] time 0.171 (0.240) data 0.000 (0.063) loss 1.1115 (0.8405) acc 74.5098 (81.0180) lr 1.5358e-03 eta 0:06:37
epoch [18/50] batch [30/51] time 0.190 (0.230) data 0.000 (0.052) loss 0.9902 (0.8552) acc 75.4717 (80.4468) lr 1.5358e-03 eta 0:06:20
epoch [18/50] batch [35/51] time 0.184 (0.223) data 0.000 (0.045) loss 0.7554 (0.8474) acc 78.2407 (80.6899) lr 1.5358e-03 eta 0:06:07
epoch [18/50] batch [40/51] time 0.180 (0.216) data 0.000 (0.039) loss 0.7002 (0.8534) acc 81.3636 (80.4081) lr 1.5358e-03 eta 0:05:55
epoch [18/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.035) loss 0.9413 (0.8610) acc 73.4694 (80.0972) lr 1.5358e-03 eta 0:05:45
epoch [18/50] batch [50/51] time 0.160 (0.207) data 0.000 (0.032) loss 1.1346 (0.8699) acc 67.9348 (79.9150) lr 1.5358e-03 eta 0:05:37
>>> alpha1: 0.269  alpha2: 0.076 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.30 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.45 <<<
epoch [19/50] batch [5/51] time 0.163 (0.568) data 0.000 (0.382) loss 0.8624 (0.8163) acc 79.2553 (82.0107) lr 1.4818e-03 eta 0:15:24
epoch [19/50] batch [10/51] time 0.168 (0.373) data 0.000 (0.191) loss 0.8548 (0.8436) acc 82.6531 (82.5108) lr 1.4818e-03 eta 0:10:04
epoch [19/50] batch [15/51] time 0.168 (0.306) data 0.000 (0.128) loss 3.1703 (0.9676) acc 44.8980 (79.8412) lr 1.4818e-03 eta 0:08:15
epoch [19/50] batch [20/51] time 0.172 (0.272) data 0.000 (0.096) loss 0.9978 (1.0215) acc 79.1667 (78.5825) lr 1.4818e-03 eta 0:07:18
epoch [19/50] batch [25/51] time 0.169 (0.252) data 0.001 (0.077) loss 0.8629 (0.9957) acc 82.1429 (78.7929) lr 1.4818e-03 eta 0:06:45
epoch [19/50] batch [30/51] time 0.187 (0.240) data 0.000 (0.064) loss 0.8562 (0.9670) acc 82.8431 (79.1185) lr 1.4818e-03 eta 0:06:24
epoch [19/50] batch [35/51] time 0.191 (0.232) data 0.000 (0.055) loss 0.7101 (0.9554) acc 86.5741 (79.1741) lr 1.4818e-03 eta 0:06:10
epoch [19/50] batch [40/51] time 0.173 (0.224) data 0.000 (0.048) loss 0.7247 (0.9315) acc 82.2115 (79.6631) lr 1.4818e-03 eta 0:05:57
epoch [19/50] batch [45/51] time 0.162 (0.218) data 0.000 (0.043) loss 0.9857 (0.9298) acc 75.5319 (79.4938) lr 1.4818e-03 eta 0:05:45
epoch [19/50] batch [50/51] time 0.169 (0.213) data 0.000 (0.038) loss 0.9539 (0.9185) acc 78.4314 (79.8059) lr 1.4818e-03 eta 0:05:37
>>> alpha1: 0.249  alpha2: 0.074 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [20/50] batch [5/51] time 0.178 (0.487) data 0.000 (0.305) loss 0.5053 (0.6983) acc 84.7222 (81.6833) lr 1.4258e-03 eta 0:12:47
epoch [20/50] batch [10/51] time 0.178 (0.331) data 0.000 (0.152) loss 0.6618 (0.7096) acc 86.5000 (82.5167) lr 1.4258e-03 eta 0:08:39
epoch [20/50] batch [15/51] time 0.171 (0.280) data 0.001 (0.102) loss 0.8496 (0.7710) acc 83.3333 (82.1997) lr 1.4258e-03 eta 0:07:17
epoch [20/50] batch [20/51] time 0.183 (0.254) data 0.000 (0.076) loss 0.9537 (0.7853) acc 72.5962 (81.5218) lr 1.4258e-03 eta 0:06:35
epoch [20/50] batch [25/51] time 0.186 (0.239) data 0.000 (0.061) loss 0.7143 (0.7847) acc 85.0962 (81.7464) lr 1.4258e-03 eta 0:06:11
epoch [20/50] batch [30/51] time 0.181 (0.229) data 0.000 (0.051) loss 0.8470 (0.7899) acc 81.0185 (81.7025) lr 1.4258e-03 eta 0:05:55
epoch [20/50] batch [35/51] time 0.187 (0.222) data 0.000 (0.044) loss 0.9150 (0.7910) acc 80.5556 (81.7127) lr 1.4258e-03 eta 0:05:43
epoch [20/50] batch [40/51] time 0.176 (0.217) data 0.000 (0.038) loss 1.0061 (0.7926) acc 67.5000 (81.5258) lr 1.4258e-03 eta 0:05:34
epoch [20/50] batch [45/51] time 0.166 (0.211) data 0.000 (0.034) loss 0.6908 (0.7909) acc 81.1225 (81.3751) lr 1.4258e-03 eta 0:05:24
epoch [20/50] batch [50/51] time 0.167 (0.207) data 0.001 (0.031) loss 0.7554 (0.7897) acc 84.6939 (81.5010) lr 1.4258e-03 eta 0:05:17
>>> alpha1: 0.226  alpha2: 0.069 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.45 <<<
epoch [21/50] batch [5/51] time 0.176 (0.539) data 0.000 (0.357) loss 0.7436 (0.6856) acc 78.3654 (83.5487) lr 1.3681e-03 eta 0:13:41
epoch [21/50] batch [10/51] time 0.174 (0.359) data 0.000 (0.179) loss 0.9932 (0.7286) acc 72.5962 (83.0822) lr 1.3681e-03 eta 0:09:05
epoch [21/50] batch [15/51] time 0.174 (0.298) data 0.000 (0.119) loss 0.8587 (0.7583) acc 75.9615 (81.4436) lr 1.3681e-03 eta 0:07:31
epoch [21/50] batch [20/51] time 0.183 (0.269) data 0.000 (0.089) loss 0.6979 (0.7471) acc 83.9623 (82.2054) lr 1.3681e-03 eta 0:06:46
epoch [21/50] batch [25/51] time 0.193 (0.253) data 0.000 (0.072) loss 0.7208 (0.7253) acc 88.5965 (82.7515) lr 1.3681e-03 eta 0:06:20
epoch [21/50] batch [30/51] time 0.173 (0.239) data 0.000 (0.060) loss 0.5616 (0.7244) acc 87.0000 (82.9478) lr 1.3681e-03 eta 0:05:57
epoch [21/50] batch [35/51] time 0.167 (0.229) data 0.000 (0.051) loss 0.6555 (0.7290) acc 83.6956 (82.7812) lr 1.3681e-03 eta 0:05:42
epoch [21/50] batch [40/51] time 0.173 (0.222) data 0.000 (0.045) loss 0.8358 (0.7400) acc 84.1346 (82.5549) lr 1.3681e-03 eta 0:05:31
epoch [21/50] batch [45/51] time 0.169 (0.216) data 0.000 (0.040) loss 0.9886 (0.7481) acc 77.5000 (82.2537) lr 1.3681e-03 eta 0:05:21
epoch [21/50] batch [50/51] time 0.177 (0.212) data 0.000 (0.036) loss 0.4065 (0.7432) acc 92.9245 (82.5492) lr 1.3681e-03 eta 0:05:14
>>> alpha1: 0.212  alpha2: 0.070 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [22/50] batch [5/51] time 0.181 (0.514) data 0.000 (0.334) loss 0.5045 (0.6276) acc 90.0000 (85.9195) lr 1.3090e-03 eta 0:12:37
epoch [22/50] batch [10/51] time 0.174 (0.345) data 0.000 (0.167) loss 0.6879 (0.6957) acc 81.3726 (83.4091) lr 1.3090e-03 eta 0:08:27
epoch [22/50] batch [15/51] time 0.179 (0.288) data 0.000 (0.112) loss 0.8591 (0.7156) acc 82.6923 (83.6457) lr 1.3090e-03 eta 0:07:01
epoch [22/50] batch [20/51] time 0.186 (0.261) data 0.000 (0.084) loss 0.6511 (0.6967) acc 81.6038 (83.8808) lr 1.3090e-03 eta 0:06:21
epoch [22/50] batch [25/51] time 0.185 (0.245) data 0.000 (0.067) loss 0.9799 (0.7164) acc 82.0000 (83.3258) lr 1.3090e-03 eta 0:05:55
epoch [22/50] batch [30/51] time 0.181 (0.234) data 0.000 (0.056) loss 1.0242 (0.7342) acc 76.4706 (82.9057) lr 1.3090e-03 eta 0:05:38
epoch [22/50] batch [35/51] time 0.184 (0.226) data 0.000 (0.048) loss 0.7144 (0.7272) acc 84.3137 (83.1773) lr 1.3090e-03 eta 0:05:26
epoch [22/50] batch [40/51] time 0.185 (0.220) data 0.000 (0.042) loss 0.6858 (0.7305) acc 82.8947 (82.9556) lr 1.3090e-03 eta 0:05:16
epoch [22/50] batch [45/51] time 0.161 (0.214) data 0.000 (0.037) loss 0.6309 (0.7305) acc 85.6383 (82.9066) lr 1.3090e-03 eta 0:05:06
epoch [22/50] batch [50/51] time 0.178 (0.210) data 0.000 (0.034) loss 0.5075 (0.7325) acc 92.1296 (82.7009) lr 1.3090e-03 eta 0:04:59
>>> alpha1: 0.206  alpha2: 0.074 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.43 <<<
epoch [23/50] batch [5/51] time 0.185 (0.511) data 0.000 (0.311) loss 0.5772 (0.7204) acc 90.9091 (84.1480) lr 1.2487e-03 eta 0:12:06
epoch [23/50] batch [10/51] time 0.183 (0.346) data 0.000 (0.156) loss 0.5343 (0.6901) acc 86.5385 (85.2348) lr 1.2487e-03 eta 0:08:10
epoch [23/50] batch [15/51] time 0.180 (0.290) data 0.001 (0.104) loss 0.6729 (0.8248) acc 83.3333 (82.7453) lr 1.2487e-03 eta 0:06:49
epoch [23/50] batch [20/51] time 0.163 (0.262) data 0.000 (0.078) loss 0.8266 (0.8096) acc 80.8511 (82.9353) lr 1.2487e-03 eta 0:06:08
epoch [23/50] batch [25/51] time 0.184 (0.246) data 0.000 (0.062) loss 0.6410 (0.7792) acc 85.0962 (83.3141) lr 1.2487e-03 eta 0:05:44
epoch [23/50] batch [30/51] time 0.179 (0.234) data 0.000 (0.052) loss 0.5378 (0.7655) acc 87.7358 (83.2324) lr 1.2487e-03 eta 0:05:27
epoch [23/50] batch [35/51] time 0.198 (0.228) data 0.000 (0.045) loss 0.6845 (0.7550) acc 82.4074 (83.3381) lr 1.2487e-03 eta 0:05:17
epoch [23/50] batch [40/51] time 0.179 (0.222) data 0.000 (0.039) loss 0.5741 (0.7517) acc 89.5455 (83.4849) lr 1.2487e-03 eta 0:05:08
epoch [23/50] batch [45/51] time 0.172 (0.216) data 0.000 (0.035) loss 0.8765 (0.7585) acc 76.9231 (83.3504) lr 1.2487e-03 eta 0:04:58
epoch [23/50] batch [50/51] time 0.177 (0.211) data 0.000 (0.031) loss 0.7020 (0.7502) acc 83.3333 (83.3864) lr 1.2487e-03 eta 0:04:51
>>> alpha1: 0.198  alpha2: 0.081 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.18 & unmatched refined noisy rate: 0.44 <<<
epoch [24/50] batch [5/51] time 0.174 (0.527) data 0.000 (0.349) loss 0.6970 (0.7204) acc 82.6923 (82.9169) lr 1.1874e-03 eta 0:12:02
epoch [24/50] batch [10/51] time 0.177 (0.352) data 0.000 (0.175) loss 0.5149 (0.6951) acc 92.6471 (84.3024) lr 1.1874e-03 eta 0:08:01
epoch [24/50] batch [15/51] time 0.162 (0.291) data 0.000 (0.117) loss 0.7809 (0.7135) acc 83.5106 (84.5361) lr 1.1874e-03 eta 0:06:36
epoch [24/50] batch [20/51] time 0.172 (0.263) data 0.000 (0.087) loss 0.8632 (0.7320) acc 86.2745 (84.1979) lr 1.1874e-03 eta 0:05:56
epoch [24/50] batch [25/51] time 0.187 (0.246) data 0.000 (0.070) loss 0.7083 (0.7272) acc 85.0000 (83.9780) lr 1.1874e-03 eta 0:05:32
epoch [24/50] batch [30/51] time 0.188 (0.237) data 0.000 (0.058) loss 0.8521 (0.7104) acc 79.4118 (84.1802) lr 1.1874e-03 eta 0:05:18
epoch [24/50] batch [35/51] time 0.222 (0.229) data 0.000 (0.050) loss 0.7029 (0.7050) acc 82.4561 (84.1785) lr 1.1874e-03 eta 0:05:07
epoch [24/50] batch [40/51] time 0.176 (0.225) data 0.000 (0.044) loss 0.7095 (0.7117) acc 87.2642 (84.0656) lr 1.1874e-03 eta 0:05:00
epoch [24/50] batch [45/51] time 0.182 (0.220) data 0.000 (0.039) loss 0.5385 (0.7034) acc 90.2778 (84.1111) lr 1.1874e-03 eta 0:04:52
epoch [24/50] batch [50/51] time 0.176 (0.215) data 0.000 (0.035) loss 0.6426 (0.6984) acc 83.4906 (84.2048) lr 1.1874e-03 eta 0:04:45
>>> alpha1: 0.191  alpha2: 0.082 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [25/50] batch [5/51] time 0.179 (0.525) data 0.000 (0.336) loss 0.5310 (0.6299) acc 86.3208 (84.4888) lr 1.1253e-03 eta 0:11:33
epoch [25/50] batch [10/51] time 0.182 (0.357) data 0.000 (0.168) loss 0.7922 (0.6486) acc 80.7692 (84.1844) lr 1.1253e-03 eta 0:07:50
epoch [25/50] batch [15/51] time 0.182 (0.301) data 0.000 (0.112) loss 0.7443 (0.6343) acc 81.2500 (85.2922) lr 1.1253e-03 eta 0:06:35
epoch [25/50] batch [20/51] time 0.188 (0.274) data 0.000 (0.084) loss 0.7971 (0.6720) acc 77.8302 (84.1277) lr 1.1253e-03 eta 0:05:58
epoch [25/50] batch [25/51] time 0.197 (0.256) data 0.000 (0.068) loss 0.7873 (0.6680) acc 75.0000 (84.1624) lr 1.1253e-03 eta 0:05:32
epoch [25/50] batch [30/51] time 0.195 (0.244) data 0.003 (0.056) loss 0.5422 (0.6650) acc 88.2075 (84.3214) lr 1.1253e-03 eta 0:05:16
epoch [25/50] batch [35/51] time 0.172 (0.235) data 0.000 (0.048) loss 0.8769 (0.6746) acc 77.9412 (84.0617) lr 1.1253e-03 eta 0:05:02
epoch [25/50] batch [40/51] time 0.182 (0.229) data 0.000 (0.042) loss 0.5882 (0.7054) acc 85.7843 (83.9514) lr 1.1253e-03 eta 0:04:54
epoch [25/50] batch [45/51] time 0.179 (0.223) data 0.000 (0.038) loss 0.7289 (0.7001) acc 83.7963 (83.8747) lr 1.1253e-03 eta 0:04:45
epoch [25/50] batch [50/51] time 0.181 (0.218) data 0.000 (0.034) loss 0.7367 (0.6984) acc 82.0000 (83.8430) lr 1.1253e-03 eta 0:04:38
>>> alpha1: 0.188  alpha2: 0.085 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [26/50] batch [5/51] time 0.173 (0.468) data 0.000 (0.280) loss 0.6464 (0.6080) acc 85.0962 (86.2592) lr 1.0628e-03 eta 0:09:54
epoch [26/50] batch [10/51] time 0.177 (0.322) data 0.000 (0.140) loss 0.7228 (0.6129) acc 85.8696 (86.1374) lr 1.0628e-03 eta 0:06:47
epoch [26/50] batch [15/51] time 0.162 (0.276) data 0.000 (0.094) loss 0.7573 (0.6538) acc 79.2553 (84.8177) lr 1.0628e-03 eta 0:05:48
epoch [26/50] batch [20/51] time 0.180 (0.253) data 0.000 (0.070) loss 0.7373 (0.6604) acc 79.2553 (84.5554) lr 1.0628e-03 eta 0:05:17
epoch [26/50] batch [25/51] time 0.177 (0.239) data 0.001 (0.056) loss 0.8405 (0.6797) acc 80.8824 (84.0796) lr 1.0628e-03 eta 0:04:58
epoch [26/50] batch [30/51] time 0.179 (0.230) data 0.000 (0.047) loss 0.9889 (0.6859) acc 73.1481 (83.8252) lr 1.0628e-03 eta 0:04:46
epoch [26/50] batch [35/51] time 0.176 (0.222) data 0.000 (0.040) loss 0.7346 (0.6804) acc 85.2041 (84.0276) lr 1.0628e-03 eta 0:04:35
epoch [26/50] batch [40/51] time 0.166 (0.217) data 0.000 (0.035) loss 0.5655 (0.6670) acc 87.2449 (84.4899) lr 1.0628e-03 eta 0:04:27
epoch [26/50] batch [45/51] time 0.177 (0.212) data 0.000 (0.031) loss 0.6386 (0.6634) acc 85.5000 (84.6611) lr 1.0628e-03 eta 0:04:20
epoch [26/50] batch [50/51] time 0.185 (0.209) data 0.001 (0.028) loss 0.5815 (0.6666) acc 83.7963 (84.5390) lr 1.0628e-03 eta 0:04:15
>>> alpha1: 0.190  alpha2: 0.091 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [27/50] batch [5/51] time 0.180 (0.457) data 0.000 (0.270) loss 0.6894 (0.8834) acc 81.1321 (82.2355) lr 1.0000e-03 eta 0:09:17
epoch [27/50] batch [10/51] time 0.195 (0.317) data 0.001 (0.135) loss 0.7559 (0.7744) acc 86.3208 (83.7424) lr 1.0000e-03 eta 0:06:24
epoch [27/50] batch [15/51] time 0.184 (0.323) data 0.000 (0.090) loss 0.6890 (0.7300) acc 81.8182 (83.6174) lr 1.0000e-03 eta 0:06:29
epoch [27/50] batch [20/51] time 0.181 (0.287) data 0.000 (0.068) loss 0.6440 (0.7183) acc 84.2593 (83.3743) lr 1.0000e-03 eta 0:05:45
epoch [27/50] batch [25/51] time 0.182 (0.266) data 0.000 (0.055) loss 0.5607 (0.6909) acc 89.0909 (84.5254) lr 1.0000e-03 eta 0:05:19
epoch [27/50] batch [30/51] time 0.169 (0.252) data 0.000 (0.046) loss 0.7039 (0.6934) acc 79.5000 (84.2061) lr 1.0000e-03 eta 0:05:01
epoch [27/50] batch [35/51] time 0.179 (0.242) data 0.000 (0.039) loss 0.8183 (0.6899) acc 78.2407 (84.0866) lr 1.0000e-03 eta 0:04:47
epoch [27/50] batch [40/51] time 0.178 (0.233) data 0.000 (0.034) loss 0.5449 (0.6894) acc 88.8889 (84.2273) lr 1.0000e-03 eta 0:04:35
epoch [27/50] batch [45/51] time 0.170 (0.226) data 0.000 (0.031) loss 0.5392 (0.6750) acc 87.0000 (84.5095) lr 1.0000e-03 eta 0:04:26
epoch [27/50] batch [50/51] time 0.175 (0.220) data 0.000 (0.027) loss 0.6536 (0.6768) acc 90.5660 (84.4603) lr 1.0000e-03 eta 0:04:18
>>> alpha1: 0.183  alpha2: 0.087 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.43 <<<
epoch [28/50] batch [5/51] time 0.185 (0.464) data 0.000 (0.268) loss 0.9351 (0.7142) acc 75.4902 (82.3899) lr 9.3721e-04 eta 0:09:02
epoch [28/50] batch [10/51] time 0.188 (0.322) data 0.000 (0.134) loss 0.5923 (0.6886) acc 82.5893 (83.9728) lr 9.3721e-04 eta 0:06:14
epoch [28/50] batch [15/51] time 0.193 (0.275) data 0.000 (0.089) loss 0.7111 (0.6729) acc 81.4815 (84.3594) lr 9.3721e-04 eta 0:05:18
epoch [28/50] batch [20/51] time 0.210 (0.253) data 0.000 (0.067) loss 0.6823 (0.6810) acc 77.6316 (83.7991) lr 9.3721e-04 eta 0:04:52
epoch [28/50] batch [25/51] time 0.174 (0.238) data 0.000 (0.054) loss 0.5699 (0.6697) acc 89.9038 (84.1678) lr 9.3721e-04 eta 0:04:32
epoch [28/50] batch [30/51] time 0.186 (0.229) data 0.000 (0.045) loss 0.6912 (0.6686) acc 85.9649 (84.2821) lr 9.3721e-04 eta 0:04:21
epoch [28/50] batch [35/51] time 0.176 (0.223) data 0.001 (0.038) loss 0.5888 (0.6721) acc 84.6154 (84.0882) lr 9.3721e-04 eta 0:04:13
epoch [28/50] batch [40/51] time 0.169 (0.216) data 0.000 (0.034) loss 0.5718 (0.6691) acc 86.2745 (83.8921) lr 9.3721e-04 eta 0:04:04
epoch [28/50] batch [45/51] time 0.177 (0.211) data 0.000 (0.030) loss 0.6321 (0.6614) acc 88.4259 (84.1008) lr 9.3721e-04 eta 0:03:58
epoch [28/50] batch [50/51] time 0.169 (0.208) data 0.000 (0.027) loss 0.7223 (0.6584) acc 87.7451 (84.3552) lr 9.3721e-04 eta 0:03:53
>>> alpha1: 0.180  alpha2: 0.087 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.42 <<<
epoch [29/50] batch [5/51] time 0.166 (0.502) data 0.000 (0.318) loss 0.4302 (0.5874) acc 88.5417 (87.2207) lr 8.7467e-04 eta 0:09:20
epoch [29/50] batch [10/51] time 0.199 (0.344) data 0.000 (0.159) loss 0.7792 (0.6113) acc 78.8462 (85.9444) lr 8.7467e-04 eta 0:06:22
epoch [29/50] batch [15/51] time 0.189 (0.289) data 0.000 (0.106) loss 0.3211 (0.6129) acc 95.0000 (86.1731) lr 8.7467e-04 eta 0:05:20
epoch [29/50] batch [20/51] time 0.183 (0.263) data 0.000 (0.080) loss 0.6873 (0.6025) acc 86.3636 (86.1001) lr 8.7467e-04 eta 0:04:49
epoch [29/50] batch [25/51] time 0.183 (0.246) data 0.000 (0.064) loss 0.6593 (0.6083) acc 84.5000 (85.9962) lr 8.7467e-04 eta 0:04:30
epoch [29/50] batch [30/51] time 0.181 (0.236) data 0.001 (0.053) loss 0.5258 (0.6090) acc 85.1852 (86.2354) lr 8.7467e-04 eta 0:04:18
epoch [29/50] batch [35/51] time 0.181 (0.227) data 0.001 (0.046) loss 0.5607 (0.6106) acc 82.8704 (85.9646) lr 8.7467e-04 eta 0:04:07
epoch [29/50] batch [40/51] time 0.163 (0.221) data 0.000 (0.040) loss 0.9386 (0.6245) acc 79.6875 (85.7915) lr 8.7467e-04 eta 0:03:58
epoch [29/50] batch [45/51] time 0.181 (0.215) data 0.000 (0.036) loss 0.4560 (0.6199) acc 90.6250 (85.9515) lr 8.7467e-04 eta 0:03:51
epoch [29/50] batch [50/51] time 0.174 (0.211) data 0.000 (0.032) loss 0.6849 (0.6206) acc 82.6923 (85.7225) lr 8.7467e-04 eta 0:03:45
>>> alpha1: 0.180  alpha2: 0.089 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.42 <<<
epoch [30/50] batch [5/51] time 0.175 (0.448) data 0.001 (0.270) loss 0.7503 (0.6074) acc 81.7308 (86.5359) lr 8.1262e-04 eta 0:07:57
epoch [30/50] batch [10/51] time 0.164 (0.310) data 0.000 (0.135) loss 0.6908 (0.6064) acc 83.3333 (86.2394) lr 8.1262e-04 eta 0:05:29
epoch [30/50] batch [15/51] time 0.186 (0.266) data 0.000 (0.090) loss 0.4689 (0.6049) acc 94.6429 (86.6909) lr 8.1262e-04 eta 0:04:41
epoch [30/50] batch [20/51] time 0.166 (0.243) data 0.000 (0.068) loss 0.5048 (0.5965) acc 85.7143 (86.2001) lr 8.1262e-04 eta 0:04:15
epoch [30/50] batch [25/51] time 0.165 (0.230) data 0.000 (0.054) loss 0.6561 (0.6086) acc 82.8125 (86.0327) lr 8.1262e-04 eta 0:04:00
epoch [30/50] batch [30/51] time 0.182 (0.223) data 0.000 (0.045) loss 0.6840 (0.6186) acc 81.6964 (85.6819) lr 8.1262e-04 eta 0:03:51
epoch [30/50] batch [35/51] time 0.181 (0.217) data 0.000 (0.039) loss 0.4756 (0.6085) acc 91.8182 (86.0716) lr 8.1262e-04 eta 0:03:45
epoch [30/50] batch [40/51] time 0.193 (0.212) data 0.000 (0.034) loss 0.7454 (0.6064) acc 80.4167 (86.2514) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [45/51] time 0.184 (0.207) data 0.000 (0.030) loss 0.4743 (0.6147) acc 91.2281 (85.9519) lr 8.1262e-04 eta 0:03:32
epoch [30/50] batch [50/51] time 0.168 (0.203) data 0.000 (0.027) loss 0.5358 (0.6119) acc 85.7843 (85.9758) lr 8.1262e-04 eta 0:03:27
>>> alpha1: 0.181  alpha2: 0.091 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.42 <<<
epoch [31/50] batch [5/51] time 0.164 (0.438) data 0.000 (0.252) loss 1.0341 (0.6901) acc 70.0000 (83.7495) lr 7.5131e-04 eta 0:07:24
epoch [31/50] batch [10/51] time 0.181 (0.306) data 0.000 (0.126) loss 0.5047 (0.6126) acc 87.9630 (85.7295) lr 7.5131e-04 eta 0:05:08
epoch [31/50] batch [15/51] time 0.184 (0.264) data 0.000 (0.084) loss 0.5290 (0.5799) acc 88.2353 (86.7293) lr 7.5131e-04 eta 0:04:25
epoch [31/50] batch [20/51] time 0.173 (0.243) data 0.000 (0.063) loss 0.8364 (0.6006) acc 81.7308 (85.9710) lr 7.5131e-04 eta 0:04:03
epoch [31/50] batch [25/51] time 0.169 (0.229) data 0.000 (0.051) loss 0.6021 (0.5918) acc 81.0000 (86.2307) lr 7.5131e-04 eta 0:03:47
epoch [31/50] batch [30/51] time 0.180 (0.221) data 0.000 (0.042) loss 0.6258 (0.5857) acc 86.5741 (86.2906) lr 7.5131e-04 eta 0:03:38
epoch [31/50] batch [35/51] time 0.179 (0.215) data 0.000 (0.036) loss 0.7542 (0.6061) acc 80.0926 (85.6321) lr 7.5131e-04 eta 0:03:31
epoch [31/50] batch [40/51] time 0.173 (0.210) data 0.000 (0.032) loss 0.4242 (0.6132) acc 88.4615 (85.5069) lr 7.5131e-04 eta 0:03:25
epoch [31/50] batch [45/51] time 0.178 (0.206) data 0.000 (0.028) loss 0.6013 (0.6052) acc 86.1111 (85.6375) lr 7.5131e-04 eta 0:03:20
epoch [31/50] batch [50/51] time 0.177 (0.202) data 0.000 (0.025) loss 0.5005 (0.6137) acc 86.3208 (85.5924) lr 7.5131e-04 eta 0:03:15
>>> alpha1: 0.177  alpha2: 0.090 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.43 <<<
epoch [32/50] batch [5/51] time 0.175 (0.460) data 0.000 (0.279) loss 0.5743 (0.5489) acc 84.6154 (87.9022) lr 6.9098e-04 eta 0:07:23
epoch [32/50] batch [10/51] time 0.186 (0.324) data 0.000 (0.140) loss 0.5774 (0.5505) acc 82.5472 (87.3261) lr 6.9098e-04 eta 0:05:10
epoch [32/50] batch [15/51] time 0.173 (0.276) data 0.001 (0.093) loss 0.3407 (0.5463) acc 94.5000 (87.2104) lr 6.9098e-04 eta 0:04:23
epoch [32/50] batch [20/51] time 0.168 (0.251) data 0.000 (0.070) loss 0.7572 (0.5798) acc 81.5000 (86.4121) lr 6.9098e-04 eta 0:03:58
epoch [32/50] batch [25/51] time 0.182 (0.236) data 0.000 (0.056) loss 0.6382 (0.5814) acc 85.4546 (86.0622) lr 6.9098e-04 eta 0:03:42
epoch [32/50] batch [30/51] time 0.170 (0.226) data 0.000 (0.047) loss 0.6283 (0.5857) acc 85.2941 (86.0823) lr 6.9098e-04 eta 0:03:32
epoch [32/50] batch [35/51] time 0.191 (0.220) data 0.000 (0.040) loss 0.3432 (0.5890) acc 92.5439 (85.8692) lr 6.9098e-04 eta 0:03:25
epoch [32/50] batch [40/51] time 0.173 (0.214) data 0.000 (0.035) loss 0.5412 (0.5902) acc 85.5769 (85.9570) lr 6.9098e-04 eta 0:03:18
epoch [32/50] batch [45/51] time 0.168 (0.209) data 0.000 (0.031) loss 0.8666 (0.5962) acc 79.5918 (85.7771) lr 6.9098e-04 eta 0:03:13
epoch [32/50] batch [50/51] time 0.180 (0.206) data 0.000 (0.028) loss 0.4754 (0.5967) acc 90.4546 (85.8014) lr 6.9098e-04 eta 0:03:09
>>> alpha1: 0.178  alpha2: 0.095 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.29 --> matched refined noisy rate: 0.19 & unmatched refined noisy rate: 0.44 <<<
epoch [33/50] batch [5/51] time 0.168 (0.476) data 0.000 (0.290) loss 0.6701 (0.6429) acc 87.5000 (85.8468) lr 6.3188e-04 eta 0:07:14
epoch [33/50] batch [10/51] time 0.172 (0.330) data 0.000 (0.145) loss 0.6709 (0.6679) acc 81.8627 (84.3078) lr 6.3188e-04 eta 0:04:59
epoch [33/50] batch [15/51] time 0.169 (0.276) data 0.000 (0.097) loss 0.6633 (0.6574) acc 79.7872 (83.8155) lr 6.3188e-04 eta 0:04:09
epoch [33/50] batch [20/51] time 0.170 (0.251) data 0.000 (0.073) loss 0.6513 (0.6341) acc 85.7843 (84.9244) lr 6.3188e-04 eta 0:03:45
epoch [33/50] batch [25/51] time 0.172 (0.236) data 0.000 (0.058) loss 0.5214 (0.6223) acc 87.9808 (85.3088) lr 6.3188e-04 eta 0:03:30
epoch [33/50] batch [30/51] time 0.161 (0.225) data 0.000 (0.049) loss 0.6529 (0.6178) acc 80.3191 (85.1976) lr 6.3188e-04 eta 0:03:20
epoch [33/50] batch [35/51] time 0.173 (0.219) data 0.000 (0.042) loss 0.4182 (0.6060) acc 90.3061 (85.6723) lr 6.3188e-04 eta 0:03:13
epoch [33/50] batch [40/51] time 0.155 (0.213) data 0.000 (0.036) loss 0.6778 (0.6087) acc 80.6818 (85.5231) lr 6.3188e-04 eta 0:03:07
epoch [33/50] batch [45/51] time 0.187 (0.209) data 0.000 (0.032) loss 0.4677 (0.6059) acc 91.2037 (85.6068) lr 6.3188e-04 eta 0:03:02
epoch [33/50] batch [50/51] time 0.168 (0.206) data 0.000 (0.029) loss 0.4711 (0.5979) acc 85.5000 (85.8020) lr 6.3188e-04 eta 0:02:58
>>> alpha1: 0.179  alpha2: 0.097 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.42 <<<
epoch [34/50] batch [5/51] time 0.173 (0.451) data 0.000 (0.276) loss 0.7289 (0.6340) acc 78.5714 (85.4499) lr 5.7422e-04 eta 0:06:29
epoch [34/50] batch [10/51] time 0.182 (0.315) data 0.000 (0.138) loss 0.5443 (0.6071) acc 87.9808 (85.5681) lr 5.7422e-04 eta 0:04:30
epoch [34/50] batch [15/51] time 0.179 (0.270) data 0.000 (0.092) loss 0.6762 (0.6238) acc 81.4815 (84.5823) lr 5.7422e-04 eta 0:03:50
epoch [34/50] batch [20/51] time 0.175 (0.247) data 0.000 (0.069) loss 0.5880 (0.5997) acc 91.3265 (85.6643) lr 5.7422e-04 eta 0:03:29
epoch [34/50] batch [25/51] time 0.177 (0.233) data 0.000 (0.055) loss 0.3534 (0.6056) acc 94.8113 (85.4179) lr 5.7422e-04 eta 0:03:16
epoch [34/50] batch [30/51] time 0.184 (0.225) data 0.000 (0.046) loss 0.7716 (0.6137) acc 84.2593 (85.3125) lr 5.7422e-04 eta 0:03:08
epoch [34/50] batch [35/51] time 0.171 (0.219) data 0.000 (0.040) loss 0.7399 (0.6150) acc 83.3333 (85.4211) lr 5.7422e-04 eta 0:03:02
epoch [34/50] batch [40/51] time 0.179 (0.214) data 0.000 (0.035) loss 0.4164 (0.5987) acc 89.3519 (85.7643) lr 5.7422e-04 eta 0:02:57
epoch [34/50] batch [45/51] time 0.177 (0.211) data 0.000 (0.031) loss 0.6378 (0.5986) acc 84.4340 (85.8418) lr 5.7422e-04 eta 0:02:53
epoch [34/50] batch [50/51] time 0.157 (0.206) data 0.000 (0.028) loss 0.8111 (0.5939) acc 73.3333 (85.8245) lr 5.7422e-04 eta 0:02:48
>>> alpha1: 0.179  alpha2: 0.098 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.42 <<<
epoch [35/50] batch [5/51] time 0.182 (0.484) data 0.000 (0.299) loss 0.3657 (0.5441) acc 93.1818 (87.9258) lr 5.1825e-04 eta 0:06:32
epoch [35/50] batch [10/51] time 0.186 (0.333) data 0.000 (0.150) loss 0.4742 (0.5084) acc 90.3509 (88.4186) lr 5.1825e-04 eta 0:04:28
epoch [35/50] batch [15/51] time 0.191 (0.283) data 0.000 (0.100) loss 0.5303 (0.5378) acc 90.5660 (88.4469) lr 5.1825e-04 eta 0:03:46
epoch [35/50] batch [20/51] time 0.175 (0.257) data 0.000 (0.075) loss 0.5249 (0.5402) acc 90.1961 (88.5137) lr 5.1825e-04 eta 0:03:24
epoch [35/50] batch [25/51] time 0.173 (0.242) data 0.000 (0.060) loss 0.6191 (0.5530) acc 83.8235 (88.1297) lr 5.1825e-04 eta 0:03:11
epoch [35/50] batch [30/51] time 0.182 (0.232) data 0.000 (0.050) loss 0.8489 (0.5746) acc 77.4038 (87.2736) lr 5.1825e-04 eta 0:03:02
epoch [35/50] batch [35/51] time 0.180 (0.224) data 0.000 (0.043) loss 0.5142 (0.5744) acc 88.7255 (87.2085) lr 5.1825e-04 eta 0:02:55
epoch [35/50] batch [40/51] time 0.167 (0.218) data 0.000 (0.038) loss 0.6034 (0.5826) acc 87.0000 (87.1265) lr 5.1825e-04 eta 0:02:49
epoch [35/50] batch [45/51] time 0.180 (0.214) data 0.000 (0.034) loss 0.4863 (0.5804) acc 94.5455 (87.0998) lr 5.1825e-04 eta 0:02:44
epoch [35/50] batch [50/51] time 0.163 (0.210) data 0.000 (0.030) loss 0.5858 (0.5810) acc 86.4583 (87.0338) lr 5.1825e-04 eta 0:02:40
>>> alpha1: 0.174  alpha2: 0.092 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [36/50] batch [5/51] time 0.192 (0.479) data 0.000 (0.288) loss 0.4805 (0.5797) acc 94.0000 (88.1100) lr 4.6417e-04 eta 0:06:04
epoch [36/50] batch [10/51] time 0.172 (0.330) data 0.000 (0.144) loss 0.3704 (0.5633) acc 95.0980 (87.8382) lr 4.6417e-04 eta 0:04:08
epoch [36/50] batch [15/51] time 0.168 (0.277) data 0.000 (0.096) loss 0.5187 (0.5503) acc 88.0208 (88.2166) lr 4.6417e-04 eta 0:03:28
epoch [36/50] batch [20/51] time 0.179 (0.254) data 0.000 (0.072) loss 0.7447 (0.5494) acc 83.3333 (88.3266) lr 4.6417e-04 eta 0:03:09
epoch [36/50] batch [25/51] time 0.172 (0.240) data 0.000 (0.058) loss 0.6485 (0.5716) acc 84.8039 (87.8104) lr 4.6417e-04 eta 0:02:57
epoch [36/50] batch [30/51] time 0.176 (0.230) data 0.000 (0.048) loss 0.6018 (0.5756) acc 86.2245 (87.7007) lr 4.6417e-04 eta 0:02:49
epoch [36/50] batch [35/51] time 0.171 (0.224) data 0.000 (0.041) loss 0.6459 (0.5693) acc 88.2353 (87.6000) lr 4.6417e-04 eta 0:02:43
epoch [36/50] batch [40/51] time 0.180 (0.218) data 0.000 (0.036) loss 0.3012 (0.5575) acc 93.6364 (87.7452) lr 4.6417e-04 eta 0:02:37
epoch [36/50] batch [45/51] time 0.172 (0.213) data 0.000 (0.032) loss 0.6037 (0.5597) acc 84.6154 (87.5266) lr 4.6417e-04 eta 0:02:33
epoch [36/50] batch [50/51] time 0.178 (0.209) data 0.000 (0.029) loss 0.6971 (0.5586) acc 86.1111 (87.6293) lr 4.6417e-04 eta 0:02:29
>>> alpha1: 0.173  alpha2: 0.094 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.40 <<<
epoch [37/50] batch [5/51] time 0.180 (0.464) data 0.000 (0.280) loss 0.5308 (0.4945) acc 88.4259 (88.0165) lr 4.1221e-04 eta 0:05:28
epoch [37/50] batch [10/51] time 0.166 (0.320) data 0.000 (0.140) loss 0.5109 (0.5017) acc 85.9375 (88.9483) lr 4.1221e-04 eta 0:03:45
epoch [37/50] batch [15/51] time 0.165 (0.274) data 0.000 (0.094) loss 0.8743 (0.5131) acc 80.2083 (88.4606) lr 4.1221e-04 eta 0:03:11
epoch [37/50] batch [20/51] time 0.206 (0.251) data 0.000 (0.070) loss 0.4418 (0.5041) acc 88.3929 (88.6801) lr 4.1221e-04 eta 0:02:54
epoch [37/50] batch [25/51] time 0.171 (0.235) data 0.000 (0.056) loss 0.5018 (0.5209) acc 88.7255 (88.2518) lr 4.1221e-04 eta 0:02:42
epoch [37/50] batch [30/51] time 0.178 (0.225) data 0.000 (0.047) loss 0.5511 (0.5352) acc 87.5000 (87.7285) lr 4.1221e-04 eta 0:02:33
epoch [37/50] batch [35/51] time 0.173 (0.218) data 0.000 (0.040) loss 0.5037 (0.5273) acc 88.5417 (87.8940) lr 4.1221e-04 eta 0:02:28
epoch [37/50] batch [40/51] time 0.159 (0.212) data 0.000 (0.035) loss 0.7230 (0.5299) acc 79.8913 (87.7290) lr 4.1221e-04 eta 0:02:23
epoch [37/50] batch [45/51] time 0.178 (0.209) data 0.000 (0.031) loss 0.5349 (0.5422) acc 87.2642 (87.2109) lr 4.1221e-04 eta 0:02:19
epoch [37/50] batch [50/51] time 0.178 (0.205) data 0.000 (0.028) loss 0.5054 (0.5444) acc 92.5926 (87.2831) lr 4.1221e-04 eta 0:02:16
>>> alpha1: 0.169  alpha2: 0.093 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.39 <<<
epoch [38/50] batch [5/51] time 0.180 (0.508) data 0.000 (0.309) loss 0.4251 (0.5402) acc 89.1509 (88.2197) lr 3.6258e-04 eta 0:05:34
epoch [38/50] batch [10/51] time 0.170 (0.341) data 0.000 (0.155) loss 0.6241 (0.5216) acc 86.0000 (88.2836) lr 3.6258e-04 eta 0:03:42
epoch [38/50] batch [15/51] time 0.173 (0.287) data 0.000 (0.103) loss 0.6626 (0.5447) acc 84.1346 (87.0981) lr 3.6258e-04 eta 0:03:05
epoch [38/50] batch [20/51] time 0.185 (0.260) data 0.000 (0.078) loss 0.4192 (0.5627) acc 90.4546 (86.9448) lr 3.6258e-04 eta 0:02:47
epoch [38/50] batch [25/51] time 0.186 (0.245) data 0.001 (0.062) loss 0.6362 (0.5675) acc 87.9808 (86.9415) lr 3.6258e-04 eta 0:02:36
epoch [38/50] batch [30/51] time 0.208 (0.235) data 0.001 (0.052) loss 0.3971 (0.5746) acc 89.0351 (86.7316) lr 3.6258e-04 eta 0:02:28
epoch [38/50] batch [35/51] time 0.174 (0.228) data 0.000 (0.045) loss 0.4111 (0.5659) acc 91.1765 (87.0408) lr 3.6258e-04 eta 0:02:22
epoch [38/50] batch [40/51] time 0.165 (0.221) data 0.000 (0.039) loss 0.6052 (0.5667) acc 90.1042 (87.1044) lr 3.6258e-04 eta 0:02:17
epoch [38/50] batch [45/51] time 0.176 (0.216) data 0.000 (0.035) loss 0.5462 (0.5729) acc 91.5094 (86.8424) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [50/51] time 0.193 (0.212) data 0.000 (0.031) loss 0.6658 (0.5672) acc 89.5455 (87.1543) lr 3.6258e-04 eta 0:02:10
>>> alpha1: 0.168  alpha2: 0.095 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.42 <<<
epoch [39/50] batch [5/51] time 0.180 (0.455) data 0.000 (0.272) loss 0.3641 (0.4871) acc 91.9811 (89.9410) lr 3.1545e-04 eta 0:04:35
epoch [39/50] batch [10/51] time 0.188 (0.320) data 0.001 (0.136) loss 0.7231 (0.5739) acc 86.7347 (86.7793) lr 3.1545e-04 eta 0:03:12
epoch [39/50] batch [15/51] time 0.186 (0.276) data 0.000 (0.091) loss 0.4701 (0.5403) acc 89.2157 (86.7231) lr 3.1545e-04 eta 0:02:45
epoch [39/50] batch [20/51] time 0.193 (0.254) data 0.000 (0.068) loss 0.3450 (0.5379) acc 91.3462 (86.6616) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [25/51] time 0.184 (0.239) data 0.000 (0.055) loss 0.4902 (0.5403) acc 89.5000 (86.7429) lr 3.1545e-04 eta 0:02:20
epoch [39/50] batch [30/51] time 0.181 (0.228) data 0.001 (0.046) loss 0.5904 (0.5651) acc 85.7843 (87.0775) lr 3.1545e-04 eta 0:02:12
epoch [39/50] batch [35/51] time 0.176 (0.221) data 0.000 (0.039) loss 0.5267 (0.5663) acc 84.1837 (86.9090) lr 3.1545e-04 eta 0:02:07
epoch [39/50] batch [40/51] time 0.173 (0.215) data 0.000 (0.034) loss 0.4076 (0.5592) acc 89.9038 (87.0636) lr 3.1545e-04 eta 0:02:02
epoch [39/50] batch [45/51] time 0.182 (0.211) data 0.000 (0.030) loss 0.6565 (0.5589) acc 88.8393 (87.3005) lr 3.1545e-04 eta 0:01:59
epoch [39/50] batch [50/51] time 0.174 (0.207) data 0.001 (0.027) loss 0.7880 (0.5550) acc 79.0816 (87.3325) lr 3.1545e-04 eta 0:01:56
>>> alpha1: 0.168  alpha2: 0.095 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.40 <<<
epoch [40/50] batch [5/51] time 0.185 (0.554) data 0.001 (0.348) loss 0.6188 (0.5507) acc 84.4340 (87.2723) lr 2.7103e-04 eta 0:05:08
epoch [40/50] batch [10/51] time 0.184 (0.368) data 0.000 (0.174) loss 0.6045 (0.5387) acc 85.5769 (87.4508) lr 2.7103e-04 eta 0:03:22
epoch [40/50] batch [15/51] time 0.193 (0.306) data 0.000 (0.117) loss 0.5840 (0.5444) acc 86.2745 (86.8010) lr 2.7103e-04 eta 0:02:46
epoch [40/50] batch [20/51] time 0.183 (0.276) data 0.000 (0.088) loss 0.3430 (0.5322) acc 92.5926 (87.2994) lr 2.7103e-04 eta 0:02:29
epoch [40/50] batch [25/51] time 0.183 (0.258) data 0.000 (0.070) loss 0.4577 (0.5348) acc 88.2653 (87.1611) lr 2.7103e-04 eta 0:02:18
epoch [40/50] batch [30/51] time 0.180 (0.245) data 0.000 (0.059) loss 0.4803 (0.5424) acc 90.0943 (87.1288) lr 2.7103e-04 eta 0:02:10
epoch [40/50] batch [35/51] time 0.198 (0.236) data 0.000 (0.050) loss 0.4118 (0.5440) acc 90.8654 (86.9662) lr 2.7103e-04 eta 0:02:04
epoch [40/50] batch [40/51] time 0.187 (0.230) data 0.000 (0.044) loss 0.4508 (0.5429) acc 89.8148 (87.0962) lr 2.7103e-04 eta 0:01:59
epoch [40/50] batch [45/51] time 0.179 (0.224) data 0.000 (0.039) loss 0.5223 (0.5438) acc 91.5094 (87.1912) lr 2.7103e-04 eta 0:01:55
epoch [40/50] batch [50/51] time 0.160 (0.218) data 0.000 (0.035) loss 0.6496 (0.5468) acc 87.5000 (87.1859) lr 2.7103e-04 eta 0:01:51
>>> alpha1: 0.171  alpha2: 0.101 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.42 <<<
epoch [41/50] batch [5/51] time 0.206 (0.497) data 0.000 (0.312) loss 0.3685 (0.4713) acc 89.8148 (88.5939) lr 2.2949e-04 eta 0:04:10
epoch [41/50] batch [10/51] time 0.177 (0.347) data 0.000 (0.156) loss 0.7707 (0.5322) acc 83.3333 (89.1456) lr 2.2949e-04 eta 0:02:53
epoch [41/50] batch [15/51] time 0.180 (0.293) data 0.000 (0.104) loss 0.4635 (0.5178) acc 92.4528 (89.9146) lr 2.2949e-04 eta 0:02:25
epoch [41/50] batch [20/51] time 0.168 (0.268) data 0.000 (0.078) loss 0.6279 (0.5152) acc 86.2245 (89.6941) lr 2.2949e-04 eta 0:02:11
epoch [41/50] batch [25/51] time 0.192 (0.251) data 0.001 (0.063) loss 0.5065 (0.5137) acc 88.2353 (89.5679) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [30/51] time 0.172 (0.239) data 0.000 (0.052) loss 0.5313 (0.5245) acc 83.3333 (88.7515) lr 2.2949e-04 eta 0:01:54
epoch [41/50] batch [35/51] time 0.176 (0.230) data 0.000 (0.045) loss 0.6071 (0.5320) acc 81.7308 (88.3557) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [40/51] time 0.173 (0.224) data 0.000 (0.039) loss 0.6336 (0.5417) acc 83.1731 (87.8889) lr 2.2949e-04 eta 0:01:45
epoch [41/50] batch [45/51] time 0.178 (0.218) data 0.000 (0.035) loss 0.5818 (0.5425) acc 85.8491 (87.9565) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [50/51] time 0.182 (0.214) data 0.000 (0.032) loss 0.7427 (0.5486) acc 86.3636 (87.9186) lr 2.2949e-04 eta 0:01:38
>>> alpha1: 0.170  alpha2: 0.101 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [42/50] batch [5/51] time 0.177 (0.505) data 0.000 (0.323) loss 0.6065 (0.5545) acc 84.5000 (86.3016) lr 1.9098e-04 eta 0:03:49
epoch [42/50] batch [10/51] time 0.165 (0.344) data 0.000 (0.162) loss 0.7840 (0.5382) acc 77.7174 (86.5026) lr 1.9098e-04 eta 0:02:34
epoch [42/50] batch [15/51] time 0.195 (0.290) data 0.000 (0.108) loss 0.5925 (0.5463) acc 82.5000 (86.4072) lr 1.9098e-04 eta 0:02:08
epoch [42/50] batch [20/51] time 0.196 (0.263) data 0.000 (0.081) loss 0.3712 (0.5447) acc 96.0526 (87.1615) lr 1.9098e-04 eta 0:01:55
epoch [42/50] batch [25/51] time 0.171 (0.248) data 0.001 (0.065) loss 0.4479 (0.5396) acc 93.0851 (87.6172) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [30/51] time 0.182 (0.237) data 0.000 (0.054) loss 0.5994 (0.5471) acc 92.1569 (87.1989) lr 1.9098e-04 eta 0:01:41
epoch [42/50] batch [35/51] time 0.170 (0.229) data 0.000 (0.046) loss 0.3962 (0.5380) acc 89.2157 (87.2098) lr 1.9098e-04 eta 0:01:37
epoch [42/50] batch [40/51] time 0.170 (0.223) data 0.000 (0.041) loss 0.6154 (0.5377) acc 85.2941 (87.4224) lr 1.9098e-04 eta 0:01:33
epoch [42/50] batch [45/51] time 0.178 (0.218) data 0.000 (0.036) loss 0.5219 (0.5500) acc 85.8491 (87.0988) lr 1.9098e-04 eta 0:01:30
epoch [42/50] batch [50/51] time 0.178 (0.214) data 0.000 (0.033) loss 0.5552 (0.5490) acc 87.2449 (87.1186) lr 1.9098e-04 eta 0:01:27
>>> alpha1: 0.170  alpha2: 0.105 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [43/50] batch [5/51] time 0.177 (0.462) data 0.000 (0.272) loss 0.6496 (0.4904) acc 88.8298 (90.2420) lr 1.5567e-04 eta 0:03:06
epoch [43/50] batch [10/51] time 0.175 (0.327) data 0.000 (0.137) loss 0.6267 (0.5215) acc 91.3265 (89.8853) lr 1.5567e-04 eta 0:02:10
epoch [43/50] batch [15/51] time 0.182 (0.280) data 0.000 (0.092) loss 0.5292 (0.5373) acc 84.1346 (88.9416) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [20/51] time 0.192 (0.256) data 0.000 (0.069) loss 0.5879 (0.5273) acc 87.0536 (88.8097) lr 1.5567e-04 eta 0:01:39
epoch [43/50] batch [25/51] time 0.185 (0.241) data 0.001 (0.055) loss 0.4116 (0.5102) acc 92.4528 (89.3746) lr 1.5567e-04 eta 0:01:32
epoch [43/50] batch [30/51] time 0.199 (0.230) data 0.000 (0.046) loss 0.5809 (0.5241) acc 85.8491 (88.5483) lr 1.5567e-04 eta 0:01:27
epoch [43/50] batch [35/51] time 0.171 (0.223) data 0.000 (0.039) loss 0.4605 (0.5167) acc 89.5000 (88.5448) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [40/51] time 0.183 (0.216) data 0.000 (0.035) loss 0.5807 (0.5313) acc 85.2941 (88.2597) lr 1.5567e-04 eta 0:01:19
epoch [43/50] batch [45/51] time 0.184 (0.213) data 0.000 (0.031) loss 0.3897 (0.5308) acc 91.5179 (88.1592) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [50/51] time 0.170 (0.209) data 0.000 (0.028) loss 0.4824 (0.5377) acc 91.6667 (87.9777) lr 1.5567e-04 eta 0:01:14
>>> alpha1: 0.166  alpha2: 0.101 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [44/50] batch [5/51] time 0.187 (0.484) data 0.000 (0.301) loss 0.4903 (0.5469) acc 87.7451 (87.0791) lr 1.2369e-04 eta 0:02:50
epoch [44/50] batch [10/51] time 0.165 (0.334) data 0.000 (0.151) loss 0.5853 (0.5100) acc 91.6667 (88.8298) lr 1.2369e-04 eta 0:01:55
epoch [44/50] batch [15/51] time 0.172 (0.283) data 0.000 (0.101) loss 0.7239 (0.5196) acc 81.3726 (88.7077) lr 1.2369e-04 eta 0:01:36
epoch [44/50] batch [20/51] time 0.174 (0.256) data 0.000 (0.076) loss 0.4456 (0.5043) acc 90.8654 (89.0558) lr 1.2369e-04 eta 0:01:26
epoch [44/50] batch [25/51] time 0.172 (0.242) data 0.000 (0.061) loss 0.6255 (0.5223) acc 85.7843 (88.7754) lr 1.2369e-04 eta 0:01:20
epoch [44/50] batch [30/51] time 0.185 (0.233) data 0.000 (0.051) loss 0.4241 (0.5169) acc 92.7885 (88.8991) lr 1.2369e-04 eta 0:01:16
epoch [44/50] batch [35/51] time 0.181 (0.226) data 0.000 (0.043) loss 0.5344 (0.5273) acc 87.9808 (88.5948) lr 1.2369e-04 eta 0:01:12
epoch [44/50] batch [40/51] time 0.177 (0.221) data 0.000 (0.038) loss 0.4966 (0.5258) acc 89.1509 (88.7688) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [45/51] time 0.164 (0.215) data 0.000 (0.034) loss 0.5173 (0.5262) acc 86.4583 (88.6305) lr 1.2369e-04 eta 0:01:07
epoch [44/50] batch [50/51] time 0.179 (0.211) data 0.000 (0.030) loss 0.5538 (0.5308) acc 87.5000 (88.4184) lr 1.2369e-04 eta 0:01:04
>>> alpha1: 0.164  alpha2: 0.102 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.22 & unmatched refined noisy rate: 0.40 <<<
epoch [45/50] batch [5/51] time 0.172 (0.489) data 0.000 (0.306) loss 0.6451 (0.5420) acc 78.4314 (85.3579) lr 9.5173e-05 eta 0:02:27
epoch [45/50] batch [10/51] time 0.192 (0.334) data 0.000 (0.153) loss 0.4918 (0.5099) acc 90.4546 (87.0229) lr 9.5173e-05 eta 0:01:39
epoch [45/50] batch [15/51] time 0.181 (0.284) data 0.000 (0.102) loss 0.6684 (0.5317) acc 86.5385 (87.3475) lr 9.5173e-05 eta 0:01:22
epoch [45/50] batch [20/51] time 0.169 (0.257) data 0.000 (0.077) loss 0.6246 (0.5334) acc 86.7347 (86.9026) lr 9.5173e-05 eta 0:01:13
epoch [45/50] batch [25/51] time 0.172 (0.243) data 0.000 (0.061) loss 0.7451 (0.5287) acc 81.3726 (87.1593) lr 9.5173e-05 eta 0:01:08
epoch [45/50] batch [30/51] time 0.185 (0.233) data 0.001 (0.051) loss 0.5361 (0.5399) acc 86.5741 (87.0340) lr 9.5173e-05 eta 0:01:04
epoch [45/50] batch [35/51] time 0.185 (0.226) data 0.000 (0.044) loss 0.6669 (0.5378) acc 86.0577 (87.3367) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [40/51] time 0.171 (0.219) data 0.000 (0.039) loss 0.5576 (0.5321) acc 89.7059 (87.7649) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [45/51] time 0.167 (0.214) data 0.000 (0.034) loss 0.4420 (0.5243) acc 86.7347 (87.6853) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [50/51] time 0.178 (0.211) data 0.001 (0.031) loss 0.4885 (0.5238) acc 88.0000 (87.6619) lr 9.5173e-05 eta 0:00:53
>>> alpha1: 0.163  alpha2: 0.101 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [46/50] batch [5/51] time 0.182 (0.544) data 0.000 (0.356) loss 0.6354 (0.5026) acc 81.9444 (87.9112) lr 7.0224e-05 eta 0:02:15
epoch [46/50] batch [10/51] time 0.188 (0.364) data 0.000 (0.178) loss 0.4789 (0.5268) acc 89.9038 (87.4944) lr 7.0224e-05 eta 0:01:29
epoch [46/50] batch [15/51] time 0.188 (0.303) data 0.000 (0.119) loss 0.5919 (0.5134) acc 85.0877 (87.9187) lr 7.0224e-05 eta 0:01:12
epoch [46/50] batch [20/51] time 0.173 (0.272) data 0.001 (0.089) loss 0.5867 (0.5292) acc 85.2941 (87.1073) lr 7.0224e-05 eta 0:01:03
epoch [46/50] batch [25/51] time 0.193 (0.254) data 0.000 (0.071) loss 0.4968 (0.5319) acc 90.0943 (87.3714) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [30/51] time 0.189 (0.243) data 0.000 (0.060) loss 0.4780 (0.5269) acc 92.7273 (87.7854) lr 7.0224e-05 eta 0:00:54
epoch [46/50] batch [35/51] time 0.205 (0.234) data 0.000 (0.051) loss 0.4509 (0.5271) acc 92.4107 (87.9208) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [40/51] time 0.175 (0.227) data 0.000 (0.045) loss 0.6169 (0.5265) acc 86.0000 (88.0527) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [45/51] time 0.176 (0.221) data 0.001 (0.040) loss 0.6372 (0.5300) acc 83.6538 (87.7672) lr 7.0224e-05 eta 0:00:46
epoch [46/50] batch [50/51] time 0.178 (0.217) data 0.000 (0.036) loss 0.4187 (0.5257) acc 91.3462 (87.8828) lr 7.0224e-05 eta 0:00:44
>>> alpha1: 0.160  alpha2: 0.098 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [47/50] batch [5/51] time 0.187 (0.526) data 0.001 (0.330) loss 0.3620 (0.5095) acc 95.9091 (90.9618) lr 4.8943e-05 eta 0:01:44
epoch [47/50] batch [10/51] time 0.187 (0.358) data 0.000 (0.165) loss 0.4375 (0.5107) acc 88.1579 (89.6050) lr 4.8943e-05 eta 0:01:09
epoch [47/50] batch [15/51] time 0.191 (0.301) data 0.001 (0.110) loss 0.7876 (0.5150) acc 81.1225 (89.0360) lr 4.8943e-05 eta 0:00:56
epoch [47/50] batch [20/51] time 0.204 (0.271) data 0.000 (0.083) loss 0.4380 (0.5088) acc 92.7885 (89.0095) lr 4.8943e-05 eta 0:00:49
epoch [47/50] batch [25/51] time 0.179 (0.253) data 0.000 (0.066) loss 0.4272 (0.5019) acc 91.0000 (89.2655) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [30/51] time 0.185 (0.242) data 0.000 (0.055) loss 0.5733 (0.4921) acc 88.8298 (89.5541) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [35/51] time 0.184 (0.233) data 0.000 (0.048) loss 0.4932 (0.4885) acc 87.0536 (89.3621) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [40/51] time 0.173 (0.226) data 0.000 (0.042) loss 0.6299 (0.4935) acc 83.6538 (89.2197) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [45/51] time 0.175 (0.220) data 0.000 (0.037) loss 0.6742 (0.5090) acc 84.1346 (88.7250) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [50/51] time 0.170 (0.216) data 0.000 (0.033) loss 0.4069 (0.5074) acc 92.3913 (88.7404) lr 4.8943e-05 eta 0:00:33
>>> alpha1: 0.158  alpha2: 0.093 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.21 & unmatched refined noisy rate: 0.40 <<<
epoch [48/50] batch [5/51] time 0.177 (0.538) data 0.000 (0.333) loss 0.5986 (0.6126) acc 84.8958 (86.8597) lr 3.1417e-05 eta 0:01:19
epoch [48/50] batch [10/51] time 0.187 (0.363) data 0.000 (0.167) loss 0.5220 (0.5526) acc 90.0862 (87.6772) lr 3.1417e-05 eta 0:00:51
epoch [48/50] batch [15/51] time 0.177 (0.304) data 0.000 (0.111) loss 0.4692 (0.5465) acc 89.6226 (87.5400) lr 3.1417e-05 eta 0:00:41
epoch [48/50] batch [20/51] time 0.189 (0.273) data 0.000 (0.084) loss 0.4010 (0.5288) acc 90.2778 (88.0019) lr 3.1417e-05 eta 0:00:36
epoch [48/50] batch [25/51] time 0.208 (0.256) data 0.001 (0.067) loss 0.4464 (0.5326) acc 93.8679 (87.8712) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [30/51] time 0.220 (0.247) data 0.001 (0.056) loss 0.4572 (0.5213) acc 90.3846 (88.0662) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [35/51] time 0.178 (0.238) data 0.000 (0.048) loss 0.5135 (0.5137) acc 86.9792 (88.1661) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [40/51] time 0.185 (0.230) data 0.000 (0.042) loss 0.6218 (0.5224) acc 86.3636 (87.9115) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [45/51] time 0.168 (0.224) data 0.000 (0.037) loss 0.4682 (0.5205) acc 89.2857 (87.9013) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [50/51] time 0.185 (0.219) data 0.000 (0.034) loss 0.4264 (0.5145) acc 89.0351 (88.0425) lr 3.1417e-05 eta 0:00:22
>>> alpha1: 0.158  alpha2: 0.092 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.42 <<<
epoch [49/50] batch [5/51] time 0.166 (0.511) data 0.001 (0.322) loss 0.3400 (0.4849) acc 89.5833 (88.2860) lr 1.7713e-05 eta 0:00:49
epoch [49/50] batch [10/51] time 0.187 (0.352) data 0.000 (0.161) loss 0.5226 (0.5092) acc 85.9091 (88.8225) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [15/51] time 0.179 (0.300) data 0.001 (0.108) loss 0.3984 (0.4950) acc 92.4528 (89.6251) lr 1.7713e-05 eta 0:00:26
epoch [49/50] batch [20/51] time 0.171 (0.268) data 0.000 (0.081) loss 0.5425 (0.5080) acc 85.5000 (88.8571) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [25/51] time 0.178 (0.251) data 0.000 (0.065) loss 0.4834 (0.4993) acc 89.9038 (89.1755) lr 1.7713e-05 eta 0:00:19
epoch [49/50] batch [30/51] time 0.190 (0.240) data 0.000 (0.054) loss 0.4134 (0.5073) acc 90.3846 (89.0947) lr 1.7713e-05 eta 0:00:17
epoch [49/50] batch [35/51] time 0.170 (0.231) data 0.000 (0.047) loss 0.5450 (0.5124) acc 89.5000 (88.8121) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [40/51] time 0.178 (0.225) data 0.000 (0.041) loss 0.5220 (0.5133) acc 88.6792 (88.6586) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [45/51] time 0.174 (0.220) data 0.000 (0.036) loss 0.4167 (0.5119) acc 90.8654 (88.6627) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [50/51] time 0.179 (0.215) data 0.000 (0.033) loss 0.6179 (0.5211) acc 84.9057 (88.2825) lr 1.7713e-05 eta 0:00:11
>>> alpha1: 0.160  alpha2: 0.093 <<<
>>> noisy rate: 0.75 --> refined noisy rate: 0.28 --> matched refined noisy rate: 0.20 & unmatched refined noisy rate: 0.41 <<<
epoch [50/50] batch [5/51] time 0.198 (0.555) data 0.000 (0.348) loss 0.6143 (0.5022) acc 85.8491 (89.4383) lr 7.8853e-06 eta 0:00:25
epoch [50/50] batch [10/51] time 0.176 (0.366) data 0.000 (0.174) loss 0.7088 (0.5517) acc 82.6923 (87.9070) lr 7.8853e-06 eta 0:00:15
epoch [50/50] batch [15/51] time 0.186 (0.304) data 0.000 (0.116) loss 0.5643 (0.5407) acc 91.2037 (88.4249) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [20/51] time 0.184 (0.273) data 0.000 (0.087) loss 0.3356 (0.5344) acc 97.2222 (88.6003) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [25/51] time 0.181 (0.254) data 0.000 (0.070) loss 0.6201 (0.5354) acc 89.0909 (88.5409) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [30/51] time 0.165 (0.241) data 0.000 (0.059) loss 0.5718 (0.5273) acc 85.9375 (88.6752) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [35/51] time 0.195 (0.233) data 0.000 (0.050) loss 0.4905 (0.5215) acc 93.1818 (88.9411) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [40/51] time 0.166 (0.225) data 0.000 (0.044) loss 0.7296 (0.5317) acc 84.6939 (88.7382) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [45/51] time 0.173 (0.219) data 0.000 (0.039) loss 0.4970 (0.5308) acc 92.1569 (88.8492) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [50/51] time 0.167 (0.214) data 0.000 (0.035) loss 0.4163 (0.5238) acc 88.2653 (88.8260) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output/oxford_flowers_ablation/rn50_16shots_12FP_symflip/nctx16_cscFalse_ctpend_gceFalse/seed3/prompt_learner/model.pth.tar-50
Finish training
* noise rate: [0.53, 0.43, 0.39, 0.37, 0.35, 0.33, 0.31, 0.3, 0.3, 0.29, 0.29, 0.29, 0.29, 0.29, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.29, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28, 0.28]
* matched noise rate: [0.28, 0.19, 0.26, 0.23, 0.24, 0.21, 0.2, 0.2, 0.2, 0.19, 0.18, 0.19, 0.2, 0.18, 0.19, 0.19, 0.19, 0.21, 0.2, 0.19, 0.19, 0.19, 0.19, 0.21, 0.21, 0.21, 0.2, 0.2, 0.2, 0.2, 0.2, 0.21, 0.21, 0.21, 0.22, 0.21, 0.21, 0.21, 0.2, 0.2]
* unmatched noise rate: [0.81, 0.59, 0.61, 0.58, 0.58, 0.53, 0.49, 0.45, 0.45, 0.44, 0.45, 0.44, 0.43, 0.44, 0.44, 0.43, 0.43, 0.43, 0.42, 0.42, 0.42, 0.43, 0.44, 0.42, 0.42, 0.4, 0.4, 0.39, 0.42, 0.4, 0.42, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.42, 0.41]
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:02<01:10,  2.92s/it] 12%|█▏        | 3/25 [00:03<00:18,  1.19it/s] 20%|██        | 5/25 [00:03<00:09,  2.19it/s] 28%|██▊       | 7/25 [00:03<00:05,  3.36it/s] 36%|███▌      | 9/25 [00:03<00:03,  4.61it/s] 44%|████▍     | 11/25 [00:03<00:02,  5.89it/s] 52%|█████▏    | 13/25 [00:04<00:01,  6.98it/s] 60%|██████    | 15/25 [00:04<00:01,  8.08it/s] 68%|██████▊   | 17/25 [00:04<00:01,  5.52it/s] 76%|███████▌  | 19/25 [00:04<00:00,  6.54it/s] 84%|████████▍ | 21/25 [00:05<00:00,  7.64it/s] 92%|█████████▏| 23/25 [00:05<00:00,  8.62it/s]100%|██████████| 25/25 [00:05<00:00,  6.76it/s]100%|██████████| 25/25 [00:05<00:00,  4.19it/s]
=> result
* total: 2,463
* correct: 1,795
* accuracy: 72.9%
* error: 27.1%
* macro_f1: 69.8%
=> per-class result
* class: 0 (pink primrose)	total: 12	correct: 12	acc: 100.0%
* class: 1 (hard-leaved pocket orchid)	total: 18	correct: 18	acc: 100.0%
* class: 2 (canterbury bells)	total: 12	correct: 4	acc: 33.3%
* class: 3 (sweet pea)	total: 17	correct: 8	acc: 47.1%
* class: 4 (english marigold)	total: 20	correct: 14	acc: 70.0%
* class: 5 (tiger lily)	total: 14	correct: 13	acc: 92.9%
* class: 6 (moon orchid)	total: 12	correct: 12	acc: 100.0%
* class: 7 (bird of paradise)	total: 26	correct: 26	acc: 100.0%
* class: 8 (monkshood)	total: 14	correct: 13	acc: 92.9%
* class: 9 (globe thistle)	total: 14	correct: 14	acc: 100.0%
* class: 10 (snapdragon)	total: 26	correct: 21	acc: 80.8%
* class: 11 (colt's foot)	total: 26	correct: 14	acc: 53.8%
* class: 12 (king protea)	total: 15	correct: 13	acc: 86.7%
* class: 13 (spear thistle)	total: 14	correct: 14	acc: 100.0%
* class: 14 (yellow iris)	total: 15	correct: 15	acc: 100.0%
* class: 15 (globe-flower)	total: 13	correct: 0	acc: 0.0%
* class: 16 (purple coneflower)	total: 26	correct: 26	acc: 100.0%
* class: 17 (peruvian lily)	total: 25	correct: 14	acc: 56.0%
* class: 18 (balloon flower)	total: 15	correct: 10	acc: 66.7%
* class: 19 (giant white arum lily)	total: 17	correct: 14	acc: 82.4%
* class: 20 (fire lily)	total: 12	correct: 9	acc: 75.0%
* class: 21 (pincushion flower)	total: 17	correct: 16	acc: 94.1%
* class: 22 (fritillary)	total: 27	correct: 22	acc: 81.5%
* class: 23 (red ginger)	total: 13	correct: 4	acc: 30.8%
* class: 24 (grape hyacinth)	total: 13	correct: 13	acc: 100.0%
* class: 25 (corn poppy)	total: 13	correct: 12	acc: 92.3%
* class: 26 (prince of wales feathers)	total: 12	correct: 0	acc: 0.0%
* class: 27 (stemless gentian)	total: 20	correct: 20	acc: 100.0%
* class: 28 (artichoke)	total: 23	correct: 20	acc: 87.0%
* class: 29 (sweet william)	total: 26	correct: 18	acc: 69.2%
* class: 30 (carnation)	total: 16	correct: 13	acc: 81.2%
* class: 31 (garden phlox)	total: 14	correct: 12	acc: 85.7%
* class: 32 (love in the mist)	total: 14	correct: 12	acc: 85.7%
* class: 33 (mexican aster)	total: 12	correct: 0	acc: 0.0%
* class: 34 (alpine sea holly)	total: 12	correct: 10	acc: 83.3%
* class: 35 (ruby-lipped cattleya)	total: 22	correct: 22	acc: 100.0%
* class: 36 (cape flower)	total: 32	correct: 0	acc: 0.0%
* class: 37 (great masterwort)	total: 17	correct: 17	acc: 100.0%
* class: 38 (siam tulip)	total: 13	correct: 0	acc: 0.0%
* class: 39 (lenten rose)	total: 20	correct: 19	acc: 95.0%
* class: 40 (barbeton daisy)	total: 38	correct: 18	acc: 47.4%
* class: 41 (daffodil)	total: 17	correct: 16	acc: 94.1%
* class: 42 (sword lily)	total: 39	correct: 0	acc: 0.0%
* class: 43 (poinsettia)	total: 28	correct: 26	acc: 92.9%
* class: 44 (bolero deep blue)	total: 12	correct: 0	acc: 0.0%
* class: 45 (wallflower)	total: 59	correct: 39	acc: 66.1%
* class: 46 (marigold)	total: 20	correct: 20	acc: 100.0%
* class: 47 (buttercup)	total: 21	correct: 17	acc: 81.0%
* class: 48 (oxeye daisy)	total: 15	correct: 14	acc: 93.3%
* class: 49 (common dandelion)	total: 28	correct: 28	acc: 100.0%
* class: 50 (petunia)	total: 77	correct: 28	acc: 36.4%
* class: 51 (wild pansy)	total: 26	correct: 26	acc: 100.0%
* class: 52 (primula)	total: 28	correct: 23	acc: 82.1%
* class: 53 (sunflower)	total: 19	correct: 19	acc: 100.0%
* class: 54 (pelargonium)	total: 21	correct: 0	acc: 0.0%
* class: 55 (bishop of llandaff)	total: 33	correct: 0	acc: 0.0%
* class: 56 (gaura)	total: 20	correct: 18	acc: 90.0%
* class: 57 (geranium)	total: 34	correct: 34	acc: 100.0%
* class: 58 (orange dahlia)	total: 20	correct: 20	acc: 100.0%
* class: 59 (pink-yellow dahlia)	total: 33	correct: 33	acc: 100.0%
* class: 60 (cautleya spicata)	total: 15	correct: 15	acc: 100.0%
* class: 61 (japanese anemone)	total: 16	correct: 14	acc: 87.5%
* class: 62 (black-eyed susan)	total: 16	correct: 16	acc: 100.0%
* class: 63 (silverbush)	total: 16	correct: 16	acc: 100.0%
* class: 64 (californian poppy)	total: 31	correct: 29	acc: 93.5%
* class: 65 (osteospermum)	total: 19	correct: 19	acc: 100.0%
* class: 66 (spring crocus)	total: 13	correct: 13	acc: 100.0%
* class: 67 (bearded iris)	total: 16	correct: 10	acc: 62.5%
* class: 68 (windflower)	total: 16	correct: 4	acc: 25.0%
* class: 69 (tree poppy)	total: 19	correct: 19	acc: 100.0%
* class: 70 (gazania)	total: 23	correct: 18	acc: 78.3%
* class: 71 (azalea)	total: 29	correct: 19	acc: 65.5%
* class: 72 (water lily)	total: 58	correct: 58	acc: 100.0%
* class: 73 (rose)	total: 51	correct: 43	acc: 84.3%
* class: 74 (thorn apple)	total: 36	correct: 4	acc: 11.1%
* class: 75 (morning glory)	total: 32	correct: 25	acc: 78.1%
* class: 76 (passion flower)	total: 75	correct: 72	acc: 96.0%
* class: 77 (lotus)	total: 42	correct: 39	acc: 92.9%
* class: 78 (toad lily)	total: 13	correct: 13	acc: 100.0%
* class: 79 (anthurium)	total: 32	correct: 29	acc: 90.6%
* class: 80 (frangipani)	total: 50	correct: 50	acc: 100.0%
* class: 81 (clematis)	total: 34	correct: 31	acc: 91.2%
* class: 82 (hibiscus)	total: 39	correct: 37	acc: 94.9%
* class: 83 (columbine)	total: 26	correct: 17	acc: 65.4%
* class: 84 (desert-rose)	total: 18	correct: 16	acc: 88.9%
* class: 85 (tree mallow)	total: 17	correct: 9	acc: 52.9%
* class: 86 (magnolia)	total: 18	correct: 17	acc: 94.4%
* class: 87 (cyclamen)	total: 46	correct: 40	acc: 87.0%
* class: 88 (watercress)	total: 55	correct: 0	acc: 0.0%
* class: 89 (canna lily)	total: 25	correct: 19	acc: 76.0%
* class: 90 (hippeastrum)	total: 23	correct: 17	acc: 73.9%
* class: 91 (bee balm)	total: 20	correct: 19	acc: 95.0%
* class: 92 (ball moss)	total: 14	correct: 0	acc: 0.0%
* class: 93 (foxglove)	total: 49	correct: 46	acc: 93.9%
* class: 94 (bougainvillea)	total: 38	correct: 32	acc: 84.2%
* class: 95 (camellia)	total: 27	correct: 23	acc: 85.2%
* class: 96 (mallow)	total: 20	correct: 0	acc: 0.0%
* class: 97 (mexican petunia)	total: 25	correct: 0	acc: 0.0%
* class: 98 (bromelia)	total: 18	correct: 17	acc: 94.4%
* class: 99 (blanket flower)	total: 15	correct: 15	acc: 100.0%
* class: 100 (trumpet creeper)	total: 17	correct: 9	acc: 52.9%
* class: 101 (blackberry lily)	total: 14	correct: 13	acc: 92.9%
* average: 73.5%
Elapsed: 0:28:21
scripts/parse.sh: line 10: output/ablation/oxford_flowers_MixDivideWarmupAugmentationBLIP/parsing_results.log: No such file or directory
